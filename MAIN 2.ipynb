{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background info\n",
    "\n",
    "### Microphysical parameters (xnames)\n",
    "\n",
    "Note that the parameter names in the namelist correspond to the microphysics Fortran code, not the Python codes used in ensemble DA experiments. Definitions are as follows:\n",
    "\n",
    "1. as = a_s, the coefficient in the snow fallspeed-diameter relationship\n",
    "2. bs = b_s, the exponent in the snow fallspeed-diameter relationship\n",
    "3. ag = a_g, the coefficient in the graupel fallspeed-diameter relationship\n",
    "4. bg = b_g, the exponent in the graupel fallspeed-diameter relationship\n",
    "5. tnw = N_0r, the intercept parameter in the (exponential) rain particle size distribution\n",
    "6. tns = N_0s, the intercept parameter in the (exponential) snow particle size distribution\n",
    "7. tng = N_0g, the intercept parameter in the (exponential) graupel particle size distribution\n",
    "8. roqs = rho_s, the snow density\n",
    "9. roqg = rho_g, the graupel density\n",
    "10. bnd21 = q_c0, the cloud-to-rain autoconversion threshold\n",
    "11. bnd1 = q_i0, the ice-to-snow autoconversion threshold (not used) \n",
    "\n",
    "### Model output (ynames)\n",
    "\n",
    "Running the cloud column model (CRM) will produce an ascii file (eg, crm1d_output.txt) containing a row of numbers (the default run contains 36 values in total).\n",
    "\n",
    "If the model is run for 1 time step only, it will output 6 values corresponding to the following variables:\n",
    "\n",
    "1. pcp: precipitation rate\n",
    "2. acc: accumulated precipitation\n",
    "3. lwp: liquid water path\n",
    "4. iwp: ice water path\n",
    "5. olr: outgoing longwave radiation\n",
    "6. osr: outgoing shortwave radiation\n",
    "\n",
    "If the model is run for \"k\" time steps, there will be a total of 6k values, where each set of 6 values (counting from the beginning) will correspond to the 6 variables above. E.g., the second 6-member set (starting from the 7th value and ending in the 12th value), will give the values of {pcp,acc,lwp,iwp,olr,osr} during the second model time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------\n",
    "#    PACKAGES    \n",
    "#----------------\n",
    "import time\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from cloud_column_model import cloud_column_model \n",
    "import create_ensemble\n",
    "from parmap_framework import parmap\n",
    "from module_runcrm import runcrm\n",
    "import csv \n",
    "import pandas as pd\n",
    "\n",
    "#--------------------------------\n",
    "#    EXPERIMENT CONFIGURATION         \n",
    "#--------------------------------\n",
    "\n",
    "### General\n",
    "rand_seed = 33; np.random.seed(rand_seed) # set random number seed for reproducibility\n",
    "num_Workers = 12\n",
    "np.set_printoptions(precision=5) \n",
    "np.set_printoptions(suppress=True) # suppress scientific notation\n",
    "\n",
    "### Input/output filenames\n",
    "expdir = './'\n",
    "input_file = expdir+'cloud_column_model/run_one_crm1d.txt' # in cloud_column_column_model dir; contains reference param vals\n",
    "output_file = expdir+'cloud_column_model/crm1d_output.txt' # appears in cloud_column_model dir under the name pert_crm1d_output.txt_{num}\n",
    "                                        # each column shows time evolution for 1 of the 6 model output vars \n",
    "namelist_file = expdir+'cloud_column_model/namelist_3h_t30-180.f90'\n",
    "\n",
    "### Ensembles and distributions\n",
    "LType = 1 # Gaussian likelihood by default; corresponds to additive and Gaussian noise\n",
    "ens_gen = 'uniform'\n",
    "n_ens = 1000 # 1000 used in Posselt and Bishop (2018)\n",
    "p1 = [              200.0,  0.3,  400.0,  0.4,  0.5,    0.5,   0.5,   0.2,     0.4,   1.e-3,  6.e-4 ] # mean parameter used in create_ensemble()\n",
    "                                                                                                      # to define Gaussian and Gamma distributions\n",
    "p2 = [              20.0,   0.05, 20.0,   0.05, 0.05,   0.05,  0.05,  0.05,    0.05,  1.e-4,  1.e-5 ] # std parameter used in create_ensemble() \n",
    "                                                                                                      # to define Gaussian and Gamma distributions\n",
    "pmin = [            50.0,   0.10, 50.0,   0.1,  0.00,   0.00,  0.00,  0.1,    0.1,  1.e-1,  2.e-6 ] # lower bound\n",
    "pmax = [            1000.0, 1.0,  1200.0, 0.90, 5.0,    5.0,   5.0,   1.0,     1.0,   3.e-0,  1.e-3 ] # upper bound\n",
    "\n",
    "### Parameters and observations\n",
    "# parameters\n",
    "xnames = [        'as',   'bs',  'ag',  'bg', 'N0r',  'N0s', 'N0g', 'rhos',  'rhog', 'qc0', 'qi0']\n",
    "XfMask = np.array([0.0,    0.0,  1.0,    1.0,  0.0,    0.0,   0.0,   0.0,     0.0,    0.0,   0.0])\n",
    "x_true = [         200.0,  0.3,  400.0,  0.4,  0.5,    0.5,   0.5,   0.2,     0.4,   1.e-3,  6.e-4 ]\n",
    "nx = len(xnames)\n",
    "nxp = np.count_nonzero(XfMask) \n",
    "xidx = np.squeeze(np.nonzero(XfMask))\n",
    "# times\n",
    "tnames =          ['30', '60', '90', '120', '150', '180']\n",
    "t_mask = np.array([1.0,  1.0,   1.0,  1.0,   1.0,   1.0])\n",
    "nt = len(tnames)\n",
    "# observations\n",
    "ynames =          ['PCP', 'ACC', 'LWP', 'IWP', 'OLR', 'OSR']\n",
    "y_mask = np.array([ 1.0,   0.0,   1.0,   1.0,   1.0,   1.0])\n",
    "y_sigma =         [ 2.0,   5.0,   0.5,   1.0,   5.0,   5.0 ] * nt\n",
    "ynames_long = [f'{y}_t{i}' for i in range(1, nt+1) for y in ynames]\n",
    "n_obs = len(ynames)\n",
    "\n",
    "### Full observation space mask\n",
    "HXfMask = np.empty(n_obs * nt)\n",
    "for t in range(nt):\n",
    "  for o in range(n_obs):\n",
    "    idx = (t * n_obs) + o\n",
    "    HXfMask[idx] = t_mask[t] * y_mask[o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the prior ensemble\n",
      "     Shape of Xf with selected vars:    (1000, 2)\n",
      "     Shape of Xf with all vars:         (1000, 11)\n",
      "     Prior mean (Xf.mean):              [618.94684   0.50474]\n",
      "     Prior variance (Xf.var):           [109954.43371      0.05424]\n",
      "Mapping prior ensemble to observation space (running the cloud model)\n",
      "    Full input to first ensemble member: ['./cloud_column_model/run_one_crm1d.txt', './cloud_column_model/crm1d_output.txt', './cloud_column_model/namelist_3h_t30-180.f90', 1, [200.0, 0.3, 335.7866465533894, 0.4599803368406364, 0.5, 0.5, 0.5, 0.2, 0.4, 0.001, 0.0006]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "parmap <function runcrm at 0x136081620>: Running in mode par with numPartitions 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        58.2910804749      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        53.0720100403      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        72.6106414795      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        74.1043777466      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        50.4893951416      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        50.2351303101      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        69.9778289795      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        62.3337516785      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        73.3973236084      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        52.6990318298      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        69.9317092896      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        55.8857040405      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        60.8782463074      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        61.5273208618      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        64.1964416504      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        70.2404251099      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        57.5507087708      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        63.5645751953      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        67.7145767212      1200.0000000000\n",
      "    State vector for ens member Ne/2:   [200.0, 0.3, 803.5114272863932, 0.6848477193785459, 0.5, 0.5, 0.5, 0.2, 0.4, 0.001, 0.0006]\n",
      "    ObSpace-mapped state vector for ens member Ne/2:  [3.7471554279, 0.0863199979, 5.085152626, 0.0008799675, 240.7511749268, 896.9605102539, 8.0385484695, 3.9377322197, 4.5294594765, 4.4273824692, 152.4221343994, 804.9404907227, 6.2242555618, 7.7818937302, 2.8728749752, 11.9328489304, 150.5088043213, 853.8159179688, 15.0625858307, 11.9454212189, 5.801486969, 13.9388914108, 183.8616333008, 804.632019043, 16.4963760376, 21.2935943604, 5.1860070229, 11.6731653214, 209.3745727539, 748.028503418, 21.1283550262, 29.9789676666, 6.6053357124, 3.8579115868, 231.498046875, 494.9621887207]\n",
      "    len(HXf), len(HXf[1]):  1000 36\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------\n",
    "#    MODEL SIMULATIONS         \n",
    "#--------------------------------\n",
    "\n",
    "### Generate prior ensemble X\n",
    "print('Generating the prior ensemble')\n",
    "p1_in   = (np.array(p1)[xidx]).tolist() # mean of selected state vars\n",
    "p2_in   = (np.array(p2)[xidx]).tolist() # std of -||-\n",
    "pmin_in = (np.array(pmin)[xidx]).tolist() # lower bound of -||-\n",
    "pmax_in = (np.array(pmax)[xidx]).tolist() # upper bound of -||-\n",
    "# only perturb selected state vars; Xf dims will be [Ne,Nx_selected]\n",
    "Xf_subset = create_ensemble.create_ensemble(n_ens,p1_in,p2_in,pmin_in,pmax_in,ens_gen)\n",
    "print('     Shape of Xf with selected vars:   ',np.array(Xf_subset).shape)\n",
    "# now construct the full prior ensemble in which selected vars are perturbed\n",
    "# and masked variables are set to their true values for all ensemble members\n",
    "Xf_mask = np.tile(x_true,n_ens).reshape(n_ens,len(x_true))\n",
    "Xf_mask[:,xidx] = np.array(Xf_subset)\n",
    "Xf = Xf_mask.tolist() # list in the form [[parms_mem{1}],...,[parms_mem{n_ens}]]\n",
    "                        # where parms_mem{k} is made of the 11 state vars (CRM params)\n",
    "print('     Shape of Xf with all vars:        ',np.array(Xf).shape)\n",
    "print('     Prior mean (Xf.mean):             ',np.mean(Xf,axis=0)[xidx])\n",
    "print('     Prior variance (Xf.var):          ',np.var(Xf,axis=0)[xidx])\n",
    "\n",
    "\n",
    "### Mapping prior ensemble to observation space h(X) (i.e., run ensemble of CRM simulations)\n",
    "print('Mapping prior ensemble to observation space (running the cloud model)')\n",
    "runs = [] # full input list for the ensemble CRM runs\n",
    "input_file_list = [input_file] * n_ens\n",
    "output_file_list = [output_file] * n_ens\n",
    "namelist_file_list = [namelist_file] * n_ens\n",
    "run_num_list = list(range(1,n_ens+1))\n",
    "runs = [list(x) for x in zip(input_file_list, output_file_list, namelist_file_list, run_num_list, Xf)]\n",
    "print(f'    Full input to first ensemble member: {runs[0]}')\n",
    "DASK_URL = 'scispark6.jpl.nasa.gov:8786'\n",
    "parmode = 'par'\n",
    "pmap = parmap.Parmap(master=DASK_URL, mode=parmode, numWorkers=num_Workers)\n",
    "HXf = pmap(runcrm, runs)\n",
    "print('    State vector for ens member Ne/2:  ', Xf[np.int32(n_ens/2)])\n",
    "print('    ObSpace-mapped state vector for ens member Ne/2: ', HXf[np.int32(n_ens/2)])\n",
    "print('    len(HXf), len(HXf[1]): ',len(HXf),len(HXf[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TRUE SIMILATION ***\n",
      "Parameter values to be used in cloud_column_model.py:  200.0 0.3 400.0 0.4 0.5 0.5 0.5 0.2 0.4 0.001 0.0006\n",
      "Input file name:     ./cloud_column_model/run_one_crm1d.txt-CRM1D-c225a3642c\n",
      "Output file name:    ./cloud_column_model/crm1d_output.txt-CRM1D-29fc716db2\n",
      "Namelist file name:  ./cloud_column_model/namelist_3h_t30-180.f90\n",
      "Output:  36 [3.7471554279, 0.0863199979, 5.0852060318, 0.0008276912, 240.7512512207, 896.9626464844, 7.9930844307, 3.937908411, 4.4381356239, 4.5186429024, 153.4401702881, 804.2377929688, 6.2555627823, 7.6948990822, 2.8587121964, 12.063126564, 160.9409637451, 848.690612793, 12.8107385635, 11.4412117004, 5.5484800339, 14.7016458511, 195.8087310791, 803.0979003906, 18.7041091919, 21.073266983, 5.6027178764, 11.48528862, 212.2489471436, 737.3928222656, 20.8552017212, 30.281162262, 6.4446291924, 3.7101278305, 231.7576751709, 493.205291748]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#------------------------\n",
    "#    TRUE SIMULATION            \n",
    "#------------------------\n",
    "\n",
    "### Run the cloud model with the true parameters\n",
    "print('*** TRUE SIMILATION ***')\n",
    "crm1d = cloud_column_model.CRM1DWrap(input_file,output_file,namelist_file, params=x_true,verbose=True)\n",
    "y_true, crm_status = crm1d() # a list of len(y_true) = nt*n_obs\n",
    "print('Output: ',len(y_true),y_true)  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving CSV file (Input Parametes and Output Parametes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[---------------------Graupel fall speed coefficient, Ag Vs outputs----------------------------------]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input parameters for 1 ensembles saved to ./cloud_column_model/ensemble_input.csv\n",
      "Output variables for 1 ensembles saved to ./cloud_column_model/ensemble_output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_csv_path = './cloud_column_model/ensemble_input.csv'\n",
    "output_csv_path = './cloud_column_model/ensemble_output.csv'\n",
    "\n",
    "# Define column names for the input parameters\n",
    "column_names_input = ['as', 'bs', 'ag', 'bg', 'N0r', 'N0s', 'N0g', 'rhos', 'rhog', 'qc0', 'qi0']\n",
    "\n",
    "# Save input ensemble (Xf) to a CSV file\n",
    "Xf_df = pd.DataFrame(Xf, columns=column_names_input)\n",
    "Xf_df.to_csv(input_csv_path, index=False)\n",
    "print(f\"Input parameters for 1 ensembles saved to {input_csv_path}\")\n",
    "\n",
    "# Define column names for the output variables\n",
    "column_names_output = [f\"{var}_t{t}\" for t in range(1, nt + 1) for var in ynames]\n",
    "\n",
    "# Save output ensemble (HXf) to a CSV file\n",
    "HXf_df = pd.DataFrame(HXf, columns=column_names_output)\n",
    "HXf_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"Output variables for 1 ensembles saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input for first iteration: [200.0, 0.3, 50.0, 0.4, 0.5, 0.5, 0.5, 0.2, 0.4, 0.001, 0.0006]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "parmap <function runcrm at 0x136081620>: Running in mode par with numPartitions 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        74.1709976196      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        50.0000000000      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        51.1510009766      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        52.3019981384      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        53.4529991150      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        54.6040000916      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        55.7550010681      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        56.9059982300      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        58.0569992065      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        59.2080001831      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        60.3590011597      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        61.5099983215      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        62.6609992981      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        63.8120002747      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        64.9629974365      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        66.1139984131      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        67.2649993896      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        68.4160003662      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        69.5670013428      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        70.7180023193      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        71.8690032959      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        73.0199966431      1200.0000000000\n",
      "Output for first ensemble member: [3.7471554279, 0.0863199979, 5.0851097107, 0.0009224159, 240.7511444092, 896.959777832, 6.8305430412, 3.865606308, 3.6832726002, 5.3417963982, 151.7407684326, 857.3449707031, 4.8954563141, 6.5638127327, 2.725605011, 13.3805494308, 139.3000946045, 895.3970336914, 6.1752095222, 9.03956604, 4.5684752464, 18.3249835968, 157.2593841553, 896.9819335938, 10.1181402206, 14.1641921997, 3.8288652897, 20.5660705566, 169.1158447266, 858.3247680664, 18.8615188599, 20.4601459503, 5.9007778168, 14.3490362167, 186.845123291, 664.3582763672]\n",
      "len(HXf), len(HXf[0]): 1000 36\n",
      "New ensemble outputs with varying 'ag' saved to ./cloud_column_model/ensemble_output_ag_vary.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cloud Column Model Parameter Sweep with Parallel Processing\n",
    "\n",
    "This script performs a parameter sweep for the 'ag' parameter in a cloud column model simulation\n",
    "It uses a parallel execution framework to run multiple instances of the CRM model with varying 'ag' values \n",
    "and saves the ensemble output to a CSV file\n",
    "\n",
    "Key Features:\n",
    "1. Fixed input parameters for the CRM model are defined in the `fixed_values` dictionary\n",
    "2. The 'ag' parameter varies from 50 to 1200, incremented by 1\n",
    "3. Input data is prepared for each run, including both fixed and varying parameters\n",
    "4. Parallel processing is implemented using DASK via the `parmap_framework` package\n",
    "5. Outputs of the CRM model are aggregated into a DataFrame and saved as a CSV\n",
    "\n",
    "Dependencies:\n",
    "- pandas\n",
    "- numpy\n",
    "- parmap_framework\n",
    "- module_runcrm (containing the `runcrm` function)\n",
    "\n",
    "Workflow:\n",
    "1. Prepare a list of input configurations (`runs`) for all parameter sweep iterations\n",
    "2. Execute the runs in parallel using the DASK cluster\n",
    "3. Process and organize the output data into a DataFrame with appropriate column names\n",
    "4. Save the DataFrame to a CSV file for further analysis\n",
    "\n",
    "Output:\n",
    "- CSV file containing the ensemble output for all 'ag' values, with 36 output variables \n",
    "  (6 variables x 6 time steps) and the corresponding 'ag' values\n",
    "\n",
    "Usage:\n",
    "Ensure all dependencies are installed, and paths for input/output files are correct before execution\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from parmap_framework import parmap\n",
    "from module_runcrm import runcrm \n",
    "\n",
    "# File path for saving the output\n",
    "output_csv_path = './cloud_column_model/ensemble_output_ag_vary.csv'\n",
    "\n",
    "# Fixed values for input parameters\n",
    "fixed_values = {\n",
    "    'as': 200.0,\n",
    "    'bs': 0.3,\n",
    "    'bg': 0.4,\n",
    "    'N0r': 0.5,\n",
    "    'N0s': 0.5,\n",
    "    'N0g': 0.5 ,\n",
    "    'rhos': 0.2,\n",
    "    'rhog':  0.4,\n",
    "    'qc0': 0.001,\n",
    "    'qi0': 0.0006\n",
    "} \n",
    "# Define the range for 'ag' parameter\n",
    "ag_values = np.arange(50, 1201, 1.151)  # From 50 to 1200 with increment of 1\n",
    "\n",
    "# Prepare input data for each iteration\n",
    "runs = []\n",
    "for ag in ag_values:\n",
    "    input_params = list(fixed_values.values())\n",
    "    input_params.insert(2, ag)  # Insert the varying 'ag' value in the correct position\n",
    "    runs.append([\n",
    "        './cloud_column_model/run_one_crm1d.txt',  # Input file (placeholder)\n",
    "        './cloud_column_model/crm1d_output.txt',  # Output file (placeholder)\n",
    "        './cloud_column_model/namelist_3h_t30-180.f90',  # Namelist file\n",
    "        len(runs) + 1,  # Run number\n",
    "        input_params  # Parameters for this run\n",
    "    ])\n",
    "\n",
    "# Print a sample input for debugging\n",
    "print(f\"Sample input for first iteration: {runs[0][-1]}\")\n",
    "\n",
    "\n",
    "# Parallel execution using DASK\n",
    "DASK_URL = 'scispark6.jpl.nasa.gov:8786'\n",
    "parmode = 'par'\n",
    "num_Workers = 12\n",
    "pmap = parmap.Parmap(master=DASK_URL, mode=parmode, numWorkers=num_Workers)\n",
    "HXf = pmap(runcrm, runs)\n",
    "\n",
    "# Print a sample output for debugging\n",
    "print('Output for first ensemble member:', HXf[0])\n",
    "print('len(HXf), len(HXf[0]):', len(HXf), len(HXf[0]))\n",
    "\n",
    "# Define column names for the output variables (6 variables x 6 time steps)\n",
    "column_names_output = [f\"{var}_t{t}\" for t in range(1, 7) for var in ['PCP', 'ACC', 'LWP', 'IWP', 'OLR', 'OSR']]\n",
    "\n",
    "# Convert the full output (36 columns per member) to a DataFrame\n",
    "HXf_df = pd.DataFrame(HXf, columns=column_names_output)\n",
    "\n",
    "# Add the 'ag' values as a separate column for reference\n",
    "HXf_df.insert(0, 'ag', ag_values)\n",
    "\n",
    "# Save to CSV\n",
    "HXf_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"New ensemble outputs with varying 'ag' saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new file which will be used as input file in the cloud model for testing. Ag value is fixed while other parameters value is remained as TRUE value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the output data\n",
    "output_data = pd.read_csv(output_csv_path)\n",
    "\n",
    "# Extract 'ag' and 'PCP_t4' values\n",
    "ag_values_plot = output_data['ag']\n",
    "pcp_t4_values = output_data['PCP_t4']\n",
    "\n",
    "# Plot 'ag' vs 'PCP_t4'\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ag_values_plot, pcp_t4_values, label='PCP_t4', linewidth=2)\n",
    "plt.xlabel('ag (Varying Parameter)', fontsize=12)\n",
    "plt.ylabel('PCP_t4 (Output Variable)', fontsize=12)\n",
    "plt.title('PCP_t4 vs ag', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input configurations with varying 'ag' saved to ./cloud_column_model/ensemble_inputs_ag_vary.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "parmap <function runcrm at 0x136081620>: Running in mode par with numPartitions 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        74.1709976196      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        50.0000000000      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        51.1510009766      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        52.3019981384      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        53.4529991150      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        54.6040000916      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        55.7550010681      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        56.9059982300      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        58.0569992065      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        59.2080001831      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        60.3590011597      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        61.5099983215      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        62.6609992981      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        63.8120002747      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        64.9629974365      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        66.1139984131      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        67.2649993896      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        68.4160003662      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        69.5670013428      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        70.7180023193      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        71.8690032959      1200.0000000000\n",
      "Warning! Parameter outside allowable range\n",
      "Ensemble member, parameter number, min, value, max:          1   3        75.0000000000        73.0199966431      1200.0000000000\n",
      "Output for first ensemble member: [3.7471554279, 0.0863199979, 5.0851097107, 0.0009224159, 240.7511444092, 896.959777832, 6.8305430412, 3.865606308, 3.6832726002, 5.3417963982, 151.7407684326, 857.3449707031, 4.8954563141, 6.5638127327, 2.725605011, 13.3805494308, 139.3000946045, 895.3970336914, 6.1752095222, 9.03956604, 4.5684752464, 18.3249835968, 157.2593841553, 896.9819335938, 10.1181402206, 14.1641921997, 3.8288652897, 20.5660705566, 169.1158447266, 858.3247680664, 18.8615188599, 20.4601459503, 5.9007778168, 14.3490362167, 186.845123291, 664.3582763672]\n",
      "len(HXf), len(HXf[0]): 1000 36\n",
      "New ensemble outputs with varying 'ag' saved to ./cloud_column_model/ensemble_output_ag_vary.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cloud Column Model Parameter Sweep with Parallel Processing\n",
    "\n",
    "This script performs a parameter sweep for the 'ag' parameter in a cloud column model simulation.\n",
    "It uses a parallel execution framework to run multiple instances of the CRM model with varying 'ag' values \n",
    "and saves the ensemble input configurations and output results to CSV files.\n",
    "\n",
    "Key Features:\n",
    "1. Fixed input parameters for the CRM model are defined in the `fixed_values` dictionary.\n",
    "2. The 'ag' parameter varies from 50 to 1200, incremented by 1.\n",
    "3. Input data is prepared for each run, including both fixed and varying parameters.\n",
    "4. Parallel processing is implemented using DASK via the `parmap_framework` package.\n",
    "5. Outputs of the CRM model are aggregated into a DataFrame and saved as a CSV.\n",
    "\n",
    "Dependencies:\n",
    "- pandas\n",
    "- numpy\n",
    "- parmap_framework\n",
    "- module_runcrm (containing the `runcrm` function)\n",
    "\n",
    "Outputs:\n",
    "1. `ensemble_inputs_ag_vary.csv`: Input configurations for all 'ag' values.\n",
    "2. `ensemble_output_ag_vary.csv`: Ensemble output for all 'ag' values.\n",
    "\n",
    "Usage:\n",
    "Ensure all dependencies are installed, and paths for input/output files are correct before execution.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from parmap_framework import parmap\n",
    "from module_runcrm import runcrm\n",
    "\n",
    "# File paths for saving input configurations and output results\n",
    "input_csv_path = './cloud_column_model/ensemble_inputs_ag_vary.csv'\n",
    "output_csv_path = './cloud_column_model/ensemble_output_ag_vary.csv'\n",
    "\n",
    "# Fixed values for input parameters\n",
    "fixed_values = {\n",
    "    'as': 200.0,\n",
    "    'bs': 0.3,\n",
    "    'bg': 0.4,\n",
    "    'N0r': 0.5,\n",
    "    'N0s': 0.5,\n",
    "    'N0g': 0.5,\n",
    "    'rhos': 0.2,\n",
    "    'rhog': 0.4,\n",
    "    'qc0': 0.001,\n",
    "    'qi0': 0.0006\n",
    "}\n",
    "\n",
    "# Define the range for the 'ag' parameter\n",
    "ag_values = np.arange(50, 1201, 1.151)  # From 50 to 1200 with increment of 1\n",
    "\n",
    "# Prepare input data for each iteration\n",
    "runs = []\n",
    "for ag in ag_values:\n",
    "    input_params = list(fixed_values.values())\n",
    "    input_params.insert(2, ag)  # Insert the varying 'ag' value in the correct position\n",
    "    runs.append([\n",
    "        './cloud_column_model/run_one_crm1d.txt',  # Input file (placeholder)\n",
    "        './cloud_column_model/crm1d_output.txt',  # Output file (placeholder)\n",
    "        './cloud_column_model/namelist_3h_t30-180.f90',  # Namelist file\n",
    "        len(runs) + 1,  # Run number\n",
    "        input_params  # Parameters for this run\n",
    "    ])\n",
    "\n",
    "# Save all input configurations to a CSV file\n",
    "input_columns = list(fixed_values.keys())\n",
    "input_columns.insert(2, 'ag')  # Insert 'ag' in the correct position\n",
    "input_data = pd.DataFrame([run[-1] for run in runs], columns=input_columns)\n",
    "input_data.to_csv(input_csv_path, index=False)\n",
    "print(f\"Input configurations with varying 'ag' saved to {input_csv_path}\")\n",
    "\n",
    "# Parallel execution using DASK\n",
    "DASK_URL = 'scispark6.jpl.nasa.gov:8786'\n",
    "parmode = 'par'\n",
    "num_Workers = 12\n",
    "pmap = parmap.Parmap(master=DASK_URL, mode=parmode, numWorkers=num_Workers)\n",
    "HXf = pmap(runcrm, runs)\n",
    "\n",
    "# Print a sample output for debugging\n",
    "print('Output for first ensemble member:', HXf[0])\n",
    "print('len(HXf), len(HXf[0]):', len(HXf), len(HXf[0]))\n",
    "\n",
    "# Define column names for the output variables (6 variables x 6 time steps)\n",
    "column_names_output = [f\"{var}_t{t}\" for t in range(1, 7) for var in ['PCP', 'ACC', 'LWP', 'IWP', 'OLR', 'OSR']]\n",
    "\n",
    "# Convert the full output (36 columns per member) to a DataFrame\n",
    "HXf_df = pd.DataFrame(HXf, columns=column_names_output)\n",
    "\n",
    "# Add the 'ag' values as a separate column for reference\n",
    "HXf_df.insert(0, 'ag', ag_values)\n",
    "\n",
    "# Save the ensemble output to a CSV file\n",
    "HXf_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"New ensemble outputs with varying 'ag' saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20000: Train Loss = 0.542463, Test Loss = 0.367280, Learning Rate = 1.000000e-03\n",
      "Epoch 2/20000: Train Loss = 0.477496, Test Loss = 0.369548, Learning Rate = 9.996200e-04\n",
      "Epoch 3/20000: Train Loss = 0.490221, Test Loss = 0.273737, Learning Rate = 9.992402e-04\n",
      "Epoch 4/20000: Train Loss = 0.476921, Test Loss = 0.304057, Learning Rate = 9.988605e-04\n",
      "Epoch 5/20000: Train Loss = 0.463274, Test Loss = 0.316549, Learning Rate = 9.984810e-04\n",
      "Epoch 6/20000: Train Loss = 0.503702, Test Loss = 0.277283, Learning Rate = 9.981016e-04\n",
      "Epoch 7/20000: Train Loss = 0.469952, Test Loss = 0.258173, Learning Rate = 9.977223e-04\n",
      "Epoch 8/20000: Train Loss = 0.454740, Test Loss = 0.270687, Learning Rate = 9.973432e-04\n",
      "Epoch 9/20000: Train Loss = 0.456667, Test Loss = 0.275984, Learning Rate = 9.969643e-04\n",
      "Epoch 10/20000: Train Loss = 0.458708, Test Loss = 0.231470, Learning Rate = 9.965854e-04\n",
      "Epoch 11/20000: Train Loss = 0.452379, Test Loss = 0.232411, Learning Rate = 9.962068e-04\n",
      "Epoch 12/20000: Train Loss = 0.450131, Test Loss = 0.237490, Learning Rate = 9.958282e-04\n",
      "Epoch 13/20000: Train Loss = 0.452309, Test Loss = 0.220135, Learning Rate = 9.954498e-04\n",
      "Epoch 14/20000: Train Loss = 0.456118, Test Loss = 0.239594, Learning Rate = 9.950716e-04\n",
      "Epoch 15/20000: Train Loss = 0.450012, Test Loss = 0.233912, Learning Rate = 9.946935e-04\n",
      "Epoch 16/20000: Train Loss = 0.450646, Test Loss = 0.267030, Learning Rate = 9.943155e-04\n",
      "Epoch 17/20000: Train Loss = 0.465493, Test Loss = 0.295021, Learning Rate = 9.939377e-04\n",
      "Epoch 18/20000: Train Loss = 0.468147, Test Loss = 0.209236, Learning Rate = 9.935601e-04\n",
      "Epoch 19/20000: Train Loss = 0.466079, Test Loss = 0.323347, Learning Rate = 9.931825e-04\n",
      "Epoch 20/20000: Train Loss = 0.453162, Test Loss = 0.203753, Learning Rate = 9.928052e-04\n",
      "Epoch 21/20000: Train Loss = 0.467787, Test Loss = 0.191895, Learning Rate = 9.924279e-04\n",
      "Epoch 22/20000: Train Loss = 0.459454, Test Loss = 0.273384, Learning Rate = 9.920508e-04\n",
      "Epoch 23/20000: Train Loss = 0.455979, Test Loss = 0.305326, Learning Rate = 9.916739e-04\n",
      "Epoch 24/20000: Train Loss = 0.459139, Test Loss = 0.301099, Learning Rate = 9.912971e-04\n",
      "Epoch 25/20000: Train Loss = 0.454412, Test Loss = 0.216231, Learning Rate = 9.909204e-04\n",
      "Epoch 26/20000: Train Loss = 0.462235, Test Loss = 0.293679, Learning Rate = 9.905439e-04\n",
      "Epoch 27/20000: Train Loss = 0.458952, Test Loss = 0.275064, Learning Rate = 9.901675e-04\n",
      "Epoch 28/20000: Train Loss = 0.453409, Test Loss = 0.257062, Learning Rate = 9.897912e-04\n",
      "Epoch 29/20000: Train Loss = 0.451337, Test Loss = 0.256718, Learning Rate = 9.894152e-04\n",
      "Epoch 30/20000: Train Loss = 0.452247, Test Loss = 0.225069, Learning Rate = 9.890392e-04\n",
      "Epoch 31/20000: Train Loss = 0.455288, Test Loss = 0.195693, Learning Rate = 9.886634e-04\n",
      "Epoch 32/20000: Train Loss = 0.454199, Test Loss = 0.225344, Learning Rate = 9.882877e-04\n",
      "Epoch 33/20000: Train Loss = 0.451455, Test Loss = 0.280567, Learning Rate = 9.879122e-04\n",
      "Epoch 34/20000: Train Loss = 0.448485, Test Loss = 0.303239, Learning Rate = 9.875368e-04\n",
      "Epoch 35/20000: Train Loss = 0.450175, Test Loss = 0.242278, Learning Rate = 9.871616e-04\n",
      "Epoch 36/20000: Train Loss = 0.451493, Test Loss = 0.251617, Learning Rate = 9.867865e-04\n",
      "Epoch 37/20000: Train Loss = 0.462181, Test Loss = 0.227078, Learning Rate = 9.864115e-04\n",
      "Epoch 38/20000: Train Loss = 0.458433, Test Loss = 0.256740, Learning Rate = 9.860367e-04\n",
      "Epoch 39/20000: Train Loss = 0.449552, Test Loss = 0.228855, Learning Rate = 9.856621e-04\n",
      "Epoch 40/20000: Train Loss = 0.451610, Test Loss = 0.220098, Learning Rate = 9.852875e-04\n",
      "Epoch 41/20000: Train Loss = 0.457400, Test Loss = 0.230370, Learning Rate = 9.849132e-04\n",
      "Epoch 42/20000: Train Loss = 0.458735, Test Loss = 0.249807, Learning Rate = 9.845389e-04\n",
      "Epoch 43/20000: Train Loss = 0.455373, Test Loss = 0.246728, Learning Rate = 9.841648e-04\n",
      "Epoch 44/20000: Train Loss = 0.462714, Test Loss = 0.227137, Learning Rate = 9.837909e-04\n",
      "Epoch 45/20000: Train Loss = 0.463383, Test Loss = 0.241259, Learning Rate = 9.834171e-04\n",
      "Epoch 46/20000: Train Loss = 0.448797, Test Loss = 0.185225, Learning Rate = 9.830434e-04\n",
      "Epoch 47/20000: Train Loss = 0.454260, Test Loss = 0.265429, Learning Rate = 9.826698e-04\n",
      "Epoch 48/20000: Train Loss = 0.454758, Test Loss = 0.230394, Learning Rate = 9.822965e-04\n",
      "Epoch 49/20000: Train Loss = 0.456169, Test Loss = 0.277750, Learning Rate = 9.819232e-04\n",
      "Epoch 50/20000: Train Loss = 0.449871, Test Loss = 0.275475, Learning Rate = 9.815501e-04\n",
      "Epoch 51/20000: Train Loss = 0.456393, Test Loss = 0.233199, Learning Rate = 9.811771e-04\n",
      "Epoch 52/20000: Train Loss = 0.454647, Test Loss = 0.248994, Learning Rate = 9.808043e-04\n",
      "Epoch 53/20000: Train Loss = 0.464055, Test Loss = 0.263323, Learning Rate = 9.804316e-04\n",
      "Epoch 54/20000: Train Loss = 0.461006, Test Loss = 0.294270, Learning Rate = 9.800591e-04\n",
      "Epoch 55/20000: Train Loss = 0.457731, Test Loss = 0.215992, Learning Rate = 9.796867e-04\n",
      "Epoch 56/20000: Train Loss = 0.447507, Test Loss = 0.277086, Learning Rate = 9.793145e-04\n",
      "Epoch 57/20000: Train Loss = 0.450274, Test Loss = 0.262463, Learning Rate = 9.789423e-04\n",
      "Epoch 58/20000: Train Loss = 0.451722, Test Loss = 0.237521, Learning Rate = 9.785704e-04\n",
      "Epoch 59/20000: Train Loss = 0.452964, Test Loss = 0.300510, Learning Rate = 9.781985e-04\n",
      "Epoch 60/20000: Train Loss = 0.452601, Test Loss = 0.237476, Learning Rate = 9.778269e-04\n",
      "Epoch 61/20000: Train Loss = 0.456950, Test Loss = 0.291194, Learning Rate = 9.774553e-04\n",
      "Epoch 62/20000: Train Loss = 0.450073, Test Loss = 0.291618, Learning Rate = 9.770839e-04\n",
      "Epoch 63/20000: Train Loss = 0.452304, Test Loss = 0.195697, Learning Rate = 9.767126e-04\n",
      "Epoch 64/20000: Train Loss = 0.447259, Test Loss = 0.254374, Learning Rate = 9.763415e-04\n",
      "Epoch 65/20000: Train Loss = 0.446504, Test Loss = 0.225672, Learning Rate = 9.759705e-04\n",
      "Epoch 66/20000: Train Loss = 0.457904, Test Loss = 0.259861, Learning Rate = 9.755997e-04\n",
      "Epoch 67/20000: Train Loss = 0.454044, Test Loss = 0.257704, Learning Rate = 9.752290e-04\n",
      "Epoch 68/20000: Train Loss = 0.456458, Test Loss = 0.217674, Learning Rate = 9.748584e-04\n",
      "Epoch 69/20000: Train Loss = 0.446884, Test Loss = 0.194321, Learning Rate = 9.744880e-04\n",
      "Epoch 70/20000: Train Loss = 0.466626, Test Loss = 0.255924, Learning Rate = 9.741177e-04\n",
      "Epoch 71/20000: Train Loss = 0.461581, Test Loss = 0.217674, Learning Rate = 9.737476e-04\n",
      "Epoch 72/20000: Train Loss = 0.452598, Test Loss = 0.204600, Learning Rate = 9.733776e-04\n",
      "Epoch 73/20000: Train Loss = 0.458847, Test Loss = 0.296861, Learning Rate = 9.730077e-04\n",
      "Epoch 74/20000: Train Loss = 0.456668, Test Loss = 0.227749, Learning Rate = 9.726380e-04\n",
      "Epoch 75/20000: Train Loss = 0.455407, Test Loss = 0.212014, Learning Rate = 9.722684e-04\n",
      "Epoch 76/20000: Train Loss = 0.450012, Test Loss = 0.236690, Learning Rate = 9.718990e-04\n",
      "Epoch 77/20000: Train Loss = 0.447807, Test Loss = 0.265917, Learning Rate = 9.715297e-04\n",
      "Epoch 78/20000: Train Loss = 0.447701, Test Loss = 0.245713, Learning Rate = 9.711606e-04\n",
      "Epoch 79/20000: Train Loss = 0.449512, Test Loss = 0.306298, Learning Rate = 9.707915e-04\n",
      "Epoch 80/20000: Train Loss = 0.453359, Test Loss = 0.242819, Learning Rate = 9.704227e-04\n",
      "Epoch 81/20000: Train Loss = 0.452602, Test Loss = 0.239251, Learning Rate = 9.700539e-04\n",
      "Epoch 82/20000: Train Loss = 0.456070, Test Loss = 0.181834, Learning Rate = 9.696853e-04\n",
      "Epoch 83/20000: Train Loss = 0.452695, Test Loss = 0.273024, Learning Rate = 9.693169e-04\n",
      "Epoch 84/20000: Train Loss = 0.462654, Test Loss = 0.254748, Learning Rate = 9.689486e-04\n",
      "Epoch 85/20000: Train Loss = 0.456817, Test Loss = 0.213583, Learning Rate = 9.685804e-04\n",
      "Epoch 86/20000: Train Loss = 0.451999, Test Loss = 0.254734, Learning Rate = 9.682124e-04\n",
      "Epoch 87/20000: Train Loss = 0.445462, Test Loss = 0.205902, Learning Rate = 9.678445e-04\n",
      "Epoch 88/20000: Train Loss = 0.458192, Test Loss = 0.304946, Learning Rate = 9.674767e-04\n",
      "Epoch 89/20000: Train Loss = 0.456639, Test Loss = 0.252724, Learning Rate = 9.671091e-04\n",
      "Epoch 90/20000: Train Loss = 0.452306, Test Loss = 0.262752, Learning Rate = 9.667416e-04\n",
      "Epoch 91/20000: Train Loss = 0.458071, Test Loss = 0.259483, Learning Rate = 9.663743e-04\n",
      "Epoch 92/20000: Train Loss = 0.449492, Test Loss = 0.196300, Learning Rate = 9.660071e-04\n",
      "Epoch 93/20000: Train Loss = 0.448302, Test Loss = 0.219837, Learning Rate = 9.656400e-04\n",
      "Epoch 94/20000: Train Loss = 0.452919, Test Loss = 0.283472, Learning Rate = 9.652731e-04\n",
      "Epoch 95/20000: Train Loss = 0.450157, Test Loss = 0.234181, Learning Rate = 9.649063e-04\n",
      "Epoch 96/20000: Train Loss = 0.449884, Test Loss = 0.212306, Learning Rate = 9.645397e-04\n",
      "Epoch 97/20000: Train Loss = 0.447388, Test Loss = 0.200631, Learning Rate = 9.641732e-04\n",
      "Epoch 98/20000: Train Loss = 0.454454, Test Loss = 0.223633, Learning Rate = 9.638068e-04\n",
      "Epoch 99/20000: Train Loss = 0.449286, Test Loss = 0.250200, Learning Rate = 9.634406e-04\n",
      "Epoch 100/20000: Train Loss = 0.453586, Test Loss = 0.228990, Learning Rate = 9.630745e-04\n",
      "Epoch 101/20000: Train Loss = 0.456121, Test Loss = 0.272811, Learning Rate = 9.627086e-04\n",
      "Epoch 102/20000: Train Loss = 0.455591, Test Loss = 0.278214, Learning Rate = 9.623428e-04\n",
      "Epoch 103/20000: Train Loss = 0.461817, Test Loss = 0.229940, Learning Rate = 9.619771e-04\n",
      "Epoch 104/20000: Train Loss = 0.455359, Test Loss = 0.220420, Learning Rate = 9.616116e-04\n",
      "Epoch 105/20000: Train Loss = 0.449371, Test Loss = 0.216152, Learning Rate = 9.612462e-04\n",
      "Epoch 106/20000: Train Loss = 0.450738, Test Loss = 0.167681, Learning Rate = 9.608810e-04\n",
      "Epoch 107/20000: Train Loss = 0.454142, Test Loss = 0.235367, Learning Rate = 9.605159e-04\n",
      "Epoch 108/20000: Train Loss = 0.454356, Test Loss = 0.224316, Learning Rate = 9.601509e-04\n",
      "Epoch 109/20000: Train Loss = 0.450386, Test Loss = 0.221831, Learning Rate = 9.597861e-04\n",
      "Epoch 110/20000: Train Loss = 0.454355, Test Loss = 0.268580, Learning Rate = 9.594214e-04\n",
      "Epoch 111/20000: Train Loss = 0.448100, Test Loss = 0.211408, Learning Rate = 9.590568e-04\n",
      "Epoch 112/20000: Train Loss = 0.449584, Test Loss = 0.289997, Learning Rate = 9.586924e-04\n",
      "Epoch 113/20000: Train Loss = 0.456315, Test Loss = 0.215282, Learning Rate = 9.583281e-04\n",
      "Epoch 114/20000: Train Loss = 0.454128, Test Loss = 0.223249, Learning Rate = 9.579640e-04\n",
      "Epoch 115/20000: Train Loss = 0.450886, Test Loss = 0.208451, Learning Rate = 9.576000e-04\n",
      "Epoch 116/20000: Train Loss = 0.461783, Test Loss = 0.218843, Learning Rate = 9.572361e-04\n",
      "Epoch 117/20000: Train Loss = 0.455194, Test Loss = 0.198689, Learning Rate = 9.568724e-04\n",
      "Epoch 118/20000: Train Loss = 0.450002, Test Loss = 0.239717, Learning Rate = 9.565088e-04\n",
      "Epoch 119/20000: Train Loss = 0.452912, Test Loss = 0.214433, Learning Rate = 9.561454e-04\n",
      "Epoch 120/20000: Train Loss = 0.449779, Test Loss = 0.263910, Learning Rate = 9.557821e-04\n",
      "Epoch 121/20000: Train Loss = 0.453914, Test Loss = 0.268032, Learning Rate = 9.554189e-04\n",
      "Epoch 122/20000: Train Loss = 0.449646, Test Loss = 0.261855, Learning Rate = 9.550558e-04\n",
      "Epoch 123/20000: Train Loss = 0.458464, Test Loss = 0.232875, Learning Rate = 9.546930e-04\n",
      "Epoch 124/20000: Train Loss = 0.449026, Test Loss = 0.302060, Learning Rate = 9.543302e-04\n",
      "Epoch 125/20000: Train Loss = 0.450499, Test Loss = 0.205356, Learning Rate = 9.539676e-04\n",
      "Epoch 126/20000: Train Loss = 0.457178, Test Loss = 0.205407, Learning Rate = 9.536051e-04\n",
      "Epoch 127/20000: Train Loss = 0.453577, Test Loss = 0.195090, Learning Rate = 9.532428e-04\n",
      "Epoch 128/20000: Train Loss = 0.464096, Test Loss = 0.235554, Learning Rate = 9.528805e-04\n",
      "Epoch 129/20000: Train Loss = 0.454225, Test Loss = 0.265467, Learning Rate = 9.525185e-04\n",
      "Epoch 130/20000: Train Loss = 0.452561, Test Loss = 0.263529, Learning Rate = 9.521565e-04\n",
      "Epoch 131/20000: Train Loss = 0.450892, Test Loss = 0.210430, Learning Rate = 9.517948e-04\n",
      "Epoch 132/20000: Train Loss = 0.448909, Test Loss = 0.251526, Learning Rate = 9.514331e-04\n",
      "Epoch 133/20000: Train Loss = 0.449178, Test Loss = 0.238842, Learning Rate = 9.510716e-04\n",
      "Epoch 134/20000: Train Loss = 0.454076, Test Loss = 0.210338, Learning Rate = 9.507102e-04\n",
      "Epoch 135/20000: Train Loss = 0.450131, Test Loss = 0.264111, Learning Rate = 9.503489e-04\n",
      "Epoch 136/20000: Train Loss = 0.445581, Test Loss = 0.277739, Learning Rate = 9.499878e-04\n",
      "Epoch 137/20000: Train Loss = 0.448471, Test Loss = 0.210545, Learning Rate = 9.496269e-04\n",
      "Epoch 138/20000: Train Loss = 0.454528, Test Loss = 0.212076, Learning Rate = 9.492660e-04\n",
      "Epoch 139/20000: Train Loss = 0.447331, Test Loss = 0.243442, Learning Rate = 9.489053e-04\n",
      "Epoch 140/20000: Train Loss = 0.456555, Test Loss = 0.235188, Learning Rate = 9.485448e-04\n",
      "Epoch 141/20000: Train Loss = 0.454826, Test Loss = 0.306055, Learning Rate = 9.481844e-04\n",
      "Epoch 142/20000: Train Loss = 0.451980, Test Loss = 0.243038, Learning Rate = 9.478241e-04\n",
      "Epoch 143/20000: Train Loss = 0.455095, Test Loss = 0.193430, Learning Rate = 9.474639e-04\n",
      "Epoch 144/20000: Train Loss = 0.451714, Test Loss = 0.219925, Learning Rate = 9.471039e-04\n",
      "Epoch 145/20000: Train Loss = 0.448220, Test Loss = 0.256577, Learning Rate = 9.467440e-04\n",
      "Epoch 146/20000: Train Loss = 0.451241, Test Loss = 0.258829, Learning Rate = 9.463843e-04\n",
      "Epoch 147/20000: Train Loss = 0.448759, Test Loss = 0.288088, Learning Rate = 9.460247e-04\n",
      "Epoch 148/20000: Train Loss = 0.453267, Test Loss = 0.290647, Learning Rate = 9.456652e-04\n",
      "Epoch 149/20000: Train Loss = 0.453055, Test Loss = 0.279437, Learning Rate = 9.453059e-04\n",
      "Epoch 150/20000: Train Loss = 0.450146, Test Loss = 0.259109, Learning Rate = 9.449467e-04\n",
      "Epoch 151/20000: Train Loss = 0.454330, Test Loss = 0.235656, Learning Rate = 9.445877e-04\n",
      "Epoch 152/20000: Train Loss = 0.449445, Test Loss = 0.180258, Learning Rate = 9.442288e-04\n",
      "Epoch 153/20000: Train Loss = 0.456689, Test Loss = 0.222803, Learning Rate = 9.438700e-04\n",
      "Epoch 154/20000: Train Loss = 0.457190, Test Loss = 0.208182, Learning Rate = 9.435113e-04\n",
      "Epoch 155/20000: Train Loss = 0.455886, Test Loss = 0.203382, Learning Rate = 9.431528e-04\n",
      "Epoch 156/20000: Train Loss = 0.449047, Test Loss = 0.210238, Learning Rate = 9.427945e-04\n",
      "Epoch 157/20000: Train Loss = 0.448342, Test Loss = 0.303409, Learning Rate = 9.424362e-04\n",
      "Epoch 158/20000: Train Loss = 0.456079, Test Loss = 0.256987, Learning Rate = 9.420781e-04\n",
      "Epoch 159/20000: Train Loss = 0.448096, Test Loss = 0.323386, Learning Rate = 9.417201e-04\n",
      "Epoch 160/20000: Train Loss = 0.457189, Test Loss = 0.265146, Learning Rate = 9.413623e-04\n",
      "Epoch 161/20000: Train Loss = 0.451134, Test Loss = 0.226799, Learning Rate = 9.410046e-04\n",
      "Epoch 162/20000: Train Loss = 0.449478, Test Loss = 0.221626, Learning Rate = 9.406471e-04\n",
      "Epoch 163/20000: Train Loss = 0.455364, Test Loss = 0.212569, Learning Rate = 9.402897e-04\n",
      "Epoch 164/20000: Train Loss = 0.451947, Test Loss = 0.268753, Learning Rate = 9.399324e-04\n",
      "Epoch 165/20000: Train Loss = 0.448871, Test Loss = 0.258480, Learning Rate = 9.395752e-04\n",
      "Epoch 166/20000: Train Loss = 0.448076, Test Loss = 0.275139, Learning Rate = 9.392182e-04\n",
      "Epoch 167/20000: Train Loss = 0.453204, Test Loss = 0.222671, Learning Rate = 9.388613e-04\n",
      "Epoch 168/20000: Train Loss = 0.451331, Test Loss = 0.237037, Learning Rate = 9.385046e-04\n",
      "Epoch 169/20000: Train Loss = 0.452909, Test Loss = 0.231349, Learning Rate = 9.381480e-04\n",
      "Epoch 170/20000: Train Loss = 0.448530, Test Loss = 0.238512, Learning Rate = 9.377915e-04\n",
      "Epoch 171/20000: Train Loss = 0.448665, Test Loss = 0.259302, Learning Rate = 9.374352e-04\n",
      "Epoch 172/20000: Train Loss = 0.448965, Test Loss = 0.304341, Learning Rate = 9.370790e-04\n",
      "Epoch 173/20000: Train Loss = 0.450009, Test Loss = 0.191699, Learning Rate = 9.367229e-04\n",
      "Epoch 174/20000: Train Loss = 0.444564, Test Loss = 0.276864, Learning Rate = 9.363670e-04\n",
      "Epoch 175/20000: Train Loss = 0.451779, Test Loss = 0.231240, Learning Rate = 9.360112e-04\n",
      "Epoch 176/20000: Train Loss = 0.448324, Test Loss = 0.256634, Learning Rate = 9.356555e-04\n",
      "Epoch 177/20000: Train Loss = 0.446382, Test Loss = 0.278228, Learning Rate = 9.353000e-04\n",
      "Epoch 178/20000: Train Loss = 0.457852, Test Loss = 0.232368, Learning Rate = 9.349446e-04\n",
      "Epoch 179/20000: Train Loss = 0.451448, Test Loss = 0.218851, Learning Rate = 9.345894e-04\n",
      "Epoch 180/20000: Train Loss = 0.459580, Test Loss = 0.267392, Learning Rate = 9.342342e-04\n",
      "Epoch 181/20000: Train Loss = 0.453396, Test Loss = 0.220841, Learning Rate = 9.338793e-04\n",
      "Epoch 182/20000: Train Loss = 0.446258, Test Loss = 0.198015, Learning Rate = 9.335244e-04\n",
      "Epoch 183/20000: Train Loss = 0.443480, Test Loss = 0.356941, Learning Rate = 9.331697e-04\n",
      "Epoch 184/20000: Train Loss = 0.447279, Test Loss = 0.287830, Learning Rate = 9.328151e-04\n",
      "Epoch 185/20000: Train Loss = 0.448933, Test Loss = 0.292770, Learning Rate = 9.324607e-04\n",
      "Epoch 186/20000: Train Loss = 0.454274, Test Loss = 0.260918, Learning Rate = 9.321064e-04\n",
      "Epoch 187/20000: Train Loss = 0.448550, Test Loss = 0.248472, Learning Rate = 9.317522e-04\n",
      "Epoch 188/20000: Train Loss = 0.447108, Test Loss = 0.258604, Learning Rate = 9.313981e-04\n",
      "Epoch 189/20000: Train Loss = 0.448974, Test Loss = 0.281025, Learning Rate = 9.310442e-04\n",
      "Epoch 190/20000: Train Loss = 0.446718, Test Loss = 0.213717, Learning Rate = 9.306905e-04\n",
      "Epoch 191/20000: Train Loss = 0.446379, Test Loss = 0.263672, Learning Rate = 9.303368e-04\n",
      "Epoch 192/20000: Train Loss = 0.455155, Test Loss = 0.243107, Learning Rate = 9.299833e-04\n",
      "Epoch 193/20000: Train Loss = 0.451176, Test Loss = 0.269244, Learning Rate = 9.296300e-04\n",
      "Epoch 194/20000: Train Loss = 0.449869, Test Loss = 0.242435, Learning Rate = 9.292767e-04\n",
      "Epoch 195/20000: Train Loss = 0.450561, Test Loss = 0.227398, Learning Rate = 9.289236e-04\n",
      "Epoch 196/20000: Train Loss = 0.451362, Test Loss = 0.290429, Learning Rate = 9.285707e-04\n",
      "Epoch 197/20000: Train Loss = 0.454884, Test Loss = 0.318234, Learning Rate = 9.282178e-04\n",
      "Epoch 198/20000: Train Loss = 0.448078, Test Loss = 0.255363, Learning Rate = 9.278651e-04\n",
      "Epoch 199/20000: Train Loss = 0.452774, Test Loss = 0.185862, Learning Rate = 9.275126e-04\n",
      "Epoch 200/20000: Train Loss = 0.451111, Test Loss = 0.217564, Learning Rate = 9.271601e-04\n",
      "Epoch 201/20000: Train Loss = 0.452325, Test Loss = 0.210395, Learning Rate = 9.268078e-04\n",
      "Epoch 202/20000: Train Loss = 0.453136, Test Loss = 0.285495, Learning Rate = 9.264557e-04\n",
      "Epoch 203/20000: Train Loss = 0.456513, Test Loss = 0.204907, Learning Rate = 9.261037e-04\n",
      "Epoch 204/20000: Train Loss = 0.458020, Test Loss = 0.252468, Learning Rate = 9.257518e-04\n",
      "Epoch 205/20000: Train Loss = 0.449880, Test Loss = 0.246726, Learning Rate = 9.254000e-04\n",
      "Epoch 206/20000: Train Loss = 0.454058, Test Loss = 0.192926, Learning Rate = 9.250484e-04\n",
      "Epoch 207/20000: Train Loss = 0.448751, Test Loss = 0.245746, Learning Rate = 9.246969e-04\n",
      "Epoch 208/20000: Train Loss = 0.447711, Test Loss = 0.184131, Learning Rate = 9.243455e-04\n",
      "Epoch 209/20000: Train Loss = 0.453521, Test Loss = 0.205678, Learning Rate = 9.239943e-04\n",
      "Epoch 210/20000: Train Loss = 0.454928, Test Loss = 0.244744, Learning Rate = 9.236432e-04\n",
      "Epoch 211/20000: Train Loss = 0.451123, Test Loss = 0.219162, Learning Rate = 9.232922e-04\n",
      "Epoch 212/20000: Train Loss = 0.447583, Test Loss = 0.220283, Learning Rate = 9.229414e-04\n",
      "Epoch 213/20000: Train Loss = 0.448252, Test Loss = 0.276635, Learning Rate = 9.225907e-04\n",
      "Epoch 214/20000: Train Loss = 0.455073, Test Loss = 0.211879, Learning Rate = 9.222402e-04\n",
      "Epoch 215/20000: Train Loss = 0.452486, Test Loss = 0.240216, Learning Rate = 9.218897e-04\n",
      "Epoch 216/20000: Train Loss = 0.452697, Test Loss = 0.197757, Learning Rate = 9.215394e-04\n",
      "Epoch 217/20000: Train Loss = 0.453492, Test Loss = 0.279900, Learning Rate = 9.211893e-04\n",
      "Epoch 218/20000: Train Loss = 0.454987, Test Loss = 0.275615, Learning Rate = 9.208393e-04\n",
      "Epoch 219/20000: Train Loss = 0.450539, Test Loss = 0.340941, Learning Rate = 9.204894e-04\n",
      "Epoch 220/20000: Train Loss = 0.456669, Test Loss = 0.243084, Learning Rate = 9.201396e-04\n",
      "Epoch 221/20000: Train Loss = 0.448931, Test Loss = 0.193079, Learning Rate = 9.197900e-04\n",
      "Epoch 222/20000: Train Loss = 0.446647, Test Loss = 0.253704, Learning Rate = 9.194405e-04\n",
      "Epoch 223/20000: Train Loss = 0.450154, Test Loss = 0.261836, Learning Rate = 9.190911e-04\n",
      "Epoch 224/20000: Train Loss = 0.450051, Test Loss = 0.290491, Learning Rate = 9.187419e-04\n",
      "Epoch 225/20000: Train Loss = 0.448046, Test Loss = 0.266869, Learning Rate = 9.183928e-04\n",
      "Epoch 226/20000: Train Loss = 0.446383, Test Loss = 0.206901, Learning Rate = 9.180438e-04\n",
      "Epoch 227/20000: Train Loss = 0.449475, Test Loss = 0.260613, Learning Rate = 9.176950e-04\n",
      "Epoch 228/20000: Train Loss = 0.452189, Test Loss = 0.246183, Learning Rate = 9.173463e-04\n",
      "Epoch 229/20000: Train Loss = 0.454711, Test Loss = 0.252815, Learning Rate = 9.169977e-04\n",
      "Epoch 230/20000: Train Loss = 0.448157, Test Loss = 0.199256, Learning Rate = 9.166493e-04\n",
      "Epoch 231/20000: Train Loss = 0.452754, Test Loss = 0.215889, Learning Rate = 9.163010e-04\n",
      "Epoch 232/20000: Train Loss = 0.449244, Test Loss = 0.251143, Learning Rate = 9.159528e-04\n",
      "Epoch 233/20000: Train Loss = 0.449971, Test Loss = 0.260994, Learning Rate = 9.156048e-04\n",
      "Epoch 234/20000: Train Loss = 0.446133, Test Loss = 0.227423, Learning Rate = 9.152569e-04\n",
      "Epoch 235/20000: Train Loss = 0.452375, Test Loss = 0.275973, Learning Rate = 9.149091e-04\n",
      "Epoch 236/20000: Train Loss = 0.446742, Test Loss = 0.259941, Learning Rate = 9.145615e-04\n",
      "Epoch 237/20000: Train Loss = 0.447480, Test Loss = 0.203680, Learning Rate = 9.142140e-04\n",
      "Epoch 238/20000: Train Loss = 0.450437, Test Loss = 0.284235, Learning Rate = 9.138666e-04\n",
      "Epoch 239/20000: Train Loss = 0.450964, Test Loss = 0.185989, Learning Rate = 9.135193e-04\n",
      "Epoch 240/20000: Train Loss = 0.453292, Test Loss = 0.305379, Learning Rate = 9.131722e-04\n",
      "Epoch 241/20000: Train Loss = 0.452559, Test Loss = 0.227027, Learning Rate = 9.128252e-04\n",
      "Epoch 242/20000: Train Loss = 0.447805, Test Loss = 0.281859, Learning Rate = 9.124784e-04\n",
      "Epoch 243/20000: Train Loss = 0.452029, Test Loss = 0.206953, Learning Rate = 9.121317e-04\n",
      "Epoch 244/20000: Train Loss = 0.449452, Test Loss = 0.249126, Learning Rate = 9.117851e-04\n",
      "Epoch 245/20000: Train Loss = 0.452389, Test Loss = 0.181223, Learning Rate = 9.114386e-04\n",
      "Epoch 246/20000: Train Loss = 0.455083, Test Loss = 0.179245, Learning Rate = 9.110923e-04\n",
      "Epoch 247/20000: Train Loss = 0.453108, Test Loss = 0.251852, Learning Rate = 9.107461e-04\n",
      "Epoch 248/20000: Train Loss = 0.448579, Test Loss = 0.268589, Learning Rate = 9.104001e-04\n",
      "Epoch 249/20000: Train Loss = 0.446058, Test Loss = 0.210622, Learning Rate = 9.100541e-04\n",
      "Epoch 250/20000: Train Loss = 0.445762, Test Loss = 0.237584, Learning Rate = 9.097083e-04\n",
      "Epoch 251/20000: Train Loss = 0.448445, Test Loss = 0.236392, Learning Rate = 9.093627e-04\n",
      "Epoch 252/20000: Train Loss = 0.452200, Test Loss = 0.275339, Learning Rate = 9.090171e-04\n",
      "Epoch 253/20000: Train Loss = 0.453448, Test Loss = 0.277277, Learning Rate = 9.086717e-04\n",
      "Epoch 254/20000: Train Loss = 0.447664, Test Loss = 0.258411, Learning Rate = 9.083265e-04\n",
      "Epoch 255/20000: Train Loss = 0.449756, Test Loss = 0.279615, Learning Rate = 9.079813e-04\n",
      "Epoch 256/20000: Train Loss = 0.450947, Test Loss = 0.319351, Learning Rate = 9.076363e-04\n",
      "Epoch 257/20000: Train Loss = 0.449923, Test Loss = 0.306716, Learning Rate = 9.072914e-04\n",
      "Epoch 258/20000: Train Loss = 0.453119, Test Loss = 0.259644, Learning Rate = 9.069467e-04\n",
      "Epoch 259/20000: Train Loss = 0.449266, Test Loss = 0.251197, Learning Rate = 9.066021e-04\n",
      "Epoch 260/20000: Train Loss = 0.454616, Test Loss = 0.216368, Learning Rate = 9.062576e-04\n",
      "Epoch 261/20000: Train Loss = 0.448630, Test Loss = 0.220752, Learning Rate = 9.059132e-04\n",
      "Epoch 262/20000: Train Loss = 0.450126, Test Loss = 0.275509, Learning Rate = 9.055690e-04\n",
      "Epoch 263/20000: Train Loss = 0.450029, Test Loss = 0.242306, Learning Rate = 9.052249e-04\n",
      "Epoch 264/20000: Train Loss = 0.447880, Test Loss = 0.241550, Learning Rate = 9.048810e-04\n",
      "Epoch 265/20000: Train Loss = 0.450635, Test Loss = 0.251532, Learning Rate = 9.045371e-04\n",
      "Epoch 266/20000: Train Loss = 0.448462, Test Loss = 0.186758, Learning Rate = 9.041934e-04\n",
      "Epoch 267/20000: Train Loss = 0.454799, Test Loss = 0.258032, Learning Rate = 9.038499e-04\n",
      "Epoch 268/20000: Train Loss = 0.449870, Test Loss = 0.218837, Learning Rate = 9.035064e-04\n",
      "Epoch 269/20000: Train Loss = 0.446940, Test Loss = 0.212719, Learning Rate = 9.031631e-04\n",
      "Epoch 270/20000: Train Loss = 0.450808, Test Loss = 0.177906, Learning Rate = 9.028199e-04\n",
      "Epoch 271/20000: Train Loss = 0.468406, Test Loss = 0.191998, Learning Rate = 9.024769e-04\n",
      "Epoch 272/20000: Train Loss = 0.464420, Test Loss = 0.212010, Learning Rate = 9.021340e-04\n",
      "Epoch 273/20000: Train Loss = 0.450985, Test Loss = 0.256962, Learning Rate = 9.017912e-04\n",
      "Epoch 274/20000: Train Loss = 0.455734, Test Loss = 0.271275, Learning Rate = 9.014485e-04\n",
      "Epoch 275/20000: Train Loss = 0.453821, Test Loss = 0.222421, Learning Rate = 9.011060e-04\n",
      "Epoch 276/20000: Train Loss = 0.454449, Test Loss = 0.249599, Learning Rate = 9.007636e-04\n",
      "Epoch 277/20000: Train Loss = 0.449386, Test Loss = 0.304344, Learning Rate = 9.004214e-04\n",
      "Epoch 278/20000: Train Loss = 0.448287, Test Loss = 0.273254, Learning Rate = 9.000792e-04\n",
      "Epoch 279/20000: Train Loss = 0.446345, Test Loss = 0.230725, Learning Rate = 8.997372e-04\n",
      "Epoch 280/20000: Train Loss = 0.446560, Test Loss = 0.253932, Learning Rate = 8.993953e-04\n",
      "Epoch 281/20000: Train Loss = 0.445787, Test Loss = 0.211407, Learning Rate = 8.990536e-04\n",
      "Epoch 282/20000: Train Loss = 0.448222, Test Loss = 0.188915, Learning Rate = 8.987120e-04\n",
      "Epoch 283/20000: Train Loss = 0.451485, Test Loss = 0.234103, Learning Rate = 8.983705e-04\n",
      "Epoch 284/20000: Train Loss = 0.449206, Test Loss = 0.301661, Learning Rate = 8.980291e-04\n",
      "Epoch 285/20000: Train Loss = 0.450105, Test Loss = 0.191726, Learning Rate = 8.976879e-04\n",
      "Epoch 286/20000: Train Loss = 0.458316, Test Loss = 0.270468, Learning Rate = 8.973468e-04\n",
      "Epoch 287/20000: Train Loss = 0.459276, Test Loss = 0.218715, Learning Rate = 8.970058e-04\n",
      "Epoch 288/20000: Train Loss = 0.454073, Test Loss = 0.234804, Learning Rate = 8.966650e-04\n",
      "Epoch 289/20000: Train Loss = 0.455990, Test Loss = 0.237327, Learning Rate = 8.963243e-04\n",
      "Epoch 290/20000: Train Loss = 0.449005, Test Loss = 0.204892, Learning Rate = 8.959837e-04\n",
      "Epoch 291/20000: Train Loss = 0.448878, Test Loss = 0.221445, Learning Rate = 8.956433e-04\n",
      "Epoch 292/20000: Train Loss = 0.448144, Test Loss = 0.286360, Learning Rate = 8.953029e-04\n",
      "Epoch 293/20000: Train Loss = 0.448395, Test Loss = 0.242938, Learning Rate = 8.949628e-04\n",
      "Epoch 294/20000: Train Loss = 0.445749, Test Loss = 0.233446, Learning Rate = 8.946227e-04\n",
      "Epoch 295/20000: Train Loss = 0.447390, Test Loss = 0.222973, Learning Rate = 8.942828e-04\n",
      "Epoch 296/20000: Train Loss = 0.448200, Test Loss = 0.224073, Learning Rate = 8.939430e-04\n",
      "Epoch 297/20000: Train Loss = 0.445915, Test Loss = 0.224974, Learning Rate = 8.936033e-04\n",
      "Epoch 298/20000: Train Loss = 0.444434, Test Loss = 0.304083, Learning Rate = 8.932637e-04\n",
      "Epoch 299/20000: Train Loss = 0.458275, Test Loss = 0.260730, Learning Rate = 8.929243e-04\n",
      "Epoch 300/20000: Train Loss = 0.447133, Test Loss = 0.229413, Learning Rate = 8.925850e-04\n",
      "Epoch 301/20000: Train Loss = 0.449360, Test Loss = 0.279047, Learning Rate = 8.922459e-04\n",
      "Epoch 302/20000: Train Loss = 0.449584, Test Loss = 0.274305, Learning Rate = 8.919068e-04\n",
      "Epoch 303/20000: Train Loss = 0.452719, Test Loss = 0.204902, Learning Rate = 8.915679e-04\n",
      "Epoch 304/20000: Train Loss = 0.450897, Test Loss = 0.329614, Learning Rate = 8.912292e-04\n",
      "Epoch 305/20000: Train Loss = 0.448992, Test Loss = 0.256391, Learning Rate = 8.908905e-04\n",
      "Epoch 306/20000: Train Loss = 0.450711, Test Loss = 0.231196, Learning Rate = 8.905520e-04\n",
      "Epoch 307/20000: Train Loss = 0.447311, Test Loss = 0.246691, Learning Rate = 8.902136e-04\n",
      "Epoch 308/20000: Train Loss = 0.447972, Test Loss = 0.218190, Learning Rate = 8.898754e-04\n",
      "Epoch 309/20000: Train Loss = 0.448095, Test Loss = 0.257805, Learning Rate = 8.895372e-04\n",
      "Epoch 310/20000: Train Loss = 0.447936, Test Loss = 0.241252, Learning Rate = 8.891992e-04\n",
      "Epoch 311/20000: Train Loss = 0.447492, Test Loss = 0.251477, Learning Rate = 8.888614e-04\n",
      "Epoch 312/20000: Train Loss = 0.449327, Test Loss = 0.178289, Learning Rate = 8.885236e-04\n",
      "Epoch 313/20000: Train Loss = 0.451062, Test Loss = 0.243674, Learning Rate = 8.881860e-04\n",
      "Epoch 314/20000: Train Loss = 0.448183, Test Loss = 0.271853, Learning Rate = 8.878485e-04\n",
      "Epoch 315/20000: Train Loss = 0.452708, Test Loss = 0.258003, Learning Rate = 8.875112e-04\n",
      "Epoch 316/20000: Train Loss = 0.453048, Test Loss = 0.235201, Learning Rate = 8.871739e-04\n",
      "Epoch 317/20000: Train Loss = 0.451038, Test Loss = 0.233924, Learning Rate = 8.868368e-04\n",
      "Epoch 318/20000: Train Loss = 0.445631, Test Loss = 0.224832, Learning Rate = 8.864999e-04\n",
      "Epoch 319/20000: Train Loss = 0.448967, Test Loss = 0.244180, Learning Rate = 8.861630e-04\n",
      "Epoch 320/20000: Train Loss = 0.448793, Test Loss = 0.203284, Learning Rate = 8.858263e-04\n",
      "Epoch 321/20000: Train Loss = 0.446702, Test Loss = 0.249794, Learning Rate = 8.854897e-04\n",
      "Epoch 322/20000: Train Loss = 0.446782, Test Loss = 0.228117, Learning Rate = 8.851533e-04\n",
      "Epoch 323/20000: Train Loss = 0.454408, Test Loss = 0.260895, Learning Rate = 8.848169e-04\n",
      "Epoch 324/20000: Train Loss = 0.449459, Test Loss = 0.275831, Learning Rate = 8.844807e-04\n",
      "Epoch 325/20000: Train Loss = 0.453292, Test Loss = 0.232332, Learning Rate = 8.841446e-04\n",
      "Epoch 326/20000: Train Loss = 0.448346, Test Loss = 0.242420, Learning Rate = 8.838087e-04\n",
      "Epoch 327/20000: Train Loss = 0.451857, Test Loss = 0.268040, Learning Rate = 8.834729e-04\n",
      "Epoch 328/20000: Train Loss = 0.447137, Test Loss = 0.338525, Learning Rate = 8.831372e-04\n",
      "Epoch 329/20000: Train Loss = 0.454379, Test Loss = 0.214376, Learning Rate = 8.828016e-04\n",
      "Epoch 330/20000: Train Loss = 0.453277, Test Loss = 0.224488, Learning Rate = 8.824662e-04\n",
      "Epoch 331/20000: Train Loss = 0.448167, Test Loss = 0.238714, Learning Rate = 8.821308e-04\n",
      "Epoch 332/20000: Train Loss = 0.458344, Test Loss = 0.332576, Learning Rate = 8.817957e-04\n",
      "Epoch 333/20000: Train Loss = 0.458069, Test Loss = 0.240904, Learning Rate = 8.814606e-04\n",
      "Epoch 334/20000: Train Loss = 0.461538, Test Loss = 0.195450, Learning Rate = 8.811257e-04\n",
      "Epoch 335/20000: Train Loss = 0.449979, Test Loss = 0.264446, Learning Rate = 8.807909e-04\n",
      "Epoch 336/20000: Train Loss = 0.457400, Test Loss = 0.279591, Learning Rate = 8.804562e-04\n",
      "Epoch 337/20000: Train Loss = 0.451997, Test Loss = 0.206181, Learning Rate = 8.801216e-04\n",
      "Epoch 338/20000: Train Loss = 0.453690, Test Loss = 0.254625, Learning Rate = 8.797872e-04\n",
      "Epoch 339/20000: Train Loss = 0.448312, Test Loss = 0.210864, Learning Rate = 8.794529e-04\n",
      "Epoch 340/20000: Train Loss = 0.446915, Test Loss = 0.216227, Learning Rate = 8.791187e-04\n",
      "Epoch 341/20000: Train Loss = 0.451281, Test Loss = 0.262657, Learning Rate = 8.787847e-04\n",
      "Epoch 342/20000: Train Loss = 0.450382, Test Loss = 0.244594, Learning Rate = 8.784508e-04\n",
      "Epoch 343/20000: Train Loss = 0.449486, Test Loss = 0.218551, Learning Rate = 8.781170e-04\n",
      "Epoch 344/20000: Train Loss = 0.447651, Test Loss = 0.217961, Learning Rate = 8.777833e-04\n",
      "Epoch 345/20000: Train Loss = 0.449539, Test Loss = 0.254809, Learning Rate = 8.774498e-04\n",
      "Epoch 346/20000: Train Loss = 0.450056, Test Loss = 0.338296, Learning Rate = 8.771164e-04\n",
      "Epoch 347/20000: Train Loss = 0.455333, Test Loss = 0.292708, Learning Rate = 8.767831e-04\n",
      "Epoch 348/20000: Train Loss = 0.452619, Test Loss = 0.235195, Learning Rate = 8.764500e-04\n",
      "Epoch 349/20000: Train Loss = 0.453938, Test Loss = 0.320393, Learning Rate = 8.761169e-04\n",
      "Epoch 350/20000: Train Loss = 0.450811, Test Loss = 0.224531, Learning Rate = 8.757840e-04\n",
      "Epoch 351/20000: Train Loss = 0.452066, Test Loss = 0.235006, Learning Rate = 8.754513e-04\n",
      "Epoch 352/20000: Train Loss = 0.450227, Test Loss = 0.241917, Learning Rate = 8.751186e-04\n",
      "Epoch 353/20000: Train Loss = 0.449053, Test Loss = 0.222931, Learning Rate = 8.747861e-04\n",
      "Epoch 354/20000: Train Loss = 0.451946, Test Loss = 0.271008, Learning Rate = 8.744537e-04\n",
      "Epoch 355/20000: Train Loss = 0.448986, Test Loss = 0.215499, Learning Rate = 8.741214e-04\n",
      "Epoch 356/20000: Train Loss = 0.451438, Test Loss = 0.224866, Learning Rate = 8.737893e-04\n",
      "Epoch 357/20000: Train Loss = 0.450496, Test Loss = 0.262393, Learning Rate = 8.734573e-04\n",
      "Epoch 358/20000: Train Loss = 0.446769, Test Loss = 0.272337, Learning Rate = 8.731254e-04\n",
      "Epoch 359/20000: Train Loss = 0.449616, Test Loss = 0.265051, Learning Rate = 8.727936e-04\n",
      "Epoch 360/20000: Train Loss = 0.446793, Test Loss = 0.243838, Learning Rate = 8.724620e-04\n",
      "Epoch 361/20000: Train Loss = 0.446474, Test Loss = 0.252399, Learning Rate = 8.721305e-04\n",
      "Epoch 362/20000: Train Loss = 0.449022, Test Loss = 0.225960, Learning Rate = 8.717991e-04\n",
      "Epoch 363/20000: Train Loss = 0.449186, Test Loss = 0.223613, Learning Rate = 8.714678e-04\n",
      "Epoch 364/20000: Train Loss = 0.448525, Test Loss = 0.210119, Learning Rate = 8.711367e-04\n",
      "Epoch 365/20000: Train Loss = 0.447201, Test Loss = 0.210698, Learning Rate = 8.708057e-04\n",
      "Epoch 366/20000: Train Loss = 0.450347, Test Loss = 0.277603, Learning Rate = 8.704748e-04\n",
      "Epoch 367/20000: Train Loss = 0.447167, Test Loss = 0.239297, Learning Rate = 8.701440e-04\n",
      "Epoch 368/20000: Train Loss = 0.446516, Test Loss = 0.228471, Learning Rate = 8.698134e-04\n",
      "Epoch 369/20000: Train Loss = 0.447776, Test Loss = 0.217342, Learning Rate = 8.694829e-04\n",
      "Epoch 370/20000: Train Loss = 0.445532, Test Loss = 0.246666, Learning Rate = 8.691525e-04\n",
      "Epoch 371/20000: Train Loss = 0.446797, Test Loss = 0.237304, Learning Rate = 8.688223e-04\n",
      "Epoch 372/20000: Train Loss = 0.452750, Test Loss = 0.314105, Learning Rate = 8.684921e-04\n",
      "Epoch 373/20000: Train Loss = 0.450105, Test Loss = 0.233121, Learning Rate = 8.681621e-04\n",
      "Epoch 374/20000: Train Loss = 0.448370, Test Loss = 0.255784, Learning Rate = 8.678323e-04\n",
      "Epoch 375/20000: Train Loss = 0.445514, Test Loss = 0.231372, Learning Rate = 8.675025e-04\n",
      "Epoch 376/20000: Train Loss = 0.447463, Test Loss = 0.218512, Learning Rate = 8.671729e-04\n",
      "Epoch 377/20000: Train Loss = 0.448744, Test Loss = 0.239376, Learning Rate = 8.668434e-04\n",
      "Epoch 378/20000: Train Loss = 0.451841, Test Loss = 0.280100, Learning Rate = 8.665140e-04\n",
      "Epoch 379/20000: Train Loss = 0.458363, Test Loss = 0.235382, Learning Rate = 8.661847e-04\n",
      "Epoch 380/20000: Train Loss = 0.451786, Test Loss = 0.237902, Learning Rate = 8.658556e-04\n",
      "Epoch 381/20000: Train Loss = 0.447865, Test Loss = 0.230679, Learning Rate = 8.655266e-04\n",
      "Epoch 382/20000: Train Loss = 0.448276, Test Loss = 0.229456, Learning Rate = 8.651977e-04\n",
      "Epoch 383/20000: Train Loss = 0.448477, Test Loss = 0.214184, Learning Rate = 8.648690e-04\n",
      "Epoch 384/20000: Train Loss = 0.446401, Test Loss = 0.234194, Learning Rate = 8.645404e-04\n",
      "Epoch 385/20000: Train Loss = 0.447073, Test Loss = 0.229749, Learning Rate = 8.642119e-04\n",
      "Epoch 386/20000: Train Loss = 0.446255, Test Loss = 0.272245, Learning Rate = 8.638835e-04\n",
      "Epoch 387/20000: Train Loss = 0.446905, Test Loss = 0.236162, Learning Rate = 8.635552e-04\n",
      "Epoch 388/20000: Train Loss = 0.447614, Test Loss = 0.197200, Learning Rate = 8.632271e-04\n",
      "Epoch 389/20000: Train Loss = 0.453237, Test Loss = 0.245226, Learning Rate = 8.628991e-04\n",
      "Epoch 390/20000: Train Loss = 0.448833, Test Loss = 0.240046, Learning Rate = 8.625712e-04\n",
      "Epoch 391/20000: Train Loss = 0.452263, Test Loss = 0.258670, Learning Rate = 8.622435e-04\n",
      "Epoch 392/20000: Train Loss = 0.447110, Test Loss = 0.237779, Learning Rate = 8.619158e-04\n",
      "Epoch 393/20000: Train Loss = 0.452339, Test Loss = 0.226544, Learning Rate = 8.615883e-04\n",
      "Epoch 394/20000: Train Loss = 0.448946, Test Loss = 0.214428, Learning Rate = 8.612610e-04\n",
      "Epoch 395/20000: Train Loss = 0.451298, Test Loss = 0.220875, Learning Rate = 8.609337e-04\n",
      "Epoch 396/20000: Train Loss = 0.446690, Test Loss = 0.180313, Learning Rate = 8.606066e-04\n",
      "Epoch 397/20000: Train Loss = 0.450065, Test Loss = 0.324902, Learning Rate = 8.602796e-04\n",
      "Epoch 398/20000: Train Loss = 0.453074, Test Loss = 0.222708, Learning Rate = 8.599527e-04\n",
      "Epoch 399/20000: Train Loss = 0.445986, Test Loss = 0.181417, Learning Rate = 8.596259e-04\n",
      "Epoch 400/20000: Train Loss = 0.446413, Test Loss = 0.252054, Learning Rate = 8.592993e-04\n",
      "Epoch 401/20000: Train Loss = 0.453100, Test Loss = 0.224886, Learning Rate = 8.589728e-04\n",
      "Epoch 402/20000: Train Loss = 0.447697, Test Loss = 0.207869, Learning Rate = 8.586464e-04\n",
      "Epoch 403/20000: Train Loss = 0.447335, Test Loss = 0.240825, Learning Rate = 8.583201e-04\n",
      "Epoch 404/20000: Train Loss = 0.451460, Test Loss = 0.307103, Learning Rate = 8.579940e-04\n",
      "Epoch 405/20000: Train Loss = 0.449183, Test Loss = 0.219056, Learning Rate = 8.576680e-04\n",
      "Epoch 406/20000: Train Loss = 0.445937, Test Loss = 0.217447, Learning Rate = 8.573421e-04\n",
      "Epoch 407/20000: Train Loss = 0.448768, Test Loss = 0.235149, Learning Rate = 8.570163e-04\n",
      "Epoch 408/20000: Train Loss = 0.446924, Test Loss = 0.271562, Learning Rate = 8.566907e-04\n",
      "Epoch 409/20000: Train Loss = 0.446428, Test Loss = 0.264321, Learning Rate = 8.563652e-04\n",
      "Epoch 410/20000: Train Loss = 0.453713, Test Loss = 0.236683, Learning Rate = 8.560398e-04\n",
      "Epoch 411/20000: Train Loss = 0.446591, Test Loss = 0.250266, Learning Rate = 8.557145e-04\n",
      "Epoch 412/20000: Train Loss = 0.447606, Test Loss = 0.290205, Learning Rate = 8.553893e-04\n",
      "Epoch 413/20000: Train Loss = 0.449957, Test Loss = 0.258455, Learning Rate = 8.550643e-04\n",
      "Epoch 414/20000: Train Loss = 0.446405, Test Loss = 0.242018, Learning Rate = 8.547394e-04\n",
      "Epoch 415/20000: Train Loss = 0.451403, Test Loss = 0.256371, Learning Rate = 8.544146e-04\n",
      "Epoch 416/20000: Train Loss = 0.448769, Test Loss = 0.231413, Learning Rate = 8.540900e-04\n",
      "Epoch 417/20000: Train Loss = 0.444133, Test Loss = 0.285081, Learning Rate = 8.537655e-04\n",
      "Epoch 418/20000: Train Loss = 0.449934, Test Loss = 0.231707, Learning Rate = 8.534410e-04\n",
      "Epoch 419/20000: Train Loss = 0.447171, Test Loss = 0.213732, Learning Rate = 8.531168e-04\n",
      "Epoch 420/20000: Train Loss = 0.447607, Test Loss = 0.227209, Learning Rate = 8.527926e-04\n",
      "Epoch 421/20000: Train Loss = 0.447739, Test Loss = 0.255429, Learning Rate = 8.524686e-04\n",
      "Epoch 422/20000: Train Loss = 0.454389, Test Loss = 0.275966, Learning Rate = 8.521446e-04\n",
      "Epoch 423/20000: Train Loss = 0.448628, Test Loss = 0.230007, Learning Rate = 8.518209e-04\n",
      "Epoch 424/20000: Train Loss = 0.453212, Test Loss = 0.231848, Learning Rate = 8.514972e-04\n",
      "Epoch 425/20000: Train Loss = 0.452311, Test Loss = 0.267670, Learning Rate = 8.511736e-04\n",
      "Epoch 426/20000: Train Loss = 0.449920, Test Loss = 0.225746, Learning Rate = 8.508502e-04\n",
      "Epoch 427/20000: Train Loss = 0.445343, Test Loss = 0.220722, Learning Rate = 8.505269e-04\n",
      "Epoch 428/20000: Train Loss = 0.446789, Test Loss = 0.237333, Learning Rate = 8.502037e-04\n",
      "Epoch 429/20000: Train Loss = 0.444297, Test Loss = 0.210754, Learning Rate = 8.498807e-04\n",
      "Epoch 430/20000: Train Loss = 0.446931, Test Loss = 0.284503, Learning Rate = 8.495578e-04\n",
      "Epoch 431/20000: Train Loss = 0.452985, Test Loss = 0.286526, Learning Rate = 8.492349e-04\n",
      "Epoch 432/20000: Train Loss = 0.447378, Test Loss = 0.237232, Learning Rate = 8.489123e-04\n",
      "Epoch 433/20000: Train Loss = 0.447667, Test Loss = 0.282951, Learning Rate = 8.485897e-04\n",
      "Epoch 434/20000: Train Loss = 0.448428, Test Loss = 0.233102, Learning Rate = 8.482673e-04\n",
      "Epoch 435/20000: Train Loss = 0.448274, Test Loss = 0.250334, Learning Rate = 8.479449e-04\n",
      "Epoch 436/20000: Train Loss = 0.446271, Test Loss = 0.251101, Learning Rate = 8.476227e-04\n",
      "Epoch 437/20000: Train Loss = 0.448239, Test Loss = 0.241944, Learning Rate = 8.473007e-04\n",
      "Epoch 438/20000: Train Loss = 0.445429, Test Loss = 0.264895, Learning Rate = 8.469787e-04\n",
      "Epoch 439/20000: Train Loss = 0.447819, Test Loss = 0.198416, Learning Rate = 8.466569e-04\n",
      "Epoch 440/20000: Train Loss = 0.445569, Test Loss = 0.271380, Learning Rate = 8.463352e-04\n",
      "Epoch 441/20000: Train Loss = 0.449860, Test Loss = 0.230374, Learning Rate = 8.460136e-04\n",
      "Epoch 442/20000: Train Loss = 0.448773, Test Loss = 0.233366, Learning Rate = 8.456921e-04\n",
      "Epoch 443/20000: Train Loss = 0.444936, Test Loss = 0.219537, Learning Rate = 8.453708e-04\n",
      "Epoch 444/20000: Train Loss = 0.446882, Test Loss = 0.231847, Learning Rate = 8.450496e-04\n",
      "Epoch 445/20000: Train Loss = 0.447405, Test Loss = 0.218802, Learning Rate = 8.447285e-04\n",
      "Epoch 446/20000: Train Loss = 0.447292, Test Loss = 0.234455, Learning Rate = 8.444075e-04\n",
      "Epoch 447/20000: Train Loss = 0.446981, Test Loss = 0.251896, Learning Rate = 8.440866e-04\n",
      "Epoch 448/20000: Train Loss = 0.448256, Test Loss = 0.209874, Learning Rate = 8.437659e-04\n",
      "Epoch 449/20000: Train Loss = 0.448003, Test Loss = 0.253063, Learning Rate = 8.434453e-04\n",
      "Epoch 450/20000: Train Loss = 0.447996, Test Loss = 0.260969, Learning Rate = 8.431248e-04\n",
      "Epoch 451/20000: Train Loss = 0.450785, Test Loss = 0.197412, Learning Rate = 8.428045e-04\n",
      "Epoch 452/20000: Train Loss = 0.451849, Test Loss = 0.259938, Learning Rate = 8.424842e-04\n",
      "Epoch 453/20000: Train Loss = 0.451959, Test Loss = 0.257900, Learning Rate = 8.421641e-04\n",
      "Epoch 454/20000: Train Loss = 0.449997, Test Loss = 0.248932, Learning Rate = 8.418441e-04\n",
      "Epoch 455/20000: Train Loss = 0.449735, Test Loss = 0.253409, Learning Rate = 8.415242e-04\n",
      "Epoch 456/20000: Train Loss = 0.448360, Test Loss = 0.232728, Learning Rate = 8.412045e-04\n",
      "Epoch 457/20000: Train Loss = 0.445192, Test Loss = 0.280793, Learning Rate = 8.408848e-04\n",
      "Epoch 458/20000: Train Loss = 0.454992, Test Loss = 0.283516, Learning Rate = 8.405653e-04\n",
      "Epoch 459/20000: Train Loss = 0.445674, Test Loss = 0.186712, Learning Rate = 8.402459e-04\n",
      "Epoch 460/20000: Train Loss = 0.453611, Test Loss = 0.182687, Learning Rate = 8.399267e-04\n",
      "Epoch 461/20000: Train Loss = 0.449670, Test Loss = 0.208041, Learning Rate = 8.396075e-04\n",
      "Epoch 462/20000: Train Loss = 0.451159, Test Loss = 0.238746, Learning Rate = 8.392885e-04\n",
      "Epoch 463/20000: Train Loss = 0.450062, Test Loss = 0.255899, Learning Rate = 8.389696e-04\n",
      "Epoch 464/20000: Train Loss = 0.447493, Test Loss = 0.268718, Learning Rate = 8.386508e-04\n",
      "Epoch 465/20000: Train Loss = 0.445475, Test Loss = 0.268816, Learning Rate = 8.383321e-04\n",
      "Epoch 466/20000: Train Loss = 0.447612, Test Loss = 0.255758, Learning Rate = 8.380136e-04\n",
      "Epoch 467/20000: Train Loss = 0.448146, Test Loss = 0.263016, Learning Rate = 8.376952e-04\n",
      "Epoch 468/20000: Train Loss = 0.448023, Test Loss = 0.243288, Learning Rate = 8.373768e-04\n",
      "Epoch 469/20000: Train Loss = 0.451646, Test Loss = 0.282877, Learning Rate = 8.370587e-04\n",
      "Epoch 470/20000: Train Loss = 0.446433, Test Loss = 0.226082, Learning Rate = 8.367406e-04\n",
      "Epoch 471/20000: Train Loss = 0.451819, Test Loss = 0.218705, Learning Rate = 8.364227e-04\n",
      "Epoch 472/20000: Train Loss = 0.448928, Test Loss = 0.263952, Learning Rate = 8.361049e-04\n",
      "Epoch 473/20000: Train Loss = 0.444872, Test Loss = 0.219075, Learning Rate = 8.357872e-04\n",
      "Epoch 474/20000: Train Loss = 0.445896, Test Loss = 0.287921, Learning Rate = 8.354696e-04\n",
      "Epoch 475/20000: Train Loss = 0.448879, Test Loss = 0.230522, Learning Rate = 8.351521e-04\n",
      "Epoch 476/20000: Train Loss = 0.449770, Test Loss = 0.226488, Learning Rate = 8.348348e-04\n",
      "Epoch 477/20000: Train Loss = 0.446284, Test Loss = 0.270011, Learning Rate = 8.345176e-04\n",
      "Epoch 478/20000: Train Loss = 0.451900, Test Loss = 0.236683, Learning Rate = 8.342005e-04\n",
      "Epoch 479/20000: Train Loss = 0.445422, Test Loss = 0.243075, Learning Rate = 8.338835e-04\n",
      "Epoch 480/20000: Train Loss = 0.444713, Test Loss = 0.232692, Learning Rate = 8.335667e-04\n",
      "Epoch 481/20000: Train Loss = 0.447589, Test Loss = 0.270769, Learning Rate = 8.332499e-04\n",
      "Epoch 482/20000: Train Loss = 0.452793, Test Loss = 0.249488, Learning Rate = 8.329333e-04\n",
      "Epoch 483/20000: Train Loss = 0.446871, Test Loss = 0.240389, Learning Rate = 8.326168e-04\n",
      "Epoch 484/20000: Train Loss = 0.450001, Test Loss = 0.212959, Learning Rate = 8.323004e-04\n",
      "Epoch 485/20000: Train Loss = 0.447868, Test Loss = 0.251364, Learning Rate = 8.319842e-04\n",
      "Epoch 486/20000: Train Loss = 0.444971, Test Loss = 0.282570, Learning Rate = 8.316681e-04\n",
      "Epoch 487/20000: Train Loss = 0.447769, Test Loss = 0.237953, Learning Rate = 8.313520e-04\n",
      "Epoch 488/20000: Train Loss = 0.445950, Test Loss = 0.248589, Learning Rate = 8.310362e-04\n",
      "Epoch 489/20000: Train Loss = 0.448228, Test Loss = 0.242687, Learning Rate = 8.307204e-04\n",
      "Epoch 490/20000: Train Loss = 0.445729, Test Loss = 0.249035, Learning Rate = 8.304047e-04\n",
      "Epoch 491/20000: Train Loss = 0.447408, Test Loss = 0.217158, Learning Rate = 8.300892e-04\n",
      "Epoch 492/20000: Train Loss = 0.446179, Test Loss = 0.211258, Learning Rate = 8.297738e-04\n",
      "Epoch 493/20000: Train Loss = 0.449972, Test Loss = 0.246398, Learning Rate = 8.294585e-04\n",
      "Epoch 494/20000: Train Loss = 0.445828, Test Loss = 0.235300, Learning Rate = 8.291433e-04\n",
      "Epoch 495/20000: Train Loss = 0.447126, Test Loss = 0.241034, Learning Rate = 8.288283e-04\n",
      "Epoch 496/20000: Train Loss = 0.446340, Test Loss = 0.244031, Learning Rate = 8.285133e-04\n",
      "Epoch 497/20000: Train Loss = 0.452972, Test Loss = 0.220486, Learning Rate = 8.281985e-04\n",
      "Epoch 498/20000: Train Loss = 0.447980, Test Loss = 0.239724, Learning Rate = 8.278838e-04\n",
      "Epoch 499/20000: Train Loss = 0.448386, Test Loss = 0.229273, Learning Rate = 8.275693e-04\n",
      "Epoch 500/20000: Train Loss = 0.445651, Test Loss = 0.243830, Learning Rate = 8.272548e-04\n",
      "Epoch 501/20000: Train Loss = 0.445664, Test Loss = 0.281561, Learning Rate = 8.269405e-04\n",
      "Epoch 502/20000: Train Loss = 0.449200, Test Loss = 0.243212, Learning Rate = 8.266263e-04\n",
      "Epoch 503/20000: Train Loss = 0.447696, Test Loss = 0.227891, Learning Rate = 8.263122e-04\n",
      "Epoch 504/20000: Train Loss = 0.445625, Test Loss = 0.228104, Learning Rate = 8.259982e-04\n",
      "Epoch 505/20000: Train Loss = 0.448262, Test Loss = 0.217521, Learning Rate = 8.256843e-04\n",
      "Epoch 506/20000: Train Loss = 0.451740, Test Loss = 0.232618, Learning Rate = 8.253706e-04\n",
      "Epoch 507/20000: Train Loss = 0.447084, Test Loss = 0.263225, Learning Rate = 8.250570e-04\n",
      "Epoch 508/20000: Train Loss = 0.452834, Test Loss = 0.241066, Learning Rate = 8.247435e-04\n",
      "Epoch 509/20000: Train Loss = 0.449348, Test Loss = 0.231975, Learning Rate = 8.244301e-04\n",
      "Epoch 510/20000: Train Loss = 0.444876, Test Loss = 0.292170, Learning Rate = 8.241168e-04\n",
      "Epoch 511/20000: Train Loss = 0.447373, Test Loss = 0.231843, Learning Rate = 8.238037e-04\n",
      "Epoch 512/20000: Train Loss = 0.447072, Test Loss = 0.273987, Learning Rate = 8.234907e-04\n",
      "Epoch 513/20000: Train Loss = 0.448578, Test Loss = 0.242264, Learning Rate = 8.231778e-04\n",
      "Epoch 514/20000: Train Loss = 0.451185, Test Loss = 0.257154, Learning Rate = 8.228650e-04\n",
      "Epoch 515/20000: Train Loss = 0.448575, Test Loss = 0.219572, Learning Rate = 8.225523e-04\n",
      "Epoch 516/20000: Train Loss = 0.448093, Test Loss = 0.222122, Learning Rate = 8.222398e-04\n",
      "Epoch 517/20000: Train Loss = 0.443070, Test Loss = 0.262778, Learning Rate = 8.219273e-04\n",
      "Epoch 518/20000: Train Loss = 0.447295, Test Loss = 0.237974, Learning Rate = 8.216150e-04\n",
      "Epoch 519/20000: Train Loss = 0.445372, Test Loss = 0.254022, Learning Rate = 8.213028e-04\n",
      "Epoch 520/20000: Train Loss = 0.450462, Test Loss = 0.256807, Learning Rate = 8.209908e-04\n",
      "Epoch 521/20000: Train Loss = 0.445396, Test Loss = 0.230792, Learning Rate = 8.206788e-04\n",
      "Epoch 522/20000: Train Loss = 0.448021, Test Loss = 0.229699, Learning Rate = 8.203670e-04\n",
      "Epoch 523/20000: Train Loss = 0.445557, Test Loss = 0.252336, Learning Rate = 8.200553e-04\n",
      "Epoch 524/20000: Train Loss = 0.448189, Test Loss = 0.298291, Learning Rate = 8.197437e-04\n",
      "Epoch 525/20000: Train Loss = 0.446302, Test Loss = 0.233349, Learning Rate = 8.194322e-04\n",
      "Epoch 526/20000: Train Loss = 0.446804, Test Loss = 0.256180, Learning Rate = 8.191208e-04\n",
      "Epoch 527/20000: Train Loss = 0.446951, Test Loss = 0.237117, Learning Rate = 8.188096e-04\n",
      "Epoch 528/20000: Train Loss = 0.450845, Test Loss = 0.232549, Learning Rate = 8.184984e-04\n",
      "Epoch 529/20000: Train Loss = 0.444061, Test Loss = 0.267140, Learning Rate = 8.181874e-04\n",
      "Epoch 530/20000: Train Loss = 0.448223, Test Loss = 0.250470, Learning Rate = 8.178766e-04\n",
      "Epoch 531/20000: Train Loss = 0.450609, Test Loss = 0.268923, Learning Rate = 8.175658e-04\n",
      "Epoch 532/20000: Train Loss = 0.451299, Test Loss = 0.300968, Learning Rate = 8.172551e-04\n",
      "Epoch 533/20000: Train Loss = 0.448549, Test Loss = 0.251063, Learning Rate = 8.169446e-04\n",
      "Epoch 534/20000: Train Loss = 0.447764, Test Loss = 0.230971, Learning Rate = 8.166342e-04\n",
      "Epoch 535/20000: Train Loss = 0.450146, Test Loss = 0.247517, Learning Rate = 8.163239e-04\n",
      "Epoch 536/20000: Train Loss = 0.448027, Test Loss = 0.241409, Learning Rate = 8.160137e-04\n",
      "Epoch 537/20000: Train Loss = 0.448112, Test Loss = 0.199800, Learning Rate = 8.157036e-04\n",
      "Epoch 538/20000: Train Loss = 0.445950, Test Loss = 0.234113, Learning Rate = 8.153937e-04\n",
      "Epoch 539/20000: Train Loss = 0.449687, Test Loss = 0.236567, Learning Rate = 8.150839e-04\n",
      "Epoch 540/20000: Train Loss = 0.448894, Test Loss = 0.224357, Learning Rate = 8.147741e-04\n",
      "Epoch 541/20000: Train Loss = 0.446708, Test Loss = 0.302588, Learning Rate = 8.144646e-04\n",
      "Epoch 542/20000: Train Loss = 0.449890, Test Loss = 0.231245, Learning Rate = 8.141551e-04\n",
      "Epoch 543/20000: Train Loss = 0.449984, Test Loss = 0.193404, Learning Rate = 8.138457e-04\n",
      "Epoch 544/20000: Train Loss = 0.449310, Test Loss = 0.213136, Learning Rate = 8.135365e-04\n",
      "Epoch 545/20000: Train Loss = 0.455845, Test Loss = 0.244183, Learning Rate = 8.132274e-04\n",
      "Epoch 546/20000: Train Loss = 0.449607, Test Loss = 0.229501, Learning Rate = 8.129184e-04\n",
      "Epoch 547/20000: Train Loss = 0.447110, Test Loss = 0.225113, Learning Rate = 8.126095e-04\n",
      "Epoch 548/20000: Train Loss = 0.449016, Test Loss = 0.240052, Learning Rate = 8.123007e-04\n",
      "Epoch 549/20000: Train Loss = 0.447629, Test Loss = 0.229584, Learning Rate = 8.119921e-04\n",
      "Epoch 550/20000: Train Loss = 0.449427, Test Loss = 0.257537, Learning Rate = 8.116835e-04\n",
      "Epoch 551/20000: Train Loss = 0.445893, Test Loss = 0.211528, Learning Rate = 8.113751e-04\n",
      "Epoch 552/20000: Train Loss = 0.450496, Test Loss = 0.251182, Learning Rate = 8.110668e-04\n",
      "Epoch 553/20000: Train Loss = 0.444777, Test Loss = 0.217167, Learning Rate = 8.107586e-04\n",
      "Epoch 554/20000: Train Loss = 0.446874, Test Loss = 0.253293, Learning Rate = 8.104505e-04\n",
      "Epoch 555/20000: Train Loss = 0.448001, Test Loss = 0.269182, Learning Rate = 8.101426e-04\n",
      "Epoch 556/20000: Train Loss = 0.446561, Test Loss = 0.252218, Learning Rate = 8.098348e-04\n",
      "Epoch 557/20000: Train Loss = 0.446942, Test Loss = 0.284758, Learning Rate = 8.095271e-04\n",
      "Epoch 558/20000: Train Loss = 0.446953, Test Loss = 0.240611, Learning Rate = 8.092195e-04\n",
      "Epoch 559/20000: Train Loss = 0.449005, Test Loss = 0.250757, Learning Rate = 8.089120e-04\n",
      "Epoch 560/20000: Train Loss = 0.444790, Test Loss = 0.218356, Learning Rate = 8.086046e-04\n",
      "Epoch 561/20000: Train Loss = 0.448117, Test Loss = 0.257021, Learning Rate = 8.082974e-04\n",
      "Epoch 562/20000: Train Loss = 0.450846, Test Loss = 0.235656, Learning Rate = 8.079902e-04\n",
      "Epoch 563/20000: Train Loss = 0.449658, Test Loss = 0.213320, Learning Rate = 8.076832e-04\n",
      "Epoch 564/20000: Train Loss = 0.449864, Test Loss = 0.213205, Learning Rate = 8.073763e-04\n",
      "Epoch 565/20000: Train Loss = 0.448172, Test Loss = 0.235184, Learning Rate = 8.070695e-04\n",
      "Epoch 566/20000: Train Loss = 0.456943, Test Loss = 0.234569, Learning Rate = 8.067629e-04\n",
      "Epoch 567/20000: Train Loss = 0.450679, Test Loss = 0.231861, Learning Rate = 8.064563e-04\n",
      "Epoch 568/20000: Train Loss = 0.448580, Test Loss = 0.234906, Learning Rate = 8.061499e-04\n",
      "Epoch 569/20000: Train Loss = 0.453314, Test Loss = 0.239753, Learning Rate = 8.058436e-04\n",
      "Epoch 570/20000: Train Loss = 0.452831, Test Loss = 0.286195, Learning Rate = 8.055374e-04\n",
      "Epoch 571/20000: Train Loss = 0.454142, Test Loss = 0.224884, Learning Rate = 8.052313e-04\n",
      "Epoch 572/20000: Train Loss = 0.453094, Test Loss = 0.193002, Learning Rate = 8.049253e-04\n",
      "Epoch 573/20000: Train Loss = 0.448031, Test Loss = 0.251399, Learning Rate = 8.046195e-04\n",
      "Epoch 574/20000: Train Loss = 0.448100, Test Loss = 0.217616, Learning Rate = 8.043137e-04\n",
      "Epoch 575/20000: Train Loss = 0.447398, Test Loss = 0.224301, Learning Rate = 8.040081e-04\n",
      "Epoch 576/20000: Train Loss = 0.445669, Test Loss = 0.265944, Learning Rate = 8.037026e-04\n",
      "Epoch 577/20000: Train Loss = 0.448991, Test Loss = 0.277764, Learning Rate = 8.033972e-04\n",
      "Epoch 578/20000: Train Loss = 0.451330, Test Loss = 0.255312, Learning Rate = 8.030920e-04\n",
      "Epoch 579/20000: Train Loss = 0.444945, Test Loss = 0.247176, Learning Rate = 8.027868e-04\n",
      "Epoch 580/20000: Train Loss = 0.447492, Test Loss = 0.224843, Learning Rate = 8.024818e-04\n",
      "Epoch 581/20000: Train Loss = 0.446911, Test Loss = 0.237503, Learning Rate = 8.021769e-04\n",
      "Epoch 582/20000: Train Loss = 0.447235, Test Loss = 0.250230, Learning Rate = 8.018721e-04\n",
      "Epoch 583/20000: Train Loss = 0.447157, Test Loss = 0.203788, Learning Rate = 8.015674e-04\n",
      "Epoch 584/20000: Train Loss = 0.449564, Test Loss = 0.246418, Learning Rate = 8.012628e-04\n",
      "Epoch 585/20000: Train Loss = 0.447135, Test Loss = 0.246402, Learning Rate = 8.009583e-04\n",
      "Epoch 586/20000: Train Loss = 0.447853, Test Loss = 0.249829, Learning Rate = 8.006540e-04\n",
      "Epoch 587/20000: Train Loss = 0.445296, Test Loss = 0.213030, Learning Rate = 8.003498e-04\n",
      "Epoch 588/20000: Train Loss = 0.446503, Test Loss = 0.201642, Learning Rate = 8.000457e-04\n",
      "Epoch 589/20000: Train Loss = 0.450287, Test Loss = 0.232011, Learning Rate = 7.997417e-04\n",
      "Epoch 590/20000: Train Loss = 0.445871, Test Loss = 0.240067, Learning Rate = 7.994378e-04\n",
      "Epoch 591/20000: Train Loss = 0.447702, Test Loss = 0.232210, Learning Rate = 7.991340e-04\n",
      "Epoch 592/20000: Train Loss = 0.447019, Test Loss = 0.250309, Learning Rate = 7.988304e-04\n",
      "Epoch 593/20000: Train Loss = 0.448878, Test Loss = 0.248707, Learning Rate = 7.985268e-04\n",
      "Epoch 594/20000: Train Loss = 0.448904, Test Loss = 0.283330, Learning Rate = 7.982234e-04\n",
      "Epoch 595/20000: Train Loss = 0.444233, Test Loss = 0.221880, Learning Rate = 7.979201e-04\n",
      "Epoch 596/20000: Train Loss = 0.446690, Test Loss = 0.265780, Learning Rate = 7.976169e-04\n",
      "Epoch 597/20000: Train Loss = 0.444928, Test Loss = 0.207560, Learning Rate = 7.973138e-04\n",
      "Epoch 598/20000: Train Loss = 0.445609, Test Loss = 0.227533, Learning Rate = 7.970109e-04\n",
      "Epoch 599/20000: Train Loss = 0.447574, Test Loss = 0.225395, Learning Rate = 7.967080e-04\n",
      "Epoch 600/20000: Train Loss = 0.447901, Test Loss = 0.221785, Learning Rate = 7.964053e-04\n",
      "Epoch 601/20000: Train Loss = 0.446097, Test Loss = 0.244690, Learning Rate = 7.961027e-04\n",
      "Epoch 602/20000: Train Loss = 0.445545, Test Loss = 0.262183, Learning Rate = 7.958002e-04\n",
      "Epoch 603/20000: Train Loss = 0.449642, Test Loss = 0.225563, Learning Rate = 7.954978e-04\n",
      "Epoch 604/20000: Train Loss = 0.450801, Test Loss = 0.270724, Learning Rate = 7.951956e-04\n",
      "Epoch 605/20000: Train Loss = 0.456414, Test Loss = 0.258450, Learning Rate = 7.948934e-04\n",
      "Epoch 606/20000: Train Loss = 0.448603, Test Loss = 0.234816, Learning Rate = 7.945914e-04\n",
      "Epoch 607/20000: Train Loss = 0.445578, Test Loss = 0.245368, Learning Rate = 7.942894e-04\n",
      "Epoch 608/20000: Train Loss = 0.445473, Test Loss = 0.251090, Learning Rate = 7.939876e-04\n",
      "Epoch 609/20000: Train Loss = 0.445345, Test Loss = 0.250345, Learning Rate = 7.936859e-04\n",
      "Epoch 610/20000: Train Loss = 0.447208, Test Loss = 0.251684, Learning Rate = 7.933844e-04\n",
      "Epoch 611/20000: Train Loss = 0.447905, Test Loss = 0.223774, Learning Rate = 7.930829e-04\n",
      "Epoch 612/20000: Train Loss = 0.450943, Test Loss = 0.206493, Learning Rate = 7.927815e-04\n",
      "Epoch 613/20000: Train Loss = 0.448305, Test Loss = 0.234689, Learning Rate = 7.924803e-04\n",
      "Epoch 614/20000: Train Loss = 0.447865, Test Loss = 0.287574, Learning Rate = 7.921792e-04\n",
      "Epoch 615/20000: Train Loss = 0.449526, Test Loss = 0.211752, Learning Rate = 7.918782e-04\n",
      "Epoch 616/20000: Train Loss = 0.446126, Test Loss = 0.211429, Learning Rate = 7.915773e-04\n",
      "Epoch 617/20000: Train Loss = 0.449448, Test Loss = 0.216093, Learning Rate = 7.912765e-04\n",
      "Epoch 618/20000: Train Loss = 0.449616, Test Loss = 0.234432, Learning Rate = 7.909759e-04\n",
      "Epoch 619/20000: Train Loss = 0.444399, Test Loss = 0.242037, Learning Rate = 7.906753e-04\n",
      "Epoch 620/20000: Train Loss = 0.446101, Test Loss = 0.223395, Learning Rate = 7.903749e-04\n",
      "Epoch 621/20000: Train Loss = 0.449817, Test Loss = 0.208197, Learning Rate = 7.900745e-04\n",
      "Epoch 622/20000: Train Loss = 0.446827, Test Loss = 0.266784, Learning Rate = 7.897743e-04\n",
      "Epoch 623/20000: Train Loss = 0.447847, Test Loss = 0.267389, Learning Rate = 7.894742e-04\n",
      "Epoch 624/20000: Train Loss = 0.449654, Test Loss = 0.269344, Learning Rate = 7.891743e-04\n",
      "Epoch 625/20000: Train Loss = 0.449278, Test Loss = 0.229768, Learning Rate = 7.888744e-04\n",
      "Epoch 626/20000: Train Loss = 0.450382, Test Loss = 0.238401, Learning Rate = 7.885747e-04\n",
      "Epoch 627/20000: Train Loss = 0.444919, Test Loss = 0.235285, Learning Rate = 7.882750e-04\n",
      "Epoch 628/20000: Train Loss = 0.448518, Test Loss = 0.264725, Learning Rate = 7.879755e-04\n",
      "Epoch 629/20000: Train Loss = 0.447221, Test Loss = 0.272335, Learning Rate = 7.876761e-04\n",
      "Epoch 630/20000: Train Loss = 0.446835, Test Loss = 0.244272, Learning Rate = 7.873768e-04\n",
      "Epoch 631/20000: Train Loss = 0.445262, Test Loss = 0.242315, Learning Rate = 7.870776e-04\n",
      "Epoch 632/20000: Train Loss = 0.445635, Test Loss = 0.235144, Learning Rate = 7.867785e-04\n",
      "Epoch 633/20000: Train Loss = 0.445561, Test Loss = 0.259850, Learning Rate = 7.864796e-04\n",
      "Epoch 634/20000: Train Loss = 0.446144, Test Loss = 0.226760, Learning Rate = 7.861807e-04\n",
      "Epoch 635/20000: Train Loss = 0.447097, Test Loss = 0.242593, Learning Rate = 7.858820e-04\n",
      "Epoch 636/20000: Train Loss = 0.445323, Test Loss = 0.238463, Learning Rate = 7.855834e-04\n",
      "Epoch 637/20000: Train Loss = 0.447003, Test Loss = 0.268769, Learning Rate = 7.852849e-04\n",
      "Epoch 638/20000: Train Loss = 0.448028, Test Loss = 0.231811, Learning Rate = 7.849865e-04\n",
      "Epoch 639/20000: Train Loss = 0.448689, Test Loss = 0.246160, Learning Rate = 7.846882e-04\n",
      "Epoch 640/20000: Train Loss = 0.452492, Test Loss = 0.230914, Learning Rate = 7.843901e-04\n",
      "Epoch 641/20000: Train Loss = 0.445723, Test Loss = 0.240828, Learning Rate = 7.840920e-04\n",
      "Epoch 642/20000: Train Loss = 0.444949, Test Loss = 0.245613, Learning Rate = 7.837941e-04\n",
      "Epoch 643/20000: Train Loss = 0.445413, Test Loss = 0.229175, Learning Rate = 7.834963e-04\n",
      "Epoch 644/20000: Train Loss = 0.445345, Test Loss = 0.254717, Learning Rate = 7.831986e-04\n",
      "Epoch 645/20000: Train Loss = 0.448379, Test Loss = 0.232046, Learning Rate = 7.829010e-04\n",
      "Epoch 646/20000: Train Loss = 0.446890, Test Loss = 0.262350, Learning Rate = 7.826035e-04\n",
      "Epoch 647/20000: Train Loss = 0.450692, Test Loss = 0.250380, Learning Rate = 7.823061e-04\n",
      "Epoch 648/20000: Train Loss = 0.451561, Test Loss = 0.242687, Learning Rate = 7.820089e-04\n",
      "Epoch 649/20000: Train Loss = 0.446706, Test Loss = 0.211682, Learning Rate = 7.817117e-04\n",
      "Epoch 650/20000: Train Loss = 0.450154, Test Loss = 0.221667, Learning Rate = 7.814147e-04\n",
      "Epoch 651/20000: Train Loss = 0.451146, Test Loss = 0.245485, Learning Rate = 7.811178e-04\n",
      "Epoch 652/20000: Train Loss = 0.449651, Test Loss = 0.222360, Learning Rate = 7.808210e-04\n",
      "Epoch 653/20000: Train Loss = 0.449583, Test Loss = 0.238393, Learning Rate = 7.805243e-04\n",
      "Epoch 654/20000: Train Loss = 0.451350, Test Loss = 0.250685, Learning Rate = 7.802277e-04\n",
      "Epoch 655/20000: Train Loss = 0.447965, Test Loss = 0.246568, Learning Rate = 7.799312e-04\n",
      "Epoch 656/20000: Train Loss = 0.448266, Test Loss = 0.208959, Learning Rate = 7.796349e-04\n",
      "Epoch 657/20000: Train Loss = 0.448274, Test Loss = 0.222849, Learning Rate = 7.793387e-04\n",
      "Epoch 658/20000: Train Loss = 0.447805, Test Loss = 0.240403, Learning Rate = 7.790425e-04\n",
      "Epoch 659/20000: Train Loss = 0.445765, Test Loss = 0.270435, Learning Rate = 7.787465e-04\n",
      "Epoch 660/20000: Train Loss = 0.450137, Test Loss = 0.242837, Learning Rate = 7.784506e-04\n",
      "Epoch 661/20000: Train Loss = 0.449612, Test Loss = 0.222767, Learning Rate = 7.781548e-04\n",
      "Epoch 662/20000: Train Loss = 0.445189, Test Loss = 0.253237, Learning Rate = 7.778591e-04\n",
      "Epoch 663/20000: Train Loss = 0.448860, Test Loss = 0.275359, Learning Rate = 7.775636e-04\n",
      "Epoch 664/20000: Train Loss = 0.447626, Test Loss = 0.233079, Learning Rate = 7.772681e-04\n",
      "Epoch 665/20000: Train Loss = 0.450100, Test Loss = 0.216597, Learning Rate = 7.769728e-04\n",
      "Epoch 666/20000: Train Loss = 0.449629, Test Loss = 0.230004, Learning Rate = 7.766776e-04\n",
      "Epoch 667/20000: Train Loss = 0.445160, Test Loss = 0.250955, Learning Rate = 7.763824e-04\n",
      "Epoch 668/20000: Train Loss = 0.447470, Test Loss = 0.253155, Learning Rate = 7.760874e-04\n",
      "Epoch 669/20000: Train Loss = 0.448385, Test Loss = 0.227741, Learning Rate = 7.757925e-04\n",
      "Epoch 670/20000: Train Loss = 0.446828, Test Loss = 0.240578, Learning Rate = 7.754978e-04\n",
      "Epoch 671/20000: Train Loss = 0.447018, Test Loss = 0.244828, Learning Rate = 7.752031e-04\n",
      "Epoch 672/20000: Train Loss = 0.446994, Test Loss = 0.247044, Learning Rate = 7.749085e-04\n",
      "Epoch 673/20000: Train Loss = 0.444381, Test Loss = 0.233836, Learning Rate = 7.746141e-04\n",
      "Epoch 674/20000: Train Loss = 0.448088, Test Loss = 0.231323, Learning Rate = 7.743198e-04\n",
      "Epoch 675/20000: Train Loss = 0.447928, Test Loss = 0.214057, Learning Rate = 7.740255e-04\n",
      "Epoch 676/20000: Train Loss = 0.447061, Test Loss = 0.226745, Learning Rate = 7.737314e-04\n",
      "Epoch 677/20000: Train Loss = 0.450959, Test Loss = 0.226646, Learning Rate = 7.734374e-04\n",
      "Epoch 678/20000: Train Loss = 0.447107, Test Loss = 0.248010, Learning Rate = 7.731435e-04\n",
      "Epoch 679/20000: Train Loss = 0.446226, Test Loss = 0.245497, Learning Rate = 7.728498e-04\n",
      "Epoch 680/20000: Train Loss = 0.446894, Test Loss = 0.233432, Learning Rate = 7.725561e-04\n",
      "Epoch 681/20000: Train Loss = 0.447057, Test Loss = 0.233689, Learning Rate = 7.722626e-04\n",
      "Epoch 682/20000: Train Loss = 0.450160, Test Loss = 0.278743, Learning Rate = 7.719691e-04\n",
      "Epoch 683/20000: Train Loss = 0.452673, Test Loss = 0.221403, Learning Rate = 7.716758e-04\n",
      "Epoch 684/20000: Train Loss = 0.448169, Test Loss = 0.241412, Learning Rate = 7.713826e-04\n",
      "Epoch 685/20000: Train Loss = 0.444507, Test Loss = 0.241750, Learning Rate = 7.710895e-04\n",
      "Epoch 686/20000: Train Loss = 0.445821, Test Loss = 0.274490, Learning Rate = 7.707965e-04\n",
      "Epoch 687/20000: Train Loss = 0.447198, Test Loss = 0.233779, Learning Rate = 7.705036e-04\n",
      "Epoch 688/20000: Train Loss = 0.444616, Test Loss = 0.271936, Learning Rate = 7.702108e-04\n",
      "Epoch 689/20000: Train Loss = 0.446186, Test Loss = 0.250756, Learning Rate = 7.699182e-04\n",
      "Epoch 690/20000: Train Loss = 0.446599, Test Loss = 0.259068, Learning Rate = 7.696256e-04\n",
      "Epoch 691/20000: Train Loss = 0.449581, Test Loss = 0.262122, Learning Rate = 7.693332e-04\n",
      "Epoch 692/20000: Train Loss = 0.444640, Test Loss = 0.243532, Learning Rate = 7.690409e-04\n",
      "Epoch 693/20000: Train Loss = 0.445866, Test Loss = 0.276797, Learning Rate = 7.687486e-04\n",
      "Epoch 694/20000: Train Loss = 0.451311, Test Loss = 0.214435, Learning Rate = 7.684565e-04\n",
      "Epoch 695/20000: Train Loss = 0.452464, Test Loss = 0.249449, Learning Rate = 7.681645e-04\n",
      "Epoch 696/20000: Train Loss = 0.446101, Test Loss = 0.214924, Learning Rate = 7.678727e-04\n",
      "Epoch 697/20000: Train Loss = 0.453202, Test Loss = 0.223411, Learning Rate = 7.675809e-04\n",
      "Epoch 698/20000: Train Loss = 0.448994, Test Loss = 0.250875, Learning Rate = 7.672892e-04\n",
      "Epoch 699/20000: Train Loss = 0.444932, Test Loss = 0.234957, Learning Rate = 7.669977e-04\n",
      "Epoch 700/20000: Train Loss = 0.447000, Test Loss = 0.208452, Learning Rate = 7.667062e-04\n",
      "Epoch 701/20000: Train Loss = 0.448220, Test Loss = 0.238724, Learning Rate = 7.664149e-04\n",
      "Epoch 702/20000: Train Loss = 0.445799, Test Loss = 0.261546, Learning Rate = 7.661237e-04\n",
      "Epoch 703/20000: Train Loss = 0.447310, Test Loss = 0.240097, Learning Rate = 7.658326e-04\n",
      "Epoch 704/20000: Train Loss = 0.446483, Test Loss = 0.233209, Learning Rate = 7.655416e-04\n",
      "Epoch 705/20000: Train Loss = 0.446237, Test Loss = 0.278099, Learning Rate = 7.652507e-04\n",
      "Epoch 706/20000: Train Loss = 0.449153, Test Loss = 0.238086, Learning Rate = 7.649599e-04\n",
      "Epoch 707/20000: Train Loss = 0.445981, Test Loss = 0.249164, Learning Rate = 7.646693e-04\n",
      "Epoch 708/20000: Train Loss = 0.445380, Test Loss = 0.233875, Learning Rate = 7.643787e-04\n",
      "Epoch 709/20000: Train Loss = 0.444697, Test Loss = 0.238011, Learning Rate = 7.640883e-04\n",
      "Epoch 710/20000: Train Loss = 0.447060, Test Loss = 0.223986, Learning Rate = 7.637979e-04\n",
      "Epoch 711/20000: Train Loss = 0.444830, Test Loss = 0.260159, Learning Rate = 7.635077e-04\n",
      "Epoch 712/20000: Train Loss = 0.450977, Test Loss = 0.229796, Learning Rate = 7.632176e-04\n",
      "Epoch 713/20000: Train Loss = 0.448054, Test Loss = 0.252658, Learning Rate = 7.629276e-04\n",
      "Epoch 714/20000: Train Loss = 0.447767, Test Loss = 0.228014, Learning Rate = 7.626377e-04\n",
      "Epoch 715/20000: Train Loss = 0.450884, Test Loss = 0.174295, Learning Rate = 7.623479e-04\n",
      "Epoch 716/20000: Train Loss = 0.453812, Test Loss = 0.237037, Learning Rate = 7.620583e-04\n",
      "Epoch 717/20000: Train Loss = 0.448053, Test Loss = 0.256091, Learning Rate = 7.617687e-04\n",
      "Epoch 718/20000: Train Loss = 0.445920, Test Loss = 0.200323, Learning Rate = 7.614793e-04\n",
      "Epoch 719/20000: Train Loss = 0.446053, Test Loss = 0.221155, Learning Rate = 7.611899e-04\n",
      "Epoch 720/20000: Train Loss = 0.451921, Test Loss = 0.153138, Learning Rate = 7.609007e-04\n",
      "Epoch 721/20000: Train Loss = 0.449735, Test Loss = 0.253948, Learning Rate = 7.606116e-04\n",
      "Epoch 722/20000: Train Loss = 0.448013, Test Loss = 0.211778, Learning Rate = 7.603225e-04\n",
      "Epoch 723/20000: Train Loss = 0.448580, Test Loss = 0.254138, Learning Rate = 7.600336e-04\n",
      "Epoch 724/20000: Train Loss = 0.447156, Test Loss = 0.264116, Learning Rate = 7.597449e-04\n",
      "Epoch 725/20000: Train Loss = 0.447205, Test Loss = 0.236576, Learning Rate = 7.594562e-04\n",
      "Epoch 726/20000: Train Loss = 0.446620, Test Loss = 0.241803, Learning Rate = 7.591676e-04\n",
      "Epoch 727/20000: Train Loss = 0.446686, Test Loss = 0.284243, Learning Rate = 7.588791e-04\n",
      "Epoch 728/20000: Train Loss = 0.445681, Test Loss = 0.224112, Learning Rate = 7.585908e-04\n",
      "Epoch 729/20000: Train Loss = 0.448772, Test Loss = 0.277604, Learning Rate = 7.583025e-04\n",
      "Epoch 730/20000: Train Loss = 0.449052, Test Loss = 0.267499, Learning Rate = 7.580144e-04\n",
      "Epoch 731/20000: Train Loss = 0.446899, Test Loss = 0.238566, Learning Rate = 7.577264e-04\n",
      "Epoch 732/20000: Train Loss = 0.448395, Test Loss = 0.226919, Learning Rate = 7.574385e-04\n",
      "Epoch 733/20000: Train Loss = 0.446158, Test Loss = 0.277500, Learning Rate = 7.571507e-04\n",
      "Epoch 734/20000: Train Loss = 0.447700, Test Loss = 0.244951, Learning Rate = 7.568630e-04\n",
      "Epoch 735/20000: Train Loss = 0.448410, Test Loss = 0.297977, Learning Rate = 7.565754e-04\n",
      "Epoch 736/20000: Train Loss = 0.447725, Test Loss = 0.262437, Learning Rate = 7.562879e-04\n",
      "Epoch 737/20000: Train Loss = 0.444162, Test Loss = 0.226380, Learning Rate = 7.560005e-04\n",
      "Epoch 738/20000: Train Loss = 0.446650, Test Loss = 0.234313, Learning Rate = 7.557133e-04\n",
      "Epoch 739/20000: Train Loss = 0.447212, Test Loss = 0.232924, Learning Rate = 7.554261e-04\n",
      "Epoch 740/20000: Train Loss = 0.444746, Test Loss = 0.282593, Learning Rate = 7.551391e-04\n",
      "Epoch 741/20000: Train Loss = 0.449750, Test Loss = 0.264633, Learning Rate = 7.548521e-04\n",
      "Epoch 742/20000: Train Loss = 0.445752, Test Loss = 0.198404, Learning Rate = 7.545653e-04\n",
      "Epoch 743/20000: Train Loss = 0.452500, Test Loss = 0.253286, Learning Rate = 7.542786e-04\n",
      "Epoch 744/20000: Train Loss = 0.447182, Test Loss = 0.234404, Learning Rate = 7.539920e-04\n",
      "Epoch 745/20000: Train Loss = 0.445849, Test Loss = 0.268449, Learning Rate = 7.537055e-04\n",
      "Epoch 746/20000: Train Loss = 0.443798, Test Loss = 0.223839, Learning Rate = 7.534191e-04\n",
      "Epoch 747/20000: Train Loss = 0.448161, Test Loss = 0.236407, Learning Rate = 7.531328e-04\n",
      "Epoch 748/20000: Train Loss = 0.450120, Test Loss = 0.234641, Learning Rate = 7.528467e-04\n",
      "Epoch 749/20000: Train Loss = 0.446216, Test Loss = 0.280271, Learning Rate = 7.525606e-04\n",
      "Epoch 750/20000: Train Loss = 0.448403, Test Loss = 0.258190, Learning Rate = 7.522746e-04\n",
      "Epoch 751/20000: Train Loss = 0.445360, Test Loss = 0.266391, Learning Rate = 7.519888e-04\n",
      "Epoch 752/20000: Train Loss = 0.445337, Test Loss = 0.237973, Learning Rate = 7.517031e-04\n",
      "Epoch 753/20000: Train Loss = 0.446832, Test Loss = 0.252443, Learning Rate = 7.514174e-04\n",
      "Epoch 754/20000: Train Loss = 0.447232, Test Loss = 0.261795, Learning Rate = 7.511319e-04\n",
      "Epoch 755/20000: Train Loss = 0.448382, Test Loss = 0.225961, Learning Rate = 7.508465e-04\n",
      "Epoch 756/20000: Train Loss = 0.445952, Test Loss = 0.243896, Learning Rate = 7.505612e-04\n",
      "Epoch 757/20000: Train Loss = 0.444976, Test Loss = 0.247024, Learning Rate = 7.502760e-04\n",
      "Epoch 758/20000: Train Loss = 0.446954, Test Loss = 0.263643, Learning Rate = 7.499909e-04\n",
      "Epoch 759/20000: Train Loss = 0.446591, Test Loss = 0.270053, Learning Rate = 7.497060e-04\n",
      "Epoch 760/20000: Train Loss = 0.453952, Test Loss = 0.242607, Learning Rate = 7.494211e-04\n",
      "Epoch 761/20000: Train Loss = 0.447817, Test Loss = 0.273144, Learning Rate = 7.491363e-04\n",
      "Epoch 762/20000: Train Loss = 0.448713, Test Loss = 0.237969, Learning Rate = 7.488517e-04\n",
      "Epoch 763/20000: Train Loss = 0.446914, Test Loss = 0.232860, Learning Rate = 7.485671e-04\n",
      "Epoch 764/20000: Train Loss = 0.446352, Test Loss = 0.254831, Learning Rate = 7.482827e-04\n",
      "Epoch 765/20000: Train Loss = 0.450869, Test Loss = 0.226020, Learning Rate = 7.479984e-04\n",
      "Epoch 766/20000: Train Loss = 0.450156, Test Loss = 0.258918, Learning Rate = 7.477142e-04\n",
      "Epoch 767/20000: Train Loss = 0.450249, Test Loss = 0.215848, Learning Rate = 7.474300e-04\n",
      "Epoch 768/20000: Train Loss = 0.449965, Test Loss = 0.256479, Learning Rate = 7.471460e-04\n",
      "Epoch 769/20000: Train Loss = 0.450553, Test Loss = 0.257786, Learning Rate = 7.468621e-04\n",
      "Epoch 770/20000: Train Loss = 0.446816, Test Loss = 0.261340, Learning Rate = 7.465784e-04\n",
      "Epoch 771/20000: Train Loss = 0.445421, Test Loss = 0.269179, Learning Rate = 7.462947e-04\n",
      "Epoch 772/20000: Train Loss = 0.448121, Test Loss = 0.225641, Learning Rate = 7.460111e-04\n",
      "Epoch 773/20000: Train Loss = 0.449697, Test Loss = 0.235262, Learning Rate = 7.457276e-04\n",
      "Epoch 774/20000: Train Loss = 0.447941, Test Loss = 0.222602, Learning Rate = 7.454443e-04\n",
      "Epoch 775/20000: Train Loss = 0.451881, Test Loss = 0.225637, Learning Rate = 7.451610e-04\n",
      "Epoch 776/20000: Train Loss = 0.449824, Test Loss = 0.246854, Learning Rate = 7.448779e-04\n",
      "Epoch 777/20000: Train Loss = 0.448170, Test Loss = 0.244300, Learning Rate = 7.445949e-04\n",
      "Epoch 778/20000: Train Loss = 0.449627, Test Loss = 0.225757, Learning Rate = 7.443119e-04\n",
      "Epoch 779/20000: Train Loss = 0.446280, Test Loss = 0.279871, Learning Rate = 7.440291e-04\n",
      "Epoch 780/20000: Train Loss = 0.449923, Test Loss = 0.195322, Learning Rate = 7.437464e-04\n",
      "Epoch 781/20000: Train Loss = 0.451775, Test Loss = 0.218198, Learning Rate = 7.434638e-04\n",
      "Epoch 782/20000: Train Loss = 0.447977, Test Loss = 0.285399, Learning Rate = 7.431813e-04\n",
      "Epoch 783/20000: Train Loss = 0.447294, Test Loss = 0.220462, Learning Rate = 7.428989e-04\n",
      "Epoch 784/20000: Train Loss = 0.452338, Test Loss = 0.294293, Learning Rate = 7.426166e-04\n",
      "Epoch 785/20000: Train Loss = 0.452612, Test Loss = 0.269813, Learning Rate = 7.423345e-04\n",
      "Epoch 786/20000: Train Loss = 0.446085, Test Loss = 0.223080, Learning Rate = 7.420524e-04\n",
      "Epoch 787/20000: Train Loss = 0.447293, Test Loss = 0.227636, Learning Rate = 7.417704e-04\n",
      "Epoch 788/20000: Train Loss = 0.447090, Test Loss = 0.252912, Learning Rate = 7.414886e-04\n",
      "Epoch 789/20000: Train Loss = 0.444537, Test Loss = 0.203915, Learning Rate = 7.412068e-04\n",
      "Epoch 790/20000: Train Loss = 0.448451, Test Loss = 0.228707, Learning Rate = 7.409252e-04\n",
      "Epoch 791/20000: Train Loss = 0.444545, Test Loss = 0.246364, Learning Rate = 7.406437e-04\n",
      "Epoch 792/20000: Train Loss = 0.444942, Test Loss = 0.237750, Learning Rate = 7.403622e-04\n",
      "Epoch 793/20000: Train Loss = 0.446208, Test Loss = 0.237440, Learning Rate = 7.400809e-04\n",
      "Epoch 794/20000: Train Loss = 0.446917, Test Loss = 0.275594, Learning Rate = 7.397997e-04\n",
      "Epoch 795/20000: Train Loss = 0.448811, Test Loss = 0.272533, Learning Rate = 7.395186e-04\n",
      "Epoch 796/20000: Train Loss = 0.447086, Test Loss = 0.280818, Learning Rate = 7.392376e-04\n",
      "Epoch 797/20000: Train Loss = 0.445951, Test Loss = 0.244552, Learning Rate = 7.389567e-04\n",
      "Epoch 798/20000: Train Loss = 0.447198, Test Loss = 0.227768, Learning Rate = 7.386759e-04\n",
      "Epoch 799/20000: Train Loss = 0.451580, Test Loss = 0.251719, Learning Rate = 7.383953e-04\n",
      "Epoch 800/20000: Train Loss = 0.446427, Test Loss = 0.278611, Learning Rate = 7.381147e-04\n",
      "Epoch 801/20000: Train Loss = 0.447285, Test Loss = 0.240917, Learning Rate = 7.378342e-04\n",
      "Epoch 802/20000: Train Loss = 0.448230, Test Loss = 0.256560, Learning Rate = 7.375539e-04\n",
      "Epoch 803/20000: Train Loss = 0.447884, Test Loss = 0.267122, Learning Rate = 7.372736e-04\n",
      "Epoch 804/20000: Train Loss = 0.445910, Test Loss = 0.234073, Learning Rate = 7.369935e-04\n",
      "Epoch 805/20000: Train Loss = 0.444954, Test Loss = 0.244382, Learning Rate = 7.367134e-04\n",
      "Epoch 806/20000: Train Loss = 0.447642, Test Loss = 0.290091, Learning Rate = 7.364335e-04\n",
      "Epoch 807/20000: Train Loss = 0.446267, Test Loss = 0.244899, Learning Rate = 7.361537e-04\n",
      "Epoch 808/20000: Train Loss = 0.446695, Test Loss = 0.218421, Learning Rate = 7.358740e-04\n",
      "Epoch 809/20000: Train Loss = 0.444990, Test Loss = 0.270076, Learning Rate = 7.355944e-04\n",
      "Epoch 810/20000: Train Loss = 0.447470, Test Loss = 0.219311, Learning Rate = 7.353148e-04\n",
      "Epoch 811/20000: Train Loss = 0.446287, Test Loss = 0.261251, Learning Rate = 7.350355e-04\n",
      "Epoch 812/20000: Train Loss = 0.451136, Test Loss = 0.224810, Learning Rate = 7.347562e-04\n",
      "Epoch 813/20000: Train Loss = 0.443975, Test Loss = 0.194967, Learning Rate = 7.344770e-04\n",
      "Epoch 814/20000: Train Loss = 0.450141, Test Loss = 0.222106, Learning Rate = 7.341979e-04\n",
      "Epoch 815/20000: Train Loss = 0.448254, Test Loss = 0.234372, Learning Rate = 7.339189e-04\n",
      "Epoch 816/20000: Train Loss = 0.444211, Test Loss = 0.256593, Learning Rate = 7.336400e-04\n",
      "Epoch 817/20000: Train Loss = 0.449031, Test Loss = 0.263112, Learning Rate = 7.333613e-04\n",
      "Epoch 818/20000: Train Loss = 0.445745, Test Loss = 0.242330, Learning Rate = 7.330826e-04\n",
      "Epoch 819/20000: Train Loss = 0.448896, Test Loss = 0.232891, Learning Rate = 7.328041e-04\n",
      "Epoch 820/20000: Train Loss = 0.447188, Test Loss = 0.236456, Learning Rate = 7.325256e-04\n",
      "Epoch 821/20000: Train Loss = 0.446425, Test Loss = 0.244869, Learning Rate = 7.322473e-04\n",
      "Epoch 822/20000: Train Loss = 0.446270, Test Loss = 0.253781, Learning Rate = 7.319691e-04\n",
      "Epoch 823/20000: Train Loss = 0.449095, Test Loss = 0.251713, Learning Rate = 7.316909e-04\n",
      "Epoch 824/20000: Train Loss = 0.448792, Test Loss = 0.238220, Learning Rate = 7.314129e-04\n",
      "Epoch 825/20000: Train Loss = 0.447434, Test Loss = 0.248098, Learning Rate = 7.311350e-04\n",
      "Epoch 826/20000: Train Loss = 0.446818, Test Loss = 0.173120, Learning Rate = 7.308572e-04\n",
      "Epoch 827/20000: Train Loss = 0.450913, Test Loss = 0.244029, Learning Rate = 7.305795e-04\n",
      "Epoch 828/20000: Train Loss = 0.445952, Test Loss = 0.264033, Learning Rate = 7.303019e-04\n",
      "Epoch 829/20000: Train Loss = 0.448886, Test Loss = 0.244387, Learning Rate = 7.300244e-04\n",
      "Epoch 830/20000: Train Loss = 0.448964, Test Loss = 0.248079, Learning Rate = 7.297470e-04\n",
      "Epoch 831/20000: Train Loss = 0.448839, Test Loss = 0.255147, Learning Rate = 7.294697e-04\n",
      "Epoch 832/20000: Train Loss = 0.449622, Test Loss = 0.238540, Learning Rate = 7.291925e-04\n",
      "Epoch 833/20000: Train Loss = 0.447066, Test Loss = 0.228024, Learning Rate = 7.289154e-04\n",
      "Epoch 834/20000: Train Loss = 0.452104, Test Loss = 0.216161, Learning Rate = 7.286385e-04\n",
      "Epoch 835/20000: Train Loss = 0.449845, Test Loss = 0.236617, Learning Rate = 7.283616e-04\n",
      "Epoch 836/20000: Train Loss = 0.445044, Test Loss = 0.239909, Learning Rate = 7.280849e-04\n",
      "Epoch 837/20000: Train Loss = 0.449436, Test Loss = 0.218141, Learning Rate = 7.278082e-04\n",
      "Epoch 838/20000: Train Loss = 0.447145, Test Loss = 0.240402, Learning Rate = 7.275317e-04\n",
      "Epoch 839/20000: Train Loss = 0.444089, Test Loss = 0.297400, Learning Rate = 7.272552e-04\n",
      "Epoch 840/20000: Train Loss = 0.451433, Test Loss = 0.250839, Learning Rate = 7.269789e-04\n",
      "Epoch 841/20000: Train Loss = 0.447579, Test Loss = 0.223920, Learning Rate = 7.267026e-04\n",
      "Epoch 842/20000: Train Loss = 0.445908, Test Loss = 0.248843, Learning Rate = 7.264265e-04\n",
      "Epoch 843/20000: Train Loss = 0.451705, Test Loss = 0.273822, Learning Rate = 7.261505e-04\n",
      "Epoch 844/20000: Train Loss = 0.448072, Test Loss = 0.260972, Learning Rate = 7.258746e-04\n",
      "Epoch 845/20000: Train Loss = 0.445590, Test Loss = 0.233567, Learning Rate = 7.255988e-04\n",
      "Epoch 846/20000: Train Loss = 0.446418, Test Loss = 0.253122, Learning Rate = 7.253231e-04\n",
      "Epoch 847/20000: Train Loss = 0.448599, Test Loss = 0.280031, Learning Rate = 7.250475e-04\n",
      "Epoch 848/20000: Train Loss = 0.446610, Test Loss = 0.205395, Learning Rate = 7.247720e-04\n",
      "Epoch 849/20000: Train Loss = 0.444636, Test Loss = 0.252701, Learning Rate = 7.244966e-04\n",
      "Epoch 850/20000: Train Loss = 0.445210, Test Loss = 0.247780, Learning Rate = 7.242213e-04\n",
      "Epoch 851/20000: Train Loss = 0.445942, Test Loss = 0.254071, Learning Rate = 7.239461e-04\n",
      "Epoch 852/20000: Train Loss = 0.446358, Test Loss = 0.249822, Learning Rate = 7.236710e-04\n",
      "Epoch 853/20000: Train Loss = 0.444869, Test Loss = 0.222342, Learning Rate = 7.233960e-04\n",
      "Epoch 854/20000: Train Loss = 0.446061, Test Loss = 0.224936, Learning Rate = 7.231212e-04\n",
      "Epoch 855/20000: Train Loss = 0.448632, Test Loss = 0.257956, Learning Rate = 7.228464e-04\n",
      "Epoch 856/20000: Train Loss = 0.447161, Test Loss = 0.217059, Learning Rate = 7.225717e-04\n",
      "Epoch 857/20000: Train Loss = 0.445153, Test Loss = 0.266938, Learning Rate = 7.222972e-04\n",
      "Epoch 858/20000: Train Loss = 0.445594, Test Loss = 0.250970, Learning Rate = 7.220227e-04\n",
      "Epoch 859/20000: Train Loss = 0.446215, Test Loss = 0.250398, Learning Rate = 7.217484e-04\n",
      "Epoch 860/20000: Train Loss = 0.446840, Test Loss = 0.225801, Learning Rate = 7.214741e-04\n",
      "Epoch 861/20000: Train Loss = 0.448202, Test Loss = 0.254259, Learning Rate = 7.212000e-04\n",
      "Epoch 862/20000: Train Loss = 0.447396, Test Loss = 0.243424, Learning Rate = 7.209260e-04\n",
      "Epoch 863/20000: Train Loss = 0.447847, Test Loss = 0.227017, Learning Rate = 7.206520e-04\n",
      "Epoch 864/20000: Train Loss = 0.446155, Test Loss = 0.248981, Learning Rate = 7.203782e-04\n",
      "Epoch 865/20000: Train Loss = 0.447339, Test Loss = 0.255566, Learning Rate = 7.201045e-04\n",
      "Epoch 866/20000: Train Loss = 0.446323, Test Loss = 0.243628, Learning Rate = 7.198308e-04\n",
      "Epoch 867/20000: Train Loss = 0.448063, Test Loss = 0.263500, Learning Rate = 7.195573e-04\n",
      "Epoch 868/20000: Train Loss = 0.446833, Test Loss = 0.242444, Learning Rate = 7.192839e-04\n",
      "Epoch 869/20000: Train Loss = 0.445271, Test Loss = 0.221019, Learning Rate = 7.190106e-04\n",
      "Epoch 870/20000: Train Loss = 0.448664, Test Loss = 0.233616, Learning Rate = 7.187374e-04\n",
      "Epoch 871/20000: Train Loss = 0.449772, Test Loss = 0.231656, Learning Rate = 7.184643e-04\n",
      "Epoch 872/20000: Train Loss = 0.456688, Test Loss = 0.259298, Learning Rate = 7.181913e-04\n",
      "Epoch 873/20000: Train Loss = 0.450345, Test Loss = 0.292751, Learning Rate = 7.179184e-04\n",
      "Epoch 874/20000: Train Loss = 0.446387, Test Loss = 0.175335, Learning Rate = 7.176456e-04\n",
      "Epoch 875/20000: Train Loss = 0.456579, Test Loss = 0.244694, Learning Rate = 7.173729e-04\n",
      "Epoch 876/20000: Train Loss = 0.447940, Test Loss = 0.214225, Learning Rate = 7.171004e-04\n",
      "Epoch 877/20000: Train Loss = 0.451471, Test Loss = 0.220553, Learning Rate = 7.168279e-04\n",
      "Epoch 878/20000: Train Loss = 0.447546, Test Loss = 0.269573, Learning Rate = 7.165555e-04\n",
      "Epoch 879/20000: Train Loss = 0.447308, Test Loss = 0.228967, Learning Rate = 7.162832e-04\n",
      "Epoch 880/20000: Train Loss = 0.451989, Test Loss = 0.253699, Learning Rate = 7.160111e-04\n",
      "Epoch 881/20000: Train Loss = 0.449375, Test Loss = 0.217366, Learning Rate = 7.157390e-04\n",
      "Epoch 882/20000: Train Loss = 0.446188, Test Loss = 0.236900, Learning Rate = 7.154670e-04\n",
      "Epoch 883/20000: Train Loss = 0.452011, Test Loss = 0.229803, Learning Rate = 7.151952e-04\n",
      "Epoch 884/20000: Train Loss = 0.445843, Test Loss = 0.244403, Learning Rate = 7.149234e-04\n",
      "Epoch 885/20000: Train Loss = 0.447585, Test Loss = 0.232164, Learning Rate = 7.146518e-04\n",
      "Epoch 886/20000: Train Loss = 0.447108, Test Loss = 0.241396, Learning Rate = 7.143802e-04\n",
      "Epoch 887/20000: Train Loss = 0.446609, Test Loss = 0.214442, Learning Rate = 7.141088e-04\n",
      "Epoch 888/20000: Train Loss = 0.449455, Test Loss = 0.217309, Learning Rate = 7.138374e-04\n",
      "Epoch 889/20000: Train Loss = 0.444477, Test Loss = 0.243230, Learning Rate = 7.135662e-04\n",
      "Epoch 890/20000: Train Loss = 0.445522, Test Loss = 0.236493, Learning Rate = 7.132951e-04\n",
      "Epoch 891/20000: Train Loss = 0.446415, Test Loss = 0.235998, Learning Rate = 7.130240e-04\n",
      "Epoch 892/20000: Train Loss = 0.445290, Test Loss = 0.250753, Learning Rate = 7.127531e-04\n",
      "Epoch 893/20000: Train Loss = 0.449328, Test Loss = 0.181643, Learning Rate = 7.124823e-04\n",
      "Epoch 894/20000: Train Loss = 0.451725, Test Loss = 0.205200, Learning Rate = 7.122115e-04\n",
      "Epoch 895/20000: Train Loss = 0.448706, Test Loss = 0.231196, Learning Rate = 7.119409e-04\n",
      "Epoch 896/20000: Train Loss = 0.446003, Test Loss = 0.242332, Learning Rate = 7.116704e-04\n",
      "Epoch 897/20000: Train Loss = 0.447291, Test Loss = 0.234069, Learning Rate = 7.114000e-04\n",
      "Epoch 898/20000: Train Loss = 0.444194, Test Loss = 0.307419, Learning Rate = 7.111297e-04\n",
      "Epoch 899/20000: Train Loss = 0.453935, Test Loss = 0.203265, Learning Rate = 7.108595e-04\n",
      "Epoch 900/20000: Train Loss = 0.447200, Test Loss = 0.247165, Learning Rate = 7.105894e-04\n",
      "Epoch 901/20000: Train Loss = 0.447441, Test Loss = 0.224518, Learning Rate = 7.103194e-04\n",
      "Epoch 902/20000: Train Loss = 0.448310, Test Loss = 0.247830, Learning Rate = 7.100495e-04\n",
      "Epoch 903/20000: Train Loss = 0.444732, Test Loss = 0.252941, Learning Rate = 7.097797e-04\n",
      "Epoch 904/20000: Train Loss = 0.445191, Test Loss = 0.242091, Learning Rate = 7.095100e-04\n",
      "Epoch 905/20000: Train Loss = 0.444883, Test Loss = 0.220274, Learning Rate = 7.092404e-04\n",
      "Epoch 906/20000: Train Loss = 0.444829, Test Loss = 0.240246, Learning Rate = 7.089709e-04\n",
      "Epoch 907/20000: Train Loss = 0.444929, Test Loss = 0.252076, Learning Rate = 7.087015e-04\n",
      "Epoch 908/20000: Train Loss = 0.445308, Test Loss = 0.235962, Learning Rate = 7.084322e-04\n",
      "Epoch 909/20000: Train Loss = 0.447853, Test Loss = 0.230150, Learning Rate = 7.081630e-04\n",
      "Epoch 910/20000: Train Loss = 0.447596, Test Loss = 0.218994, Learning Rate = 7.078939e-04\n",
      "Epoch 911/20000: Train Loss = 0.450034, Test Loss = 0.243288, Learning Rate = 7.076249e-04\n",
      "Epoch 912/20000: Train Loss = 0.449122, Test Loss = 0.226065, Learning Rate = 7.073561e-04\n",
      "Epoch 913/20000: Train Loss = 0.449027, Test Loss = 0.250397, Learning Rate = 7.070873e-04\n",
      "Epoch 914/20000: Train Loss = 0.446469, Test Loss = 0.268918, Learning Rate = 7.068186e-04\n",
      "Epoch 915/20000: Train Loss = 0.446745, Test Loss = 0.258564, Learning Rate = 7.065500e-04\n",
      "Epoch 916/20000: Train Loss = 0.448278, Test Loss = 0.235205, Learning Rate = 7.062816e-04\n",
      "Epoch 917/20000: Train Loss = 0.445091, Test Loss = 0.237051, Learning Rate = 7.060132e-04\n",
      "Epoch 918/20000: Train Loss = 0.444506, Test Loss = 0.233316, Learning Rate = 7.057449e-04\n",
      "Epoch 919/20000: Train Loss = 0.444583, Test Loss = 0.260001, Learning Rate = 7.054768e-04\n",
      "Epoch 920/20000: Train Loss = 0.446444, Test Loss = 0.249122, Learning Rate = 7.052087e-04\n",
      "Epoch 921/20000: Train Loss = 0.449494, Test Loss = 0.217080, Learning Rate = 7.049408e-04\n",
      "Epoch 922/20000: Train Loss = 0.444875, Test Loss = 0.260663, Learning Rate = 7.046729e-04\n",
      "Epoch 923/20000: Train Loss = 0.447478, Test Loss = 0.199570, Learning Rate = 7.044051e-04\n",
      "Epoch 924/20000: Train Loss = 0.445604, Test Loss = 0.260625, Learning Rate = 7.041375e-04\n",
      "Epoch 925/20000: Train Loss = 0.445889, Test Loss = 0.237453, Learning Rate = 7.038699e-04\n",
      "Epoch 926/20000: Train Loss = 0.450337, Test Loss = 0.199509, Learning Rate = 7.036025e-04\n",
      "Epoch 927/20000: Train Loss = 0.446893, Test Loss = 0.267571, Learning Rate = 7.033351e-04\n",
      "Epoch 928/20000: Train Loss = 0.447556, Test Loss = 0.251781, Learning Rate = 7.030679e-04\n",
      "Epoch 929/20000: Train Loss = 0.445680, Test Loss = 0.257056, Learning Rate = 7.028007e-04\n",
      "Epoch 930/20000: Train Loss = 0.446815, Test Loss = 0.235875, Learning Rate = 7.025337e-04\n",
      "Epoch 931/20000: Train Loss = 0.444991, Test Loss = 0.224400, Learning Rate = 7.022667e-04\n",
      "Epoch 932/20000: Train Loss = 0.446733, Test Loss = 0.240231, Learning Rate = 7.019999e-04\n",
      "Epoch 933/20000: Train Loss = 0.448098, Test Loss = 0.250290, Learning Rate = 7.017332e-04\n",
      "Epoch 934/20000: Train Loss = 0.446874, Test Loss = 0.294544, Learning Rate = 7.014665e-04\n",
      "Epoch 935/20000: Train Loss = 0.450569, Test Loss = 0.266944, Learning Rate = 7.012000e-04\n",
      "Epoch 936/20000: Train Loss = 0.452556, Test Loss = 0.263544, Learning Rate = 7.009335e-04\n",
      "Epoch 937/20000: Train Loss = 0.449269, Test Loss = 0.266853, Learning Rate = 7.006672e-04\n",
      "Epoch 938/20000: Train Loss = 0.448372, Test Loss = 0.220938, Learning Rate = 7.004010e-04\n",
      "Epoch 939/20000: Train Loss = 0.446803, Test Loss = 0.231263, Learning Rate = 7.001348e-04\n",
      "Epoch 940/20000: Train Loss = 0.449421, Test Loss = 0.269656, Learning Rate = 6.998688e-04\n",
      "Epoch 941/20000: Train Loss = 0.448148, Test Loss = 0.227288, Learning Rate = 6.996029e-04\n",
      "Epoch 942/20000: Train Loss = 0.445574, Test Loss = 0.222459, Learning Rate = 6.993371e-04\n",
      "Epoch 943/20000: Train Loss = 0.444972, Test Loss = 0.252477, Learning Rate = 6.990713e-04\n",
      "Epoch 944/20000: Train Loss = 0.447031, Test Loss = 0.222776, Learning Rate = 6.988057e-04\n",
      "Epoch 945/20000: Train Loss = 0.444150, Test Loss = 0.269570, Learning Rate = 6.985402e-04\n",
      "Epoch 946/20000: Train Loss = 0.447602, Test Loss = 0.205632, Learning Rate = 6.982747e-04\n",
      "Epoch 947/20000: Train Loss = 0.447438, Test Loss = 0.252596, Learning Rate = 6.980094e-04\n",
      "Epoch 948/20000: Train Loss = 0.449927, Test Loss = 0.254086, Learning Rate = 6.977442e-04\n",
      "Epoch 949/20000: Train Loss = 0.447645, Test Loss = 0.235529, Learning Rate = 6.974791e-04\n",
      "Epoch 950/20000: Train Loss = 0.445590, Test Loss = 0.265432, Learning Rate = 6.972140e-04\n",
      "Epoch 951/20000: Train Loss = 0.448527, Test Loss = 0.271396, Learning Rate = 6.969491e-04\n",
      "Epoch 952/20000: Train Loss = 0.444429, Test Loss = 0.226141, Learning Rate = 6.966843e-04\n",
      "Epoch 953/20000: Train Loss = 0.449560, Test Loss = 0.232896, Learning Rate = 6.964196e-04\n",
      "Epoch 954/20000: Train Loss = 0.447929, Test Loss = 0.248440, Learning Rate = 6.961550e-04\n",
      "Epoch 955/20000: Train Loss = 0.448790, Test Loss = 0.246101, Learning Rate = 6.958904e-04\n",
      "Epoch 956/20000: Train Loss = 0.444052, Test Loss = 0.238998, Learning Rate = 6.956260e-04\n",
      "Epoch 957/20000: Train Loss = 0.446647, Test Loss = 0.251638, Learning Rate = 6.953617e-04\n",
      "Epoch 958/20000: Train Loss = 0.450722, Test Loss = 0.245407, Learning Rate = 6.950975e-04\n",
      "Epoch 959/20000: Train Loss = 0.444684, Test Loss = 0.224453, Learning Rate = 6.948334e-04\n",
      "Epoch 960/20000: Train Loss = 0.443871, Test Loss = 0.284279, Learning Rate = 6.945693e-04\n",
      "Epoch 961/20000: Train Loss = 0.447960, Test Loss = 0.265552, Learning Rate = 6.943054e-04\n",
      "Epoch 962/20000: Train Loss = 0.446235, Test Loss = 0.277322, Learning Rate = 6.940416e-04\n",
      "Epoch 963/20000: Train Loss = 0.448625, Test Loss = 0.257007, Learning Rate = 6.937779e-04\n",
      "Epoch 964/20000: Train Loss = 0.447474, Test Loss = 0.267167, Learning Rate = 6.935143e-04\n",
      "Epoch 965/20000: Train Loss = 0.445274, Test Loss = 0.294837, Learning Rate = 6.932508e-04\n",
      "Epoch 966/20000: Train Loss = 0.446533, Test Loss = 0.248694, Learning Rate = 6.929873e-04\n",
      "Epoch 967/20000: Train Loss = 0.445345, Test Loss = 0.219194, Learning Rate = 6.927240e-04\n",
      "Epoch 968/20000: Train Loss = 0.447081, Test Loss = 0.222216, Learning Rate = 6.924608e-04\n",
      "Epoch 969/20000: Train Loss = 0.447541, Test Loss = 0.258144, Learning Rate = 6.921977e-04\n",
      "Epoch 970/20000: Train Loss = 0.449891, Test Loss = 0.211980, Learning Rate = 6.919347e-04\n",
      "Epoch 971/20000: Train Loss = 0.448548, Test Loss = 0.201149, Learning Rate = 6.916718e-04\n",
      "Epoch 972/20000: Train Loss = 0.445748, Test Loss = 0.267417, Learning Rate = 6.914089e-04\n",
      "Epoch 973/20000: Train Loss = 0.451667, Test Loss = 0.226512, Learning Rate = 6.911462e-04\n",
      "Epoch 974/20000: Train Loss = 0.449777, Test Loss = 0.247046, Learning Rate = 6.908836e-04\n",
      "Epoch 975/20000: Train Loss = 0.446113, Test Loss = 0.259015, Learning Rate = 6.906211e-04\n",
      "Epoch 976/20000: Train Loss = 0.445648, Test Loss = 0.241544, Learning Rate = 6.903587e-04\n",
      "Epoch 977/20000: Train Loss = 0.444944, Test Loss = 0.260030, Learning Rate = 6.900964e-04\n",
      "Epoch 978/20000: Train Loss = 0.446166, Test Loss = 0.251703, Learning Rate = 6.898341e-04\n",
      "Epoch 979/20000: Train Loss = 0.444580, Test Loss = 0.234729, Learning Rate = 6.895720e-04\n",
      "Epoch 980/20000: Train Loss = 0.448685, Test Loss = 0.278841, Learning Rate = 6.893100e-04\n",
      "Epoch 981/20000: Train Loss = 0.444944, Test Loss = 0.232593, Learning Rate = 6.890481e-04\n",
      "Epoch 982/20000: Train Loss = 0.447129, Test Loss = 0.241691, Learning Rate = 6.887863e-04\n",
      "Epoch 983/20000: Train Loss = 0.447647, Test Loss = 0.220334, Learning Rate = 6.885245e-04\n",
      "Epoch 984/20000: Train Loss = 0.445891, Test Loss = 0.239324, Learning Rate = 6.882629e-04\n",
      "Epoch 985/20000: Train Loss = 0.450109, Test Loss = 0.194550, Learning Rate = 6.880014e-04\n",
      "Epoch 986/20000: Train Loss = 0.446590, Test Loss = 0.267165, Learning Rate = 6.877400e-04\n",
      "Epoch 987/20000: Train Loss = 0.445048, Test Loss = 0.226196, Learning Rate = 6.874787e-04\n",
      "Epoch 988/20000: Train Loss = 0.447262, Test Loss = 0.274961, Learning Rate = 6.872174e-04\n",
      "Epoch 989/20000: Train Loss = 0.447150, Test Loss = 0.290910, Learning Rate = 6.869563e-04\n",
      "Epoch 990/20000: Train Loss = 0.446682, Test Loss = 0.235330, Learning Rate = 6.866953e-04\n",
      "Epoch 991/20000: Train Loss = 0.446028, Test Loss = 0.246979, Learning Rate = 6.864344e-04\n",
      "Epoch 992/20000: Train Loss = 0.448332, Test Loss = 0.257498, Learning Rate = 6.861735e-04\n",
      "Epoch 993/20000: Train Loss = 0.444215, Test Loss = 0.217390, Learning Rate = 6.859128e-04\n",
      "Epoch 994/20000: Train Loss = 0.446946, Test Loss = 0.242281, Learning Rate = 6.856522e-04\n",
      "Epoch 995/20000: Train Loss = 0.447207, Test Loss = 0.257293, Learning Rate = 6.853917e-04\n",
      "Epoch 996/20000: Train Loss = 0.445930, Test Loss = 0.253141, Learning Rate = 6.851312e-04\n",
      "Epoch 997/20000: Train Loss = 0.445062, Test Loss = 0.230673, Learning Rate = 6.848709e-04\n",
      "Epoch 998/20000: Train Loss = 0.448863, Test Loss = 0.266655, Learning Rate = 6.846107e-04\n",
      "Epoch 999/20000: Train Loss = 0.446187, Test Loss = 0.226754, Learning Rate = 6.843505e-04\n",
      "Epoch 1000/20000: Train Loss = 0.447337, Test Loss = 0.251373, Learning Rate = 6.840905e-04\n",
      "Epoch 1001/20000: Train Loss = 0.447813, Test Loss = 0.237766, Learning Rate = 6.838306e-04\n",
      "Epoch 1002/20000: Train Loss = 0.449815, Test Loss = 0.187149, Learning Rate = 6.835707e-04\n",
      "Epoch 1003/20000: Train Loss = 0.449040, Test Loss = 0.230621, Learning Rate = 6.833110e-04\n",
      "Epoch 1004/20000: Train Loss = 0.450114, Test Loss = 0.239083, Learning Rate = 6.830513e-04\n",
      "Epoch 1005/20000: Train Loss = 0.450094, Test Loss = 0.239814, Learning Rate = 6.827918e-04\n",
      "Epoch 1006/20000: Train Loss = 0.448391, Test Loss = 0.227559, Learning Rate = 6.825324e-04\n",
      "Epoch 1007/20000: Train Loss = 0.446482, Test Loss = 0.247344, Learning Rate = 6.822730e-04\n",
      "Epoch 1008/20000: Train Loss = 0.447178, Test Loss = 0.235473, Learning Rate = 6.820138e-04\n",
      "Epoch 1009/20000: Train Loss = 0.445975, Test Loss = 0.232405, Learning Rate = 6.817546e-04\n",
      "Epoch 1010/20000: Train Loss = 0.447438, Test Loss = 0.262127, Learning Rate = 6.814956e-04\n",
      "Epoch 1011/20000: Train Loss = 0.446419, Test Loss = 0.254571, Learning Rate = 6.812366e-04\n",
      "Epoch 1012/20000: Train Loss = 0.445491, Test Loss = 0.249929, Learning Rate = 6.809778e-04\n",
      "Epoch 1013/20000: Train Loss = 0.446597, Test Loss = 0.269051, Learning Rate = 6.807190e-04\n",
      "Epoch 1014/20000: Train Loss = 0.446158, Test Loss = 0.246115, Learning Rate = 6.804604e-04\n",
      "Epoch 1015/20000: Train Loss = 0.444233, Test Loss = 0.268028, Learning Rate = 6.802018e-04\n",
      "Epoch 1016/20000: Train Loss = 0.446266, Test Loss = 0.249084, Learning Rate = 6.799433e-04\n",
      "Epoch 1017/20000: Train Loss = 0.449226, Test Loss = 0.288435, Learning Rate = 6.796850e-04\n",
      "Epoch 1018/20000: Train Loss = 0.448090, Test Loss = 0.245516, Learning Rate = 6.794267e-04\n",
      "Epoch 1019/20000: Train Loss = 0.445285, Test Loss = 0.266262, Learning Rate = 6.791686e-04\n",
      "Epoch 1020/20000: Train Loss = 0.446198, Test Loss = 0.227374, Learning Rate = 6.789105e-04\n",
      "Epoch 1021/20000: Train Loss = 0.446304, Test Loss = 0.246679, Learning Rate = 6.786525e-04\n",
      "Epoch 1022/20000: Train Loss = 0.446149, Test Loss = 0.231487, Learning Rate = 6.783947e-04\n",
      "Epoch 1023/20000: Train Loss = 0.444472, Test Loss = 0.284947, Learning Rate = 6.781369e-04\n",
      "Epoch 1024/20000: Train Loss = 0.447619, Test Loss = 0.261912, Learning Rate = 6.778792e-04\n",
      "Epoch 1025/20000: Train Loss = 0.445161, Test Loss = 0.248449, Learning Rate = 6.776216e-04\n",
      "Epoch 1026/20000: Train Loss = 0.445127, Test Loss = 0.232324, Learning Rate = 6.773642e-04\n",
      "Epoch 1027/20000: Train Loss = 0.447379, Test Loss = 0.223651, Learning Rate = 6.771068e-04\n",
      "Epoch 1028/20000: Train Loss = 0.448930, Test Loss = 0.212367, Learning Rate = 6.768495e-04\n",
      "Epoch 1029/20000: Train Loss = 0.448312, Test Loss = 0.304212, Learning Rate = 6.765923e-04\n",
      "Epoch 1030/20000: Train Loss = 0.446162, Test Loss = 0.232171, Learning Rate = 6.763352e-04\n",
      "Epoch 1031/20000: Train Loss = 0.449667, Test Loss = 0.248487, Learning Rate = 6.760782e-04\n",
      "Epoch 1032/20000: Train Loss = 0.445869, Test Loss = 0.216710, Learning Rate = 6.758213e-04\n",
      "Epoch 1033/20000: Train Loss = 0.447586, Test Loss = 0.254403, Learning Rate = 6.755646e-04\n",
      "Epoch 1034/20000: Train Loss = 0.449626, Test Loss = 0.226980, Learning Rate = 6.753079e-04\n",
      "Epoch 1035/20000: Train Loss = 0.448102, Test Loss = 0.246190, Learning Rate = 6.750513e-04\n",
      "Epoch 1036/20000: Train Loss = 0.447604, Test Loss = 0.240793, Learning Rate = 6.747948e-04\n",
      "Epoch 1037/20000: Train Loss = 0.445245, Test Loss = 0.229742, Learning Rate = 6.745384e-04\n",
      "Epoch 1038/20000: Train Loss = 0.449501, Test Loss = 0.239920, Learning Rate = 6.742820e-04\n",
      "Epoch 1039/20000: Train Loss = 0.446572, Test Loss = 0.240508, Learning Rate = 6.740258e-04\n",
      "Epoch 1040/20000: Train Loss = 0.445980, Test Loss = 0.250610, Learning Rate = 6.737697e-04\n",
      "Epoch 1041/20000: Train Loss = 0.445200, Test Loss = 0.221730, Learning Rate = 6.735137e-04\n",
      "Epoch 1042/20000: Train Loss = 0.447191, Test Loss = 0.247070, Learning Rate = 6.732578e-04\n",
      "Epoch 1043/20000: Train Loss = 0.445624, Test Loss = 0.233944, Learning Rate = 6.730020e-04\n",
      "Epoch 1044/20000: Train Loss = 0.449105, Test Loss = 0.260528, Learning Rate = 6.727463e-04\n",
      "Epoch 1045/20000: Train Loss = 0.452038, Test Loss = 0.237757, Learning Rate = 6.724906e-04\n",
      "Epoch 1046/20000: Train Loss = 0.447525, Test Loss = 0.235840, Learning Rate = 6.722351e-04\n",
      "Epoch 1047/20000: Train Loss = 0.446955, Test Loss = 0.202138, Learning Rate = 6.719797e-04\n",
      "Epoch 1048/20000: Train Loss = 0.448190, Test Loss = 0.232252, Learning Rate = 6.717243e-04\n",
      "Epoch 1049/20000: Train Loss = 0.446789, Test Loss = 0.241707, Learning Rate = 6.714691e-04\n",
      "Epoch 1050/20000: Train Loss = 0.447538, Test Loss = 0.250291, Learning Rate = 6.712140e-04\n",
      "Epoch 1051/20000: Train Loss = 0.445750, Test Loss = 0.212360, Learning Rate = 6.709589e-04\n",
      "Epoch 1052/20000: Train Loss = 0.446041, Test Loss = 0.250665, Learning Rate = 6.707040e-04\n",
      "Epoch 1053/20000: Train Loss = 0.445901, Test Loss = 0.234995, Learning Rate = 6.704491e-04\n",
      "Epoch 1054/20000: Train Loss = 0.444524, Test Loss = 0.219697, Learning Rate = 6.701944e-04\n",
      "Epoch 1055/20000: Train Loss = 0.445227, Test Loss = 0.263084, Learning Rate = 6.699397e-04\n",
      "Epoch 1056/20000: Train Loss = 0.445370, Test Loss = 0.243390, Learning Rate = 6.696851e-04\n",
      "Epoch 1057/20000: Train Loss = 0.447297, Test Loss = 0.256021, Learning Rate = 6.694307e-04\n",
      "Epoch 1058/20000: Train Loss = 0.444369, Test Loss = 0.288758, Learning Rate = 6.691763e-04\n",
      "Epoch 1059/20000: Train Loss = 0.447019, Test Loss = 0.231174, Learning Rate = 6.689221e-04\n",
      "Epoch 1060/20000: Train Loss = 0.445975, Test Loss = 0.289785, Learning Rate = 6.686679e-04\n",
      "Epoch 1061/20000: Train Loss = 0.446742, Test Loss = 0.273043, Learning Rate = 6.684138e-04\n",
      "Epoch 1062/20000: Train Loss = 0.449693, Test Loss = 0.238760, Learning Rate = 6.681598e-04\n",
      "Epoch 1063/20000: Train Loss = 0.446981, Test Loss = 0.257417, Learning Rate = 6.679059e-04\n",
      "Epoch 1064/20000: Train Loss = 0.445113, Test Loss = 0.225032, Learning Rate = 6.676522e-04\n",
      "Epoch 1065/20000: Train Loss = 0.446884, Test Loss = 0.251418, Learning Rate = 6.673985e-04\n",
      "Epoch 1066/20000: Train Loss = 0.448748, Test Loss = 0.268299, Learning Rate = 6.671449e-04\n",
      "Epoch 1067/20000: Train Loss = 0.446038, Test Loss = 0.230808, Learning Rate = 6.668914e-04\n",
      "Epoch 1068/20000: Train Loss = 0.445637, Test Loss = 0.293565, Learning Rate = 6.666380e-04\n",
      "Epoch 1069/20000: Train Loss = 0.449188, Test Loss = 0.256383, Learning Rate = 6.663847e-04\n",
      "Epoch 1070/20000: Train Loss = 0.446251, Test Loss = 0.249934, Learning Rate = 6.661315e-04\n",
      "Epoch 1071/20000: Train Loss = 0.447025, Test Loss = 0.290149, Learning Rate = 6.658784e-04\n",
      "Epoch 1072/20000: Train Loss = 0.446979, Test Loss = 0.221564, Learning Rate = 6.656253e-04\n",
      "Epoch 1073/20000: Train Loss = 0.444727, Test Loss = 0.274922, Learning Rate = 6.653724e-04\n",
      "Epoch 1074/20000: Train Loss = 0.448862, Test Loss = 0.240564, Learning Rate = 6.651196e-04\n",
      "Epoch 1075/20000: Train Loss = 0.448404, Test Loss = 0.242271, Learning Rate = 6.648669e-04\n",
      "Epoch 1076/20000: Train Loss = 0.446271, Test Loss = 0.254315, Learning Rate = 6.646142e-04\n",
      "Epoch 1077/20000: Train Loss = 0.448734, Test Loss = 0.275177, Learning Rate = 6.643617e-04\n",
      "Epoch 1078/20000: Train Loss = 0.444300, Test Loss = 0.253238, Learning Rate = 6.641093e-04\n",
      "Epoch 1079/20000: Train Loss = 0.446924, Test Loss = 0.232241, Learning Rate = 6.638569e-04\n",
      "Epoch 1080/20000: Train Loss = 0.446999, Test Loss = 0.275854, Learning Rate = 6.636047e-04\n",
      "Epoch 1081/20000: Train Loss = 0.446585, Test Loss = 0.244867, Learning Rate = 6.633525e-04\n",
      "Epoch 1082/20000: Train Loss = 0.445206, Test Loss = 0.263041, Learning Rate = 6.631005e-04\n",
      "Epoch 1083/20000: Train Loss = 0.446170, Test Loss = 0.241001, Learning Rate = 6.628485e-04\n",
      "Epoch 1084/20000: Train Loss = 0.446291, Test Loss = 0.211991, Learning Rate = 6.625966e-04\n",
      "Epoch 1085/20000: Train Loss = 0.445780, Test Loss = 0.202193, Learning Rate = 6.623449e-04\n",
      "Epoch 1086/20000: Train Loss = 0.448920, Test Loss = 0.217258, Learning Rate = 6.620932e-04\n",
      "Epoch 1087/20000: Train Loss = 0.446573, Test Loss = 0.235764, Learning Rate = 6.618416e-04\n",
      "Epoch 1088/20000: Train Loss = 0.445618, Test Loss = 0.253215, Learning Rate = 6.615901e-04\n",
      "Epoch 1089/20000: Train Loss = 0.447393, Test Loss = 0.250363, Learning Rate = 6.613387e-04\n",
      "Epoch 1090/20000: Train Loss = 0.444145, Test Loss = 0.211435, Learning Rate = 6.610875e-04\n",
      "Epoch 1091/20000: Train Loss = 0.446421, Test Loss = 0.243230, Learning Rate = 6.608363e-04\n",
      "Epoch 1092/20000: Train Loss = 0.446023, Test Loss = 0.231274, Learning Rate = 6.605852e-04\n",
      "Epoch 1093/20000: Train Loss = 0.445894, Test Loss = 0.271344, Learning Rate = 6.603342e-04\n",
      "Epoch 1094/20000: Train Loss = 0.449293, Test Loss = 0.266626, Learning Rate = 6.600832e-04\n",
      "Epoch 1095/20000: Train Loss = 0.446748, Test Loss = 0.235507, Learning Rate = 6.598324e-04\n",
      "Epoch 1096/20000: Train Loss = 0.445354, Test Loss = 0.220589, Learning Rate = 6.595817e-04\n",
      "Epoch 1097/20000: Train Loss = 0.444318, Test Loss = 0.272223, Learning Rate = 6.593311e-04\n",
      "Epoch 1098/20000: Train Loss = 0.449380, Test Loss = 0.257223, Learning Rate = 6.590806e-04\n",
      "Epoch 1099/20000: Train Loss = 0.446007, Test Loss = 0.201139, Learning Rate = 6.588301e-04\n",
      "Epoch 1100/20000: Train Loss = 0.448136, Test Loss = 0.212106, Learning Rate = 6.585798e-04\n",
      "Epoch 1101/20000: Train Loss = 0.444375, Test Loss = 0.229858, Learning Rate = 6.583296e-04\n",
      "Epoch 1102/20000: Train Loss = 0.449365, Test Loss = 0.296630, Learning Rate = 6.580794e-04\n",
      "Epoch 1103/20000: Train Loss = 0.449654, Test Loss = 0.257523, Learning Rate = 6.578294e-04\n",
      "Epoch 1104/20000: Train Loss = 0.449864, Test Loss = 0.242979, Learning Rate = 6.575794e-04\n",
      "Epoch 1105/20000: Train Loss = 0.446372, Test Loss = 0.215024, Learning Rate = 6.573295e-04\n",
      "Epoch 1106/20000: Train Loss = 0.449044, Test Loss = 0.240743, Learning Rate = 6.570798e-04\n",
      "Epoch 1107/20000: Train Loss = 0.448729, Test Loss = 0.260065, Learning Rate = 6.568301e-04\n",
      "Epoch 1108/20000: Train Loss = 0.446646, Test Loss = 0.259522, Learning Rate = 6.565805e-04\n",
      "Epoch 1109/20000: Train Loss = 0.450670, Test Loss = 0.251518, Learning Rate = 6.563310e-04\n",
      "Epoch 1110/20000: Train Loss = 0.446188, Test Loss = 0.257949, Learning Rate = 6.560816e-04\n",
      "Epoch 1111/20000: Train Loss = 0.446128, Test Loss = 0.237200, Learning Rate = 6.558324e-04\n",
      "Epoch 1112/20000: Train Loss = 0.447467, Test Loss = 0.234017, Learning Rate = 6.555832e-04\n",
      "Epoch 1113/20000: Train Loss = 0.446835, Test Loss = 0.222169, Learning Rate = 6.553340e-04\n",
      "Epoch 1114/20000: Train Loss = 0.446546, Test Loss = 0.264697, Learning Rate = 6.550850e-04\n",
      "Epoch 1115/20000: Train Loss = 0.450340, Test Loss = 0.267304, Learning Rate = 6.548361e-04\n",
      "Epoch 1116/20000: Train Loss = 0.445634, Test Loss = 0.227596, Learning Rate = 6.545873e-04\n",
      "Epoch 1117/20000: Train Loss = 0.445461, Test Loss = 0.231008, Learning Rate = 6.543386e-04\n",
      "Epoch 1118/20000: Train Loss = 0.449977, Test Loss = 0.205963, Learning Rate = 6.540899e-04\n",
      "Epoch 1119/20000: Train Loss = 0.449137, Test Loss = 0.237252, Learning Rate = 6.538414e-04\n",
      "Epoch 1120/20000: Train Loss = 0.448184, Test Loss = 0.244042, Learning Rate = 6.535930e-04\n",
      "Epoch 1121/20000: Train Loss = 0.446424, Test Loss = 0.236783, Learning Rate = 6.533446e-04\n",
      "Epoch 1122/20000: Train Loss = 0.448042, Test Loss = 0.230732, Learning Rate = 6.530964e-04\n",
      "Epoch 1123/20000: Train Loss = 0.446536, Test Loss = 0.246910, Learning Rate = 6.528482e-04\n",
      "Epoch 1124/20000: Train Loss = 0.444561, Test Loss = 0.252174, Learning Rate = 6.526001e-04\n",
      "Epoch 1125/20000: Train Loss = 0.445424, Test Loss = 0.263454, Learning Rate = 6.523522e-04\n",
      "Epoch 1126/20000: Train Loss = 0.449871, Test Loss = 0.246856, Learning Rate = 6.521043e-04\n",
      "Epoch 1127/20000: Train Loss = 0.445696, Test Loss = 0.238067, Learning Rate = 6.518565e-04\n",
      "Epoch 1128/20000: Train Loss = 0.447088, Test Loss = 0.245312, Learning Rate = 6.516088e-04\n",
      "Epoch 1129/20000: Train Loss = 0.447659, Test Loss = 0.230151, Learning Rate = 6.513612e-04\n",
      "Epoch 1130/20000: Train Loss = 0.450409, Test Loss = 0.199815, Learning Rate = 6.511137e-04\n",
      "Epoch 1131/20000: Train Loss = 0.446712, Test Loss = 0.246718, Learning Rate = 6.508663e-04\n",
      "Epoch 1132/20000: Train Loss = 0.452321, Test Loss = 0.236380, Learning Rate = 6.506190e-04\n",
      "Epoch 1133/20000: Train Loss = 0.446900, Test Loss = 0.254112, Learning Rate = 6.503718e-04\n",
      "Epoch 1134/20000: Train Loss = 0.447584, Test Loss = 0.248615, Learning Rate = 6.501247e-04\n",
      "Epoch 1135/20000: Train Loss = 0.445696, Test Loss = 0.265598, Learning Rate = 6.498776e-04\n",
      "Epoch 1136/20000: Train Loss = 0.445380, Test Loss = 0.230122, Learning Rate = 6.496307e-04\n",
      "Epoch 1137/20000: Train Loss = 0.447083, Test Loss = 0.232833, Learning Rate = 6.493839e-04\n",
      "Epoch 1138/20000: Train Loss = 0.446363, Test Loss = 0.261038, Learning Rate = 6.491371e-04\n",
      "Epoch 1139/20000: Train Loss = 0.445464, Test Loss = 0.212802, Learning Rate = 6.488905e-04\n",
      "Epoch 1140/20000: Train Loss = 0.448026, Test Loss = 0.242667, Learning Rate = 6.486439e-04\n",
      "Epoch 1141/20000: Train Loss = 0.445166, Test Loss = 0.222453, Learning Rate = 6.483974e-04\n",
      "Epoch 1142/20000: Train Loss = 0.448323, Test Loss = 0.231063, Learning Rate = 6.481511e-04\n",
      "Epoch 1143/20000: Train Loss = 0.445292, Test Loss = 0.226968, Learning Rate = 6.479048e-04\n",
      "Epoch 1144/20000: Train Loss = 0.445174, Test Loss = 0.232292, Learning Rate = 6.476586e-04\n",
      "Epoch 1145/20000: Train Loss = 0.446592, Test Loss = 0.255005, Learning Rate = 6.474125e-04\n",
      "Epoch 1146/20000: Train Loss = 0.448134, Test Loss = 0.221515, Learning Rate = 6.471665e-04\n",
      "Epoch 1147/20000: Train Loss = 0.446065, Test Loss = 0.238087, Learning Rate = 6.469206e-04\n",
      "Epoch 1148/20000: Train Loss = 0.448721, Test Loss = 0.229614, Learning Rate = 6.466748e-04\n",
      "Epoch 1149/20000: Train Loss = 0.445974, Test Loss = 0.239985, Learning Rate = 6.464291e-04\n",
      "Epoch 1150/20000: Train Loss = 0.448331, Test Loss = 0.227471, Learning Rate = 6.461834e-04\n",
      "Epoch 1151/20000: Train Loss = 0.447847, Test Loss = 0.237163, Learning Rate = 6.459379e-04\n",
      "Epoch 1152/20000: Train Loss = 0.447081, Test Loss = 0.262073, Learning Rate = 6.456925e-04\n",
      "Epoch 1153/20000: Train Loss = 0.445399, Test Loss = 0.234862, Learning Rate = 6.454471e-04\n",
      "Epoch 1154/20000: Train Loss = 0.445902, Test Loss = 0.238921, Learning Rate = 6.452019e-04\n",
      "Epoch 1155/20000: Train Loss = 0.445452, Test Loss = 0.240842, Learning Rate = 6.449567e-04\n",
      "Epoch 1156/20000: Train Loss = 0.443802, Test Loss = 0.264863, Learning Rate = 6.447116e-04\n",
      "Epoch 1157/20000: Train Loss = 0.445962, Test Loss = 0.232210, Learning Rate = 6.444667e-04\n",
      "Epoch 1158/20000: Train Loss = 0.445185, Test Loss = 0.244834, Learning Rate = 6.442218e-04\n",
      "Epoch 1159/20000: Train Loss = 0.446926, Test Loss = 0.250629, Learning Rate = 6.439770e-04\n",
      "Epoch 1160/20000: Train Loss = 0.445210, Test Loss = 0.260628, Learning Rate = 6.437323e-04\n",
      "Epoch 1161/20000: Train Loss = 0.445422, Test Loss = 0.231012, Learning Rate = 6.434877e-04\n",
      "Epoch 1162/20000: Train Loss = 0.445446, Test Loss = 0.243934, Learning Rate = 6.432432e-04\n",
      "Epoch 1163/20000: Train Loss = 0.446961, Test Loss = 0.228857, Learning Rate = 6.429988e-04\n",
      "Epoch 1164/20000: Train Loss = 0.444711, Test Loss = 0.234122, Learning Rate = 6.427545e-04\n",
      "Epoch 1165/20000: Train Loss = 0.444810, Test Loss = 0.240087, Learning Rate = 6.425102e-04\n",
      "Epoch 1166/20000: Train Loss = 0.446762, Test Loss = 0.254310, Learning Rate = 6.422661e-04\n",
      "Epoch 1167/20000: Train Loss = 0.444747, Test Loss = 0.236956, Learning Rate = 6.420221e-04\n",
      "Epoch 1168/20000: Train Loss = 0.445842, Test Loss = 0.243725, Learning Rate = 6.417781e-04\n",
      "Epoch 1169/20000: Train Loss = 0.447062, Test Loss = 0.266903, Learning Rate = 6.415343e-04\n",
      "Epoch 1170/20000: Train Loss = 0.448036, Test Loss = 0.296609, Learning Rate = 6.412905e-04\n",
      "Epoch 1171/20000: Train Loss = 0.449531, Test Loss = 0.216547, Learning Rate = 6.410468e-04\n",
      "Epoch 1172/20000: Train Loss = 0.446169, Test Loss = 0.226488, Learning Rate = 6.408032e-04\n",
      "Epoch 1173/20000: Train Loss = 0.449611, Test Loss = 0.220558, Learning Rate = 6.405597e-04\n",
      "Epoch 1174/20000: Train Loss = 0.447030, Test Loss = 0.263913, Learning Rate = 6.403163e-04\n",
      "Epoch 1175/20000: Train Loss = 0.450461, Test Loss = 0.280270, Learning Rate = 6.400730e-04\n",
      "Epoch 1176/20000: Train Loss = 0.447065, Test Loss = 0.243057, Learning Rate = 6.398298e-04\n",
      "Epoch 1177/20000: Train Loss = 0.446621, Test Loss = 0.203677, Learning Rate = 6.395867e-04\n",
      "Epoch 1178/20000: Train Loss = 0.446467, Test Loss = 0.247006, Learning Rate = 6.393437e-04\n",
      "Epoch 1179/20000: Train Loss = 0.445900, Test Loss = 0.262343, Learning Rate = 6.391008e-04\n",
      "Epoch 1180/20000: Train Loss = 0.446553, Test Loss = 0.219220, Learning Rate = 6.388579e-04\n",
      "Epoch 1181/20000: Train Loss = 0.446266, Test Loss = 0.242978, Learning Rate = 6.386152e-04\n",
      "Epoch 1182/20000: Train Loss = 0.447067, Test Loss = 0.251359, Learning Rate = 6.383725e-04\n",
      "Epoch 1183/20000: Train Loss = 0.444546, Test Loss = 0.242265, Learning Rate = 6.381299e-04\n",
      "Epoch 1184/20000: Train Loss = 0.446926, Test Loss = 0.235500, Learning Rate = 6.378875e-04\n",
      "Epoch 1185/20000: Train Loss = 0.446712, Test Loss = 0.226797, Learning Rate = 6.376451e-04\n",
      "Epoch 1186/20000: Train Loss = 0.445373, Test Loss = 0.252027, Learning Rate = 6.374028e-04\n",
      "Epoch 1187/20000: Train Loss = 0.445474, Test Loss = 0.232710, Learning Rate = 6.371606e-04\n",
      "Epoch 1188/20000: Train Loss = 0.447380, Test Loss = 0.248949, Learning Rate = 6.369185e-04\n",
      "Epoch 1189/20000: Train Loss = 0.450838, Test Loss = 0.251314, Learning Rate = 6.366765e-04\n",
      "Epoch 1190/20000: Train Loss = 0.448837, Test Loss = 0.253419, Learning Rate = 6.364346e-04\n",
      "Epoch 1191/20000: Train Loss = 0.448433, Test Loss = 0.259860, Learning Rate = 6.361927e-04\n",
      "Epoch 1192/20000: Train Loss = 0.446631, Test Loss = 0.261730, Learning Rate = 6.359510e-04\n",
      "Epoch 1193/20000: Train Loss = 0.445890, Test Loss = 0.245674, Learning Rate = 6.357094e-04\n",
      "Epoch 1194/20000: Train Loss = 0.445490, Test Loss = 0.229525, Learning Rate = 6.354678e-04\n",
      "Epoch 1195/20000: Train Loss = 0.445630, Test Loss = 0.254734, Learning Rate = 6.352264e-04\n",
      "Epoch 1196/20000: Train Loss = 0.445748, Test Loss = 0.225695, Learning Rate = 6.349850e-04\n",
      "Epoch 1197/20000: Train Loss = 0.447930, Test Loss = 0.273296, Learning Rate = 6.347437e-04\n",
      "Epoch 1198/20000: Train Loss = 0.448141, Test Loss = 0.288545, Learning Rate = 6.345025e-04\n",
      "Epoch 1199/20000: Train Loss = 0.451552, Test Loss = 0.267364, Learning Rate = 6.342614e-04\n",
      "Epoch 1200/20000: Train Loss = 0.445375, Test Loss = 0.219774, Learning Rate = 6.340204e-04\n",
      "Epoch 1201/20000: Train Loss = 0.446869, Test Loss = 0.276257, Learning Rate = 6.337795e-04\n",
      "Epoch 1202/20000: Train Loss = 0.446396, Test Loss = 0.238905, Learning Rate = 6.335387e-04\n",
      "Epoch 1203/20000: Train Loss = 0.446019, Test Loss = 0.229545, Learning Rate = 6.332980e-04\n",
      "Epoch 1204/20000: Train Loss = 0.445934, Test Loss = 0.250922, Learning Rate = 6.330573e-04\n",
      "Epoch 1205/20000: Train Loss = 0.444789, Test Loss = 0.221904, Learning Rate = 6.328168e-04\n",
      "Epoch 1206/20000: Train Loss = 0.445437, Test Loss = 0.243968, Learning Rate = 6.325763e-04\n",
      "Epoch 1207/20000: Train Loss = 0.445810, Test Loss = 0.217773, Learning Rate = 6.323360e-04\n",
      "Epoch 1208/20000: Train Loss = 0.447946, Test Loss = 0.270429, Learning Rate = 6.320957e-04\n",
      "Epoch 1209/20000: Train Loss = 0.445482, Test Loss = 0.235490, Learning Rate = 6.318555e-04\n",
      "Epoch 1210/20000: Train Loss = 0.444878, Test Loss = 0.236475, Learning Rate = 6.316154e-04\n",
      "Epoch 1211/20000: Train Loss = 0.445109, Test Loss = 0.258289, Learning Rate = 6.313754e-04\n",
      "Epoch 1212/20000: Train Loss = 0.445780, Test Loss = 0.235921, Learning Rate = 6.311355e-04\n",
      "Epoch 1213/20000: Train Loss = 0.444509, Test Loss = 0.229279, Learning Rate = 6.308957e-04\n",
      "Epoch 1214/20000: Train Loss = 0.445641, Test Loss = 0.242597, Learning Rate = 6.306560e-04\n",
      "Epoch 1215/20000: Train Loss = 0.446674, Test Loss = 0.261583, Learning Rate = 6.304164e-04\n",
      "Epoch 1216/20000: Train Loss = 0.444830, Test Loss = 0.232170, Learning Rate = 6.301768e-04\n",
      "Epoch 1217/20000: Train Loss = 0.446742, Test Loss = 0.241556, Learning Rate = 6.299374e-04\n",
      "Epoch 1218/20000: Train Loss = 0.445657, Test Loss = 0.246102, Learning Rate = 6.296980e-04\n",
      "Epoch 1219/20000: Train Loss = 0.444845, Test Loss = 0.224199, Learning Rate = 6.294587e-04\n",
      "Epoch 1220/20000: Train Loss = 0.447733, Test Loss = 0.255627, Learning Rate = 6.292196e-04\n",
      "Epoch 1221/20000: Train Loss = 0.448269, Test Loss = 0.230762, Learning Rate = 6.289805e-04\n",
      "Epoch 1222/20000: Train Loss = 0.449382, Test Loss = 0.242882, Learning Rate = 6.287415e-04\n",
      "Epoch 1223/20000: Train Loss = 0.446731, Test Loss = 0.266647, Learning Rate = 6.285026e-04\n",
      "Epoch 1224/20000: Train Loss = 0.445011, Test Loss = 0.268856, Learning Rate = 6.282638e-04\n",
      "Epoch 1225/20000: Train Loss = 0.445498, Test Loss = 0.248820, Learning Rate = 6.280250e-04\n",
      "Epoch 1226/20000: Train Loss = 0.446930, Test Loss = 0.227881, Learning Rate = 6.277864e-04\n",
      "Epoch 1227/20000: Train Loss = 0.446492, Test Loss = 0.258536, Learning Rate = 6.275479e-04\n",
      "Epoch 1228/20000: Train Loss = 0.444264, Test Loss = 0.224317, Learning Rate = 6.273094e-04\n",
      "Epoch 1229/20000: Train Loss = 0.450374, Test Loss = 0.258481, Learning Rate = 6.270711e-04\n",
      "Epoch 1230/20000: Train Loss = 0.445931, Test Loss = 0.246945, Learning Rate = 6.268328e-04\n",
      "Epoch 1231/20000: Train Loss = 0.444522, Test Loss = 0.242029, Learning Rate = 6.265946e-04\n",
      "Epoch 1232/20000: Train Loss = 0.447915, Test Loss = 0.272889, Learning Rate = 6.263565e-04\n",
      "Epoch 1233/20000: Train Loss = 0.446831, Test Loss = 0.263887, Learning Rate = 6.261185e-04\n",
      "Epoch 1234/20000: Train Loss = 0.445700, Test Loss = 0.252279, Learning Rate = 6.258806e-04\n",
      "Epoch 1235/20000: Train Loss = 0.447677, Test Loss = 0.243983, Learning Rate = 6.256428e-04\n",
      "Epoch 1236/20000: Train Loss = 0.446660, Test Loss = 0.250773, Learning Rate = 6.254051e-04\n",
      "Epoch 1237/20000: Train Loss = 0.448493, Test Loss = 0.229580, Learning Rate = 6.251674e-04\n",
      "Epoch 1238/20000: Train Loss = 0.444444, Test Loss = 0.230686, Learning Rate = 6.249299e-04\n",
      "Epoch 1239/20000: Train Loss = 0.445504, Test Loss = 0.261355, Learning Rate = 6.246924e-04\n",
      "Epoch 1240/20000: Train Loss = 0.446379, Test Loss = 0.243207, Learning Rate = 6.244551e-04\n",
      "Epoch 1241/20000: Train Loss = 0.445739, Test Loss = 0.227521, Learning Rate = 6.242178e-04\n",
      "Epoch 1242/20000: Train Loss = 0.447621, Test Loss = 0.219355, Learning Rate = 6.239806e-04\n",
      "Epoch 1243/20000: Train Loss = 0.448886, Test Loss = 0.250723, Learning Rate = 6.237435e-04\n",
      "Epoch 1244/20000: Train Loss = 0.445099, Test Loss = 0.226889, Learning Rate = 6.235065e-04\n",
      "Epoch 1245/20000: Train Loss = 0.445033, Test Loss = 0.267986, Learning Rate = 6.232696e-04\n",
      "Epoch 1246/20000: Train Loss = 0.445268, Test Loss = 0.244807, Learning Rate = 6.230328e-04\n",
      "Epoch 1247/20000: Train Loss = 0.448998, Test Loss = 0.232705, Learning Rate = 6.227960e-04\n",
      "Epoch 1248/20000: Train Loss = 0.446329, Test Loss = 0.241474, Learning Rate = 6.225594e-04\n",
      "Epoch 1249/20000: Train Loss = 0.445874, Test Loss = 0.292754, Learning Rate = 6.223228e-04\n",
      "Epoch 1250/20000: Train Loss = 0.444227, Test Loss = 0.215307, Learning Rate = 6.220864e-04\n",
      "Epoch 1251/20000: Train Loss = 0.446374, Test Loss = 0.260798, Learning Rate = 6.218500e-04\n",
      "Epoch 1252/20000: Train Loss = 0.446207, Test Loss = 0.237272, Learning Rate = 6.216137e-04\n",
      "Epoch 1253/20000: Train Loss = 0.448130, Test Loss = 0.261634, Learning Rate = 6.213775e-04\n",
      "Epoch 1254/20000: Train Loss = 0.444152, Test Loss = 0.252196, Learning Rate = 6.211414e-04\n",
      "Epoch 1255/20000: Train Loss = 0.445791, Test Loss = 0.235408, Learning Rate = 6.209054e-04\n",
      "Epoch 1256/20000: Train Loss = 0.445463, Test Loss = 0.230968, Learning Rate = 6.206694e-04\n",
      "Epoch 1257/20000: Train Loss = 0.445285, Test Loss = 0.229119, Learning Rate = 6.204336e-04\n",
      "Epoch 1258/20000: Train Loss = 0.448247, Test Loss = 0.210421, Learning Rate = 6.201979e-04\n",
      "Epoch 1259/20000: Train Loss = 0.447663, Test Loss = 0.233270, Learning Rate = 6.199622e-04\n",
      "Epoch 1260/20000: Train Loss = 0.446439, Test Loss = 0.265285, Learning Rate = 6.197266e-04\n",
      "Epoch 1261/20000: Train Loss = 0.445539, Test Loss = 0.259687, Learning Rate = 6.194912e-04\n",
      "Epoch 1262/20000: Train Loss = 0.446550, Test Loss = 0.242097, Learning Rate = 6.192558e-04\n",
      "Epoch 1263/20000: Train Loss = 0.445958, Test Loss = 0.254857, Learning Rate = 6.190205e-04\n",
      "Epoch 1264/20000: Train Loss = 0.445207, Test Loss = 0.263939, Learning Rate = 6.187853e-04\n",
      "Epoch 1265/20000: Train Loss = 0.447586, Test Loss = 0.270805, Learning Rate = 6.185501e-04\n",
      "Epoch 1266/20000: Train Loss = 0.445554, Test Loss = 0.262028, Learning Rate = 6.183151e-04\n",
      "Epoch 1267/20000: Train Loss = 0.445445, Test Loss = 0.259874, Learning Rate = 6.180802e-04\n",
      "Epoch 1268/20000: Train Loss = 0.447393, Test Loss = 0.253815, Learning Rate = 6.178453e-04\n",
      "Epoch 1269/20000: Train Loss = 0.445963, Test Loss = 0.241490, Learning Rate = 6.176105e-04\n",
      "Epoch 1270/20000: Train Loss = 0.446298, Test Loss = 0.223441, Learning Rate = 6.173759e-04\n",
      "Epoch 1271/20000: Train Loss = 0.446130, Test Loss = 0.245930, Learning Rate = 6.171413e-04\n",
      "Epoch 1272/20000: Train Loss = 0.446717, Test Loss = 0.228715, Learning Rate = 6.169068e-04\n",
      "Epoch 1273/20000: Train Loss = 0.449384, Test Loss = 0.214816, Learning Rate = 6.166724e-04\n",
      "Epoch 1274/20000: Train Loss = 0.447039, Test Loss = 0.251797, Learning Rate = 6.164381e-04\n",
      "Epoch 1275/20000: Train Loss = 0.452715, Test Loss = 0.204981, Learning Rate = 6.162038e-04\n",
      "Epoch 1276/20000: Train Loss = 0.449309, Test Loss = 0.264731, Learning Rate = 6.159697e-04\n",
      "Epoch 1277/20000: Train Loss = 0.451390, Test Loss = 0.260153, Learning Rate = 6.157356e-04\n",
      "Epoch 1278/20000: Train Loss = 0.445155, Test Loss = 0.256043, Learning Rate = 6.155017e-04\n",
      "Epoch 1279/20000: Train Loss = 0.448513, Test Loss = 0.219100, Learning Rate = 6.152678e-04\n",
      "Epoch 1280/20000: Train Loss = 0.448036, Test Loss = 0.250278, Learning Rate = 6.150340e-04\n",
      "Epoch 1281/20000: Train Loss = 0.447013, Test Loss = 0.246046, Learning Rate = 6.148003e-04\n",
      "Epoch 1282/20000: Train Loss = 0.446516, Test Loss = 0.204761, Learning Rate = 6.145667e-04\n",
      "Epoch 1283/20000: Train Loss = 0.449102, Test Loss = 0.241125, Learning Rate = 6.143332e-04\n",
      "Epoch 1284/20000: Train Loss = 0.445658, Test Loss = 0.238550, Learning Rate = 6.140998e-04\n",
      "Epoch 1285/20000: Train Loss = 0.446968, Test Loss = 0.261623, Learning Rate = 6.138664e-04\n",
      "Epoch 1286/20000: Train Loss = 0.446125, Test Loss = 0.249275, Learning Rate = 6.136332e-04\n",
      "Epoch 1287/20000: Train Loss = 0.445593, Test Loss = 0.225193, Learning Rate = 6.134000e-04\n",
      "Epoch 1288/20000: Train Loss = 0.447174, Test Loss = 0.238635, Learning Rate = 6.131669e-04\n",
      "Epoch 1289/20000: Train Loss = 0.444815, Test Loss = 0.219634, Learning Rate = 6.129339e-04\n",
      "Epoch 1290/20000: Train Loss = 0.450196, Test Loss = 0.242029, Learning Rate = 6.127010e-04\n",
      "Epoch 1291/20000: Train Loss = 0.445373, Test Loss = 0.250265, Learning Rate = 6.124682e-04\n",
      "Epoch 1292/20000: Train Loss = 0.447572, Test Loss = 0.259698, Learning Rate = 6.122355e-04\n",
      "Epoch 1293/20000: Train Loss = 0.445716, Test Loss = 0.240741, Learning Rate = 6.120029e-04\n",
      "Epoch 1294/20000: Train Loss = 0.448715, Test Loss = 0.253396, Learning Rate = 6.117703e-04\n",
      "Epoch 1295/20000: Train Loss = 0.444627, Test Loss = 0.267426, Learning Rate = 6.115379e-04\n",
      "Epoch 1296/20000: Train Loss = 0.447813, Test Loss = 0.241018, Learning Rate = 6.113055e-04\n",
      "Epoch 1297/20000: Train Loss = 0.449128, Test Loss = 0.224965, Learning Rate = 6.110732e-04\n",
      "Epoch 1298/20000: Train Loss = 0.445251, Test Loss = 0.232242, Learning Rate = 6.108410e-04\n",
      "Epoch 1299/20000: Train Loss = 0.445625, Test Loss = 0.222533, Learning Rate = 6.106089e-04\n",
      "Epoch 1300/20000: Train Loss = 0.447125, Test Loss = 0.254388, Learning Rate = 6.103769e-04\n",
      "Epoch 1301/20000: Train Loss = 0.448249, Test Loss = 0.272935, Learning Rate = 6.101450e-04\n",
      "Epoch 1302/20000: Train Loss = 0.444884, Test Loss = 0.220241, Learning Rate = 6.099132e-04\n",
      "Epoch 1303/20000: Train Loss = 0.444137, Test Loss = 0.256758, Learning Rate = 6.096814e-04\n",
      "Epoch 1304/20000: Train Loss = 0.445094, Test Loss = 0.249481, Learning Rate = 6.094497e-04\n",
      "Epoch 1305/20000: Train Loss = 0.445289, Test Loss = 0.243309, Learning Rate = 6.092182e-04\n",
      "Epoch 1306/20000: Train Loss = 0.448171, Test Loss = 0.236561, Learning Rate = 6.089867e-04\n",
      "Epoch 1307/20000: Train Loss = 0.445457, Test Loss = 0.248790, Learning Rate = 6.087553e-04\n",
      "Epoch 1308/20000: Train Loss = 0.447782, Test Loss = 0.256048, Learning Rate = 6.085240e-04\n",
      "Epoch 1309/20000: Train Loss = 0.449773, Test Loss = 0.232665, Learning Rate = 6.082927e-04\n",
      "Epoch 1310/20000: Train Loss = 0.446536, Test Loss = 0.232271, Learning Rate = 6.080616e-04\n",
      "Epoch 1311/20000: Train Loss = 0.447394, Test Loss = 0.257527, Learning Rate = 6.078306e-04\n",
      "Epoch 1312/20000: Train Loss = 0.448156, Test Loss = 0.226829, Learning Rate = 6.075996e-04\n",
      "Epoch 1313/20000: Train Loss = 0.445413, Test Loss = 0.225747, Learning Rate = 6.073687e-04\n",
      "Epoch 1314/20000: Train Loss = 0.446114, Test Loss = 0.232308, Learning Rate = 6.071380e-04\n",
      "Epoch 1315/20000: Train Loss = 0.447050, Test Loss = 0.251827, Learning Rate = 6.069073e-04\n",
      "Epoch 1316/20000: Train Loss = 0.444793, Test Loss = 0.259117, Learning Rate = 6.066766e-04\n",
      "Epoch 1317/20000: Train Loss = 0.445713, Test Loss = 0.239184, Learning Rate = 6.064461e-04\n",
      "Epoch 1318/20000: Train Loss = 0.444086, Test Loss = 0.268043, Learning Rate = 6.062157e-04\n",
      "Epoch 1319/20000: Train Loss = 0.448667, Test Loss = 0.256264, Learning Rate = 6.059853e-04\n",
      "Epoch 1320/20000: Train Loss = 0.447992, Test Loss = 0.255413, Learning Rate = 6.057551e-04\n",
      "Epoch 1321/20000: Train Loss = 0.445839, Test Loss = 0.260345, Learning Rate = 6.055249e-04\n",
      "Epoch 1322/20000: Train Loss = 0.444771, Test Loss = 0.234494, Learning Rate = 6.052948e-04\n",
      "Epoch 1323/20000: Train Loss = 0.443829, Test Loss = 0.277829, Learning Rate = 6.050648e-04\n",
      "Epoch 1324/20000: Train Loss = 0.450180, Test Loss = 0.244804, Learning Rate = 6.048349e-04\n",
      "Epoch 1325/20000: Train Loss = 0.453335, Test Loss = 0.219296, Learning Rate = 6.046051e-04\n",
      "Epoch 1326/20000: Train Loss = 0.449037, Test Loss = 0.266258, Learning Rate = 6.043754e-04\n",
      "Epoch 1327/20000: Train Loss = 0.447706, Test Loss = 0.269222, Learning Rate = 6.041457e-04\n",
      "Epoch 1328/20000: Train Loss = 0.445175, Test Loss = 0.249379, Learning Rate = 6.039162e-04\n",
      "Epoch 1329/20000: Train Loss = 0.447263, Test Loss = 0.237126, Learning Rate = 6.036867e-04\n",
      "Epoch 1330/20000: Train Loss = 0.448107, Test Loss = 0.229497, Learning Rate = 6.034573e-04\n",
      "Epoch 1331/20000: Train Loss = 0.448263, Test Loss = 0.228278, Learning Rate = 6.032280e-04\n",
      "Epoch 1332/20000: Train Loss = 0.446885, Test Loss = 0.230028, Learning Rate = 6.029988e-04\n",
      "Epoch 1333/20000: Train Loss = 0.444292, Test Loss = 0.265612, Learning Rate = 6.027697e-04\n",
      "Epoch 1334/20000: Train Loss = 0.446475, Test Loss = 0.234346, Learning Rate = 6.025406e-04\n",
      "Epoch 1335/20000: Train Loss = 0.445498, Test Loss = 0.222111, Learning Rate = 6.023117e-04\n",
      "Epoch 1336/20000: Train Loss = 0.446179, Test Loss = 0.229229, Learning Rate = 6.020828e-04\n",
      "Epoch 1337/20000: Train Loss = 0.446132, Test Loss = 0.224264, Learning Rate = 6.018541e-04\n",
      "Epoch 1338/20000: Train Loss = 0.446736, Test Loss = 0.247070, Learning Rate = 6.016254e-04\n",
      "Epoch 1339/20000: Train Loss = 0.448005, Test Loss = 0.238559, Learning Rate = 6.013968e-04\n",
      "Epoch 1340/20000: Train Loss = 0.445690, Test Loss = 0.241714, Learning Rate = 6.011683e-04\n",
      "Epoch 1341/20000: Train Loss = 0.445797, Test Loss = 0.239067, Learning Rate = 6.009398e-04\n",
      "Epoch 1342/20000: Train Loss = 0.449977, Test Loss = 0.280177, Learning Rate = 6.007115e-04\n",
      "Epoch 1343/20000: Train Loss = 0.447629, Test Loss = 0.272367, Learning Rate = 6.004832e-04\n",
      "Epoch 1344/20000: Train Loss = 0.446633, Test Loss = 0.258339, Learning Rate = 6.002551e-04\n",
      "Epoch 1345/20000: Train Loss = 0.445261, Test Loss = 0.248659, Learning Rate = 6.000270e-04\n",
      "Epoch 1346/20000: Train Loss = 0.444736, Test Loss = 0.239598, Learning Rate = 5.997990e-04\n",
      "Epoch 1347/20000: Train Loss = 0.445810, Test Loss = 0.260293, Learning Rate = 5.995711e-04\n",
      "Epoch 1348/20000: Train Loss = 0.444962, Test Loss = 0.256570, Learning Rate = 5.993433e-04\n",
      "Epoch 1349/20000: Train Loss = 0.447130, Test Loss = 0.256293, Learning Rate = 5.991155e-04\n",
      "Epoch 1350/20000: Train Loss = 0.445680, Test Loss = 0.221867, Learning Rate = 5.988879e-04\n",
      "Epoch 1351/20000: Train Loss = 0.445581, Test Loss = 0.231788, Learning Rate = 5.986603e-04\n",
      "Epoch 1352/20000: Train Loss = 0.448014, Test Loss = 0.242028, Learning Rate = 5.984328e-04\n",
      "Epoch 1353/20000: Train Loss = 0.444592, Test Loss = 0.275977, Learning Rate = 5.982055e-04\n",
      "Epoch 1354/20000: Train Loss = 0.452256, Test Loss = 0.218009, Learning Rate = 5.979782e-04\n",
      "Epoch 1355/20000: Train Loss = 0.446220, Test Loss = 0.271955, Learning Rate = 5.977509e-04\n",
      "Epoch 1356/20000: Train Loss = 0.446297, Test Loss = 0.199271, Learning Rate = 5.975238e-04\n",
      "Epoch 1357/20000: Train Loss = 0.445699, Test Loss = 0.264217, Learning Rate = 5.972968e-04\n",
      "Epoch 1358/20000: Train Loss = 0.447466, Test Loss = 0.250534, Learning Rate = 5.970698e-04\n",
      "Epoch 1359/20000: Train Loss = 0.446081, Test Loss = 0.224627, Learning Rate = 5.968429e-04\n",
      "Epoch 1360/20000: Train Loss = 0.444875, Test Loss = 0.267944, Learning Rate = 5.966162e-04\n",
      "Epoch 1361/20000: Train Loss = 0.444012, Test Loss = 0.220844, Learning Rate = 5.963895e-04\n",
      "Epoch 1362/20000: Train Loss = 0.446424, Test Loss = 0.263313, Learning Rate = 5.961628e-04\n",
      "Epoch 1363/20000: Train Loss = 0.446773, Test Loss = 0.230804, Learning Rate = 5.959363e-04\n",
      "Epoch 1364/20000: Train Loss = 0.446623, Test Loss = 0.231663, Learning Rate = 5.957099e-04\n",
      "Epoch 1365/20000: Train Loss = 0.449987, Test Loss = 0.275539, Learning Rate = 5.954835e-04\n",
      "Epoch 1366/20000: Train Loss = 0.446735, Test Loss = 0.264347, Learning Rate = 5.952573e-04\n",
      "Epoch 1367/20000: Train Loss = 0.445742, Test Loss = 0.243204, Learning Rate = 5.950311e-04\n",
      "Epoch 1368/20000: Train Loss = 0.445767, Test Loss = 0.247532, Learning Rate = 5.948050e-04\n",
      "Epoch 1369/20000: Train Loss = 0.446660, Test Loss = 0.248912, Learning Rate = 5.945790e-04\n",
      "Epoch 1370/20000: Train Loss = 0.445271, Test Loss = 0.236025, Learning Rate = 5.943531e-04\n",
      "Epoch 1371/20000: Train Loss = 0.445377, Test Loss = 0.266964, Learning Rate = 5.941272e-04\n",
      "Epoch 1372/20000: Train Loss = 0.445267, Test Loss = 0.247999, Learning Rate = 5.939015e-04\n",
      "Epoch 1373/20000: Train Loss = 0.445265, Test Loss = 0.237808, Learning Rate = 5.936758e-04\n",
      "Epoch 1374/20000: Train Loss = 0.446504, Test Loss = 0.238374, Learning Rate = 5.934502e-04\n",
      "Epoch 1375/20000: Train Loss = 0.446552, Test Loss = 0.230246, Learning Rate = 5.932247e-04\n",
      "Epoch 1376/20000: Train Loss = 0.445386, Test Loss = 0.244451, Learning Rate = 5.929993e-04\n",
      "Epoch 1377/20000: Train Loss = 0.445092, Test Loss = 0.260205, Learning Rate = 5.927740e-04\n",
      "Epoch 1378/20000: Train Loss = 0.445982, Test Loss = 0.243100, Learning Rate = 5.925487e-04\n",
      "Epoch 1379/20000: Train Loss = 0.444039, Test Loss = 0.235015, Learning Rate = 5.923236e-04\n",
      "Epoch 1380/20000: Train Loss = 0.444715, Test Loss = 0.235356, Learning Rate = 5.920985e-04\n",
      "Epoch 1381/20000: Train Loss = 0.449863, Test Loss = 0.233619, Learning Rate = 5.918735e-04\n",
      "Epoch 1382/20000: Train Loss = 0.445854, Test Loss = 0.246240, Learning Rate = 5.916487e-04\n",
      "Epoch 1383/20000: Train Loss = 0.445838, Test Loss = 0.232362, Learning Rate = 5.914238e-04\n",
      "Epoch 1384/20000: Train Loss = 0.446785, Test Loss = 0.287535, Learning Rate = 5.911991e-04\n",
      "Epoch 1385/20000: Train Loss = 0.446238, Test Loss = 0.225546, Learning Rate = 5.909745e-04\n",
      "Epoch 1386/20000: Train Loss = 0.445532, Test Loss = 0.252959, Learning Rate = 5.907499e-04\n",
      "Epoch 1387/20000: Train Loss = 0.445904, Test Loss = 0.246648, Learning Rate = 5.905255e-04\n",
      "Epoch 1388/20000: Train Loss = 0.445301, Test Loss = 0.239767, Learning Rate = 5.903011e-04\n",
      "Epoch 1389/20000: Train Loss = 0.447183, Test Loss = 0.239997, Learning Rate = 5.900768e-04\n",
      "Epoch 1390/20000: Train Loss = 0.446902, Test Loss = 0.263704, Learning Rate = 5.898526e-04\n",
      "Epoch 1391/20000: Train Loss = 0.443738, Test Loss = 0.228667, Learning Rate = 5.896284e-04\n",
      "Epoch 1392/20000: Train Loss = 0.444138, Test Loss = 0.257833, Learning Rate = 5.894044e-04\n",
      "Epoch 1393/20000: Train Loss = 0.445446, Test Loss = 0.265946, Learning Rate = 5.891804e-04\n",
      "Epoch 1394/20000: Train Loss = 0.446719, Test Loss = 0.247783, Learning Rate = 5.889566e-04\n",
      "Epoch 1395/20000: Train Loss = 0.447358, Test Loss = 0.225848, Learning Rate = 5.887328e-04\n",
      "Epoch 1396/20000: Train Loss = 0.445605, Test Loss = 0.267110, Learning Rate = 5.885091e-04\n",
      "Epoch 1397/20000: Train Loss = 0.448049, Test Loss = 0.288549, Learning Rate = 5.882854e-04\n",
      "Epoch 1398/20000: Train Loss = 0.446048, Test Loss = 0.236859, Learning Rate = 5.880619e-04\n",
      "Epoch 1399/20000: Train Loss = 0.445110, Test Loss = 0.243599, Learning Rate = 5.878385e-04\n",
      "Epoch 1400/20000: Train Loss = 0.448155, Test Loss = 0.234610, Learning Rate = 5.876151e-04\n",
      "Epoch 1401/20000: Train Loss = 0.445494, Test Loss = 0.244303, Learning Rate = 5.873918e-04\n",
      "Epoch 1402/20000: Train Loss = 0.445886, Test Loss = 0.256859, Learning Rate = 5.871686e-04\n",
      "Epoch 1403/20000: Train Loss = 0.446642, Test Loss = 0.241852, Learning Rate = 5.869455e-04\n",
      "Epoch 1404/20000: Train Loss = 0.446250, Test Loss = 0.232876, Learning Rate = 5.867225e-04\n",
      "Epoch 1405/20000: Train Loss = 0.445291, Test Loss = 0.245750, Learning Rate = 5.864996e-04\n",
      "Epoch 1406/20000: Train Loss = 0.444118, Test Loss = 0.260880, Learning Rate = 5.862767e-04\n",
      "Epoch 1407/20000: Train Loss = 0.445174, Test Loss = 0.239601, Learning Rate = 5.860539e-04\n",
      "Epoch 1408/20000: Train Loss = 0.446731, Test Loss = 0.228536, Learning Rate = 5.858313e-04\n",
      "Epoch 1409/20000: Train Loss = 0.444667, Test Loss = 0.273754, Learning Rate = 5.856087e-04\n",
      "Epoch 1410/20000: Train Loss = 0.445611, Test Loss = 0.218868, Learning Rate = 5.853861e-04\n",
      "Epoch 1411/20000: Train Loss = 0.446852, Test Loss = 0.274044, Learning Rate = 5.851637e-04\n",
      "Epoch 1412/20000: Train Loss = 0.445062, Test Loss = 0.216159, Learning Rate = 5.849414e-04\n",
      "Epoch 1413/20000: Train Loss = 0.445608, Test Loss = 0.224637, Learning Rate = 5.847191e-04\n",
      "Epoch 1414/20000: Train Loss = 0.446397, Test Loss = 0.245621, Learning Rate = 5.844969e-04\n",
      "Epoch 1415/20000: Train Loss = 0.446650, Test Loss = 0.250142, Learning Rate = 5.842748e-04\n",
      "Epoch 1416/20000: Train Loss = 0.448320, Test Loss = 0.319167, Learning Rate = 5.840528e-04\n",
      "Epoch 1417/20000: Train Loss = 0.446728, Test Loss = 0.217096, Learning Rate = 5.838309e-04\n",
      "Epoch 1418/20000: Train Loss = 0.446674, Test Loss = 0.235738, Learning Rate = 5.836091e-04\n",
      "Epoch 1419/20000: Train Loss = 0.444230, Test Loss = 0.216507, Learning Rate = 5.833873e-04\n",
      "Epoch 1420/20000: Train Loss = 0.449054, Test Loss = 0.244855, Learning Rate = 5.831656e-04\n",
      "Epoch 1421/20000: Train Loss = 0.444511, Test Loss = 0.218896, Learning Rate = 5.829440e-04\n",
      "Epoch 1422/20000: Train Loss = 0.447727, Test Loss = 0.235175, Learning Rate = 5.827225e-04\n",
      "Epoch 1423/20000: Train Loss = 0.447306, Test Loss = 0.231559, Learning Rate = 5.825011e-04\n",
      "Epoch 1424/20000: Train Loss = 0.445464, Test Loss = 0.226700, Learning Rate = 5.822798e-04\n",
      "Epoch 1425/20000: Train Loss = 0.444518, Test Loss = 0.242943, Learning Rate = 5.820585e-04\n",
      "Epoch 1426/20000: Train Loss = 0.447831, Test Loss = 0.237527, Learning Rate = 5.818374e-04\n",
      "Epoch 1427/20000: Train Loss = 0.446489, Test Loss = 0.232920, Learning Rate = 5.816163e-04\n",
      "Epoch 1428/20000: Train Loss = 0.445862, Test Loss = 0.242663, Learning Rate = 5.813953e-04\n",
      "Epoch 1429/20000: Train Loss = 0.445308, Test Loss = 0.283950, Learning Rate = 5.811744e-04\n",
      "Epoch 1430/20000: Train Loss = 0.448407, Test Loss = 0.231803, Learning Rate = 5.809535e-04\n",
      "Epoch 1431/20000: Train Loss = 0.443843, Test Loss = 0.267625, Learning Rate = 5.807328e-04\n",
      "Epoch 1432/20000: Train Loss = 0.444535, Test Loss = 0.247511, Learning Rate = 5.805121e-04\n",
      "Epoch 1433/20000: Train Loss = 0.446547, Test Loss = 0.234810, Learning Rate = 5.802916e-04\n",
      "Epoch 1434/20000: Train Loss = 0.448338, Test Loss = 0.287330, Learning Rate = 5.800711e-04\n",
      "Epoch 1435/20000: Train Loss = 0.448031, Test Loss = 0.241169, Learning Rate = 5.798507e-04\n",
      "Epoch 1436/20000: Train Loss = 0.444067, Test Loss = 0.259025, Learning Rate = 5.796303e-04\n",
      "Epoch 1437/20000: Train Loss = 0.444659, Test Loss = 0.243724, Learning Rate = 5.794101e-04\n",
      "Epoch 1438/20000: Train Loss = 0.448427, Test Loss = 0.241263, Learning Rate = 5.791899e-04\n",
      "Epoch 1439/20000: Train Loss = 0.445446, Test Loss = 0.225368, Learning Rate = 5.789698e-04\n",
      "Epoch 1440/20000: Train Loss = 0.448793, Test Loss = 0.253283, Learning Rate = 5.787499e-04\n",
      "Epoch 1441/20000: Train Loss = 0.449824, Test Loss = 0.253208, Learning Rate = 5.785299e-04\n",
      "Epoch 1442/20000: Train Loss = 0.447790, Test Loss = 0.265320, Learning Rate = 5.783101e-04\n",
      "Epoch 1443/20000: Train Loss = 0.444587, Test Loss = 0.246628, Learning Rate = 5.780904e-04\n",
      "Epoch 1444/20000: Train Loss = 0.445149, Test Loss = 0.228984, Learning Rate = 5.778707e-04\n",
      "Epoch 1445/20000: Train Loss = 0.445442, Test Loss = 0.249827, Learning Rate = 5.776511e-04\n",
      "Epoch 1446/20000: Train Loss = 0.445296, Test Loss = 0.232993, Learning Rate = 5.774316e-04\n",
      "Epoch 1447/20000: Train Loss = 0.445042, Test Loss = 0.222190, Learning Rate = 5.772122e-04\n",
      "Epoch 1448/20000: Train Loss = 0.444622, Test Loss = 0.272443, Learning Rate = 5.769929e-04\n",
      "Epoch 1449/20000: Train Loss = 0.446695, Test Loss = 0.300202, Learning Rate = 5.767737e-04\n",
      "Epoch 1450/20000: Train Loss = 0.445187, Test Loss = 0.218057, Learning Rate = 5.765545e-04\n",
      "Epoch 1451/20000: Train Loss = 0.444987, Test Loss = 0.254402, Learning Rate = 5.763354e-04\n",
      "Epoch 1452/20000: Train Loss = 0.445531, Test Loss = 0.263014, Learning Rate = 5.761164e-04\n",
      "Epoch 1453/20000: Train Loss = 0.445641, Test Loss = 0.253348, Learning Rate = 5.758975e-04\n",
      "Epoch 1454/20000: Train Loss = 0.446904, Test Loss = 0.245684, Learning Rate = 5.756787e-04\n",
      "Epoch 1455/20000: Train Loss = 0.448534, Test Loss = 0.233091, Learning Rate = 5.754600e-04\n",
      "Epoch 1456/20000: Train Loss = 0.446264, Test Loss = 0.240838, Learning Rate = 5.752413e-04\n",
      "Epoch 1457/20000: Train Loss = 0.448100, Test Loss = 0.281399, Learning Rate = 5.750227e-04\n",
      "Epoch 1458/20000: Train Loss = 0.446334, Test Loss = 0.241353, Learning Rate = 5.748042e-04\n",
      "Epoch 1459/20000: Train Loss = 0.444832, Test Loss = 0.249198, Learning Rate = 5.745858e-04\n",
      "Epoch 1460/20000: Train Loss = 0.445492, Test Loss = 0.206222, Learning Rate = 5.743675e-04\n",
      "Epoch 1461/20000: Train Loss = 0.448078, Test Loss = 0.229469, Learning Rate = 5.741493e-04\n",
      "Epoch 1462/20000: Train Loss = 0.445074, Test Loss = 0.243320, Learning Rate = 5.739311e-04\n",
      "Epoch 1463/20000: Train Loss = 0.449552, Test Loss = 0.261503, Learning Rate = 5.737130e-04\n",
      "Epoch 1464/20000: Train Loss = 0.451018, Test Loss = 0.290117, Learning Rate = 5.734950e-04\n",
      "Epoch 1465/20000: Train Loss = 0.450831, Test Loss = 0.219320, Learning Rate = 5.732771e-04\n",
      "Epoch 1466/20000: Train Loss = 0.445519, Test Loss = 0.261227, Learning Rate = 5.730593e-04\n",
      "Epoch 1467/20000: Train Loss = 0.445886, Test Loss = 0.254722, Learning Rate = 5.728415e-04\n",
      "Epoch 1468/20000: Train Loss = 0.447493, Test Loss = 0.248647, Learning Rate = 5.726239e-04\n",
      "Epoch 1469/20000: Train Loss = 0.444580, Test Loss = 0.262707, Learning Rate = 5.724063e-04\n",
      "Epoch 1470/20000: Train Loss = 0.445292, Test Loss = 0.224793, Learning Rate = 5.721888e-04\n",
      "Epoch 1471/20000: Train Loss = 0.446117, Test Loss = 0.233884, Learning Rate = 5.719714e-04\n",
      "Epoch 1472/20000: Train Loss = 0.444617, Test Loss = 0.242145, Learning Rate = 5.717540e-04\n",
      "Epoch 1473/20000: Train Loss = 0.443983, Test Loss = 0.226014, Learning Rate = 5.715368e-04\n",
      "Epoch 1474/20000: Train Loss = 0.445068, Test Loss = 0.293920, Learning Rate = 5.713196e-04\n",
      "Epoch 1475/20000: Train Loss = 0.447759, Test Loss = 0.224803, Learning Rate = 5.711025e-04\n",
      "Epoch 1476/20000: Train Loss = 0.445176, Test Loss = 0.253744, Learning Rate = 5.708855e-04\n",
      "Epoch 1477/20000: Train Loss = 0.447151, Test Loss = 0.224287, Learning Rate = 5.706686e-04\n",
      "Epoch 1478/20000: Train Loss = 0.444148, Test Loss = 0.268536, Learning Rate = 5.704518e-04\n",
      "Epoch 1479/20000: Train Loss = 0.445504, Test Loss = 0.239345, Learning Rate = 5.702350e-04\n",
      "Epoch 1480/20000: Train Loss = 0.446141, Test Loss = 0.248594, Learning Rate = 5.700183e-04\n",
      "Epoch 1481/20000: Train Loss = 0.447132, Test Loss = 0.245523, Learning Rate = 5.698018e-04\n",
      "Epoch 1482/20000: Train Loss = 0.445455, Test Loss = 0.245993, Learning Rate = 5.695852e-04\n",
      "Epoch 1483/20000: Train Loss = 0.444907, Test Loss = 0.270628, Learning Rate = 5.693688e-04\n",
      "Epoch 1484/20000: Train Loss = 0.443063, Test Loss = 0.223538, Learning Rate = 5.691525e-04\n",
      "Epoch 1485/20000: Train Loss = 0.445710, Test Loss = 0.256648, Learning Rate = 5.689362e-04\n",
      "Epoch 1486/20000: Train Loss = 0.445445, Test Loss = 0.232906, Learning Rate = 5.687200e-04\n",
      "Epoch 1487/20000: Train Loss = 0.449029, Test Loss = 0.228257, Learning Rate = 5.685039e-04\n",
      "Epoch 1488/20000: Train Loss = 0.444250, Test Loss = 0.254646, Learning Rate = 5.682879e-04\n",
      "Epoch 1489/20000: Train Loss = 0.445507, Test Loss = 0.256035, Learning Rate = 5.680720e-04\n",
      "Epoch 1490/20000: Train Loss = 0.446221, Test Loss = 0.238028, Learning Rate = 5.678561e-04\n",
      "Epoch 1491/20000: Train Loss = 0.444121, Test Loss = 0.261564, Learning Rate = 5.676404e-04\n",
      "Epoch 1492/20000: Train Loss = 0.445668, Test Loss = 0.234676, Learning Rate = 5.674247e-04\n",
      "Epoch 1493/20000: Train Loss = 0.447562, Test Loss = 0.266651, Learning Rate = 5.672091e-04\n",
      "Epoch 1494/20000: Train Loss = 0.446570, Test Loss = 0.232294, Learning Rate = 5.669935e-04\n",
      "Epoch 1495/20000: Train Loss = 0.444497, Test Loss = 0.215887, Learning Rate = 5.667781e-04\n",
      "Epoch 1496/20000: Train Loss = 0.448991, Test Loss = 0.235137, Learning Rate = 5.665627e-04\n",
      "Epoch 1497/20000: Train Loss = 0.444924, Test Loss = 0.258157, Learning Rate = 5.663475e-04\n",
      "Epoch 1498/20000: Train Loss = 0.446227, Test Loss = 0.248931, Learning Rate = 5.661323e-04\n",
      "Epoch 1499/20000: Train Loss = 0.445769, Test Loss = 0.233115, Learning Rate = 5.659171e-04\n",
      "Epoch 1500/20000: Train Loss = 0.445716, Test Loss = 0.245781, Learning Rate = 5.657021e-04\n",
      "Epoch 1501/20000: Train Loss = 0.446303, Test Loss = 0.262377, Learning Rate = 5.654872e-04\n",
      "Epoch 1502/20000: Train Loss = 0.444936, Test Loss = 0.267020, Learning Rate = 5.652723e-04\n",
      "Epoch 1503/20000: Train Loss = 0.448721, Test Loss = 0.250936, Learning Rate = 5.650575e-04\n",
      "Epoch 1504/20000: Train Loss = 0.444830, Test Loss = 0.211810, Learning Rate = 5.648428e-04\n",
      "Epoch 1505/20000: Train Loss = 0.446922, Test Loss = 0.239105, Learning Rate = 5.646282e-04\n",
      "Epoch 1506/20000: Train Loss = 0.446695, Test Loss = 0.249047, Learning Rate = 5.644136e-04\n",
      "Epoch 1507/20000: Train Loss = 0.445536, Test Loss = 0.259100, Learning Rate = 5.641992e-04\n",
      "Epoch 1508/20000: Train Loss = 0.446094, Test Loss = 0.273023, Learning Rate = 5.639848e-04\n",
      "Epoch 1509/20000: Train Loss = 0.447896, Test Loss = 0.269760, Learning Rate = 5.637705e-04\n",
      "Epoch 1510/20000: Train Loss = 0.446673, Test Loss = 0.271483, Learning Rate = 5.635563e-04\n",
      "Epoch 1511/20000: Train Loss = 0.444211, Test Loss = 0.237438, Learning Rate = 5.633421e-04\n",
      "Epoch 1512/20000: Train Loss = 0.445766, Test Loss = 0.247678, Learning Rate = 5.631281e-04\n",
      "Epoch 1513/20000: Train Loss = 0.445871, Test Loss = 0.263515, Learning Rate = 5.629141e-04\n",
      "Epoch 1514/20000: Train Loss = 0.445242, Test Loss = 0.234220, Learning Rate = 5.627002e-04\n",
      "Epoch 1515/20000: Train Loss = 0.444517, Test Loss = 0.240162, Learning Rate = 5.624864e-04\n",
      "Epoch 1516/20000: Train Loss = 0.447489, Test Loss = 0.220809, Learning Rate = 5.622727e-04\n",
      "Epoch 1517/20000: Train Loss = 0.447103, Test Loss = 0.234087, Learning Rate = 5.620590e-04\n",
      "Epoch 1518/20000: Train Loss = 0.444743, Test Loss = 0.266717, Learning Rate = 5.618455e-04\n",
      "Epoch 1519/20000: Train Loss = 0.445721, Test Loss = 0.255240, Learning Rate = 5.616320e-04\n",
      "Epoch 1520/20000: Train Loss = 0.445941, Test Loss = 0.249029, Learning Rate = 5.614186e-04\n",
      "Epoch 1521/20000: Train Loss = 0.444856, Test Loss = 0.236250, Learning Rate = 5.612052e-04\n",
      "Epoch 1522/20000: Train Loss = 0.443587, Test Loss = 0.254587, Learning Rate = 5.609920e-04\n",
      "Epoch 1523/20000: Train Loss = 0.446144, Test Loss = 0.250923, Learning Rate = 5.607788e-04\n",
      "Epoch 1524/20000: Train Loss = 0.445389, Test Loss = 0.242649, Learning Rate = 5.605658e-04\n",
      "Epoch 1525/20000: Train Loss = 0.445024, Test Loss = 0.242862, Learning Rate = 5.603528e-04\n",
      "Epoch 1526/20000: Train Loss = 0.445260, Test Loss = 0.237123, Learning Rate = 5.601398e-04\n",
      "Epoch 1527/20000: Train Loss = 0.444194, Test Loss = 0.262970, Learning Rate = 5.599270e-04\n",
      "Epoch 1528/20000: Train Loss = 0.444134, Test Loss = 0.232192, Learning Rate = 5.597142e-04\n",
      "Epoch 1529/20000: Train Loss = 0.444091, Test Loss = 0.251092, Learning Rate = 5.595016e-04\n",
      "Epoch 1530/20000: Train Loss = 0.443834, Test Loss = 0.252388, Learning Rate = 5.592890e-04\n",
      "Epoch 1531/20000: Train Loss = 0.449947, Test Loss = 0.209818, Learning Rate = 5.590765e-04\n",
      "Epoch 1532/20000: Train Loss = 0.445757, Test Loss = 0.256681, Learning Rate = 5.588640e-04\n",
      "Epoch 1533/20000: Train Loss = 0.445486, Test Loss = 0.200659, Learning Rate = 5.586517e-04\n",
      "Epoch 1534/20000: Train Loss = 0.448558, Test Loss = 0.222298, Learning Rate = 5.584394e-04\n",
      "Epoch 1535/20000: Train Loss = 0.445239, Test Loss = 0.247652, Learning Rate = 5.582272e-04\n",
      "Epoch 1536/20000: Train Loss = 0.445545, Test Loss = 0.266391, Learning Rate = 5.580151e-04\n",
      "Epoch 1537/20000: Train Loss = 0.446380, Test Loss = 0.287512, Learning Rate = 5.578031e-04\n",
      "Epoch 1538/20000: Train Loss = 0.448538, Test Loss = 0.245033, Learning Rate = 5.575911e-04\n",
      "Epoch 1539/20000: Train Loss = 0.445433, Test Loss = 0.244696, Learning Rate = 5.573792e-04\n",
      "Epoch 1540/20000: Train Loss = 0.444985, Test Loss = 0.245510, Learning Rate = 5.571675e-04\n",
      "Epoch 1541/20000: Train Loss = 0.447745, Test Loss = 0.221776, Learning Rate = 5.569557e-04\n",
      "Epoch 1542/20000: Train Loss = 0.448143, Test Loss = 0.237047, Learning Rate = 5.567441e-04\n",
      "Epoch 1543/20000: Train Loss = 0.445939, Test Loss = 0.246548, Learning Rate = 5.565326e-04\n",
      "Epoch 1544/20000: Train Loss = 0.443639, Test Loss = 0.226970, Learning Rate = 5.563211e-04\n",
      "Epoch 1545/20000: Train Loss = 0.446233, Test Loss = 0.254111, Learning Rate = 5.561097e-04\n",
      "Epoch 1546/20000: Train Loss = 0.446610, Test Loss = 0.236288, Learning Rate = 5.558984e-04\n",
      "Epoch 1547/20000: Train Loss = 0.445168, Test Loss = 0.242978, Learning Rate = 5.556872e-04\n",
      "Epoch 1548/20000: Train Loss = 0.445466, Test Loss = 0.247011, Learning Rate = 5.554760e-04\n",
      "Epoch 1549/20000: Train Loss = 0.444580, Test Loss = 0.249266, Learning Rate = 5.552650e-04\n",
      "Epoch 1550/20000: Train Loss = 0.444463, Test Loss = 0.242604, Learning Rate = 5.550540e-04\n",
      "Epoch 1551/20000: Train Loss = 0.444668, Test Loss = 0.253529, Learning Rate = 5.548431e-04\n",
      "Epoch 1552/20000: Train Loss = 0.445496, Test Loss = 0.216015, Learning Rate = 5.546323e-04\n",
      "Epoch 1553/20000: Train Loss = 0.447618, Test Loss = 0.238884, Learning Rate = 5.544215e-04\n",
      "Epoch 1554/20000: Train Loss = 0.445894, Test Loss = 0.232439, Learning Rate = 5.542108e-04\n",
      "Epoch 1555/20000: Train Loss = 0.445880, Test Loss = 0.264630, Learning Rate = 5.540003e-04\n",
      "Epoch 1556/20000: Train Loss = 0.447117, Test Loss = 0.270517, Learning Rate = 5.537898e-04\n",
      "Epoch 1557/20000: Train Loss = 0.448126, Test Loss = 0.270490, Learning Rate = 5.535793e-04\n",
      "Epoch 1558/20000: Train Loss = 0.447857, Test Loss = 0.203449, Learning Rate = 5.533690e-04\n",
      "Epoch 1559/20000: Train Loss = 0.445409, Test Loss = 0.268971, Learning Rate = 5.531587e-04\n",
      "Epoch 1560/20000: Train Loss = 0.446406, Test Loss = 0.225917, Learning Rate = 5.529485e-04\n",
      "Epoch 1561/20000: Train Loss = 0.447104, Test Loss = 0.229519, Learning Rate = 5.527384e-04\n",
      "Epoch 1562/20000: Train Loss = 0.446883, Test Loss = 0.251268, Learning Rate = 5.525284e-04\n",
      "Epoch 1563/20000: Train Loss = 0.449127, Test Loss = 0.255260, Learning Rate = 5.523185e-04\n",
      "Epoch 1564/20000: Train Loss = 0.453169, Test Loss = 0.257538, Learning Rate = 5.521086e-04\n",
      "Epoch 1565/20000: Train Loss = 0.445036, Test Loss = 0.238553, Learning Rate = 5.518988e-04\n",
      "Epoch 1566/20000: Train Loss = 0.447131, Test Loss = 0.249358, Learning Rate = 5.516891e-04\n",
      "Epoch 1567/20000: Train Loss = 0.444437, Test Loss = 0.244165, Learning Rate = 5.514795e-04\n",
      "Epoch 1568/20000: Train Loss = 0.443958, Test Loss = 0.220200, Learning Rate = 5.512699e-04\n",
      "Epoch 1569/20000: Train Loss = 0.446604, Test Loss = 0.230715, Learning Rate = 5.510605e-04\n",
      "Epoch 1570/20000: Train Loss = 0.445621, Test Loss = 0.238326, Learning Rate = 5.508511e-04\n",
      "Epoch 1571/20000: Train Loss = 0.444293, Test Loss = 0.260227, Learning Rate = 5.506418e-04\n",
      "Epoch 1572/20000: Train Loss = 0.446857, Test Loss = 0.232088, Learning Rate = 5.504325e-04\n",
      "Epoch 1573/20000: Train Loss = 0.447547, Test Loss = 0.240729, Learning Rate = 5.502234e-04\n",
      "Epoch 1574/20000: Train Loss = 0.444440, Test Loss = 0.218509, Learning Rate = 5.500143e-04\n",
      "Epoch 1575/20000: Train Loss = 0.450215, Test Loss = 0.248359, Learning Rate = 5.498053e-04\n",
      "Epoch 1576/20000: Train Loss = 0.447274, Test Loss = 0.243604, Learning Rate = 5.495964e-04\n",
      "Epoch 1577/20000: Train Loss = 0.445987, Test Loss = 0.224283, Learning Rate = 5.493876e-04\n",
      "Epoch 1578/20000: Train Loss = 0.446728, Test Loss = 0.233884, Learning Rate = 5.491788e-04\n",
      "Epoch 1579/20000: Train Loss = 0.444660, Test Loss = 0.229612, Learning Rate = 5.489702e-04\n",
      "Epoch 1580/20000: Train Loss = 0.444445, Test Loss = 0.282284, Learning Rate = 5.487616e-04\n",
      "Epoch 1581/20000: Train Loss = 0.452650, Test Loss = 0.243968, Learning Rate = 5.485530e-04\n",
      "Epoch 1582/20000: Train Loss = 0.445568, Test Loss = 0.233498, Learning Rate = 5.483446e-04\n",
      "Epoch 1583/20000: Train Loss = 0.444434, Test Loss = 0.249960, Learning Rate = 5.481363e-04\n",
      "Epoch 1584/20000: Train Loss = 0.444594, Test Loss = 0.238050, Learning Rate = 5.479280e-04\n",
      "Epoch 1585/20000: Train Loss = 0.443903, Test Loss = 0.241900, Learning Rate = 5.477198e-04\n",
      "Epoch 1586/20000: Train Loss = 0.445635, Test Loss = 0.232318, Learning Rate = 5.475117e-04\n",
      "Epoch 1587/20000: Train Loss = 0.444789, Test Loss = 0.232264, Learning Rate = 5.473036e-04\n",
      "Epoch 1588/20000: Train Loss = 0.445915, Test Loss = 0.235139, Learning Rate = 5.470957e-04\n",
      "Epoch 1589/20000: Train Loss = 0.443884, Test Loss = 0.263513, Learning Rate = 5.468878e-04\n",
      "Epoch 1590/20000: Train Loss = 0.448955, Test Loss = 0.225883, Learning Rate = 5.466800e-04\n",
      "Epoch 1591/20000: Train Loss = 0.445070, Test Loss = 0.229324, Learning Rate = 5.464723e-04\n",
      "Epoch 1592/20000: Train Loss = 0.445942, Test Loss = 0.265839, Learning Rate = 5.462646e-04\n",
      "Epoch 1593/20000: Train Loss = 0.447414, Test Loss = 0.243916, Learning Rate = 5.460570e-04\n",
      "Epoch 1594/20000: Train Loss = 0.446712, Test Loss = 0.232377, Learning Rate = 5.458496e-04\n",
      "Epoch 1595/20000: Train Loss = 0.447298, Test Loss = 0.256730, Learning Rate = 5.456421e-04\n",
      "Epoch 1596/20000: Train Loss = 0.445943, Test Loss = 0.249162, Learning Rate = 5.454348e-04\n",
      "Epoch 1597/20000: Train Loss = 0.445772, Test Loss = 0.230831, Learning Rate = 5.452276e-04\n",
      "Epoch 1598/20000: Train Loss = 0.444998, Test Loss = 0.249832, Learning Rate = 5.450204e-04\n",
      "Epoch 1599/20000: Train Loss = 0.446135, Test Loss = 0.232789, Learning Rate = 5.448133e-04\n",
      "Epoch 1600/20000: Train Loss = 0.444337, Test Loss = 0.265954, Learning Rate = 5.446063e-04\n",
      "Epoch 1601/20000: Train Loss = 0.445185, Test Loss = 0.248160, Learning Rate = 5.443994e-04\n",
      "Epoch 1602/20000: Train Loss = 0.445153, Test Loss = 0.245030, Learning Rate = 5.441925e-04\n",
      "Epoch 1603/20000: Train Loss = 0.446953, Test Loss = 0.320959, Learning Rate = 5.439857e-04\n",
      "Epoch 1604/20000: Train Loss = 0.449480, Test Loss = 0.247443, Learning Rate = 5.437790e-04\n",
      "Epoch 1605/20000: Train Loss = 0.447534, Test Loss = 0.243913, Learning Rate = 5.435724e-04\n",
      "Epoch 1606/20000: Train Loss = 0.443721, Test Loss = 0.239759, Learning Rate = 5.433659e-04\n",
      "Epoch 1607/20000: Train Loss = 0.445140, Test Loss = 0.240223, Learning Rate = 5.431594e-04\n",
      "Epoch 1608/20000: Train Loss = 0.445501, Test Loss = 0.267084, Learning Rate = 5.429530e-04\n",
      "Epoch 1609/20000: Train Loss = 0.447128, Test Loss = 0.248549, Learning Rate = 5.427467e-04\n",
      "Epoch 1610/20000: Train Loss = 0.444712, Test Loss = 0.207540, Learning Rate = 5.425405e-04\n",
      "Epoch 1611/20000: Train Loss = 0.449496, Test Loss = 0.223802, Learning Rate = 5.423343e-04\n",
      "Epoch 1612/20000: Train Loss = 0.445908, Test Loss = 0.247580, Learning Rate = 5.421282e-04\n",
      "Epoch 1613/20000: Train Loss = 0.448945, Test Loss = 0.242297, Learning Rate = 5.419222e-04\n",
      "Epoch 1614/20000: Train Loss = 0.449148, Test Loss = 0.237934, Learning Rate = 5.417163e-04\n",
      "Epoch 1615/20000: Train Loss = 0.446600, Test Loss = 0.253123, Learning Rate = 5.415105e-04\n",
      "Epoch 1616/20000: Train Loss = 0.445305, Test Loss = 0.216703, Learning Rate = 5.413047e-04\n",
      "Epoch 1617/20000: Train Loss = 0.447448, Test Loss = 0.229262, Learning Rate = 5.410991e-04\n",
      "Epoch 1618/20000: Train Loss = 0.446136, Test Loss = 0.227593, Learning Rate = 5.408935e-04\n",
      "Epoch 1619/20000: Train Loss = 0.446280, Test Loss = 0.259313, Learning Rate = 5.406879e-04\n",
      "Epoch 1620/20000: Train Loss = 0.445895, Test Loss = 0.247754, Learning Rate = 5.404825e-04\n",
      "Epoch 1621/20000: Train Loss = 0.445496, Test Loss = 0.261329, Learning Rate = 5.402771e-04\n",
      "Epoch 1622/20000: Train Loss = 0.445759, Test Loss = 0.247217, Learning Rate = 5.400718e-04\n",
      "Epoch 1623/20000: Train Loss = 0.445173, Test Loss = 0.223087, Learning Rate = 5.398666e-04\n",
      "Epoch 1624/20000: Train Loss = 0.446492, Test Loss = 0.237872, Learning Rate = 5.396615e-04\n",
      "Epoch 1625/20000: Train Loss = 0.446138, Test Loss = 0.221542, Learning Rate = 5.394564e-04\n",
      "Epoch 1626/20000: Train Loss = 0.448070, Test Loss = 0.235648, Learning Rate = 5.392514e-04\n",
      "Epoch 1627/20000: Train Loss = 0.446681, Test Loss = 0.249134, Learning Rate = 5.390465e-04\n",
      "Epoch 1628/20000: Train Loss = 0.445790, Test Loss = 0.225412, Learning Rate = 5.388417e-04\n",
      "Epoch 1629/20000: Train Loss = 0.444766, Test Loss = 0.243793, Learning Rate = 5.386370e-04\n",
      "Epoch 1630/20000: Train Loss = 0.444784, Test Loss = 0.241830, Learning Rate = 5.384323e-04\n",
      "Epoch 1631/20000: Train Loss = 0.447298, Test Loss = 0.269620, Learning Rate = 5.382277e-04\n",
      "Epoch 1632/20000: Train Loss = 0.449384, Test Loss = 0.265746, Learning Rate = 5.380232e-04\n",
      "Epoch 1633/20000: Train Loss = 0.448678, Test Loss = 0.216291, Learning Rate = 5.378188e-04\n",
      "Epoch 1634/20000: Train Loss = 0.446376, Test Loss = 0.251917, Learning Rate = 5.376144e-04\n",
      "Epoch 1635/20000: Train Loss = 0.446237, Test Loss = 0.249874, Learning Rate = 5.374101e-04\n",
      "Epoch 1636/20000: Train Loss = 0.446356, Test Loss = 0.241859, Learning Rate = 5.372059e-04\n",
      "Epoch 1637/20000: Train Loss = 0.445922, Test Loss = 0.228960, Learning Rate = 5.370018e-04\n",
      "Epoch 1638/20000: Train Loss = 0.443919, Test Loss = 0.241320, Learning Rate = 5.367978e-04\n",
      "Epoch 1639/20000: Train Loss = 0.446282, Test Loss = 0.272923, Learning Rate = 5.365938e-04\n",
      "Epoch 1640/20000: Train Loss = 0.446075, Test Loss = 0.260900, Learning Rate = 5.363899e-04\n",
      "Epoch 1641/20000: Train Loss = 0.445537, Test Loss = 0.269551, Learning Rate = 5.361861e-04\n",
      "Epoch 1642/20000: Train Loss = 0.445190, Test Loss = 0.228836, Learning Rate = 5.359824e-04\n",
      "Epoch 1643/20000: Train Loss = 0.444848, Test Loss = 0.240153, Learning Rate = 5.357787e-04\n",
      "Epoch 1644/20000: Train Loss = 0.448788, Test Loss = 0.238590, Learning Rate = 5.355751e-04\n",
      "Epoch 1645/20000: Train Loss = 0.446906, Test Loss = 0.236207, Learning Rate = 5.353716e-04\n",
      "Epoch 1646/20000: Train Loss = 0.444485, Test Loss = 0.240425, Learning Rate = 5.351682e-04\n",
      "Epoch 1647/20000: Train Loss = 0.445227, Test Loss = 0.247685, Learning Rate = 5.349648e-04\n",
      "Epoch 1648/20000: Train Loss = 0.446827, Test Loss = 0.220604, Learning Rate = 5.347616e-04\n",
      "Epoch 1649/20000: Train Loss = 0.444104, Test Loss = 0.248121, Learning Rate = 5.345584e-04\n",
      "Epoch 1650/20000: Train Loss = 0.445183, Test Loss = 0.251762, Learning Rate = 5.343552e-04\n",
      "Epoch 1651/20000: Train Loss = 0.444069, Test Loss = 0.241461, Learning Rate = 5.341522e-04\n",
      "Epoch 1652/20000: Train Loss = 0.445145, Test Loss = 0.253386, Learning Rate = 5.339492e-04\n",
      "Epoch 1653/20000: Train Loss = 0.445877, Test Loss = 0.238966, Learning Rate = 5.337464e-04\n",
      "Epoch 1654/20000: Train Loss = 0.446025, Test Loss = 0.249148, Learning Rate = 5.335435e-04\n",
      "Epoch 1655/20000: Train Loss = 0.444496, Test Loss = 0.253034, Learning Rate = 5.333408e-04\n",
      "Epoch 1656/20000: Train Loss = 0.445038, Test Loss = 0.240009, Learning Rate = 5.331382e-04\n",
      "Epoch 1657/20000: Train Loss = 0.445794, Test Loss = 0.270315, Learning Rate = 5.329356e-04\n",
      "Epoch 1658/20000: Train Loss = 0.447538, Test Loss = 0.215347, Learning Rate = 5.327331e-04\n",
      "Epoch 1659/20000: Train Loss = 0.446471, Test Loss = 0.248913, Learning Rate = 5.325307e-04\n",
      "Epoch 1660/20000: Train Loss = 0.443781, Test Loss = 0.219906, Learning Rate = 5.323283e-04\n",
      "Epoch 1661/20000: Train Loss = 0.446791, Test Loss = 0.247065, Learning Rate = 5.321260e-04\n",
      "Epoch 1662/20000: Train Loss = 0.445272, Test Loss = 0.235442, Learning Rate = 5.319238e-04\n",
      "Epoch 1663/20000: Train Loss = 0.444753, Test Loss = 0.242089, Learning Rate = 5.317217e-04\n",
      "Epoch 1664/20000: Train Loss = 0.445495, Test Loss = 0.245621, Learning Rate = 5.315197e-04\n",
      "Epoch 1665/20000: Train Loss = 0.447839, Test Loss = 0.277681, Learning Rate = 5.313177e-04\n",
      "Epoch 1666/20000: Train Loss = 0.448025, Test Loss = 0.246657, Learning Rate = 5.311158e-04\n",
      "Epoch 1667/20000: Train Loss = 0.448735, Test Loss = 0.237213, Learning Rate = 5.309140e-04\n",
      "Epoch 1668/20000: Train Loss = 0.444912, Test Loss = 0.251708, Learning Rate = 5.307123e-04\n",
      "Epoch 1669/20000: Train Loss = 0.447295, Test Loss = 0.229632, Learning Rate = 5.305106e-04\n",
      "Epoch 1670/20000: Train Loss = 0.448711, Test Loss = 0.236158, Learning Rate = 5.303091e-04\n",
      "Epoch 1671/20000: Train Loss = 0.445832, Test Loss = 0.245074, Learning Rate = 5.301076e-04\n",
      "Epoch 1672/20000: Train Loss = 0.445439, Test Loss = 0.234405, Learning Rate = 5.299061e-04\n",
      "Epoch 1673/20000: Train Loss = 0.445416, Test Loss = 0.252151, Learning Rate = 5.297048e-04\n",
      "Epoch 1674/20000: Train Loss = 0.446564, Test Loss = 0.223197, Learning Rate = 5.295035e-04\n",
      "Epoch 1675/20000: Train Loss = 0.449217, Test Loss = 0.212432, Learning Rate = 5.293023e-04\n",
      "Epoch 1676/20000: Train Loss = 0.449841, Test Loss = 0.206975, Learning Rate = 5.291012e-04\n",
      "Epoch 1677/20000: Train Loss = 0.446201, Test Loss = 0.242371, Learning Rate = 5.289001e-04\n",
      "Epoch 1678/20000: Train Loss = 0.443957, Test Loss = 0.235432, Learning Rate = 5.286992e-04\n",
      "Epoch 1679/20000: Train Loss = 0.444523, Test Loss = 0.237416, Learning Rate = 5.284983e-04\n",
      "Epoch 1680/20000: Train Loss = 0.444640, Test Loss = 0.239683, Learning Rate = 5.282975e-04\n",
      "Epoch 1681/20000: Train Loss = 0.444437, Test Loss = 0.248405, Learning Rate = 5.280967e-04\n",
      "Epoch 1682/20000: Train Loss = 0.444156, Test Loss = 0.252254, Learning Rate = 5.278961e-04\n",
      "Epoch 1683/20000: Train Loss = 0.446223, Test Loss = 0.238723, Learning Rate = 5.276955e-04\n",
      "Epoch 1684/20000: Train Loss = 0.446429, Test Loss = 0.220758, Learning Rate = 5.274950e-04\n",
      "Epoch 1685/20000: Train Loss = 0.444282, Test Loss = 0.256182, Learning Rate = 5.272945e-04\n",
      "Epoch 1686/20000: Train Loss = 0.444256, Test Loss = 0.253642, Learning Rate = 5.270942e-04\n",
      "Epoch 1687/20000: Train Loss = 0.445932, Test Loss = 0.253344, Learning Rate = 5.268939e-04\n",
      "Epoch 1688/20000: Train Loss = 0.444677, Test Loss = 0.250028, Learning Rate = 5.266937e-04\n",
      "Epoch 1689/20000: Train Loss = 0.447403, Test Loss = 0.236826, Learning Rate = 5.264936e-04\n",
      "Epoch 1690/20000: Train Loss = 0.447357, Test Loss = 0.288640, Learning Rate = 5.262935e-04\n",
      "Epoch 1691/20000: Train Loss = 0.447833, Test Loss = 0.246926, Learning Rate = 5.260935e-04\n",
      "Epoch 1692/20000: Train Loss = 0.445685, Test Loss = 0.288614, Learning Rate = 5.258936e-04\n",
      "Epoch 1693/20000: Train Loss = 0.445044, Test Loss = 0.229996, Learning Rate = 5.256938e-04\n",
      "Epoch 1694/20000: Train Loss = 0.445984, Test Loss = 0.264403, Learning Rate = 5.254941e-04\n",
      "Epoch 1695/20000: Train Loss = 0.448699, Test Loss = 0.280454, Learning Rate = 5.252944e-04\n",
      "Epoch 1696/20000: Train Loss = 0.453078, Test Loss = 0.257480, Learning Rate = 5.250948e-04\n",
      "Epoch 1697/20000: Train Loss = 0.446056, Test Loss = 0.235821, Learning Rate = 5.248953e-04\n",
      "Epoch 1698/20000: Train Loss = 0.443929, Test Loss = 0.254802, Learning Rate = 5.246958e-04\n",
      "Epoch 1699/20000: Train Loss = 0.444632, Test Loss = 0.240558, Learning Rate = 5.244965e-04\n",
      "Epoch 1700/20000: Train Loss = 0.445899, Test Loss = 0.225398, Learning Rate = 5.242972e-04\n",
      "Epoch 1701/20000: Train Loss = 0.446811, Test Loss = 0.233824, Learning Rate = 5.240979e-04\n",
      "Epoch 1702/20000: Train Loss = 0.444508, Test Loss = 0.242290, Learning Rate = 5.238988e-04\n",
      "Epoch 1703/20000: Train Loss = 0.444742, Test Loss = 0.238430, Learning Rate = 5.236997e-04\n",
      "Epoch 1704/20000: Train Loss = 0.445437, Test Loss = 0.262284, Learning Rate = 5.235007e-04\n",
      "Epoch 1705/20000: Train Loss = 0.444804, Test Loss = 0.241897, Learning Rate = 5.233018e-04\n",
      "Epoch 1706/20000: Train Loss = 0.446538, Test Loss = 0.254716, Learning Rate = 5.231030e-04\n",
      "Epoch 1707/20000: Train Loss = 0.449771, Test Loss = 0.245985, Learning Rate = 5.229042e-04\n",
      "Epoch 1708/20000: Train Loss = 0.446844, Test Loss = 0.244144, Learning Rate = 5.227055e-04\n",
      "Epoch 1709/20000: Train Loss = 0.444886, Test Loss = 0.238682, Learning Rate = 5.225069e-04\n",
      "Epoch 1710/20000: Train Loss = 0.446895, Test Loss = 0.247916, Learning Rate = 5.223084e-04\n",
      "Epoch 1711/20000: Train Loss = 0.444621, Test Loss = 0.243060, Learning Rate = 5.221099e-04\n",
      "Epoch 1712/20000: Train Loss = 0.444197, Test Loss = 0.250334, Learning Rate = 5.219115e-04\n",
      "Epoch 1713/20000: Train Loss = 0.444250, Test Loss = 0.228925, Learning Rate = 5.217132e-04\n",
      "Epoch 1714/20000: Train Loss = 0.446043, Test Loss = 0.264698, Learning Rate = 5.215150e-04\n",
      "Epoch 1715/20000: Train Loss = 0.443818, Test Loss = 0.247661, Learning Rate = 5.213168e-04\n",
      "Epoch 1716/20000: Train Loss = 0.444058, Test Loss = 0.247080, Learning Rate = 5.211187e-04\n",
      "Epoch 1717/20000: Train Loss = 0.447769, Test Loss = 0.255365, Learning Rate = 5.209207e-04\n",
      "Epoch 1718/20000: Train Loss = 0.447613, Test Loss = 0.221351, Learning Rate = 5.207228e-04\n",
      "Epoch 1719/20000: Train Loss = 0.446641, Test Loss = 0.238982, Learning Rate = 5.205249e-04\n",
      "Epoch 1720/20000: Train Loss = 0.448940, Test Loss = 0.242749, Learning Rate = 5.203271e-04\n",
      "Epoch 1721/20000: Train Loss = 0.445563, Test Loss = 0.248167, Learning Rate = 5.201294e-04\n",
      "Epoch 1722/20000: Train Loss = 0.445491, Test Loss = 0.252923, Learning Rate = 5.199318e-04\n",
      "Epoch 1723/20000: Train Loss = 0.444435, Test Loss = 0.229962, Learning Rate = 5.197342e-04\n",
      "Epoch 1724/20000: Train Loss = 0.447823, Test Loss = 0.223483, Learning Rate = 5.195367e-04\n",
      "Epoch 1725/20000: Train Loss = 0.445614, Test Loss = 0.232401, Learning Rate = 5.193393e-04\n",
      "Epoch 1726/20000: Train Loss = 0.445861, Test Loss = 0.233089, Learning Rate = 5.191420e-04\n",
      "Epoch 1727/20000: Train Loss = 0.445116, Test Loss = 0.245930, Learning Rate = 5.189447e-04\n",
      "Epoch 1728/20000: Train Loss = 0.444441, Test Loss = 0.228097, Learning Rate = 5.187476e-04\n",
      "Epoch 1729/20000: Train Loss = 0.446176, Test Loss = 0.236905, Learning Rate = 5.185504e-04\n",
      "Epoch 1730/20000: Train Loss = 0.446140, Test Loss = 0.222428, Learning Rate = 5.183534e-04\n",
      "Epoch 1731/20000: Train Loss = 0.445736, Test Loss = 0.229195, Learning Rate = 5.181564e-04\n",
      "Epoch 1732/20000: Train Loss = 0.446271, Test Loss = 0.227177, Learning Rate = 5.179596e-04\n",
      "Epoch 1733/20000: Train Loss = 0.443608, Test Loss = 0.277243, Learning Rate = 5.177628e-04\n",
      "Epoch 1734/20000: Train Loss = 0.451515, Test Loss = 0.255777, Learning Rate = 5.175660e-04\n",
      "Epoch 1735/20000: Train Loss = 0.447458, Test Loss = 0.248724, Learning Rate = 5.173694e-04\n",
      "Epoch 1736/20000: Train Loss = 0.444940, Test Loss = 0.226994, Learning Rate = 5.171728e-04\n",
      "Epoch 1737/20000: Train Loss = 0.446405, Test Loss = 0.226041, Learning Rate = 5.169763e-04\n",
      "Epoch 1738/20000: Train Loss = 0.446450, Test Loss = 0.222179, Learning Rate = 5.167798e-04\n",
      "Epoch 1739/20000: Train Loss = 0.447319, Test Loss = 0.217371, Learning Rate = 5.165835e-04\n",
      "Epoch 1740/20000: Train Loss = 0.445361, Test Loss = 0.250173, Learning Rate = 5.163872e-04\n",
      "Epoch 1741/20000: Train Loss = 0.448372, Test Loss = 0.246288, Learning Rate = 5.161910e-04\n",
      "Epoch 1742/20000: Train Loss = 0.444842, Test Loss = 0.241587, Learning Rate = 5.159948e-04\n",
      "Epoch 1743/20000: Train Loss = 0.446202, Test Loss = 0.236350, Learning Rate = 5.157988e-04\n",
      "Epoch 1744/20000: Train Loss = 0.444072, Test Loss = 0.253941, Learning Rate = 5.156028e-04\n",
      "Epoch 1745/20000: Train Loss = 0.446245, Test Loss = 0.229587, Learning Rate = 5.154068e-04\n",
      "Epoch 1746/20000: Train Loss = 0.445267, Test Loss = 0.225875, Learning Rate = 5.152110e-04\n",
      "Epoch 1747/20000: Train Loss = 0.446792, Test Loss = 0.227693, Learning Rate = 5.150152e-04\n",
      "Epoch 1748/20000: Train Loss = 0.444468, Test Loss = 0.258470, Learning Rate = 5.148195e-04\n",
      "Epoch 1749/20000: Train Loss = 0.444562, Test Loss = 0.224768, Learning Rate = 5.146239e-04\n",
      "Epoch 1750/20000: Train Loss = 0.446662, Test Loss = 0.261049, Learning Rate = 5.144284e-04\n",
      "Epoch 1751/20000: Train Loss = 0.447056, Test Loss = 0.252356, Learning Rate = 5.142329e-04\n",
      "Epoch 1752/20000: Train Loss = 0.447095, Test Loss = 0.247001, Learning Rate = 5.140375e-04\n",
      "Epoch 1753/20000: Train Loss = 0.445005, Test Loss = 0.234756, Learning Rate = 5.138422e-04\n",
      "Epoch 1754/20000: Train Loss = 0.448296, Test Loss = 0.262446, Learning Rate = 5.136470e-04\n",
      "Epoch 1755/20000: Train Loss = 0.446489, Test Loss = 0.265395, Learning Rate = 5.134518e-04\n",
      "Epoch 1756/20000: Train Loss = 0.449041, Test Loss = 0.238192, Learning Rate = 5.132567e-04\n",
      "Epoch 1757/20000: Train Loss = 0.444760, Test Loss = 0.235989, Learning Rate = 5.130617e-04\n",
      "Epoch 1758/20000: Train Loss = 0.447814, Test Loss = 0.238859, Learning Rate = 5.128667e-04\n",
      "Epoch 1759/20000: Train Loss = 0.445376, Test Loss = 0.246979, Learning Rate = 5.126718e-04\n",
      "Epoch 1760/20000: Train Loss = 0.445265, Test Loss = 0.225722, Learning Rate = 5.124770e-04\n",
      "Epoch 1761/20000: Train Loss = 0.444185, Test Loss = 0.239862, Learning Rate = 5.122823e-04\n",
      "Epoch 1762/20000: Train Loss = 0.446436, Test Loss = 0.245981, Learning Rate = 5.120877e-04\n",
      "Epoch 1763/20000: Train Loss = 0.446649, Test Loss = 0.244548, Learning Rate = 5.118931e-04\n",
      "Epoch 1764/20000: Train Loss = 0.445629, Test Loss = 0.227423, Learning Rate = 5.116986e-04\n",
      "Epoch 1765/20000: Train Loss = 0.445612, Test Loss = 0.240869, Learning Rate = 5.115041e-04\n",
      "Epoch 1766/20000: Train Loss = 0.445910, Test Loss = 0.236692, Learning Rate = 5.113098e-04\n",
      "Epoch 1767/20000: Train Loss = 0.446067, Test Loss = 0.261674, Learning Rate = 5.111155e-04\n",
      "Epoch 1768/20000: Train Loss = 0.444280, Test Loss = 0.231780, Learning Rate = 5.109213e-04\n",
      "Epoch 1769/20000: Train Loss = 0.446593, Test Loss = 0.240651, Learning Rate = 5.107272e-04\n",
      "Epoch 1770/20000: Train Loss = 0.444818, Test Loss = 0.239746, Learning Rate = 5.105331e-04\n",
      "Epoch 1771/20000: Train Loss = 0.445749, Test Loss = 0.222283, Learning Rate = 5.103391e-04\n",
      "Epoch 1772/20000: Train Loss = 0.444055, Test Loss = 0.261771, Learning Rate = 5.101452e-04\n",
      "Epoch 1773/20000: Train Loss = 0.446051, Test Loss = 0.251170, Learning Rate = 5.099513e-04\n",
      "Epoch 1774/20000: Train Loss = 0.444968, Test Loss = 0.251471, Learning Rate = 5.097576e-04\n",
      "Epoch 1775/20000: Train Loss = 0.445643, Test Loss = 0.244828, Learning Rate = 5.095639e-04\n",
      "Epoch 1776/20000: Train Loss = 0.444876, Test Loss = 0.242420, Learning Rate = 5.093703e-04\n",
      "Epoch 1777/20000: Train Loss = 0.443464, Test Loss = 0.254035, Learning Rate = 5.091767e-04\n",
      "Epoch 1778/20000: Train Loss = 0.446064, Test Loss = 0.274990, Learning Rate = 5.089832e-04\n",
      "Epoch 1779/20000: Train Loss = 0.446884, Test Loss = 0.263294, Learning Rate = 5.087898e-04\n",
      "Epoch 1780/20000: Train Loss = 0.446313, Test Loss = 0.227896, Learning Rate = 5.085965e-04\n",
      "Epoch 1781/20000: Train Loss = 0.445035, Test Loss = 0.227934, Learning Rate = 5.084033e-04\n",
      "Epoch 1782/20000: Train Loss = 0.446545, Test Loss = 0.224956, Learning Rate = 5.082101e-04\n",
      "Epoch 1783/20000: Train Loss = 0.444905, Test Loss = 0.253037, Learning Rate = 5.080170e-04\n",
      "Epoch 1784/20000: Train Loss = 0.444646, Test Loss = 0.240359, Learning Rate = 5.078239e-04\n",
      "Epoch 1785/20000: Train Loss = 0.445320, Test Loss = 0.217409, Learning Rate = 5.076310e-04\n",
      "Epoch 1786/20000: Train Loss = 0.449759, Test Loss = 0.256526, Learning Rate = 5.074381e-04\n",
      "Epoch 1787/20000: Train Loss = 0.445920, Test Loss = 0.262586, Learning Rate = 5.072453e-04\n",
      "Epoch 1788/20000: Train Loss = 0.445581, Test Loss = 0.256007, Learning Rate = 5.070525e-04\n",
      "Epoch 1789/20000: Train Loss = 0.444412, Test Loss = 0.250961, Learning Rate = 5.068599e-04\n",
      "Epoch 1790/20000: Train Loss = 0.446682, Test Loss = 0.235343, Learning Rate = 5.066673e-04\n",
      "Epoch 1791/20000: Train Loss = 0.445576, Test Loss = 0.283385, Learning Rate = 5.064748e-04\n",
      "Epoch 1792/20000: Train Loss = 0.446557, Test Loss = 0.262024, Learning Rate = 5.062823e-04\n",
      "Epoch 1793/20000: Train Loss = 0.446593, Test Loss = 0.237661, Learning Rate = 5.060899e-04\n",
      "Epoch 1794/20000: Train Loss = 0.446341, Test Loss = 0.259449, Learning Rate = 5.058976e-04\n",
      "Epoch 1795/20000: Train Loss = 0.444798, Test Loss = 0.218726, Learning Rate = 5.057054e-04\n",
      "Epoch 1796/20000: Train Loss = 0.444388, Test Loss = 0.232909, Learning Rate = 5.055133e-04\n",
      "Epoch 1797/20000: Train Loss = 0.446699, Test Loss = 0.244992, Learning Rate = 5.053212e-04\n",
      "Epoch 1798/20000: Train Loss = 0.446039, Test Loss = 0.243296, Learning Rate = 5.051292e-04\n",
      "Epoch 1799/20000: Train Loss = 0.443937, Test Loss = 0.245402, Learning Rate = 5.049372e-04\n",
      "Epoch 1800/20000: Train Loss = 0.443712, Test Loss = 0.244855, Learning Rate = 5.047454e-04\n",
      "Epoch 1801/20000: Train Loss = 0.446383, Test Loss = 0.263126, Learning Rate = 5.045536e-04\n",
      "Epoch 1802/20000: Train Loss = 0.444702, Test Loss = 0.251789, Learning Rate = 5.043619e-04\n",
      "Epoch 1803/20000: Train Loss = 0.445132, Test Loss = 0.244198, Learning Rate = 5.041702e-04\n",
      "Epoch 1804/20000: Train Loss = 0.445481, Test Loss = 0.228497, Learning Rate = 5.039787e-04\n",
      "Epoch 1805/20000: Train Loss = 0.445659, Test Loss = 0.257279, Learning Rate = 5.037872e-04\n",
      "Epoch 1806/20000: Train Loss = 0.448505, Test Loss = 0.242445, Learning Rate = 5.035957e-04\n",
      "Epoch 1807/20000: Train Loss = 0.446078, Test Loss = 0.259608, Learning Rate = 5.034044e-04\n",
      "Epoch 1808/20000: Train Loss = 0.444951, Test Loss = 0.227469, Learning Rate = 5.032131e-04\n",
      "Epoch 1809/20000: Train Loss = 0.444175, Test Loss = 0.255858, Learning Rate = 5.030219e-04\n",
      "Epoch 1810/20000: Train Loss = 0.445604, Test Loss = 0.251279, Learning Rate = 5.028308e-04\n",
      "Epoch 1811/20000: Train Loss = 0.445666, Test Loss = 0.230775, Learning Rate = 5.026397e-04\n",
      "Epoch 1812/20000: Train Loss = 0.445434, Test Loss = 0.252320, Learning Rate = 5.024487e-04\n",
      "Epoch 1813/20000: Train Loss = 0.444192, Test Loss = 0.244675, Learning Rate = 5.022578e-04\n",
      "Epoch 1814/20000: Train Loss = 0.448094, Test Loss = 0.229225, Learning Rate = 5.020669e-04\n",
      "Epoch 1815/20000: Train Loss = 0.445657, Test Loss = 0.256472, Learning Rate = 5.018762e-04\n",
      "Epoch 1816/20000: Train Loss = 0.444452, Test Loss = 0.253826, Learning Rate = 5.016855e-04\n",
      "Epoch 1817/20000: Train Loss = 0.444963, Test Loss = 0.237693, Learning Rate = 5.014948e-04\n",
      "Epoch 1818/20000: Train Loss = 0.445797, Test Loss = 0.255945, Learning Rate = 5.013043e-04\n",
      "Epoch 1819/20000: Train Loss = 0.445605, Test Loss = 0.235392, Learning Rate = 5.011138e-04\n",
      "Epoch 1820/20000: Train Loss = 0.444390, Test Loss = 0.240691, Learning Rate = 5.009234e-04\n",
      "Epoch 1821/20000: Train Loss = 0.445144, Test Loss = 0.232110, Learning Rate = 5.007331e-04\n",
      "Epoch 1822/20000: Train Loss = 0.446694, Test Loss = 0.242863, Learning Rate = 5.005428e-04\n",
      "Epoch 1823/20000: Train Loss = 0.444895, Test Loss = 0.245938, Learning Rate = 5.003526e-04\n",
      "Epoch 1824/20000: Train Loss = 0.444975, Test Loss = 0.238691, Learning Rate = 5.001625e-04\n",
      "Epoch 1825/20000: Train Loss = 0.445125, Test Loss = 0.240656, Learning Rate = 4.999724e-04\n",
      "Epoch 1826/20000: Train Loss = 0.449270, Test Loss = 0.237376, Learning Rate = 4.997825e-04\n",
      "Epoch 1827/20000: Train Loss = 0.447094, Test Loss = 0.239609, Learning Rate = 4.995926e-04\n",
      "Epoch 1828/20000: Train Loss = 0.446551, Test Loss = 0.240254, Learning Rate = 4.994027e-04\n",
      "Epoch 1829/20000: Train Loss = 0.445953, Test Loss = 0.278439, Learning Rate = 4.992130e-04\n",
      "Epoch 1830/20000: Train Loss = 0.448631, Test Loss = 0.256607, Learning Rate = 4.990233e-04\n",
      "Epoch 1831/20000: Train Loss = 0.446047, Test Loss = 0.279848, Learning Rate = 4.988337e-04\n",
      "Epoch 1832/20000: Train Loss = 0.445672, Test Loss = 0.253770, Learning Rate = 4.986441e-04\n",
      "Epoch 1833/20000: Train Loss = 0.446175, Test Loss = 0.254337, Learning Rate = 4.984547e-04\n",
      "Epoch 1834/20000: Train Loss = 0.448154, Test Loss = 0.255430, Learning Rate = 4.982653e-04\n",
      "Epoch 1835/20000: Train Loss = 0.446862, Test Loss = 0.231546, Learning Rate = 4.980759e-04\n",
      "Epoch 1836/20000: Train Loss = 0.447527, Test Loss = 0.234762, Learning Rate = 4.978867e-04\n",
      "Epoch 1837/20000: Train Loss = 0.445243, Test Loss = 0.239891, Learning Rate = 4.976975e-04\n",
      "Epoch 1838/20000: Train Loss = 0.446914, Test Loss = 0.247310, Learning Rate = 4.975084e-04\n",
      "Epoch 1839/20000: Train Loss = 0.444646, Test Loss = 0.246419, Learning Rate = 4.973193e-04\n",
      "Epoch 1840/20000: Train Loss = 0.445407, Test Loss = 0.257680, Learning Rate = 4.971304e-04\n",
      "Epoch 1841/20000: Train Loss = 0.445578, Test Loss = 0.261711, Learning Rate = 4.969415e-04\n",
      "Epoch 1842/20000: Train Loss = 0.445726, Test Loss = 0.252245, Learning Rate = 4.967526e-04\n",
      "Epoch 1843/20000: Train Loss = 0.446197, Test Loss = 0.240600, Learning Rate = 4.965639e-04\n",
      "Epoch 1844/20000: Train Loss = 0.445392, Test Loss = 0.262127, Learning Rate = 4.963752e-04\n",
      "Epoch 1845/20000: Train Loss = 0.446703, Test Loss = 0.243132, Learning Rate = 4.961866e-04\n",
      "Epoch 1846/20000: Train Loss = 0.447074, Test Loss = 0.225242, Learning Rate = 4.959981e-04\n",
      "Epoch 1847/20000: Train Loss = 0.446058, Test Loss = 0.259284, Learning Rate = 4.958096e-04\n",
      "Epoch 1848/20000: Train Loss = 0.444177, Test Loss = 0.219007, Learning Rate = 4.956212e-04\n",
      "Epoch 1849/20000: Train Loss = 0.445192, Test Loss = 0.263281, Learning Rate = 4.954329e-04\n",
      "Epoch 1850/20000: Train Loss = 0.443835, Test Loss = 0.236693, Learning Rate = 4.952446e-04\n",
      "Epoch 1851/20000: Train Loss = 0.444414, Test Loss = 0.258164, Learning Rate = 4.950565e-04\n",
      "Epoch 1852/20000: Train Loss = 0.446130, Test Loss = 0.236883, Learning Rate = 4.948683e-04\n",
      "Epoch 1853/20000: Train Loss = 0.445587, Test Loss = 0.234654, Learning Rate = 4.946803e-04\n",
      "Epoch 1854/20000: Train Loss = 0.445126, Test Loss = 0.246060, Learning Rate = 4.944923e-04\n",
      "Epoch 1855/20000: Train Loss = 0.444733, Test Loss = 0.231483, Learning Rate = 4.943044e-04\n",
      "Epoch 1856/20000: Train Loss = 0.446741, Test Loss = 0.228836, Learning Rate = 4.941166e-04\n",
      "Epoch 1857/20000: Train Loss = 0.444502, Test Loss = 0.240783, Learning Rate = 4.939289e-04\n",
      "Epoch 1858/20000: Train Loss = 0.444253, Test Loss = 0.231776, Learning Rate = 4.937412e-04\n",
      "Epoch 1859/20000: Train Loss = 0.445016, Test Loss = 0.251275, Learning Rate = 4.935536e-04\n",
      "Epoch 1860/20000: Train Loss = 0.446838, Test Loss = 0.231090, Learning Rate = 4.933661e-04\n",
      "Epoch 1861/20000: Train Loss = 0.445257, Test Loss = 0.241859, Learning Rate = 4.931786e-04\n",
      "Epoch 1862/20000: Train Loss = 0.444501, Test Loss = 0.246834, Learning Rate = 4.929912e-04\n",
      "Epoch 1863/20000: Train Loss = 0.445097, Test Loss = 0.231504, Learning Rate = 4.928039e-04\n",
      "Epoch 1864/20000: Train Loss = 0.447989, Test Loss = 0.239660, Learning Rate = 4.926166e-04\n",
      "Epoch 1865/20000: Train Loss = 0.445273, Test Loss = 0.251993, Learning Rate = 4.924294e-04\n",
      "Epoch 1866/20000: Train Loss = 0.444648, Test Loss = 0.257338, Learning Rate = 4.922423e-04\n",
      "Epoch 1867/20000: Train Loss = 0.446094, Test Loss = 0.259672, Learning Rate = 4.920553e-04\n",
      "Epoch 1868/20000: Train Loss = 0.445261, Test Loss = 0.230158, Learning Rate = 4.918683e-04\n",
      "Epoch 1869/20000: Train Loss = 0.445712, Test Loss = 0.240655, Learning Rate = 4.916814e-04\n",
      "Epoch 1870/20000: Train Loss = 0.445770, Test Loss = 0.241241, Learning Rate = 4.914946e-04\n",
      "Epoch 1871/20000: Train Loss = 0.446070, Test Loss = 0.258373, Learning Rate = 4.913078e-04\n",
      "Epoch 1872/20000: Train Loss = 0.441650, Test Loss = 0.226757, Learning Rate = 4.911212e-04\n",
      "Epoch 1873/20000: Train Loss = 0.451968, Test Loss = 0.233465, Learning Rate = 4.909345e-04\n",
      "Epoch 1874/20000: Train Loss = 0.446239, Test Loss = 0.244008, Learning Rate = 4.907480e-04\n",
      "Epoch 1875/20000: Train Loss = 0.444644, Test Loss = 0.228945, Learning Rate = 4.905615e-04\n",
      "Epoch 1876/20000: Train Loss = 0.446646, Test Loss = 0.251858, Learning Rate = 4.903751e-04\n",
      "Epoch 1877/20000: Train Loss = 0.447729, Test Loss = 0.249962, Learning Rate = 4.901888e-04\n",
      "Epoch 1878/20000: Train Loss = 0.445598, Test Loss = 0.234756, Learning Rate = 4.900025e-04\n",
      "Epoch 1879/20000: Train Loss = 0.444084, Test Loss = 0.262096, Learning Rate = 4.898164e-04\n",
      "Epoch 1880/20000: Train Loss = 0.444141, Test Loss = 0.244524, Learning Rate = 4.896302e-04\n",
      "Epoch 1881/20000: Train Loss = 0.444869, Test Loss = 0.267993, Learning Rate = 4.894442e-04\n",
      "Epoch 1882/20000: Train Loss = 0.446255, Test Loss = 0.228334, Learning Rate = 4.892582e-04\n",
      "Epoch 1883/20000: Train Loss = 0.444129, Test Loss = 0.244990, Learning Rate = 4.890723e-04\n",
      "Epoch 1884/20000: Train Loss = 0.446605, Test Loss = 0.231516, Learning Rate = 4.888865e-04\n",
      "Epoch 1885/20000: Train Loss = 0.446702, Test Loss = 0.234655, Learning Rate = 4.887007e-04\n",
      "Epoch 1886/20000: Train Loss = 0.451500, Test Loss = 0.288943, Learning Rate = 4.885150e-04\n",
      "Epoch 1887/20000: Train Loss = 0.447463, Test Loss = 0.261242, Learning Rate = 4.883294e-04\n",
      "Epoch 1888/20000: Train Loss = 0.446455, Test Loss = 0.220773, Learning Rate = 4.881438e-04\n",
      "Epoch 1889/20000: Train Loss = 0.447444, Test Loss = 0.255466, Learning Rate = 4.879584e-04\n",
      "Epoch 1890/20000: Train Loss = 0.445262, Test Loss = 0.238488, Learning Rate = 4.877730e-04\n",
      "Epoch 1891/20000: Train Loss = 0.445100, Test Loss = 0.241300, Learning Rate = 4.875876e-04\n",
      "Epoch 1892/20000: Train Loss = 0.447737, Test Loss = 0.225883, Learning Rate = 4.874023e-04\n",
      "Epoch 1893/20000: Train Loss = 0.444901, Test Loss = 0.237053, Learning Rate = 4.872171e-04\n",
      "Epoch 1894/20000: Train Loss = 0.445612, Test Loss = 0.239207, Learning Rate = 4.870320e-04\n",
      "Epoch 1895/20000: Train Loss = 0.445672, Test Loss = 0.248630, Learning Rate = 4.868470e-04\n",
      "Epoch 1896/20000: Train Loss = 0.444807, Test Loss = 0.238759, Learning Rate = 4.866620e-04\n",
      "Epoch 1897/20000: Train Loss = 0.447011, Test Loss = 0.237508, Learning Rate = 4.864770e-04\n",
      "Epoch 1898/20000: Train Loss = 0.449702, Test Loss = 0.248215, Learning Rate = 4.862922e-04\n",
      "Epoch 1899/20000: Train Loss = 0.444896, Test Loss = 0.230620, Learning Rate = 4.861074e-04\n",
      "Epoch 1900/20000: Train Loss = 0.443538, Test Loss = 0.268785, Learning Rate = 4.859227e-04\n",
      "Epoch 1901/20000: Train Loss = 0.449599, Test Loss = 0.259180, Learning Rate = 4.857381e-04\n",
      "Epoch 1902/20000: Train Loss = 0.444650, Test Loss = 0.256842, Learning Rate = 4.855535e-04\n",
      "Epoch 1903/20000: Train Loss = 0.445497, Test Loss = 0.253178, Learning Rate = 4.853690e-04\n",
      "Epoch 1904/20000: Train Loss = 0.446144, Test Loss = 0.261264, Learning Rate = 4.851846e-04\n",
      "Epoch 1905/20000: Train Loss = 0.446350, Test Loss = 0.255138, Learning Rate = 4.850002e-04\n",
      "Epoch 1906/20000: Train Loss = 0.446142, Test Loss = 0.227905, Learning Rate = 4.848159e-04\n",
      "Epoch 1907/20000: Train Loss = 0.446974, Test Loss = 0.227219, Learning Rate = 4.846317e-04\n",
      "Epoch 1908/20000: Train Loss = 0.443923, Test Loss = 0.257003, Learning Rate = 4.844476e-04\n",
      "Epoch 1909/20000: Train Loss = 0.445738, Test Loss = 0.241745, Learning Rate = 4.842635e-04\n",
      "Epoch 1910/20000: Train Loss = 0.444513, Test Loss = 0.240133, Learning Rate = 4.840795e-04\n",
      "Epoch 1911/20000: Train Loss = 0.446789, Test Loss = 0.257158, Learning Rate = 4.838956e-04\n",
      "Epoch 1912/20000: Train Loss = 0.444588, Test Loss = 0.234858, Learning Rate = 4.837117e-04\n",
      "Epoch 1913/20000: Train Loss = 0.446526, Test Loss = 0.250267, Learning Rate = 4.835279e-04\n",
      "Epoch 1914/20000: Train Loss = 0.445152, Test Loss = 0.245138, Learning Rate = 4.833442e-04\n",
      "Epoch 1915/20000: Train Loss = 0.444461, Test Loss = 0.245098, Learning Rate = 4.831605e-04\n",
      "Epoch 1916/20000: Train Loss = 0.446425, Test Loss = 0.236162, Learning Rate = 4.829769e-04\n",
      "Epoch 1917/20000: Train Loss = 0.444268, Test Loss = 0.253457, Learning Rate = 4.827934e-04\n",
      "Epoch 1918/20000: Train Loss = 0.445673, Test Loss = 0.238053, Learning Rate = 4.826100e-04\n",
      "Epoch 1919/20000: Train Loss = 0.445256, Test Loss = 0.257852, Learning Rate = 4.824266e-04\n",
      "Epoch 1920/20000: Train Loss = 0.445670, Test Loss = 0.257211, Learning Rate = 4.822433e-04\n",
      "Epoch 1921/20000: Train Loss = 0.446814, Test Loss = 0.253230, Learning Rate = 4.820600e-04\n",
      "Epoch 1922/20000: Train Loss = 0.444592, Test Loss = 0.249212, Learning Rate = 4.818769e-04\n",
      "Epoch 1923/20000: Train Loss = 0.447404, Test Loss = 0.258772, Learning Rate = 4.816938e-04\n",
      "Epoch 1924/20000: Train Loss = 0.446406, Test Loss = 0.259890, Learning Rate = 4.815107e-04\n",
      "Epoch 1925/20000: Train Loss = 0.445317, Test Loss = 0.233042, Learning Rate = 4.813278e-04\n",
      "Epoch 1926/20000: Train Loss = 0.446954, Test Loss = 0.232744, Learning Rate = 4.811449e-04\n",
      "Epoch 1927/20000: Train Loss = 0.446368, Test Loss = 0.276581, Learning Rate = 4.809621e-04\n",
      "Epoch 1928/20000: Train Loss = 0.444386, Test Loss = 0.238380, Learning Rate = 4.807793e-04\n",
      "Epoch 1929/20000: Train Loss = 0.444476, Test Loss = 0.249415, Learning Rate = 4.805966e-04\n",
      "Epoch 1930/20000: Train Loss = 0.446159, Test Loss = 0.237848, Learning Rate = 4.804140e-04\n",
      "Epoch 1931/20000: Train Loss = 0.445780, Test Loss = 0.227532, Learning Rate = 4.802315e-04\n",
      "Epoch 1932/20000: Train Loss = 0.447450, Test Loss = 0.228193, Learning Rate = 4.800490e-04\n",
      "Epoch 1933/20000: Train Loss = 0.444132, Test Loss = 0.252097, Learning Rate = 4.798666e-04\n",
      "Epoch 1934/20000: Train Loss = 0.444803, Test Loss = 0.243112, Learning Rate = 4.796842e-04\n",
      "Epoch 1935/20000: Train Loss = 0.443977, Test Loss = 0.235299, Learning Rate = 4.795020e-04\n",
      "Epoch 1936/20000: Train Loss = 0.445394, Test Loss = 0.257356, Learning Rate = 4.793198e-04\n",
      "Epoch 1937/20000: Train Loss = 0.445540, Test Loss = 0.223399, Learning Rate = 4.791376e-04\n",
      "Epoch 1938/20000: Train Loss = 0.443806, Test Loss = 0.236459, Learning Rate = 4.789556e-04\n",
      "Epoch 1939/20000: Train Loss = 0.445207, Test Loss = 0.229871, Learning Rate = 4.787736e-04\n",
      "Epoch 1940/20000: Train Loss = 0.445592, Test Loss = 0.255207, Learning Rate = 4.785917e-04\n",
      "Epoch 1941/20000: Train Loss = 0.444266, Test Loss = 0.246734, Learning Rate = 4.784098e-04\n",
      "Epoch 1942/20000: Train Loss = 0.445669, Test Loss = 0.234218, Learning Rate = 4.782280e-04\n",
      "Epoch 1943/20000: Train Loss = 0.452999, Test Loss = 0.252783, Learning Rate = 4.780463e-04\n",
      "Epoch 1944/20000: Train Loss = 0.444769, Test Loss = 0.263809, Learning Rate = 4.778647e-04\n",
      "Epoch 1945/20000: Train Loss = 0.445859, Test Loss = 0.230833, Learning Rate = 4.776831e-04\n",
      "Epoch 1946/20000: Train Loss = 0.445216, Test Loss = 0.251730, Learning Rate = 4.775016e-04\n",
      "Epoch 1947/20000: Train Loss = 0.447028, Test Loss = 0.236539, Learning Rate = 4.773202e-04\n",
      "Epoch 1948/20000: Train Loss = 0.446679, Test Loss = 0.257168, Learning Rate = 4.771388e-04\n",
      "Epoch 1949/20000: Train Loss = 0.445232, Test Loss = 0.256584, Learning Rate = 4.769575e-04\n",
      "Epoch 1950/20000: Train Loss = 0.443963, Test Loss = 0.254211, Learning Rate = 4.767763e-04\n",
      "Epoch 1951/20000: Train Loss = 0.446367, Test Loss = 0.248179, Learning Rate = 4.765951e-04\n",
      "Epoch 1952/20000: Train Loss = 0.444923, Test Loss = 0.258871, Learning Rate = 4.764140e-04\n",
      "Epoch 1953/20000: Train Loss = 0.447015, Test Loss = 0.223901, Learning Rate = 4.762330e-04\n",
      "Epoch 1954/20000: Train Loss = 0.446565, Test Loss = 0.256162, Learning Rate = 4.760520e-04\n",
      "Epoch 1955/20000: Train Loss = 0.444928, Test Loss = 0.243334, Learning Rate = 4.758711e-04\n",
      "Epoch 1956/20000: Train Loss = 0.445090, Test Loss = 0.236006, Learning Rate = 4.756903e-04\n",
      "Epoch 1957/20000: Train Loss = 0.445810, Test Loss = 0.208032, Learning Rate = 4.755096e-04\n",
      "Epoch 1958/20000: Train Loss = 0.446696, Test Loss = 0.232956, Learning Rate = 4.753289e-04\n",
      "Epoch 1959/20000: Train Loss = 0.445437, Test Loss = 0.249873, Learning Rate = 4.751483e-04\n",
      "Epoch 1960/20000: Train Loss = 0.444991, Test Loss = 0.236006, Learning Rate = 4.749677e-04\n",
      "Epoch 1961/20000: Train Loss = 0.444758, Test Loss = 0.237191, Learning Rate = 4.747873e-04\n",
      "Epoch 1962/20000: Train Loss = 0.443562, Test Loss = 0.254506, Learning Rate = 4.746069e-04\n",
      "Epoch 1963/20000: Train Loss = 0.444940, Test Loss = 0.257837, Learning Rate = 4.744265e-04\n",
      "Epoch 1964/20000: Train Loss = 0.446776, Test Loss = 0.264744, Learning Rate = 4.742463e-04\n",
      "Epoch 1965/20000: Train Loss = 0.444377, Test Loss = 0.241304, Learning Rate = 4.740660e-04\n",
      "Epoch 1966/20000: Train Loss = 0.444776, Test Loss = 0.248558, Learning Rate = 4.738859e-04\n",
      "Epoch 1967/20000: Train Loss = 0.446203, Test Loss = 0.226598, Learning Rate = 4.737059e-04\n",
      "Epoch 1968/20000: Train Loss = 0.446519, Test Loss = 0.225248, Learning Rate = 4.735259e-04\n",
      "Epoch 1969/20000: Train Loss = 0.444955, Test Loss = 0.257548, Learning Rate = 4.733459e-04\n",
      "Epoch 1970/20000: Train Loss = 0.444973, Test Loss = 0.237017, Learning Rate = 4.731661e-04\n",
      "Epoch 1971/20000: Train Loss = 0.444412, Test Loss = 0.241892, Learning Rate = 4.729863e-04\n",
      "Epoch 1972/20000: Train Loss = 0.447549, Test Loss = 0.249358, Learning Rate = 4.728066e-04\n",
      "Epoch 1973/20000: Train Loss = 0.445297, Test Loss = 0.256525, Learning Rate = 4.726269e-04\n",
      "Epoch 1974/20000: Train Loss = 0.445147, Test Loss = 0.243499, Learning Rate = 4.724473e-04\n",
      "Epoch 1975/20000: Train Loss = 0.445622, Test Loss = 0.254692, Learning Rate = 4.722678e-04\n",
      "Epoch 1976/20000: Train Loss = 0.444314, Test Loss = 0.240781, Learning Rate = 4.720884e-04\n",
      "Epoch 1977/20000: Train Loss = 0.443800, Test Loss = 0.237602, Learning Rate = 4.719090e-04\n",
      "Epoch 1978/20000: Train Loss = 0.445610, Test Loss = 0.233659, Learning Rate = 4.717297e-04\n",
      "Epoch 1979/20000: Train Loss = 0.444334, Test Loss = 0.243090, Learning Rate = 4.715504e-04\n",
      "Epoch 1980/20000: Train Loss = 0.444992, Test Loss = 0.239687, Learning Rate = 4.713712e-04\n",
      "Epoch 1981/20000: Train Loss = 0.447474, Test Loss = 0.234300, Learning Rate = 4.711921e-04\n",
      "Epoch 1982/20000: Train Loss = 0.445040, Test Loss = 0.254273, Learning Rate = 4.710131e-04\n",
      "Epoch 1983/20000: Train Loss = 0.445119, Test Loss = 0.231789, Learning Rate = 4.708341e-04\n",
      "Epoch 1984/20000: Train Loss = 0.445474, Test Loss = 0.234837, Learning Rate = 4.706552e-04\n",
      "Epoch 1985/20000: Train Loss = 0.444505, Test Loss = 0.236058, Learning Rate = 4.704764e-04\n",
      "Epoch 1986/20000: Train Loss = 0.445240, Test Loss = 0.237670, Learning Rate = 4.702976e-04\n",
      "Epoch 1987/20000: Train Loss = 0.444788, Test Loss = 0.241189, Learning Rate = 4.701189e-04\n",
      "Epoch 1988/20000: Train Loss = 0.444834, Test Loss = 0.234762, Learning Rate = 4.699403e-04\n",
      "Epoch 1989/20000: Train Loss = 0.443528, Test Loss = 0.243058, Learning Rate = 4.697617e-04\n",
      "Epoch 1990/20000: Train Loss = 0.446198, Test Loss = 0.217924, Learning Rate = 4.695832e-04\n",
      "Epoch 1991/20000: Train Loss = 0.446892, Test Loss = 0.234951, Learning Rate = 4.694048e-04\n",
      "Epoch 1992/20000: Train Loss = 0.445820, Test Loss = 0.251151, Learning Rate = 4.692264e-04\n",
      "Epoch 1993/20000: Train Loss = 0.447211, Test Loss = 0.238786, Learning Rate = 4.690481e-04\n",
      "Epoch 1994/20000: Train Loss = 0.444277, Test Loss = 0.276531, Learning Rate = 4.688699e-04\n",
      "Epoch 1995/20000: Train Loss = 0.445008, Test Loss = 0.221187, Learning Rate = 4.686918e-04\n",
      "Epoch 1996/20000: Train Loss = 0.446789, Test Loss = 0.253227, Learning Rate = 4.685137e-04\n",
      "Epoch 1997/20000: Train Loss = 0.443801, Test Loss = 0.233530, Learning Rate = 4.683356e-04\n",
      "Epoch 1998/20000: Train Loss = 0.443839, Test Loss = 0.233615, Learning Rate = 4.681577e-04\n",
      "Epoch 1999/20000: Train Loss = 0.446236, Test Loss = 0.233040, Learning Rate = 4.679798e-04\n",
      "Epoch 2000/20000: Train Loss = 0.445956, Test Loss = 0.241529, Learning Rate = 4.678020e-04\n",
      "Epoch 2001/20000: Train Loss = 0.445459, Test Loss = 0.247774, Learning Rate = 4.676242e-04\n",
      "Epoch 2002/20000: Train Loss = 0.445142, Test Loss = 0.236152, Learning Rate = 4.674465e-04\n",
      "Epoch 2003/20000: Train Loss = 0.447797, Test Loss = 0.230767, Learning Rate = 4.672689e-04\n",
      "Epoch 2004/20000: Train Loss = 0.446001, Test Loss = 0.251785, Learning Rate = 4.670914e-04\n",
      "Epoch 2005/20000: Train Loss = 0.446450, Test Loss = 0.235834, Learning Rate = 4.669139e-04\n",
      "Epoch 2006/20000: Train Loss = 0.444136, Test Loss = 0.235216, Learning Rate = 4.667365e-04\n",
      "Epoch 2007/20000: Train Loss = 0.446920, Test Loss = 0.241506, Learning Rate = 4.665591e-04\n",
      "Epoch 2008/20000: Train Loss = 0.445410, Test Loss = 0.238929, Learning Rate = 4.663818e-04\n",
      "Epoch 2009/20000: Train Loss = 0.444460, Test Loss = 0.256969, Learning Rate = 4.662046e-04\n",
      "Epoch 2010/20000: Train Loss = 0.448869, Test Loss = 0.238208, Learning Rate = 4.660275e-04\n",
      "Epoch 2011/20000: Train Loss = 0.447942, Test Loss = 0.232237, Learning Rate = 4.658504e-04\n",
      "Epoch 2012/20000: Train Loss = 0.448135, Test Loss = 0.214301, Learning Rate = 4.656734e-04\n",
      "Epoch 2013/20000: Train Loss = 0.446974, Test Loss = 0.236489, Learning Rate = 4.654965e-04\n",
      "Epoch 2014/20000: Train Loss = 0.445200, Test Loss = 0.215353, Learning Rate = 4.653196e-04\n",
      "Epoch 2015/20000: Train Loss = 0.446603, Test Loss = 0.237371, Learning Rate = 4.651428e-04\n",
      "Epoch 2016/20000: Train Loss = 0.445326, Test Loss = 0.246683, Learning Rate = 4.649660e-04\n",
      "Epoch 2017/20000: Train Loss = 0.444611, Test Loss = 0.236892, Learning Rate = 4.647894e-04\n",
      "Epoch 2018/20000: Train Loss = 0.445787, Test Loss = 0.249439, Learning Rate = 4.646128e-04\n",
      "Epoch 2019/20000: Train Loss = 0.444722, Test Loss = 0.236380, Learning Rate = 4.644362e-04\n",
      "Epoch 2020/20000: Train Loss = 0.445610, Test Loss = 0.250345, Learning Rate = 4.642597e-04\n",
      "Epoch 2021/20000: Train Loss = 0.446845, Test Loss = 0.223583, Learning Rate = 4.640833e-04\n",
      "Epoch 2022/20000: Train Loss = 0.445608, Test Loss = 0.239829, Learning Rate = 4.639070e-04\n",
      "Epoch 2023/20000: Train Loss = 0.445089, Test Loss = 0.239342, Learning Rate = 4.637307e-04\n",
      "Epoch 2024/20000: Train Loss = 0.444639, Test Loss = 0.243038, Learning Rate = 4.635545e-04\n",
      "Epoch 2025/20000: Train Loss = 0.444754, Test Loss = 0.247969, Learning Rate = 4.633784e-04\n",
      "Epoch 2026/20000: Train Loss = 0.444982, Test Loss = 0.231668, Learning Rate = 4.632023e-04\n",
      "Epoch 2027/20000: Train Loss = 0.444108, Test Loss = 0.255172, Learning Rate = 4.630263e-04\n",
      "Epoch 2028/20000: Train Loss = 0.444458, Test Loss = 0.241340, Learning Rate = 4.628504e-04\n",
      "Epoch 2029/20000: Train Loss = 0.446624, Test Loss = 0.235097, Learning Rate = 4.626745e-04\n",
      "Epoch 2030/20000: Train Loss = 0.444034, Test Loss = 0.245947, Learning Rate = 4.624987e-04\n",
      "Epoch 2031/20000: Train Loss = 0.445875, Test Loss = 0.255150, Learning Rate = 4.623230e-04\n",
      "Epoch 2032/20000: Train Loss = 0.445258, Test Loss = 0.260010, Learning Rate = 4.621473e-04\n",
      "Epoch 2033/20000: Train Loss = 0.448073, Test Loss = 0.232568, Learning Rate = 4.619717e-04\n",
      "Epoch 2034/20000: Train Loss = 0.445331, Test Loss = 0.262289, Learning Rate = 4.617961e-04\n",
      "Epoch 2035/20000: Train Loss = 0.445401, Test Loss = 0.234902, Learning Rate = 4.616207e-04\n",
      "Epoch 2036/20000: Train Loss = 0.447713, Test Loss = 0.239053, Learning Rate = 4.614453e-04\n",
      "Epoch 2037/20000: Train Loss = 0.446643, Test Loss = 0.256255, Learning Rate = 4.612699e-04\n",
      "Epoch 2038/20000: Train Loss = 0.447429, Test Loss = 0.255153, Learning Rate = 4.610947e-04\n",
      "Epoch 2039/20000: Train Loss = 0.445640, Test Loss = 0.246203, Learning Rate = 4.609195e-04\n",
      "Epoch 2040/20000: Train Loss = 0.445704, Test Loss = 0.256927, Learning Rate = 4.607443e-04\n",
      "Epoch 2041/20000: Train Loss = 0.445460, Test Loss = 0.239141, Learning Rate = 4.605693e-04\n",
      "Epoch 2042/20000: Train Loss = 0.445150, Test Loss = 0.244898, Learning Rate = 4.603942e-04\n",
      "Epoch 2043/20000: Train Loss = 0.443788, Test Loss = 0.253979, Learning Rate = 4.602193e-04\n",
      "Epoch 2044/20000: Train Loss = 0.445220, Test Loss = 0.251923, Learning Rate = 4.600444e-04\n",
      "Epoch 2045/20000: Train Loss = 0.444735, Test Loss = 0.236886, Learning Rate = 4.598696e-04\n",
      "Epoch 2046/20000: Train Loss = 0.445258, Test Loss = 0.250573, Learning Rate = 4.596949e-04\n",
      "Epoch 2047/20000: Train Loss = 0.449801, Test Loss = 0.247901, Learning Rate = 4.595202e-04\n",
      "Epoch 2048/20000: Train Loss = 0.445502, Test Loss = 0.245564, Learning Rate = 4.593456e-04\n",
      "Epoch 2049/20000: Train Loss = 0.443410, Test Loss = 0.240693, Learning Rate = 4.591711e-04\n",
      "Epoch 2050/20000: Train Loss = 0.444471, Test Loss = 0.240782, Learning Rate = 4.589966e-04\n",
      "Epoch 2051/20000: Train Loss = 0.444363, Test Loss = 0.233332, Learning Rate = 4.588222e-04\n",
      "Epoch 2052/20000: Train Loss = 0.445938, Test Loss = 0.237853, Learning Rate = 4.586479e-04\n",
      "Epoch 2053/20000: Train Loss = 0.444921, Test Loss = 0.251586, Learning Rate = 4.584736e-04\n",
      "Epoch 2054/20000: Train Loss = 0.444687, Test Loss = 0.233156, Learning Rate = 4.582994e-04\n",
      "Epoch 2055/20000: Train Loss = 0.445924, Test Loss = 0.266417, Learning Rate = 4.581252e-04\n",
      "Epoch 2056/20000: Train Loss = 0.445841, Test Loss = 0.254197, Learning Rate = 4.579512e-04\n",
      "Epoch 2057/20000: Train Loss = 0.445912, Test Loss = 0.249798, Learning Rate = 4.577772e-04\n",
      "Epoch 2058/20000: Train Loss = 0.443345, Test Loss = 0.240817, Learning Rate = 4.576032e-04\n",
      "Epoch 2059/20000: Train Loss = 0.444321, Test Loss = 0.245041, Learning Rate = 4.574293e-04\n",
      "Epoch 2060/20000: Train Loss = 0.443950, Test Loss = 0.247347, Learning Rate = 4.572555e-04\n",
      "Epoch 2061/20000: Train Loss = 0.444727, Test Loss = 0.247259, Learning Rate = 4.570818e-04\n",
      "Epoch 2062/20000: Train Loss = 0.444043, Test Loss = 0.237536, Learning Rate = 4.569081e-04\n",
      "Epoch 2063/20000: Train Loss = 0.443611, Test Loss = 0.256910, Learning Rate = 4.567345e-04\n",
      "Epoch 2064/20000: Train Loss = 0.443613, Test Loss = 0.250325, Learning Rate = 4.565609e-04\n",
      "Epoch 2065/20000: Train Loss = 0.445931, Test Loss = 0.248132, Learning Rate = 4.563875e-04\n",
      "Epoch 2066/20000: Train Loss = 0.445757, Test Loss = 0.254422, Learning Rate = 4.562140e-04\n",
      "Epoch 2067/20000: Train Loss = 0.446318, Test Loss = 0.225761, Learning Rate = 4.560407e-04\n",
      "Epoch 2068/20000: Train Loss = 0.444652, Test Loss = 0.254988, Learning Rate = 4.558674e-04\n",
      "Epoch 2069/20000: Train Loss = 0.444263, Test Loss = 0.240683, Learning Rate = 4.556942e-04\n",
      "Epoch 2070/20000: Train Loss = 0.443451, Test Loss = 0.233931, Learning Rate = 4.555210e-04\n",
      "Epoch 2071/20000: Train Loss = 0.445062, Test Loss = 0.253411, Learning Rate = 4.553480e-04\n",
      "Epoch 2072/20000: Train Loss = 0.446236, Test Loss = 0.258220, Learning Rate = 4.551749e-04\n",
      "Epoch 2073/20000: Train Loss = 0.444276, Test Loss = 0.247205, Learning Rate = 4.550020e-04\n",
      "Epoch 2074/20000: Train Loss = 0.444522, Test Loss = 0.225140, Learning Rate = 4.548291e-04\n",
      "Epoch 2075/20000: Train Loss = 0.445167, Test Loss = 0.258871, Learning Rate = 4.546563e-04\n",
      "Epoch 2076/20000: Train Loss = 0.444141, Test Loss = 0.253706, Learning Rate = 4.544835e-04\n",
      "Epoch 2077/20000: Train Loss = 0.444486, Test Loss = 0.253183, Learning Rate = 4.543108e-04\n",
      "Epoch 2078/20000: Train Loss = 0.444102, Test Loss = 0.257143, Learning Rate = 4.541382e-04\n",
      "Epoch 2079/20000: Train Loss = 0.443557, Test Loss = 0.249022, Learning Rate = 4.539656e-04\n",
      "Epoch 2080/20000: Train Loss = 0.444291, Test Loss = 0.251811, Learning Rate = 4.537931e-04\n",
      "Epoch 2081/20000: Train Loss = 0.444712, Test Loss = 0.251628, Learning Rate = 4.536207e-04\n",
      "Epoch 2082/20000: Train Loss = 0.444813, Test Loss = 0.249646, Learning Rate = 4.534484e-04\n",
      "Epoch 2083/20000: Train Loss = 0.445499, Test Loss = 0.236589, Learning Rate = 4.532761e-04\n",
      "Epoch 2084/20000: Train Loss = 0.444983, Test Loss = 0.241770, Learning Rate = 4.531038e-04\n",
      "Epoch 2085/20000: Train Loss = 0.444470, Test Loss = 0.229047, Learning Rate = 4.529317e-04\n",
      "Epoch 2086/20000: Train Loss = 0.443999, Test Loss = 0.250260, Learning Rate = 4.527596e-04\n",
      "Epoch 2087/20000: Train Loss = 0.448005, Test Loss = 0.240302, Learning Rate = 4.525875e-04\n",
      "Epoch 2088/20000: Train Loss = 0.447124, Test Loss = 0.247905, Learning Rate = 4.524155e-04\n",
      "Epoch 2089/20000: Train Loss = 0.446660, Test Loss = 0.250556, Learning Rate = 4.522436e-04\n",
      "Epoch 2090/20000: Train Loss = 0.444804, Test Loss = 0.255185, Learning Rate = 4.520718e-04\n",
      "Epoch 2091/20000: Train Loss = 0.445511, Test Loss = 0.236603, Learning Rate = 4.519000e-04\n",
      "Epoch 2092/20000: Train Loss = 0.445635, Test Loss = 0.233343, Learning Rate = 4.517283e-04\n",
      "Epoch 2093/20000: Train Loss = 0.443972, Test Loss = 0.258324, Learning Rate = 4.515567e-04\n",
      "Epoch 2094/20000: Train Loss = 0.444523, Test Loss = 0.240107, Learning Rate = 4.513851e-04\n",
      "Epoch 2095/20000: Train Loss = 0.445973, Test Loss = 0.235293, Learning Rate = 4.512136e-04\n",
      "Epoch 2096/20000: Train Loss = 0.444852, Test Loss = 0.262381, Learning Rate = 4.510421e-04\n",
      "Epoch 2097/20000: Train Loss = 0.445233, Test Loss = 0.258415, Learning Rate = 4.508707e-04\n",
      "Epoch 2098/20000: Train Loss = 0.446330, Test Loss = 0.252445, Learning Rate = 4.506994e-04\n",
      "Epoch 2099/20000: Train Loss = 0.445126, Test Loss = 0.252006, Learning Rate = 4.505282e-04\n",
      "Epoch 2100/20000: Train Loss = 0.444170, Test Loss = 0.259146, Learning Rate = 4.503570e-04\n",
      "Epoch 2101/20000: Train Loss = 0.444680, Test Loss = 0.249348, Learning Rate = 4.501859e-04\n",
      "Epoch 2102/20000: Train Loss = 0.445025, Test Loss = 0.242943, Learning Rate = 4.500148e-04\n",
      "Epoch 2103/20000: Train Loss = 0.444349, Test Loss = 0.238178, Learning Rate = 4.498438e-04\n",
      "Epoch 2104/20000: Train Loss = 0.446531, Test Loss = 0.256874, Learning Rate = 4.496729e-04\n",
      "Epoch 2105/20000: Train Loss = 0.445099, Test Loss = 0.265383, Learning Rate = 4.495020e-04\n",
      "Epoch 2106/20000: Train Loss = 0.447357, Test Loss = 0.261384, Learning Rate = 4.493312e-04\n",
      "Epoch 2107/20000: Train Loss = 0.446299, Test Loss = 0.239624, Learning Rate = 4.491605e-04\n",
      "Epoch 2108/20000: Train Loss = 0.445399, Test Loss = 0.247732, Learning Rate = 4.489898e-04\n",
      "Epoch 2109/20000: Train Loss = 0.446416, Test Loss = 0.243314, Learning Rate = 4.488192e-04\n",
      "Epoch 2110/20000: Train Loss = 0.446585, Test Loss = 0.270598, Learning Rate = 4.486487e-04\n",
      "Epoch 2111/20000: Train Loss = 0.446957, Test Loss = 0.231340, Learning Rate = 4.484782e-04\n",
      "Epoch 2112/20000: Train Loss = 0.447986, Test Loss = 0.258968, Learning Rate = 4.483078e-04\n",
      "Epoch 2113/20000: Train Loss = 0.445872, Test Loss = 0.242049, Learning Rate = 4.481374e-04\n",
      "Epoch 2114/20000: Train Loss = 0.447658, Test Loss = 0.245391, Learning Rate = 4.479672e-04\n",
      "Epoch 2115/20000: Train Loss = 0.445780, Test Loss = 0.270264, Learning Rate = 4.477969e-04\n",
      "Epoch 2116/20000: Train Loss = 0.447516, Test Loss = 0.260241, Learning Rate = 4.476268e-04\n",
      "Epoch 2117/20000: Train Loss = 0.446232, Test Loss = 0.254367, Learning Rate = 4.474567e-04\n",
      "Epoch 2118/20000: Train Loss = 0.446320, Test Loss = 0.256024, Learning Rate = 4.472867e-04\n",
      "Epoch 2119/20000: Train Loss = 0.447667, Test Loss = 0.241049, Learning Rate = 4.471167e-04\n",
      "Epoch 2120/20000: Train Loss = 0.444397, Test Loss = 0.231743, Learning Rate = 4.469468e-04\n",
      "Epoch 2121/20000: Train Loss = 0.444800, Test Loss = 0.255392, Learning Rate = 4.467770e-04\n",
      "Epoch 2122/20000: Train Loss = 0.444524, Test Loss = 0.247587, Learning Rate = 4.466073e-04\n",
      "Epoch 2123/20000: Train Loss = 0.445285, Test Loss = 0.244022, Learning Rate = 4.464376e-04\n",
      "Epoch 2124/20000: Train Loss = 0.445305, Test Loss = 0.233849, Learning Rate = 4.462679e-04\n",
      "Epoch 2125/20000: Train Loss = 0.446267, Test Loss = 0.237093, Learning Rate = 4.460983e-04\n",
      "Epoch 2126/20000: Train Loss = 0.446600, Test Loss = 0.260332, Learning Rate = 4.459288e-04\n",
      "Epoch 2127/20000: Train Loss = 0.445207, Test Loss = 0.235538, Learning Rate = 4.457594e-04\n",
      "Epoch 2128/20000: Train Loss = 0.444602, Test Loss = 0.251172, Learning Rate = 4.455900e-04\n",
      "Epoch 2129/20000: Train Loss = 0.444891, Test Loss = 0.238571, Learning Rate = 4.454207e-04\n",
      "Epoch 2130/20000: Train Loss = 0.446076, Test Loss = 0.243713, Learning Rate = 4.452515e-04\n",
      "Epoch 2131/20000: Train Loss = 0.444775, Test Loss = 0.260492, Learning Rate = 4.450823e-04\n",
      "Epoch 2132/20000: Train Loss = 0.444526, Test Loss = 0.236539, Learning Rate = 4.449132e-04\n",
      "Epoch 2133/20000: Train Loss = 0.444372, Test Loss = 0.249678, Learning Rate = 4.447441e-04\n",
      "Epoch 2134/20000: Train Loss = 0.444451, Test Loss = 0.256046, Learning Rate = 4.445751e-04\n",
      "Epoch 2135/20000: Train Loss = 0.443975, Test Loss = 0.238265, Learning Rate = 4.444062e-04\n",
      "Epoch 2136/20000: Train Loss = 0.444114, Test Loss = 0.256116, Learning Rate = 4.442373e-04\n",
      "Epoch 2137/20000: Train Loss = 0.448872, Test Loss = 0.227128, Learning Rate = 4.440685e-04\n",
      "Epoch 2138/20000: Train Loss = 0.445016, Test Loss = 0.239477, Learning Rate = 4.438998e-04\n",
      "Epoch 2139/20000: Train Loss = 0.444477, Test Loss = 0.239101, Learning Rate = 4.437311e-04\n",
      "Epoch 2140/20000: Train Loss = 0.445965, Test Loss = 0.251967, Learning Rate = 4.435625e-04\n",
      "Epoch 2141/20000: Train Loss = 0.445237, Test Loss = 0.247258, Learning Rate = 4.433940e-04\n",
      "Epoch 2142/20000: Train Loss = 0.444267, Test Loss = 0.238962, Learning Rate = 4.432255e-04\n",
      "Epoch 2143/20000: Train Loss = 0.447363, Test Loss = 0.270699, Learning Rate = 4.430571e-04\n",
      "Epoch 2144/20000: Train Loss = 0.444511, Test Loss = 0.245466, Learning Rate = 4.428887e-04\n",
      "Epoch 2145/20000: Train Loss = 0.444154, Test Loss = 0.234115, Learning Rate = 4.427205e-04\n",
      "Epoch 2146/20000: Train Loss = 0.443415, Test Loss = 0.269622, Learning Rate = 4.425522e-04\n",
      "Epoch 2147/20000: Train Loss = 0.445315, Test Loss = 0.235861, Learning Rate = 4.423841e-04\n",
      "Epoch 2148/20000: Train Loss = 0.443908, Test Loss = 0.253272, Learning Rate = 4.422160e-04\n",
      "Epoch 2149/20000: Train Loss = 0.446592, Test Loss = 0.230205, Learning Rate = 4.420479e-04\n",
      "Epoch 2150/20000: Train Loss = 0.444575, Test Loss = 0.257528, Learning Rate = 4.418800e-04\n",
      "Epoch 2151/20000: Train Loss = 0.445207, Test Loss = 0.239803, Learning Rate = 4.417121e-04\n",
      "Epoch 2152/20000: Train Loss = 0.447732, Test Loss = 0.228920, Learning Rate = 4.415442e-04\n",
      "Epoch 2153/20000: Train Loss = 0.444541, Test Loss = 0.242655, Learning Rate = 4.413765e-04\n",
      "Epoch 2154/20000: Train Loss = 0.450558, Test Loss = 0.272025, Learning Rate = 4.412088e-04\n",
      "Epoch 2155/20000: Train Loss = 0.445849, Test Loss = 0.236000, Learning Rate = 4.410411e-04\n",
      "Epoch 2156/20000: Train Loss = 0.446201, Test Loss = 0.224555, Learning Rate = 4.408735e-04\n",
      "Epoch 2157/20000: Train Loss = 0.446449, Test Loss = 0.239790, Learning Rate = 4.407060e-04\n",
      "Epoch 2158/20000: Train Loss = 0.445799, Test Loss = 0.237804, Learning Rate = 4.405385e-04\n",
      "Epoch 2159/20000: Train Loss = 0.445077, Test Loss = 0.255930, Learning Rate = 4.403712e-04\n",
      "Epoch 2160/20000: Train Loss = 0.445328, Test Loss = 0.251525, Learning Rate = 4.402038e-04\n",
      "Epoch 2161/20000: Train Loss = 0.446246, Test Loss = 0.252075, Learning Rate = 4.400366e-04\n",
      "Epoch 2162/20000: Train Loss = 0.445413, Test Loss = 0.239221, Learning Rate = 4.398694e-04\n",
      "Epoch 2163/20000: Train Loss = 0.444482, Test Loss = 0.233673, Learning Rate = 4.397022e-04\n",
      "Epoch 2164/20000: Train Loss = 0.445234, Test Loss = 0.244845, Learning Rate = 4.395351e-04\n",
      "Epoch 2165/20000: Train Loss = 0.446240, Test Loss = 0.231418, Learning Rate = 4.393681e-04\n",
      "Epoch 2166/20000: Train Loss = 0.444917, Test Loss = 0.238972, Learning Rate = 4.392012e-04\n",
      "Epoch 2167/20000: Train Loss = 0.443227, Test Loss = 0.248202, Learning Rate = 4.390343e-04\n",
      "Epoch 2168/20000: Train Loss = 0.446892, Test Loss = 0.256452, Learning Rate = 4.388675e-04\n",
      "Epoch 2169/20000: Train Loss = 0.444783, Test Loss = 0.236365, Learning Rate = 4.387007e-04\n",
      "Epoch 2170/20000: Train Loss = 0.445857, Test Loss = 0.243708, Learning Rate = 4.385340e-04\n",
      "Epoch 2171/20000: Train Loss = 0.444706, Test Loss = 0.249164, Learning Rate = 4.383674e-04\n",
      "Epoch 2172/20000: Train Loss = 0.445600, Test Loss = 0.240759, Learning Rate = 4.382008e-04\n",
      "Epoch 2173/20000: Train Loss = 0.444815, Test Loss = 0.240105, Learning Rate = 4.380343e-04\n",
      "Epoch 2174/20000: Train Loss = 0.444807, Test Loss = 0.260648, Learning Rate = 4.378679e-04\n",
      "Epoch 2175/20000: Train Loss = 0.445446, Test Loss = 0.260000, Learning Rate = 4.377015e-04\n",
      "Epoch 2176/20000: Train Loss = 0.445810, Test Loss = 0.229320, Learning Rate = 4.375352e-04\n",
      "Epoch 2177/20000: Train Loss = 0.447165, Test Loss = 0.237736, Learning Rate = 4.373689e-04\n",
      "Epoch 2178/20000: Train Loss = 0.444738, Test Loss = 0.275641, Learning Rate = 4.372027e-04\n",
      "Epoch 2179/20000: Train Loss = 0.447577, Test Loss = 0.252426, Learning Rate = 4.370366e-04\n",
      "Epoch 2180/20000: Train Loss = 0.445756, Test Loss = 0.293409, Learning Rate = 4.368706e-04\n",
      "Epoch 2181/20000: Train Loss = 0.447108, Test Loss = 0.224059, Learning Rate = 4.367046e-04\n",
      "Epoch 2182/20000: Train Loss = 0.445628, Test Loss = 0.243878, Learning Rate = 4.365386e-04\n",
      "Epoch 2183/20000: Train Loss = 0.446178, Test Loss = 0.250982, Learning Rate = 4.363728e-04\n",
      "Epoch 2184/20000: Train Loss = 0.445338, Test Loss = 0.248093, Learning Rate = 4.362069e-04\n",
      "Epoch 2185/20000: Train Loss = 0.446783, Test Loss = 0.263173, Learning Rate = 4.360412e-04\n",
      "Epoch 2186/20000: Train Loss = 0.445496, Test Loss = 0.217741, Learning Rate = 4.358755e-04\n",
      "Epoch 2187/20000: Train Loss = 0.445568, Test Loss = 0.264916, Learning Rate = 4.357099e-04\n",
      "Epoch 2188/20000: Train Loss = 0.444802, Test Loss = 0.232409, Learning Rate = 4.355443e-04\n",
      "Epoch 2189/20000: Train Loss = 0.445711, Test Loss = 0.250189, Learning Rate = 4.353788e-04\n",
      "Epoch 2190/20000: Train Loss = 0.445384, Test Loss = 0.238923, Learning Rate = 4.352134e-04\n",
      "Epoch 2191/20000: Train Loss = 0.444696, Test Loss = 0.260567, Learning Rate = 4.350480e-04\n",
      "Epoch 2192/20000: Train Loss = 0.444337, Test Loss = 0.228067, Learning Rate = 4.348827e-04\n",
      "Epoch 2193/20000: Train Loss = 0.444191, Test Loss = 0.250163, Learning Rate = 4.347175e-04\n",
      "Epoch 2194/20000: Train Loss = 0.444068, Test Loss = 0.245933, Learning Rate = 4.345523e-04\n",
      "Epoch 2195/20000: Train Loss = 0.444264, Test Loss = 0.227670, Learning Rate = 4.343872e-04\n",
      "Epoch 2196/20000: Train Loss = 0.444390, Test Loss = 0.243750, Learning Rate = 4.342221e-04\n",
      "Epoch 2197/20000: Train Loss = 0.446323, Test Loss = 0.243466, Learning Rate = 4.340571e-04\n",
      "Epoch 2198/20000: Train Loss = 0.444501, Test Loss = 0.248000, Learning Rate = 4.338922e-04\n",
      "Epoch 2199/20000: Train Loss = 0.444175, Test Loss = 0.234256, Learning Rate = 4.337273e-04\n",
      "Epoch 2200/20000: Train Loss = 0.444305, Test Loss = 0.253438, Learning Rate = 4.335625e-04\n",
      "Epoch 2201/20000: Train Loss = 0.444145, Test Loss = 0.256970, Learning Rate = 4.333978e-04\n",
      "Epoch 2202/20000: Train Loss = 0.444510, Test Loss = 0.249082, Learning Rate = 4.332331e-04\n",
      "Epoch 2203/20000: Train Loss = 0.444875, Test Loss = 0.237358, Learning Rate = 4.330685e-04\n",
      "Epoch 2204/20000: Train Loss = 0.447025, Test Loss = 0.236137, Learning Rate = 4.329039e-04\n",
      "Epoch 2205/20000: Train Loss = 0.447247, Test Loss = 0.228908, Learning Rate = 4.327395e-04\n",
      "Epoch 2206/20000: Train Loss = 0.444243, Test Loss = 0.252090, Learning Rate = 4.325750e-04\n",
      "Epoch 2207/20000: Train Loss = 0.443940, Test Loss = 0.244133, Learning Rate = 4.324107e-04\n",
      "Epoch 2208/20000: Train Loss = 0.445782, Test Loss = 0.252429, Learning Rate = 4.322464e-04\n",
      "Epoch 2209/20000: Train Loss = 0.444731, Test Loss = 0.245133, Learning Rate = 4.320821e-04\n",
      "Epoch 2210/20000: Train Loss = 0.443554, Test Loss = 0.232926, Learning Rate = 4.319179e-04\n",
      "Epoch 2211/20000: Train Loss = 0.447942, Test Loss = 0.244411, Learning Rate = 4.317538e-04\n",
      "Epoch 2212/20000: Train Loss = 0.445317, Test Loss = 0.249215, Learning Rate = 4.315898e-04\n",
      "Epoch 2213/20000: Train Loss = 0.445097, Test Loss = 0.241676, Learning Rate = 4.314258e-04\n",
      "Epoch 2214/20000: Train Loss = 0.444178, Test Loss = 0.257517, Learning Rate = 4.312618e-04\n",
      "Epoch 2215/20000: Train Loss = 0.445439, Test Loss = 0.255345, Learning Rate = 4.310980e-04\n",
      "Epoch 2216/20000: Train Loss = 0.444129, Test Loss = 0.246000, Learning Rate = 4.309342e-04\n",
      "Epoch 2217/20000: Train Loss = 0.446786, Test Loss = 0.242083, Learning Rate = 4.307704e-04\n",
      "Epoch 2218/20000: Train Loss = 0.444737, Test Loss = 0.239761, Learning Rate = 4.306067e-04\n",
      "Epoch 2219/20000: Train Loss = 0.444282, Test Loss = 0.246076, Learning Rate = 4.304431e-04\n",
      "Epoch 2220/20000: Train Loss = 0.443949, Test Loss = 0.237253, Learning Rate = 4.302796e-04\n",
      "Epoch 2221/20000: Train Loss = 0.444729, Test Loss = 0.244432, Learning Rate = 4.301161e-04\n",
      "Epoch 2222/20000: Train Loss = 0.445404, Test Loss = 0.236229, Learning Rate = 4.299526e-04\n",
      "Epoch 2223/20000: Train Loss = 0.444894, Test Loss = 0.226623, Learning Rate = 4.297893e-04\n",
      "Epoch 2224/20000: Train Loss = 0.445260, Test Loss = 0.267254, Learning Rate = 4.296260e-04\n",
      "Epoch 2225/20000: Train Loss = 0.446774, Test Loss = 0.272974, Learning Rate = 4.294627e-04\n",
      "Epoch 2226/20000: Train Loss = 0.449280, Test Loss = 0.255838, Learning Rate = 4.292995e-04\n",
      "Epoch 2227/20000: Train Loss = 0.445960, Test Loss = 0.251199, Learning Rate = 4.291364e-04\n",
      "Epoch 2228/20000: Train Loss = 0.444877, Test Loss = 0.252654, Learning Rate = 4.289733e-04\n",
      "Epoch 2229/20000: Train Loss = 0.444078, Test Loss = 0.251173, Learning Rate = 4.288103e-04\n",
      "Epoch 2230/20000: Train Loss = 0.444213, Test Loss = 0.248475, Learning Rate = 4.286474e-04\n",
      "Epoch 2231/20000: Train Loss = 0.444168, Test Loss = 0.249851, Learning Rate = 4.284845e-04\n",
      "Epoch 2232/20000: Train Loss = 0.444456, Test Loss = 0.243512, Learning Rate = 4.283217e-04\n",
      "Epoch 2233/20000: Train Loss = 0.444923, Test Loss = 0.242805, Learning Rate = 4.281590e-04\n",
      "Epoch 2234/20000: Train Loss = 0.444374, Test Loss = 0.237951, Learning Rate = 4.279963e-04\n",
      "Epoch 2235/20000: Train Loss = 0.444131, Test Loss = 0.246289, Learning Rate = 4.278337e-04\n",
      "Epoch 2236/20000: Train Loss = 0.445066, Test Loss = 0.241239, Learning Rate = 4.276711e-04\n",
      "Epoch 2237/20000: Train Loss = 0.444807, Test Loss = 0.261252, Learning Rate = 4.275086e-04\n",
      "Epoch 2238/20000: Train Loss = 0.444295, Test Loss = 0.240771, Learning Rate = 4.273461e-04\n",
      "Epoch 2239/20000: Train Loss = 0.444577, Test Loss = 0.240437, Learning Rate = 4.271838e-04\n",
      "Epoch 2240/20000: Train Loss = 0.445412, Test Loss = 0.238650, Learning Rate = 4.270215e-04\n",
      "Epoch 2241/20000: Train Loss = 0.444549, Test Loss = 0.263976, Learning Rate = 4.268592e-04\n",
      "Epoch 2242/20000: Train Loss = 0.446056, Test Loss = 0.255349, Learning Rate = 4.266970e-04\n",
      "Epoch 2243/20000: Train Loss = 0.444430, Test Loss = 0.241918, Learning Rate = 4.265349e-04\n",
      "Epoch 2244/20000: Train Loss = 0.446059, Test Loss = 0.257937, Learning Rate = 4.263728e-04\n",
      "Epoch 2245/20000: Train Loss = 0.449362, Test Loss = 0.242366, Learning Rate = 4.262108e-04\n",
      "Epoch 2246/20000: Train Loss = 0.443262, Test Loss = 0.261864, Learning Rate = 4.260488e-04\n",
      "Epoch 2247/20000: Train Loss = 0.445918, Test Loss = 0.248973, Learning Rate = 4.258869e-04\n",
      "Epoch 2248/20000: Train Loss = 0.446985, Test Loss = 0.261333, Learning Rate = 4.257251e-04\n",
      "Epoch 2249/20000: Train Loss = 0.446053, Test Loss = 0.268653, Learning Rate = 4.255634e-04\n",
      "Epoch 2250/20000: Train Loss = 0.446278, Test Loss = 0.237508, Learning Rate = 4.254017e-04\n",
      "Epoch 2251/20000: Train Loss = 0.448198, Test Loss = 0.246986, Learning Rate = 4.252400e-04\n",
      "Epoch 2252/20000: Train Loss = 0.445668, Test Loss = 0.225107, Learning Rate = 4.250784e-04\n",
      "Epoch 2253/20000: Train Loss = 0.447960, Test Loss = 0.247758, Learning Rate = 4.249169e-04\n",
      "Epoch 2254/20000: Train Loss = 0.445535, Test Loss = 0.243149, Learning Rate = 4.247555e-04\n",
      "Epoch 2255/20000: Train Loss = 0.447078, Test Loss = 0.235484, Learning Rate = 4.245941e-04\n",
      "Epoch 2256/20000: Train Loss = 0.446200, Test Loss = 0.231583, Learning Rate = 4.244327e-04\n",
      "Epoch 2257/20000: Train Loss = 0.445440, Test Loss = 0.238358, Learning Rate = 4.242715e-04\n",
      "Epoch 2258/20000: Train Loss = 0.445234, Test Loss = 0.245044, Learning Rate = 4.241102e-04\n",
      "Epoch 2259/20000: Train Loss = 0.445190, Test Loss = 0.234780, Learning Rate = 4.239491e-04\n",
      "Epoch 2260/20000: Train Loss = 0.445057, Test Loss = 0.256773, Learning Rate = 4.237880e-04\n",
      "Epoch 2261/20000: Train Loss = 0.443805, Test Loss = 0.240706, Learning Rate = 4.236270e-04\n",
      "Epoch 2262/20000: Train Loss = 0.444517, Test Loss = 0.242920, Learning Rate = 4.234660e-04\n",
      "Epoch 2263/20000: Train Loss = 0.445588, Test Loss = 0.234133, Learning Rate = 4.233051e-04\n",
      "Epoch 2264/20000: Train Loss = 0.444346, Test Loss = 0.246788, Learning Rate = 4.231443e-04\n",
      "Epoch 2265/20000: Train Loss = 0.445190, Test Loss = 0.265814, Learning Rate = 4.229835e-04\n",
      "Epoch 2266/20000: Train Loss = 0.446148, Test Loss = 0.257220, Learning Rate = 4.228228e-04\n",
      "Epoch 2267/20000: Train Loss = 0.444976, Test Loss = 0.245803, Learning Rate = 4.226621e-04\n",
      "Epoch 2268/20000: Train Loss = 0.445770, Test Loss = 0.242658, Learning Rate = 4.225015e-04\n",
      "Epoch 2269/20000: Train Loss = 0.443955, Test Loss = 0.243083, Learning Rate = 4.223410e-04\n",
      "Epoch 2270/20000: Train Loss = 0.443717, Test Loss = 0.254340, Learning Rate = 4.221805e-04\n",
      "Epoch 2271/20000: Train Loss = 0.443833, Test Loss = 0.240867, Learning Rate = 4.220201e-04\n",
      "Epoch 2272/20000: Train Loss = 0.444375, Test Loss = 0.248715, Learning Rate = 4.218597e-04\n",
      "Epoch 2273/20000: Train Loss = 0.443999, Test Loss = 0.245161, Learning Rate = 4.216994e-04\n",
      "Epoch 2274/20000: Train Loss = 0.443508, Test Loss = 0.234684, Learning Rate = 4.215392e-04\n",
      "Epoch 2275/20000: Train Loss = 0.444363, Test Loss = 0.233236, Learning Rate = 4.213790e-04\n",
      "Epoch 2276/20000: Train Loss = 0.443704, Test Loss = 0.243350, Learning Rate = 4.212189e-04\n",
      "Epoch 2277/20000: Train Loss = 0.446329, Test Loss = 0.235712, Learning Rate = 4.210588e-04\n",
      "Epoch 2278/20000: Train Loss = 0.444147, Test Loss = 0.234655, Learning Rate = 4.208988e-04\n",
      "Epoch 2279/20000: Train Loss = 0.444784, Test Loss = 0.255139, Learning Rate = 4.207389e-04\n",
      "Epoch 2280/20000: Train Loss = 0.445405, Test Loss = 0.220654, Learning Rate = 4.205790e-04\n",
      "Epoch 2281/20000: Train Loss = 0.443935, Test Loss = 0.257982, Learning Rate = 4.204192e-04\n",
      "Epoch 2282/20000: Train Loss = 0.449604, Test Loss = 0.260204, Learning Rate = 4.202595e-04\n",
      "Epoch 2283/20000: Train Loss = 0.445791, Test Loss = 0.224159, Learning Rate = 4.200998e-04\n",
      "Epoch 2284/20000: Train Loss = 0.445630, Test Loss = 0.228343, Learning Rate = 4.199402e-04\n",
      "Epoch 2285/20000: Train Loss = 0.446395, Test Loss = 0.220450, Learning Rate = 4.197806e-04\n",
      "Epoch 2286/20000: Train Loss = 0.447769, Test Loss = 0.257173, Learning Rate = 4.196211e-04\n",
      "Epoch 2287/20000: Train Loss = 0.443487, Test Loss = 0.231118, Learning Rate = 4.194617e-04\n",
      "Epoch 2288/20000: Train Loss = 0.444183, Test Loss = 0.258059, Learning Rate = 4.193023e-04\n",
      "Epoch 2289/20000: Train Loss = 0.445739, Test Loss = 0.240429, Learning Rate = 4.191430e-04\n",
      "Epoch 2290/20000: Train Loss = 0.447104, Test Loss = 0.247547, Learning Rate = 4.189837e-04\n",
      "Epoch 2291/20000: Train Loss = 0.444280, Test Loss = 0.244967, Learning Rate = 4.188245e-04\n",
      "Epoch 2292/20000: Train Loss = 0.445572, Test Loss = 0.253969, Learning Rate = 4.186653e-04\n",
      "Epoch 2293/20000: Train Loss = 0.445220, Test Loss = 0.254190, Learning Rate = 4.185063e-04\n",
      "Epoch 2294/20000: Train Loss = 0.445475, Test Loss = 0.243403, Learning Rate = 4.183472e-04\n",
      "Epoch 2295/20000: Train Loss = 0.446090, Test Loss = 0.226455, Learning Rate = 4.181883e-04\n",
      "Epoch 2296/20000: Train Loss = 0.447120, Test Loss = 0.242959, Learning Rate = 4.180294e-04\n",
      "Epoch 2297/20000: Train Loss = 0.444581, Test Loss = 0.235129, Learning Rate = 4.178705e-04\n",
      "Epoch 2298/20000: Train Loss = 0.445762, Test Loss = 0.255510, Learning Rate = 4.177118e-04\n",
      "Epoch 2299/20000: Train Loss = 0.446612, Test Loss = 0.255508, Learning Rate = 4.175530e-04\n",
      "Epoch 2300/20000: Train Loss = 0.445647, Test Loss = 0.252402, Learning Rate = 4.173944e-04\n",
      "Epoch 2301/20000: Train Loss = 0.444180, Test Loss = 0.247625, Learning Rate = 4.172358e-04\n",
      "Epoch 2302/20000: Train Loss = 0.444332, Test Loss = 0.245166, Learning Rate = 4.170772e-04\n",
      "Epoch 2303/20000: Train Loss = 0.444501, Test Loss = 0.241854, Learning Rate = 4.169188e-04\n",
      "Epoch 2304/20000: Train Loss = 0.444420, Test Loss = 0.262108, Learning Rate = 4.167604e-04\n",
      "Epoch 2305/20000: Train Loss = 0.446599, Test Loss = 0.250335, Learning Rate = 4.166020e-04\n",
      "Epoch 2306/20000: Train Loss = 0.449523, Test Loss = 0.256188, Learning Rate = 4.164437e-04\n",
      "Epoch 2307/20000: Train Loss = 0.445400, Test Loss = 0.248825, Learning Rate = 4.162855e-04\n",
      "Epoch 2308/20000: Train Loss = 0.445361, Test Loss = 0.223146, Learning Rate = 4.161273e-04\n",
      "Epoch 2309/20000: Train Loss = 0.445750, Test Loss = 0.243151, Learning Rate = 4.159692e-04\n",
      "Epoch 2310/20000: Train Loss = 0.446903, Test Loss = 0.235177, Learning Rate = 4.158111e-04\n",
      "Epoch 2311/20000: Train Loss = 0.445817, Test Loss = 0.237847, Learning Rate = 4.156531e-04\n",
      "Epoch 2312/20000: Train Loss = 0.443950, Test Loss = 0.250468, Learning Rate = 4.154952e-04\n",
      "Epoch 2313/20000: Train Loss = 0.444592, Test Loss = 0.240134, Learning Rate = 4.153373e-04\n",
      "Epoch 2314/20000: Train Loss = 0.445785, Test Loss = 0.251462, Learning Rate = 4.151795e-04\n",
      "Epoch 2315/20000: Train Loss = 0.444461, Test Loss = 0.255663, Learning Rate = 4.150217e-04\n",
      "Epoch 2316/20000: Train Loss = 0.444383, Test Loss = 0.245106, Learning Rate = 4.148640e-04\n",
      "Epoch 2317/20000: Train Loss = 0.444276, Test Loss = 0.248467, Learning Rate = 4.147064e-04\n",
      "Epoch 2318/20000: Train Loss = 0.444538, Test Loss = 0.243460, Learning Rate = 4.145488e-04\n",
      "Epoch 2319/20000: Train Loss = 0.447027, Test Loss = 0.239246, Learning Rate = 4.143913e-04\n",
      "Epoch 2320/20000: Train Loss = 0.445649, Test Loss = 0.235752, Learning Rate = 4.142338e-04\n",
      "Epoch 2321/20000: Train Loss = 0.446835, Test Loss = 0.251262, Learning Rate = 4.140764e-04\n",
      "Epoch 2322/20000: Train Loss = 0.448033, Test Loss = 0.225753, Learning Rate = 4.139191e-04\n",
      "Epoch 2323/20000: Train Loss = 0.446419, Test Loss = 0.242109, Learning Rate = 4.137618e-04\n",
      "Epoch 2324/20000: Train Loss = 0.446614, Test Loss = 0.241582, Learning Rate = 4.136046e-04\n",
      "Epoch 2325/20000: Train Loss = 0.445797, Test Loss = 0.256359, Learning Rate = 4.134474e-04\n",
      "Epoch 2326/20000: Train Loss = 0.446477, Test Loss = 0.242852, Learning Rate = 4.132903e-04\n",
      "Epoch 2327/20000: Train Loss = 0.444271, Test Loss = 0.245322, Learning Rate = 4.131333e-04\n",
      "Epoch 2328/20000: Train Loss = 0.443649, Test Loss = 0.250645, Learning Rate = 4.129763e-04\n",
      "Epoch 2329/20000: Train Loss = 0.444129, Test Loss = 0.253918, Learning Rate = 4.128194e-04\n",
      "Epoch 2330/20000: Train Loss = 0.444594, Test Loss = 0.253691, Learning Rate = 4.126625e-04\n",
      "Epoch 2331/20000: Train Loss = 0.444822, Test Loss = 0.234987, Learning Rate = 4.125057e-04\n",
      "Epoch 2332/20000: Train Loss = 0.444682, Test Loss = 0.253820, Learning Rate = 4.123490e-04\n",
      "Epoch 2333/20000: Train Loss = 0.444013, Test Loss = 0.249994, Learning Rate = 4.121923e-04\n",
      "Epoch 2334/20000: Train Loss = 0.444415, Test Loss = 0.243940, Learning Rate = 4.120357e-04\n",
      "Epoch 2335/20000: Train Loss = 0.445427, Test Loss = 0.232544, Learning Rate = 4.118791e-04\n",
      "Epoch 2336/20000: Train Loss = 0.445566, Test Loss = 0.253258, Learning Rate = 4.117226e-04\n",
      "Epoch 2337/20000: Train Loss = 0.444127, Test Loss = 0.238381, Learning Rate = 4.115662e-04\n",
      "Epoch 2338/20000: Train Loss = 0.444260, Test Loss = 0.236190, Learning Rate = 4.114098e-04\n",
      "Epoch 2339/20000: Train Loss = 0.445550, Test Loss = 0.248579, Learning Rate = 4.112535e-04\n",
      "Epoch 2340/20000: Train Loss = 0.444081, Test Loss = 0.262592, Learning Rate = 4.110972e-04\n",
      "Epoch 2341/20000: Train Loss = 0.447280, Test Loss = 0.246544, Learning Rate = 4.109410e-04\n",
      "Epoch 2342/20000: Train Loss = 0.445224, Test Loss = 0.254210, Learning Rate = 4.107849e-04\n",
      "Epoch 2343/20000: Train Loss = 0.444397, Test Loss = 0.237613, Learning Rate = 4.106288e-04\n",
      "Epoch 2344/20000: Train Loss = 0.445265, Test Loss = 0.243977, Learning Rate = 4.104728e-04\n",
      "Epoch 2345/20000: Train Loss = 0.445157, Test Loss = 0.253102, Learning Rate = 4.103168e-04\n",
      "Epoch 2346/20000: Train Loss = 0.445422, Test Loss = 0.256060, Learning Rate = 4.101609e-04\n",
      "Epoch 2347/20000: Train Loss = 0.444053, Test Loss = 0.251385, Learning Rate = 4.100050e-04\n",
      "Epoch 2348/20000: Train Loss = 0.445419, Test Loss = 0.246103, Learning Rate = 4.098492e-04\n",
      "Epoch 2349/20000: Train Loss = 0.446104, Test Loss = 0.227273, Learning Rate = 4.096935e-04\n",
      "Epoch 2350/20000: Train Loss = 0.448065, Test Loss = 0.242187, Learning Rate = 4.095378e-04\n",
      "Epoch 2351/20000: Train Loss = 0.444312, Test Loss = 0.244157, Learning Rate = 4.093822e-04\n",
      "Epoch 2352/20000: Train Loss = 0.445695, Test Loss = 0.234451, Learning Rate = 4.092267e-04\n",
      "Epoch 2353/20000: Train Loss = 0.444101, Test Loss = 0.249901, Learning Rate = 4.090712e-04\n",
      "Epoch 2354/20000: Train Loss = 0.444440, Test Loss = 0.242871, Learning Rate = 4.089157e-04\n",
      "Epoch 2355/20000: Train Loss = 0.445563, Test Loss = 0.250632, Learning Rate = 4.087604e-04\n",
      "Epoch 2356/20000: Train Loss = 0.446992, Test Loss = 0.247369, Learning Rate = 4.086050e-04\n",
      "Epoch 2357/20000: Train Loss = 0.447312, Test Loss = 0.256036, Learning Rate = 4.084498e-04\n",
      "Epoch 2358/20000: Train Loss = 0.444537, Test Loss = 0.254142, Learning Rate = 4.082946e-04\n",
      "Epoch 2359/20000: Train Loss = 0.445055, Test Loss = 0.226366, Learning Rate = 4.081394e-04\n",
      "Epoch 2360/20000: Train Loss = 0.446977, Test Loss = 0.223603, Learning Rate = 4.079844e-04\n",
      "Epoch 2361/20000: Train Loss = 0.447378, Test Loss = 0.224919, Learning Rate = 4.078293e-04\n",
      "Epoch 2362/20000: Train Loss = 0.444884, Test Loss = 0.260096, Learning Rate = 4.076744e-04\n",
      "Epoch 2363/20000: Train Loss = 0.445743, Test Loss = 0.233678, Learning Rate = 4.075195e-04\n",
      "Epoch 2364/20000: Train Loss = 0.444898, Test Loss = 0.249408, Learning Rate = 4.073646e-04\n",
      "Epoch 2365/20000: Train Loss = 0.445748, Test Loss = 0.234592, Learning Rate = 4.072098e-04\n",
      "Epoch 2366/20000: Train Loss = 0.444800, Test Loss = 0.251971, Learning Rate = 4.070551e-04\n",
      "Epoch 2367/20000: Train Loss = 0.444803, Test Loss = 0.229019, Learning Rate = 4.069004e-04\n",
      "Epoch 2368/20000: Train Loss = 0.444094, Test Loss = 0.248581, Learning Rate = 4.067458e-04\n",
      "Epoch 2369/20000: Train Loss = 0.445842, Test Loss = 0.249237, Learning Rate = 4.065913e-04\n",
      "Epoch 2370/20000: Train Loss = 0.445069, Test Loss = 0.250981, Learning Rate = 4.064368e-04\n",
      "Epoch 2371/20000: Train Loss = 0.445418, Test Loss = 0.260300, Learning Rate = 4.062823e-04\n",
      "Epoch 2372/20000: Train Loss = 0.443097, Test Loss = 0.227526, Learning Rate = 4.061280e-04\n",
      "Epoch 2373/20000: Train Loss = 0.443826, Test Loss = 0.264164, Learning Rate = 4.059736e-04\n",
      "Epoch 2374/20000: Train Loss = 0.444968, Test Loss = 0.246127, Learning Rate = 4.058194e-04\n",
      "Epoch 2375/20000: Train Loss = 0.444488, Test Loss = 0.249915, Learning Rate = 4.056652e-04\n",
      "Epoch 2376/20000: Train Loss = 0.444273, Test Loss = 0.239764, Learning Rate = 4.055110e-04\n",
      "Epoch 2377/20000: Train Loss = 0.444499, Test Loss = 0.231213, Learning Rate = 4.053570e-04\n",
      "Epoch 2378/20000: Train Loss = 0.443801, Test Loss = 0.237189, Learning Rate = 4.052029e-04\n",
      "Epoch 2379/20000: Train Loss = 0.444382, Test Loss = 0.240912, Learning Rate = 4.050490e-04\n",
      "Epoch 2380/20000: Train Loss = 0.443792, Test Loss = 0.258527, Learning Rate = 4.048951e-04\n",
      "Epoch 2381/20000: Train Loss = 0.447437, Test Loss = 0.242773, Learning Rate = 4.047412e-04\n",
      "Epoch 2382/20000: Train Loss = 0.444498, Test Loss = 0.237999, Learning Rate = 4.045874e-04\n",
      "Epoch 2383/20000: Train Loss = 0.444439, Test Loss = 0.242051, Learning Rate = 4.044337e-04\n",
      "Epoch 2384/20000: Train Loss = 0.446284, Test Loss = 0.244201, Learning Rate = 4.042800e-04\n",
      "Epoch 2385/20000: Train Loss = 0.446407, Test Loss = 0.246415, Learning Rate = 4.041264e-04\n",
      "Epoch 2386/20000: Train Loss = 0.444515, Test Loss = 0.233802, Learning Rate = 4.039728e-04\n",
      "Epoch 2387/20000: Train Loss = 0.444087, Test Loss = 0.253382, Learning Rate = 4.038193e-04\n",
      "Epoch 2388/20000: Train Loss = 0.444108, Test Loss = 0.232003, Learning Rate = 4.036659e-04\n",
      "Epoch 2389/20000: Train Loss = 0.444460, Test Loss = 0.270010, Learning Rate = 4.035125e-04\n",
      "Epoch 2390/20000: Train Loss = 0.444286, Test Loss = 0.233980, Learning Rate = 4.033592e-04\n",
      "Epoch 2391/20000: Train Loss = 0.445008, Test Loss = 0.247273, Learning Rate = 4.032059e-04\n",
      "Epoch 2392/20000: Train Loss = 0.445188, Test Loss = 0.237816, Learning Rate = 4.030527e-04\n",
      "Epoch 2393/20000: Train Loss = 0.445921, Test Loss = 0.232030, Learning Rate = 4.028996e-04\n",
      "Epoch 2394/20000: Train Loss = 0.445476, Test Loss = 0.261800, Learning Rate = 4.027465e-04\n",
      "Epoch 2395/20000: Train Loss = 0.446393, Test Loss = 0.247612, Learning Rate = 4.025935e-04\n",
      "Epoch 2396/20000: Train Loss = 0.446477, Test Loss = 0.243311, Learning Rate = 4.024405e-04\n",
      "Epoch 2397/20000: Train Loss = 0.443887, Test Loss = 0.251866, Learning Rate = 4.022876e-04\n",
      "Epoch 2398/20000: Train Loss = 0.447018, Test Loss = 0.237200, Learning Rate = 4.021347e-04\n",
      "Epoch 2399/20000: Train Loss = 0.444445, Test Loss = 0.248839, Learning Rate = 4.019819e-04\n",
      "Epoch 2400/20000: Train Loss = 0.444206, Test Loss = 0.238941, Learning Rate = 4.018292e-04\n",
      "Epoch 2401/20000: Train Loss = 0.443843, Test Loss = 0.235973, Learning Rate = 4.016765e-04\n",
      "Epoch 2402/20000: Train Loss = 0.445963, Test Loss = 0.240552, Learning Rate = 4.015239e-04\n",
      "Epoch 2403/20000: Train Loss = 0.446738, Test Loss = 0.236974, Learning Rate = 4.013713e-04\n",
      "Epoch 2404/20000: Train Loss = 0.452090, Test Loss = 0.225648, Learning Rate = 4.012188e-04\n",
      "Epoch 2405/20000: Train Loss = 0.445983, Test Loss = 0.233694, Learning Rate = 4.010663e-04\n",
      "Epoch 2406/20000: Train Loss = 0.451746, Test Loss = 0.195739, Learning Rate = 4.009139e-04\n",
      "Epoch 2407/20000: Train Loss = 0.449245, Test Loss = 0.253266, Learning Rate = 4.007616e-04\n",
      "Epoch 2408/20000: Train Loss = 0.444825, Test Loss = 0.213568, Learning Rate = 4.006093e-04\n",
      "Epoch 2409/20000: Train Loss = 0.446909, Test Loss = 0.234318, Learning Rate = 4.004571e-04\n",
      "Epoch 2410/20000: Train Loss = 0.448273, Test Loss = 0.241786, Learning Rate = 4.003049e-04\n",
      "Epoch 2411/20000: Train Loss = 0.444372, Test Loss = 0.255724, Learning Rate = 4.001528e-04\n",
      "Epoch 2412/20000: Train Loss = 0.445025, Test Loss = 0.246527, Learning Rate = 4.000008e-04\n",
      "Epoch 2413/20000: Train Loss = 0.446246, Test Loss = 0.243990, Learning Rate = 3.998488e-04\n",
      "Epoch 2414/20000: Train Loss = 0.446958, Test Loss = 0.230896, Learning Rate = 3.996969e-04\n",
      "Epoch 2415/20000: Train Loss = 0.446091, Test Loss = 0.239277, Learning Rate = 3.995450e-04\n",
      "Epoch 2416/20000: Train Loss = 0.443674, Test Loss = 0.229794, Learning Rate = 3.993932e-04\n",
      "Epoch 2417/20000: Train Loss = 0.445190, Test Loss = 0.237798, Learning Rate = 3.992414e-04\n",
      "Epoch 2418/20000: Train Loss = 0.444665, Test Loss = 0.250963, Learning Rate = 3.990897e-04\n",
      "Epoch 2419/20000: Train Loss = 0.444626, Test Loss = 0.265971, Learning Rate = 3.989381e-04\n",
      "Epoch 2420/20000: Train Loss = 0.445713, Test Loss = 0.245351, Learning Rate = 3.987865e-04\n",
      "Epoch 2421/20000: Train Loss = 0.444393, Test Loss = 0.232406, Learning Rate = 3.986349e-04\n",
      "Epoch 2422/20000: Train Loss = 0.443627, Test Loss = 0.241688, Learning Rate = 3.984835e-04\n",
      "Epoch 2423/20000: Train Loss = 0.446624, Test Loss = 0.231148, Learning Rate = 3.983321e-04\n",
      "Epoch 2424/20000: Train Loss = 0.445148, Test Loss = 0.240986, Learning Rate = 3.981807e-04\n",
      "Epoch 2425/20000: Train Loss = 0.443699, Test Loss = 0.257852, Learning Rate = 3.980294e-04\n",
      "Epoch 2426/20000: Train Loss = 0.444561, Test Loss = 0.246296, Learning Rate = 3.978782e-04\n",
      "Epoch 2427/20000: Train Loss = 0.443443, Test Loss = 0.249046, Learning Rate = 3.977270e-04\n",
      "Epoch 2428/20000: Train Loss = 0.444826, Test Loss = 0.239637, Learning Rate = 3.975759e-04\n",
      "Epoch 2429/20000: Train Loss = 0.443628, Test Loss = 0.248330, Learning Rate = 3.974248e-04\n",
      "Epoch 2430/20000: Train Loss = 0.446170, Test Loss = 0.249909, Learning Rate = 3.972738e-04\n",
      "Epoch 2431/20000: Train Loss = 0.445102, Test Loss = 0.256250, Learning Rate = 3.971228e-04\n",
      "Epoch 2432/20000: Train Loss = 0.443014, Test Loss = 0.245746, Learning Rate = 3.969719e-04\n",
      "Epoch 2433/20000: Train Loss = 0.444365, Test Loss = 0.233388, Learning Rate = 3.968211e-04\n",
      "Epoch 2434/20000: Train Loss = 0.445761, Test Loss = 0.242523, Learning Rate = 3.966703e-04\n",
      "Epoch 2435/20000: Train Loss = 0.443921, Test Loss = 0.242381, Learning Rate = 3.965196e-04\n",
      "Epoch 2436/20000: Train Loss = 0.444036, Test Loss = 0.250607, Learning Rate = 3.963689e-04\n",
      "Epoch 2437/20000: Train Loss = 0.443340, Test Loss = 0.242732, Learning Rate = 3.962183e-04\n",
      "Epoch 2438/20000: Train Loss = 0.444271, Test Loss = 0.250885, Learning Rate = 3.960678e-04\n",
      "Epoch 2439/20000: Train Loss = 0.444970, Test Loss = 0.254295, Learning Rate = 3.959173e-04\n",
      "Epoch 2440/20000: Train Loss = 0.446402, Test Loss = 0.265229, Learning Rate = 3.957668e-04\n",
      "Epoch 2441/20000: Train Loss = 0.447565, Test Loss = 0.262621, Learning Rate = 3.956164e-04\n",
      "Epoch 2442/20000: Train Loss = 0.445228, Test Loss = 0.234699, Learning Rate = 3.954661e-04\n",
      "Epoch 2443/20000: Train Loss = 0.446515, Test Loss = 0.240515, Learning Rate = 3.953159e-04\n",
      "Epoch 2444/20000: Train Loss = 0.443726, Test Loss = 0.253149, Learning Rate = 3.951656e-04\n",
      "Epoch 2445/20000: Train Loss = 0.445124, Test Loss = 0.245627, Learning Rate = 3.950155e-04\n",
      "Epoch 2446/20000: Train Loss = 0.445612, Test Loss = 0.262224, Learning Rate = 3.948654e-04\n",
      "Epoch 2447/20000: Train Loss = 0.445781, Test Loss = 0.234405, Learning Rate = 3.947154e-04\n",
      "Epoch 2448/20000: Train Loss = 0.446623, Test Loss = 0.258262, Learning Rate = 3.945654e-04\n",
      "Epoch 2449/20000: Train Loss = 0.446231, Test Loss = 0.274934, Learning Rate = 3.944155e-04\n",
      "Epoch 2450/20000: Train Loss = 0.446970, Test Loss = 0.238150, Learning Rate = 3.942656e-04\n",
      "Epoch 2451/20000: Train Loss = 0.444174, Test Loss = 0.243342, Learning Rate = 3.941158e-04\n",
      "Epoch 2452/20000: Train Loss = 0.444924, Test Loss = 0.248557, Learning Rate = 3.939660e-04\n",
      "Epoch 2453/20000: Train Loss = 0.444247, Test Loss = 0.253376, Learning Rate = 3.938163e-04\n",
      "Epoch 2454/20000: Train Loss = 0.445385, Test Loss = 0.243027, Learning Rate = 3.936667e-04\n",
      "Epoch 2455/20000: Train Loss = 0.443562, Test Loss = 0.233972, Learning Rate = 3.935171e-04\n",
      "Epoch 2456/20000: Train Loss = 0.445217, Test Loss = 0.238048, Learning Rate = 3.933676e-04\n",
      "Epoch 2457/20000: Train Loss = 0.445838, Test Loss = 0.256065, Learning Rate = 3.932181e-04\n",
      "Epoch 2458/20000: Train Loss = 0.445546, Test Loss = 0.239503, Learning Rate = 3.930687e-04\n",
      "Epoch 2459/20000: Train Loss = 0.444898, Test Loss = 0.243280, Learning Rate = 3.929193e-04\n",
      "Epoch 2460/20000: Train Loss = 0.446202, Test Loss = 0.222619, Learning Rate = 3.927700e-04\n",
      "Epoch 2461/20000: Train Loss = 0.443743, Test Loss = 0.255041, Learning Rate = 3.926208e-04\n",
      "Epoch 2462/20000: Train Loss = 0.443988, Test Loss = 0.242670, Learning Rate = 3.924716e-04\n",
      "Epoch 2463/20000: Train Loss = 0.445539, Test Loss = 0.262516, Learning Rate = 3.923225e-04\n",
      "Epoch 2464/20000: Train Loss = 0.445008, Test Loss = 0.243789, Learning Rate = 3.921734e-04\n",
      "Epoch 2465/20000: Train Loss = 0.444069, Test Loss = 0.251759, Learning Rate = 3.920244e-04\n",
      "Epoch 2466/20000: Train Loss = 0.444786, Test Loss = 0.257585, Learning Rate = 3.918754e-04\n",
      "Epoch 2467/20000: Train Loss = 0.443792, Test Loss = 0.242133, Learning Rate = 3.917265e-04\n",
      "Epoch 2468/20000: Train Loss = 0.443518, Test Loss = 0.241351, Learning Rate = 3.915777e-04\n",
      "Epoch 2469/20000: Train Loss = 0.443892, Test Loss = 0.245241, Learning Rate = 3.914289e-04\n",
      "Epoch 2470/20000: Train Loss = 0.444012, Test Loss = 0.239123, Learning Rate = 3.912802e-04\n",
      "Epoch 2471/20000: Train Loss = 0.443789, Test Loss = 0.243827, Learning Rate = 3.911315e-04\n",
      "Epoch 2472/20000: Train Loss = 0.444272, Test Loss = 0.248006, Learning Rate = 3.909829e-04\n",
      "Epoch 2473/20000: Train Loss = 0.445204, Test Loss = 0.259257, Learning Rate = 3.908343e-04\n",
      "Epoch 2474/20000: Train Loss = 0.445970, Test Loss = 0.235878, Learning Rate = 3.906858e-04\n",
      "Epoch 2475/20000: Train Loss = 0.444990, Test Loss = 0.262959, Learning Rate = 3.905374e-04\n",
      "Epoch 2476/20000: Train Loss = 0.445622, Test Loss = 0.235046, Learning Rate = 3.903890e-04\n",
      "Epoch 2477/20000: Train Loss = 0.445140, Test Loss = 0.254210, Learning Rate = 3.902406e-04\n",
      "Epoch 2478/20000: Train Loss = 0.443619, Test Loss = 0.237426, Learning Rate = 3.900924e-04\n",
      "Epoch 2479/20000: Train Loss = 0.448114, Test Loss = 0.249387, Learning Rate = 3.899441e-04\n",
      "Epoch 2480/20000: Train Loss = 0.447588, Test Loss = 0.247760, Learning Rate = 3.897960e-04\n",
      "Epoch 2481/20000: Train Loss = 0.445786, Test Loss = 0.237302, Learning Rate = 3.896478e-04\n",
      "Epoch 2482/20000: Train Loss = 0.446080, Test Loss = 0.247546, Learning Rate = 3.894998e-04\n",
      "Epoch 2483/20000: Train Loss = 0.444897, Test Loss = 0.249988, Learning Rate = 3.893518e-04\n",
      "Epoch 2484/20000: Train Loss = 0.444538, Test Loss = 0.247056, Learning Rate = 3.892038e-04\n",
      "Epoch 2485/20000: Train Loss = 0.443890, Test Loss = 0.245546, Learning Rate = 3.890560e-04\n",
      "Epoch 2486/20000: Train Loss = 0.443980, Test Loss = 0.230792, Learning Rate = 3.889081e-04\n",
      "Epoch 2487/20000: Train Loss = 0.444696, Test Loss = 0.263821, Learning Rate = 3.887604e-04\n",
      "Epoch 2488/20000: Train Loss = 0.444786, Test Loss = 0.242168, Learning Rate = 3.886126e-04\n",
      "Epoch 2489/20000: Train Loss = 0.443803, Test Loss = 0.238375, Learning Rate = 3.884650e-04\n",
      "Epoch 2490/20000: Train Loss = 0.445918, Test Loss = 0.249027, Learning Rate = 3.883174e-04\n",
      "Epoch 2491/20000: Train Loss = 0.444504, Test Loss = 0.249147, Learning Rate = 3.881698e-04\n",
      "Epoch 2492/20000: Train Loss = 0.445478, Test Loss = 0.236473, Learning Rate = 3.880223e-04\n",
      "Epoch 2493/20000: Train Loss = 0.444189, Test Loss = 0.251721, Learning Rate = 3.878749e-04\n",
      "Epoch 2494/20000: Train Loss = 0.444613, Test Loss = 0.242729, Learning Rate = 3.877275e-04\n",
      "Epoch 2495/20000: Train Loss = 0.446564, Test Loss = 0.241523, Learning Rate = 3.875802e-04\n",
      "Epoch 2496/20000: Train Loss = 0.445294, Test Loss = 0.234904, Learning Rate = 3.874329e-04\n",
      "Epoch 2497/20000: Train Loss = 0.444747, Test Loss = 0.239507, Learning Rate = 3.872857e-04\n",
      "Epoch 2498/20000: Train Loss = 0.443947, Test Loss = 0.232597, Learning Rate = 3.871385e-04\n",
      "Epoch 2499/20000: Train Loss = 0.446967, Test Loss = 0.253968, Learning Rate = 3.869914e-04\n",
      "Epoch 2500/20000: Train Loss = 0.444721, Test Loss = 0.230920, Learning Rate = 3.868444e-04\n",
      "Epoch 2501/20000: Train Loss = 0.444359, Test Loss = 0.255833, Learning Rate = 3.866974e-04\n",
      "Epoch 2502/20000: Train Loss = 0.443963, Test Loss = 0.250872, Learning Rate = 3.865505e-04\n",
      "Epoch 2503/20000: Train Loss = 0.443887, Test Loss = 0.250532, Learning Rate = 3.864036e-04\n",
      "Epoch 2504/20000: Train Loss = 0.443473, Test Loss = 0.234945, Learning Rate = 3.862568e-04\n",
      "Epoch 2505/20000: Train Loss = 0.444723, Test Loss = 0.256870, Learning Rate = 3.861100e-04\n",
      "Epoch 2506/20000: Train Loss = 0.445735, Test Loss = 0.249571, Learning Rate = 3.859633e-04\n",
      "Epoch 2507/20000: Train Loss = 0.444683, Test Loss = 0.244123, Learning Rate = 3.858166e-04\n",
      "Epoch 2508/20000: Train Loss = 0.444000, Test Loss = 0.249491, Learning Rate = 3.856700e-04\n",
      "Epoch 2509/20000: Train Loss = 0.443588, Test Loss = 0.247456, Learning Rate = 3.855235e-04\n",
      "Epoch 2510/20000: Train Loss = 0.445253, Test Loss = 0.247746, Learning Rate = 3.853770e-04\n",
      "Epoch 2511/20000: Train Loss = 0.444605, Test Loss = 0.257792, Learning Rate = 3.852306e-04\n",
      "Epoch 2512/20000: Train Loss = 0.443581, Test Loss = 0.252877, Learning Rate = 3.850842e-04\n",
      "Epoch 2513/20000: Train Loss = 0.446029, Test Loss = 0.231821, Learning Rate = 3.849379e-04\n",
      "Epoch 2514/20000: Train Loss = 0.443937, Test Loss = 0.247054, Learning Rate = 3.847916e-04\n",
      "Epoch 2515/20000: Train Loss = 0.445116, Test Loss = 0.257700, Learning Rate = 3.846454e-04\n",
      "Epoch 2516/20000: Train Loss = 0.447491, Test Loss = 0.260518, Learning Rate = 3.844992e-04\n",
      "Epoch 2517/20000: Train Loss = 0.443273, Test Loss = 0.223593, Learning Rate = 3.843531e-04\n",
      "Epoch 2518/20000: Train Loss = 0.444079, Test Loss = 0.252328, Learning Rate = 3.842071e-04\n",
      "Epoch 2519/20000: Train Loss = 0.443877, Test Loss = 0.249040, Learning Rate = 3.840611e-04\n",
      "Epoch 2520/20000: Train Loss = 0.444171, Test Loss = 0.250830, Learning Rate = 3.839152e-04\n",
      "Epoch 2521/20000: Train Loss = 0.444321, Test Loss = 0.248602, Learning Rate = 3.837693e-04\n",
      "Epoch 2522/20000: Train Loss = 0.446122, Test Loss = 0.257263, Learning Rate = 3.836235e-04\n",
      "Epoch 2523/20000: Train Loss = 0.444751, Test Loss = 0.255517, Learning Rate = 3.834777e-04\n",
      "Epoch 2524/20000: Train Loss = 0.444084, Test Loss = 0.245553, Learning Rate = 3.833320e-04\n",
      "Epoch 2525/20000: Train Loss = 0.446128, Test Loss = 0.216366, Learning Rate = 3.831863e-04\n",
      "Epoch 2526/20000: Train Loss = 0.446536, Test Loss = 0.245203, Learning Rate = 3.830407e-04\n",
      "Epoch 2527/20000: Train Loss = 0.447145, Test Loss = 0.248242, Learning Rate = 3.828952e-04\n",
      "Epoch 2528/20000: Train Loss = 0.445699, Test Loss = 0.238280, Learning Rate = 3.827497e-04\n",
      "Epoch 2529/20000: Train Loss = 0.445492, Test Loss = 0.233327, Learning Rate = 3.826043e-04\n",
      "Epoch 2530/20000: Train Loss = 0.448772, Test Loss = 0.245199, Learning Rate = 3.824589e-04\n",
      "Epoch 2531/20000: Train Loss = 0.445587, Test Loss = 0.247843, Learning Rate = 3.823136e-04\n",
      "Epoch 2532/20000: Train Loss = 0.444875, Test Loss = 0.247904, Learning Rate = 3.821683e-04\n",
      "Epoch 2533/20000: Train Loss = 0.444241, Test Loss = 0.239197, Learning Rate = 3.820231e-04\n",
      "Epoch 2534/20000: Train Loss = 0.444204, Test Loss = 0.240828, Learning Rate = 3.818779e-04\n",
      "Epoch 2535/20000: Train Loss = 0.443844, Test Loss = 0.242069, Learning Rate = 3.817328e-04\n",
      "Epoch 2536/20000: Train Loss = 0.443811, Test Loss = 0.253261, Learning Rate = 3.815878e-04\n",
      "Epoch 2537/20000: Train Loss = 0.444433, Test Loss = 0.251031, Learning Rate = 3.814428e-04\n",
      "Epoch 2538/20000: Train Loss = 0.444294, Test Loss = 0.247295, Learning Rate = 3.812978e-04\n",
      "Epoch 2539/20000: Train Loss = 0.443922, Test Loss = 0.244859, Learning Rate = 3.811530e-04\n",
      "Epoch 2540/20000: Train Loss = 0.445047, Test Loss = 0.234637, Learning Rate = 3.810081e-04\n",
      "Epoch 2541/20000: Train Loss = 0.445687, Test Loss = 0.243285, Learning Rate = 3.808634e-04\n",
      "Epoch 2542/20000: Train Loss = 0.444628, Test Loss = 0.269685, Learning Rate = 3.807186e-04\n",
      "Epoch 2543/20000: Train Loss = 0.443817, Test Loss = 0.245684, Learning Rate = 3.805740e-04\n",
      "Epoch 2544/20000: Train Loss = 0.443726, Test Loss = 0.248583, Learning Rate = 3.804294e-04\n",
      "Epoch 2545/20000: Train Loss = 0.446102, Test Loss = 0.247386, Learning Rate = 3.802848e-04\n",
      "Epoch 2546/20000: Train Loss = 0.445959, Test Loss = 0.239308, Learning Rate = 3.801403e-04\n",
      "Epoch 2547/20000: Train Loss = 0.446800, Test Loss = 0.245829, Learning Rate = 3.799959e-04\n",
      "Epoch 2548/20000: Train Loss = 0.446218, Test Loss = 0.259188, Learning Rate = 3.798515e-04\n",
      "Epoch 2549/20000: Train Loss = 0.448028, Test Loss = 0.235594, Learning Rate = 3.797072e-04\n",
      "Epoch 2550/20000: Train Loss = 0.445518, Test Loss = 0.247275, Learning Rate = 3.795629e-04\n",
      "Epoch 2551/20000: Train Loss = 0.445921, Test Loss = 0.242518, Learning Rate = 3.794187e-04\n",
      "Epoch 2552/20000: Train Loss = 0.445907, Test Loss = 0.235061, Learning Rate = 3.792745e-04\n",
      "Epoch 2553/20000: Train Loss = 0.444802, Test Loss = 0.241660, Learning Rate = 3.791304e-04\n",
      "Epoch 2554/20000: Train Loss = 0.443584, Test Loss = 0.249462, Learning Rate = 3.789863e-04\n",
      "Epoch 2555/20000: Train Loss = 0.444611, Test Loss = 0.235083, Learning Rate = 3.788423e-04\n",
      "Epoch 2556/20000: Train Loss = 0.445015, Test Loss = 0.243473, Learning Rate = 3.786984e-04\n",
      "Epoch 2557/20000: Train Loss = 0.444965, Test Loss = 0.262490, Learning Rate = 3.785545e-04\n",
      "Epoch 2558/20000: Train Loss = 0.444852, Test Loss = 0.233384, Learning Rate = 3.784106e-04\n",
      "Epoch 2559/20000: Train Loss = 0.445213, Test Loss = 0.251786, Learning Rate = 3.782668e-04\n",
      "Epoch 2560/20000: Train Loss = 0.443947, Test Loss = 0.245156, Learning Rate = 3.781231e-04\n",
      "Epoch 2561/20000: Train Loss = 0.443839, Test Loss = 0.249879, Learning Rate = 3.779794e-04\n",
      "Epoch 2562/20000: Train Loss = 0.444123, Test Loss = 0.248706, Learning Rate = 3.778358e-04\n",
      "Epoch 2563/20000: Train Loss = 0.444598, Test Loss = 0.237712, Learning Rate = 3.776922e-04\n",
      "Epoch 2564/20000: Train Loss = 0.444801, Test Loss = 0.242611, Learning Rate = 3.775487e-04\n",
      "Epoch 2565/20000: Train Loss = 0.446249, Test Loss = 0.249547, Learning Rate = 3.774053e-04\n",
      "Epoch 2566/20000: Train Loss = 0.444318, Test Loss = 0.239349, Learning Rate = 3.772619e-04\n",
      "Epoch 2567/20000: Train Loss = 0.444675, Test Loss = 0.243603, Learning Rate = 3.771185e-04\n",
      "Epoch 2568/20000: Train Loss = 0.444331, Test Loss = 0.239350, Learning Rate = 3.769752e-04\n",
      "Epoch 2569/20000: Train Loss = 0.443884, Test Loss = 0.242821, Learning Rate = 3.768320e-04\n",
      "Epoch 2570/20000: Train Loss = 0.444007, Test Loss = 0.251005, Learning Rate = 3.766888e-04\n",
      "Epoch 2571/20000: Train Loss = 0.444518, Test Loss = 0.255748, Learning Rate = 3.765457e-04\n",
      "Epoch 2572/20000: Train Loss = 0.445701, Test Loss = 0.249319, Learning Rate = 3.764026e-04\n",
      "Epoch 2573/20000: Train Loss = 0.443813, Test Loss = 0.240171, Learning Rate = 3.762596e-04\n",
      "Epoch 2574/20000: Train Loss = 0.445327, Test Loss = 0.243088, Learning Rate = 3.761166e-04\n",
      "Epoch 2575/20000: Train Loss = 0.443978, Test Loss = 0.248969, Learning Rate = 3.759737e-04\n",
      "Epoch 2576/20000: Train Loss = 0.443548, Test Loss = 0.240522, Learning Rate = 3.758308e-04\n",
      "Epoch 2577/20000: Train Loss = 0.445346, Test Loss = 0.246918, Learning Rate = 3.756880e-04\n",
      "Epoch 2578/20000: Train Loss = 0.444055, Test Loss = 0.255619, Learning Rate = 3.755453e-04\n",
      "Epoch 2579/20000: Train Loss = 0.444625, Test Loss = 0.253765, Learning Rate = 3.754026e-04\n",
      "Epoch 2580/20000: Train Loss = 0.444859, Test Loss = 0.242143, Learning Rate = 3.752599e-04\n",
      "Epoch 2581/20000: Train Loss = 0.444811, Test Loss = 0.259033, Learning Rate = 3.751173e-04\n",
      "Epoch 2582/20000: Train Loss = 0.446038, Test Loss = 0.245511, Learning Rate = 3.749748e-04\n",
      "Epoch 2583/20000: Train Loss = 0.445033, Test Loss = 0.255154, Learning Rate = 3.748323e-04\n",
      "Epoch 2584/20000: Train Loss = 0.444492, Test Loss = 0.265017, Learning Rate = 3.746899e-04\n",
      "Epoch 2585/20000: Train Loss = 0.446842, Test Loss = 0.222962, Learning Rate = 3.745475e-04\n",
      "Epoch 2586/20000: Train Loss = 0.445484, Test Loss = 0.231254, Learning Rate = 3.744052e-04\n",
      "Epoch 2587/20000: Train Loss = 0.444890, Test Loss = 0.251802, Learning Rate = 3.742629e-04\n",
      "Epoch 2588/20000: Train Loss = 0.445254, Test Loss = 0.248042, Learning Rate = 3.741207e-04\n",
      "Epoch 2589/20000: Train Loss = 0.445479, Test Loss = 0.256744, Learning Rate = 3.739786e-04\n",
      "Epoch 2590/20000: Train Loss = 0.444386, Test Loss = 0.251863, Learning Rate = 3.738365e-04\n",
      "Epoch 2591/20000: Train Loss = 0.444491, Test Loss = 0.249019, Learning Rate = 3.736944e-04\n",
      "Epoch 2592/20000: Train Loss = 0.443021, Test Loss = 0.287441, Learning Rate = 3.735524e-04\n",
      "Epoch 2593/20000: Train Loss = 0.445794, Test Loss = 0.247233, Learning Rate = 3.734105e-04\n",
      "Epoch 2594/20000: Train Loss = 0.448143, Test Loss = 0.230862, Learning Rate = 3.732686e-04\n",
      "Epoch 2595/20000: Train Loss = 0.444753, Test Loss = 0.247944, Learning Rate = 3.731268e-04\n",
      "Epoch 2596/20000: Train Loss = 0.445759, Test Loss = 0.233192, Learning Rate = 3.729850e-04\n",
      "Epoch 2597/20000: Train Loss = 0.446064, Test Loss = 0.242877, Learning Rate = 3.728433e-04\n",
      "Epoch 2598/20000: Train Loss = 0.444804, Test Loss = 0.230423, Learning Rate = 3.727016e-04\n",
      "Epoch 2599/20000: Train Loss = 0.443463, Test Loss = 0.249498, Learning Rate = 3.725600e-04\n",
      "Epoch 2600/20000: Train Loss = 0.444392, Test Loss = 0.247061, Learning Rate = 3.724184e-04\n",
      "Epoch 2601/20000: Train Loss = 0.447288, Test Loss = 0.261060, Learning Rate = 3.722769e-04\n",
      "Epoch 2602/20000: Train Loss = 0.445335, Test Loss = 0.261657, Learning Rate = 3.721355e-04\n",
      "Epoch 2603/20000: Train Loss = 0.444353, Test Loss = 0.245183, Learning Rate = 3.719941e-04\n",
      "Epoch 2604/20000: Train Loss = 0.444545, Test Loss = 0.245840, Learning Rate = 3.718527e-04\n",
      "Epoch 2605/20000: Train Loss = 0.443941, Test Loss = 0.240893, Learning Rate = 3.717114e-04\n",
      "Epoch 2606/20000: Train Loss = 0.444964, Test Loss = 0.246387, Learning Rate = 3.715702e-04\n",
      "Epoch 2607/20000: Train Loss = 0.443648, Test Loss = 0.240853, Learning Rate = 3.714290e-04\n",
      "Epoch 2608/20000: Train Loss = 0.444770, Test Loss = 0.252250, Learning Rate = 3.712879e-04\n",
      "Epoch 2609/20000: Train Loss = 0.444013, Test Loss = 0.245332, Learning Rate = 3.711468e-04\n",
      "Epoch 2610/20000: Train Loss = 0.443781, Test Loss = 0.234905, Learning Rate = 3.710057e-04\n",
      "Epoch 2611/20000: Train Loss = 0.444656, Test Loss = 0.244787, Learning Rate = 3.708648e-04\n",
      "Epoch 2612/20000: Train Loss = 0.445067, Test Loss = 0.257215, Learning Rate = 3.707239e-04\n",
      "Epoch 2613/20000: Train Loss = 0.445876, Test Loss = 0.232109, Learning Rate = 3.705830e-04\n",
      "Epoch 2614/20000: Train Loss = 0.444593, Test Loss = 0.264724, Learning Rate = 3.704422e-04\n",
      "Epoch 2615/20000: Train Loss = 0.445454, Test Loss = 0.244347, Learning Rate = 3.703014e-04\n",
      "Epoch 2616/20000: Train Loss = 0.445090, Test Loss = 0.248308, Learning Rate = 3.701607e-04\n",
      "Epoch 2617/20000: Train Loss = 0.444060, Test Loss = 0.243245, Learning Rate = 3.700201e-04\n",
      "Epoch 2618/20000: Train Loss = 0.443273, Test Loss = 0.243183, Learning Rate = 3.698795e-04\n",
      "Epoch 2619/20000: Train Loss = 0.443862, Test Loss = 0.252723, Learning Rate = 3.697389e-04\n",
      "Epoch 2620/20000: Train Loss = 0.444250, Test Loss = 0.245772, Learning Rate = 3.695984e-04\n",
      "Epoch 2621/20000: Train Loss = 0.445271, Test Loss = 0.257390, Learning Rate = 3.694580e-04\n",
      "Epoch 2622/20000: Train Loss = 0.444136, Test Loss = 0.246927, Learning Rate = 3.693176e-04\n",
      "Epoch 2623/20000: Train Loss = 0.444367, Test Loss = 0.247427, Learning Rate = 3.691773e-04\n",
      "Epoch 2624/20000: Train Loss = 0.443889, Test Loss = 0.244762, Learning Rate = 3.690370e-04\n",
      "Epoch 2625/20000: Train Loss = 0.443950, Test Loss = 0.255750, Learning Rate = 3.688968e-04\n",
      "Epoch 2626/20000: Train Loss = 0.444201, Test Loss = 0.237465, Learning Rate = 3.687566e-04\n",
      "Epoch 2627/20000: Train Loss = 0.446128, Test Loss = 0.255647, Learning Rate = 3.686165e-04\n",
      "Epoch 2628/20000: Train Loss = 0.445061, Test Loss = 0.235213, Learning Rate = 3.684764e-04\n",
      "Epoch 2629/20000: Train Loss = 0.443489, Test Loss = 0.241545, Learning Rate = 3.683364e-04\n",
      "Epoch 2630/20000: Train Loss = 0.444653, Test Loss = 0.244916, Learning Rate = 3.681965e-04\n",
      "Epoch 2631/20000: Train Loss = 0.443982, Test Loss = 0.243640, Learning Rate = 3.680566e-04\n",
      "Epoch 2632/20000: Train Loss = 0.445363, Test Loss = 0.239072, Learning Rate = 3.679167e-04\n",
      "Epoch 2633/20000: Train Loss = 0.443841, Test Loss = 0.257624, Learning Rate = 3.677769e-04\n",
      "Epoch 2634/20000: Train Loss = 0.446967, Test Loss = 0.244851, Learning Rate = 3.676372e-04\n",
      "Epoch 2635/20000: Train Loss = 0.445048, Test Loss = 0.245022, Learning Rate = 3.674975e-04\n",
      "Epoch 2636/20000: Train Loss = 0.443954, Test Loss = 0.243878, Learning Rate = 3.673578e-04\n",
      "Epoch 2637/20000: Train Loss = 0.446324, Test Loss = 0.245038, Learning Rate = 3.672182e-04\n",
      "Epoch 2638/20000: Train Loss = 0.444311, Test Loss = 0.259833, Learning Rate = 3.670787e-04\n",
      "Epoch 2639/20000: Train Loss = 0.445163, Test Loss = 0.247273, Learning Rate = 3.669392e-04\n",
      "Epoch 2640/20000: Train Loss = 0.444102, Test Loss = 0.241926, Learning Rate = 3.667998e-04\n",
      "Epoch 2641/20000: Train Loss = 0.445065, Test Loss = 0.244812, Learning Rate = 3.666604e-04\n",
      "Epoch 2642/20000: Train Loss = 0.446501, Test Loss = 0.239602, Learning Rate = 3.665211e-04\n",
      "Epoch 2643/20000: Train Loss = 0.445278, Test Loss = 0.243539, Learning Rate = 3.663818e-04\n",
      "Epoch 2644/20000: Train Loss = 0.445436, Test Loss = 0.241478, Learning Rate = 3.662426e-04\n",
      "Epoch 2645/20000: Train Loss = 0.443914, Test Loss = 0.242878, Learning Rate = 3.661035e-04\n",
      "Epoch 2646/20000: Train Loss = 0.444785, Test Loss = 0.238805, Learning Rate = 3.659644e-04\n",
      "Epoch 2647/20000: Train Loss = 0.446414, Test Loss = 0.251450, Learning Rate = 3.658253e-04\n",
      "Epoch 2648/20000: Train Loss = 0.444791, Test Loss = 0.244439, Learning Rate = 3.656863e-04\n",
      "Epoch 2649/20000: Train Loss = 0.446263, Test Loss = 0.247599, Learning Rate = 3.655473e-04\n",
      "Epoch 2650/20000: Train Loss = 0.446708, Test Loss = 0.250465, Learning Rate = 3.654084e-04\n",
      "Epoch 2651/20000: Train Loss = 0.445476, Test Loss = 0.247560, Learning Rate = 3.652696e-04\n",
      "Epoch 2652/20000: Train Loss = 0.445101, Test Loss = 0.229384, Learning Rate = 3.651308e-04\n",
      "Epoch 2653/20000: Train Loss = 0.444449, Test Loss = 0.250769, Learning Rate = 3.649921e-04\n",
      "Epoch 2654/20000: Train Loss = 0.443704, Test Loss = 0.251530, Learning Rate = 3.648534e-04\n",
      "Epoch 2655/20000: Train Loss = 0.444773, Test Loss = 0.251531, Learning Rate = 3.647147e-04\n",
      "Epoch 2656/20000: Train Loss = 0.445156, Test Loss = 0.257505, Learning Rate = 3.645762e-04\n",
      "Epoch 2657/20000: Train Loss = 0.444130, Test Loss = 0.242902, Learning Rate = 3.644376e-04\n",
      "Epoch 2658/20000: Train Loss = 0.444528, Test Loss = 0.247802, Learning Rate = 3.642992e-04\n",
      "Epoch 2659/20000: Train Loss = 0.443625, Test Loss = 0.240912, Learning Rate = 3.641607e-04\n",
      "Epoch 2660/20000: Train Loss = 0.444071, Test Loss = 0.247279, Learning Rate = 3.640224e-04\n",
      "Epoch 2661/20000: Train Loss = 0.444990, Test Loss = 0.251576, Learning Rate = 3.638840e-04\n",
      "Epoch 2662/20000: Train Loss = 0.443818, Test Loss = 0.253108, Learning Rate = 3.637458e-04\n",
      "Epoch 2663/20000: Train Loss = 0.445501, Test Loss = 0.239297, Learning Rate = 3.636076e-04\n",
      "Epoch 2664/20000: Train Loss = 0.443569, Test Loss = 0.244287, Learning Rate = 3.634694e-04\n",
      "Epoch 2665/20000: Train Loss = 0.443484, Test Loss = 0.241231, Learning Rate = 3.633313e-04\n",
      "Epoch 2666/20000: Train Loss = 0.444339, Test Loss = 0.254698, Learning Rate = 3.631932e-04\n",
      "Epoch 2667/20000: Train Loss = 0.445737, Test Loss = 0.236982, Learning Rate = 3.630552e-04\n",
      "Epoch 2668/20000: Train Loss = 0.443885, Test Loss = 0.247355, Learning Rate = 3.629173e-04\n",
      "Epoch 2669/20000: Train Loss = 0.445018, Test Loss = 0.236891, Learning Rate = 3.627794e-04\n",
      "Epoch 2670/20000: Train Loss = 0.444160, Test Loss = 0.235197, Learning Rate = 3.626415e-04\n",
      "Epoch 2671/20000: Train Loss = 0.446085, Test Loss = 0.240915, Learning Rate = 3.625037e-04\n",
      "Epoch 2672/20000: Train Loss = 0.445853, Test Loss = 0.255489, Learning Rate = 3.623660e-04\n",
      "Epoch 2673/20000: Train Loss = 0.447692, Test Loss = 0.248795, Learning Rate = 3.622283e-04\n",
      "Epoch 2674/20000: Train Loss = 0.446105, Test Loss = 0.242578, Learning Rate = 3.620907e-04\n",
      "Epoch 2675/20000: Train Loss = 0.443583, Test Loss = 0.249893, Learning Rate = 3.619531e-04\n",
      "Epoch 2676/20000: Train Loss = 0.445157, Test Loss = 0.244841, Learning Rate = 3.618156e-04\n",
      "Epoch 2677/20000: Train Loss = 0.443564, Test Loss = 0.241880, Learning Rate = 3.616781e-04\n",
      "Epoch 2678/20000: Train Loss = 0.443794, Test Loss = 0.249585, Learning Rate = 3.615407e-04\n",
      "Epoch 2679/20000: Train Loss = 0.445814, Test Loss = 0.242490, Learning Rate = 3.614033e-04\n",
      "Epoch 2680/20000: Train Loss = 0.444937, Test Loss = 0.260860, Learning Rate = 3.612660e-04\n",
      "Epoch 2681/20000: Train Loss = 0.445561, Test Loss = 0.247874, Learning Rate = 3.611287e-04\n",
      "Epoch 2682/20000: Train Loss = 0.443536, Test Loss = 0.251008, Learning Rate = 3.609915e-04\n",
      "Epoch 2683/20000: Train Loss = 0.444538, Test Loss = 0.237737, Learning Rate = 3.608543e-04\n",
      "Epoch 2684/20000: Train Loss = 0.446274, Test Loss = 0.259061, Learning Rate = 3.607172e-04\n",
      "Epoch 2685/20000: Train Loss = 0.444836, Test Loss = 0.251372, Learning Rate = 3.605801e-04\n",
      "Epoch 2686/20000: Train Loss = 0.445808, Test Loss = 0.251065, Learning Rate = 3.604431e-04\n",
      "Epoch 2687/20000: Train Loss = 0.443562, Test Loss = 0.234759, Learning Rate = 3.603061e-04\n",
      "Epoch 2688/20000: Train Loss = 0.444179, Test Loss = 0.247805, Learning Rate = 3.601692e-04\n",
      "Epoch 2689/20000: Train Loss = 0.444396, Test Loss = 0.237647, Learning Rate = 3.600324e-04\n",
      "Epoch 2690/20000: Train Loss = 0.443880, Test Loss = 0.242602, Learning Rate = 3.598956e-04\n",
      "Epoch 2691/20000: Train Loss = 0.444046, Test Loss = 0.253382, Learning Rate = 3.597588e-04\n",
      "Epoch 2692/20000: Train Loss = 0.444949, Test Loss = 0.247624, Learning Rate = 3.596221e-04\n",
      "Epoch 2693/20000: Train Loss = 0.443882, Test Loss = 0.239450, Learning Rate = 3.594855e-04\n",
      "Epoch 2694/20000: Train Loss = 0.444683, Test Loss = 0.247520, Learning Rate = 3.593489e-04\n",
      "Epoch 2695/20000: Train Loss = 0.443403, Test Loss = 0.242121, Learning Rate = 3.592123e-04\n",
      "Epoch 2696/20000: Train Loss = 0.444221, Test Loss = 0.245708, Learning Rate = 3.590759e-04\n",
      "Epoch 2697/20000: Train Loss = 0.445481, Test Loss = 0.240573, Learning Rate = 3.589394e-04\n",
      "Epoch 2698/20000: Train Loss = 0.444788, Test Loss = 0.249155, Learning Rate = 3.588030e-04\n",
      "Epoch 2699/20000: Train Loss = 0.445139, Test Loss = 0.244653, Learning Rate = 3.586667e-04\n",
      "Epoch 2700/20000: Train Loss = 0.445370, Test Loss = 0.256679, Learning Rate = 3.585304e-04\n",
      "Epoch 2701/20000: Train Loss = 0.445134, Test Loss = 0.238773, Learning Rate = 3.583942e-04\n",
      "Epoch 2702/20000: Train Loss = 0.446077, Test Loss = 0.262306, Learning Rate = 3.582580e-04\n",
      "Epoch 2703/20000: Train Loss = 0.444455, Test Loss = 0.237850, Learning Rate = 3.581219e-04\n",
      "Epoch 2704/20000: Train Loss = 0.443931, Test Loss = 0.240867, Learning Rate = 3.579858e-04\n",
      "Epoch 2705/20000: Train Loss = 0.444508, Test Loss = 0.243156, Learning Rate = 3.578498e-04\n",
      "Epoch 2706/20000: Train Loss = 0.445831, Test Loss = 0.243921, Learning Rate = 3.577138e-04\n",
      "Epoch 2707/20000: Train Loss = 0.444686, Test Loss = 0.233163, Learning Rate = 3.575779e-04\n",
      "Epoch 2708/20000: Train Loss = 0.444415, Test Loss = 0.238509, Learning Rate = 3.574420e-04\n",
      "Epoch 2709/20000: Train Loss = 0.444418, Test Loss = 0.245747, Learning Rate = 3.573062e-04\n",
      "Epoch 2710/20000: Train Loss = 0.443425, Test Loss = 0.236607, Learning Rate = 3.571704e-04\n",
      "Epoch 2711/20000: Train Loss = 0.444259, Test Loss = 0.242295, Learning Rate = 3.570347e-04\n",
      "Epoch 2712/20000: Train Loss = 0.444891, Test Loss = 0.238081, Learning Rate = 3.568990e-04\n",
      "Epoch 2713/20000: Train Loss = 0.443276, Test Loss = 0.253248, Learning Rate = 3.567634e-04\n",
      "Epoch 2714/20000: Train Loss = 0.445732, Test Loss = 0.246800, Learning Rate = 3.566279e-04\n",
      "Epoch 2715/20000: Train Loss = 0.445156, Test Loss = 0.251533, Learning Rate = 3.564924e-04\n",
      "Epoch 2716/20000: Train Loss = 0.444169, Test Loss = 0.241206, Learning Rate = 3.563569e-04\n",
      "Epoch 2717/20000: Train Loss = 0.444896, Test Loss = 0.252723, Learning Rate = 3.562215e-04\n",
      "Epoch 2718/20000: Train Loss = 0.443763, Test Loss = 0.235418, Learning Rate = 3.560861e-04\n",
      "Epoch 2719/20000: Train Loss = 0.444536, Test Loss = 0.250361, Learning Rate = 3.559508e-04\n",
      "Epoch 2720/20000: Train Loss = 0.443900, Test Loss = 0.243555, Learning Rate = 3.558156e-04\n",
      "Epoch 2721/20000: Train Loss = 0.443598, Test Loss = 0.237108, Learning Rate = 3.556804e-04\n",
      "Epoch 2722/20000: Train Loss = 0.445769, Test Loss = 0.243657, Learning Rate = 3.555452e-04\n",
      "Epoch 2723/20000: Train Loss = 0.444387, Test Loss = 0.246916, Learning Rate = 3.554101e-04\n",
      "Epoch 2724/20000: Train Loss = 0.444178, Test Loss = 0.245564, Learning Rate = 3.552751e-04\n",
      "Epoch 2725/20000: Train Loss = 0.444607, Test Loss = 0.234582, Learning Rate = 3.551401e-04\n",
      "Epoch 2726/20000: Train Loss = 0.446328, Test Loss = 0.248250, Learning Rate = 3.550052e-04\n",
      "Epoch 2727/20000: Train Loss = 0.444325, Test Loss = 0.247268, Learning Rate = 3.548703e-04\n",
      "Epoch 2728/20000: Train Loss = 0.445794, Test Loss = 0.242477, Learning Rate = 3.547354e-04\n",
      "Epoch 2729/20000: Train Loss = 0.444616, Test Loss = 0.239834, Learning Rate = 3.546006e-04\n",
      "Epoch 2730/20000: Train Loss = 0.444073, Test Loss = 0.237103, Learning Rate = 3.544659e-04\n",
      "Epoch 2731/20000: Train Loss = 0.444822, Test Loss = 0.243824, Learning Rate = 3.543312e-04\n",
      "Epoch 2732/20000: Train Loss = 0.443552, Test Loss = 0.244733, Learning Rate = 3.541966e-04\n",
      "Epoch 2733/20000: Train Loss = 0.444017, Test Loss = 0.244693, Learning Rate = 3.540620e-04\n",
      "Epoch 2734/20000: Train Loss = 0.445042, Test Loss = 0.238498, Learning Rate = 3.539275e-04\n",
      "Epoch 2735/20000: Train Loss = 0.444836, Test Loss = 0.248967, Learning Rate = 3.537930e-04\n",
      "Epoch 2736/20000: Train Loss = 0.444574, Test Loss = 0.243177, Learning Rate = 3.536585e-04\n",
      "Epoch 2737/20000: Train Loss = 0.443846, Test Loss = 0.255245, Learning Rate = 3.535242e-04\n",
      "Epoch 2738/20000: Train Loss = 0.444362, Test Loss = 0.237887, Learning Rate = 3.533898e-04\n",
      "Epoch 2739/20000: Train Loss = 0.446537, Test Loss = 0.247585, Learning Rate = 3.532555e-04\n",
      "Epoch 2740/20000: Train Loss = 0.445568, Test Loss = 0.249792, Learning Rate = 3.531213e-04\n",
      "Epoch 2741/20000: Train Loss = 0.444176, Test Loss = 0.241806, Learning Rate = 3.529871e-04\n",
      "Epoch 2742/20000: Train Loss = 0.444412, Test Loss = 0.247265, Learning Rate = 3.528530e-04\n",
      "Epoch 2743/20000: Train Loss = 0.443448, Test Loss = 0.259062, Learning Rate = 3.527189e-04\n",
      "Epoch 2744/20000: Train Loss = 0.445221, Test Loss = 0.243495, Learning Rate = 3.525849e-04\n",
      "Epoch 2745/20000: Train Loss = 0.444368, Test Loss = 0.252169, Learning Rate = 3.524509e-04\n",
      "Epoch 2746/20000: Train Loss = 0.444471, Test Loss = 0.245985, Learning Rate = 3.523170e-04\n",
      "Epoch 2747/20000: Train Loss = 0.444498, Test Loss = 0.250880, Learning Rate = 3.521832e-04\n",
      "Epoch 2748/20000: Train Loss = 0.446151, Test Loss = 0.240726, Learning Rate = 3.520493e-04\n",
      "Epoch 2749/20000: Train Loss = 0.445767, Test Loss = 0.267324, Learning Rate = 3.519156e-04\n",
      "Epoch 2750/20000: Train Loss = 0.444166, Test Loss = 0.240276, Learning Rate = 3.517818e-04\n",
      "Epoch 2751/20000: Train Loss = 0.445119, Test Loss = 0.246196, Learning Rate = 3.516482e-04\n",
      "Epoch 2752/20000: Train Loss = 0.445105, Test Loss = 0.242864, Learning Rate = 3.515146e-04\n",
      "Epoch 2753/20000: Train Loss = 0.445188, Test Loss = 0.235077, Learning Rate = 3.513810e-04\n",
      "Epoch 2754/20000: Train Loss = 0.445776, Test Loss = 0.238380, Learning Rate = 3.512475e-04\n",
      "Epoch 2755/20000: Train Loss = 0.445840, Test Loss = 0.242312, Learning Rate = 3.511140e-04\n",
      "Epoch 2756/20000: Train Loss = 0.444642, Test Loss = 0.238114, Learning Rate = 3.509806e-04\n",
      "Epoch 2757/20000: Train Loss = 0.444268, Test Loss = 0.251349, Learning Rate = 3.508472e-04\n",
      "Epoch 2758/20000: Train Loss = 0.444412, Test Loss = 0.240082, Learning Rate = 3.507139e-04\n",
      "Epoch 2759/20000: Train Loss = 0.443314, Test Loss = 0.256903, Learning Rate = 3.505807e-04\n",
      "Epoch 2760/20000: Train Loss = 0.443507, Test Loss = 0.243516, Learning Rate = 3.504475e-04\n",
      "Epoch 2761/20000: Train Loss = 0.444463, Test Loss = 0.241866, Learning Rate = 3.503143e-04\n",
      "Epoch 2762/20000: Train Loss = 0.447657, Test Loss = 0.246464, Learning Rate = 3.501812e-04\n",
      "Epoch 2763/20000: Train Loss = 0.444353, Test Loss = 0.241958, Learning Rate = 3.500481e-04\n",
      "Epoch 2764/20000: Train Loss = 0.444416, Test Loss = 0.253220, Learning Rate = 3.499151e-04\n",
      "Epoch 2765/20000: Train Loss = 0.446434, Test Loss = 0.246105, Learning Rate = 3.497822e-04\n",
      "Epoch 2766/20000: Train Loss = 0.446973, Test Loss = 0.242102, Learning Rate = 3.496493e-04\n",
      "Epoch 2767/20000: Train Loss = 0.445602, Test Loss = 0.262021, Learning Rate = 3.495164e-04\n",
      "Epoch 2768/20000: Train Loss = 0.444789, Test Loss = 0.238622, Learning Rate = 3.493836e-04\n",
      "Epoch 2769/20000: Train Loss = 0.447385, Test Loss = 0.239948, Learning Rate = 3.492508e-04\n",
      "Epoch 2770/20000: Train Loss = 0.444897, Test Loss = 0.251582, Learning Rate = 3.491181e-04\n",
      "Epoch 2771/20000: Train Loss = 0.443880, Test Loss = 0.243529, Learning Rate = 3.489855e-04\n",
      "Epoch 2772/20000: Train Loss = 0.443226, Test Loss = 0.242548, Learning Rate = 3.488529e-04\n",
      "Epoch 2773/20000: Train Loss = 0.444418, Test Loss = 0.259958, Learning Rate = 3.487203e-04\n",
      "Epoch 2774/20000: Train Loss = 0.444061, Test Loss = 0.246760, Learning Rate = 3.485878e-04\n",
      "Epoch 2775/20000: Train Loss = 0.444272, Test Loss = 0.241619, Learning Rate = 3.484554e-04\n",
      "Epoch 2776/20000: Train Loss = 0.443534, Test Loss = 0.244024, Learning Rate = 3.483229e-04\n",
      "Epoch 2777/20000: Train Loss = 0.443492, Test Loss = 0.255354, Learning Rate = 3.481906e-04\n",
      "Epoch 2778/20000: Train Loss = 0.446138, Test Loss = 0.245230, Learning Rate = 3.480583e-04\n",
      "Epoch 2779/20000: Train Loss = 0.444802, Test Loss = 0.254061, Learning Rate = 3.479260e-04\n",
      "Epoch 2780/20000: Train Loss = 0.443097, Test Loss = 0.248254, Learning Rate = 3.477938e-04\n",
      "Epoch 2781/20000: Train Loss = 0.443769, Test Loss = 0.244621, Learning Rate = 3.476617e-04\n",
      "Epoch 2782/20000: Train Loss = 0.444483, Test Loss = 0.249817, Learning Rate = 3.475296e-04\n",
      "Epoch 2783/20000: Train Loss = 0.444215, Test Loss = 0.253219, Learning Rate = 3.473975e-04\n",
      "Epoch 2784/20000: Train Loss = 0.443673, Test Loss = 0.255737, Learning Rate = 3.472655e-04\n",
      "Epoch 2785/20000: Train Loss = 0.443919, Test Loss = 0.245773, Learning Rate = 3.471336e-04\n",
      "Epoch 2786/20000: Train Loss = 0.444340, Test Loss = 0.263358, Learning Rate = 3.470017e-04\n",
      "Epoch 2787/20000: Train Loss = 0.443920, Test Loss = 0.226766, Learning Rate = 3.468698e-04\n",
      "Epoch 2788/20000: Train Loss = 0.444006, Test Loss = 0.245179, Learning Rate = 3.467380e-04\n",
      "Epoch 2789/20000: Train Loss = 0.443764, Test Loss = 0.241319, Learning Rate = 3.466063e-04\n",
      "Epoch 2790/20000: Train Loss = 0.444456, Test Loss = 0.237377, Learning Rate = 3.464746e-04\n",
      "Epoch 2791/20000: Train Loss = 0.443459, Test Loss = 0.256396, Learning Rate = 3.463429e-04\n",
      "Epoch 2792/20000: Train Loss = 0.444456, Test Loss = 0.245151, Learning Rate = 3.462113e-04\n",
      "Epoch 2793/20000: Train Loss = 0.444596, Test Loss = 0.227638, Learning Rate = 3.460798e-04\n",
      "Epoch 2794/20000: Train Loss = 0.444064, Test Loss = 0.247013, Learning Rate = 3.459483e-04\n",
      "Epoch 2795/20000: Train Loss = 0.443694, Test Loss = 0.234763, Learning Rate = 3.458168e-04\n",
      "Epoch 2796/20000: Train Loss = 0.444855, Test Loss = 0.249334, Learning Rate = 3.456854e-04\n",
      "Epoch 2797/20000: Train Loss = 0.444464, Test Loss = 0.238188, Learning Rate = 3.455541e-04\n",
      "Epoch 2798/20000: Train Loss = 0.444856, Test Loss = 0.243018, Learning Rate = 3.454228e-04\n",
      "Epoch 2799/20000: Train Loss = 0.446023, Test Loss = 0.236617, Learning Rate = 3.452915e-04\n",
      "Epoch 2800/20000: Train Loss = 0.445528, Test Loss = 0.232397, Learning Rate = 3.451603e-04\n",
      "Epoch 2801/20000: Train Loss = 0.444333, Test Loss = 0.242285, Learning Rate = 3.450292e-04\n",
      "Epoch 2802/20000: Train Loss = 0.443979, Test Loss = 0.238446, Learning Rate = 3.448981e-04\n",
      "Epoch 2803/20000: Train Loss = 0.446050, Test Loss = 0.246830, Learning Rate = 3.447670e-04\n",
      "Epoch 2804/20000: Train Loss = 0.445576, Test Loss = 0.238548, Learning Rate = 3.446360e-04\n",
      "Epoch 2805/20000: Train Loss = 0.443448, Test Loss = 0.235240, Learning Rate = 3.445051e-04\n",
      "Epoch 2806/20000: Train Loss = 0.444342, Test Loss = 0.255069, Learning Rate = 3.443741e-04\n",
      "Epoch 2807/20000: Train Loss = 0.443365, Test Loss = 0.228593, Learning Rate = 3.442433e-04\n",
      "Epoch 2808/20000: Train Loss = 0.446224, Test Loss = 0.232560, Learning Rate = 3.441125e-04\n",
      "Epoch 2809/20000: Train Loss = 0.446215, Test Loss = 0.237296, Learning Rate = 3.439817e-04\n",
      "Epoch 2810/20000: Train Loss = 0.444879, Test Loss = 0.239032, Learning Rate = 3.438510e-04\n",
      "Epoch 2811/20000: Train Loss = 0.443876, Test Loss = 0.244324, Learning Rate = 3.437204e-04\n",
      "Epoch 2812/20000: Train Loss = 0.444192, Test Loss = 0.238082, Learning Rate = 3.435898e-04\n",
      "Epoch 2813/20000: Train Loss = 0.443232, Test Loss = 0.242146, Learning Rate = 3.434592e-04\n",
      "Epoch 2814/20000: Train Loss = 0.443519, Test Loss = 0.248500, Learning Rate = 3.433287e-04\n",
      "Epoch 2815/20000: Train Loss = 0.443970, Test Loss = 0.239998, Learning Rate = 3.431983e-04\n",
      "Epoch 2816/20000: Train Loss = 0.444984, Test Loss = 0.253145, Learning Rate = 3.430679e-04\n",
      "Epoch 2817/20000: Train Loss = 0.443720, Test Loss = 0.245123, Learning Rate = 3.429375e-04\n",
      "Epoch 2818/20000: Train Loss = 0.444430, Test Loss = 0.257467, Learning Rate = 3.428072e-04\n",
      "Epoch 2819/20000: Train Loss = 0.444320, Test Loss = 0.234203, Learning Rate = 3.426769e-04\n",
      "Epoch 2820/20000: Train Loss = 0.445651, Test Loss = 0.243302, Learning Rate = 3.425467e-04\n",
      "Epoch 2821/20000: Train Loss = 0.444434, Test Loss = 0.248053, Learning Rate = 3.424166e-04\n",
      "Epoch 2822/20000: Train Loss = 0.446636, Test Loss = 0.248266, Learning Rate = 3.422865e-04\n",
      "Epoch 2823/20000: Train Loss = 0.444641, Test Loss = 0.241376, Learning Rate = 3.421564e-04\n",
      "Epoch 2824/20000: Train Loss = 0.445829, Test Loss = 0.235604, Learning Rate = 3.420264e-04\n",
      "Epoch 2825/20000: Train Loss = 0.444328, Test Loss = 0.250944, Learning Rate = 3.418964e-04\n",
      "Epoch 2826/20000: Train Loss = 0.444823, Test Loss = 0.236362, Learning Rate = 3.417665e-04\n",
      "Epoch 2827/20000: Train Loss = 0.444268, Test Loss = 0.249986, Learning Rate = 3.416367e-04\n",
      "Epoch 2828/20000: Train Loss = 0.444226, Test Loss = 0.241646, Learning Rate = 3.415068e-04\n",
      "Epoch 2829/20000: Train Loss = 0.446047, Test Loss = 0.246226, Learning Rate = 3.413771e-04\n",
      "Epoch 2830/20000: Train Loss = 0.444173, Test Loss = 0.233198, Learning Rate = 3.412474e-04\n",
      "Epoch 2831/20000: Train Loss = 0.446251, Test Loss = 0.241865, Learning Rate = 3.411177e-04\n",
      "Epoch 2832/20000: Train Loss = 0.444452, Test Loss = 0.251194, Learning Rate = 3.409881e-04\n",
      "Epoch 2833/20000: Train Loss = 0.446284, Test Loss = 0.251028, Learning Rate = 3.408585e-04\n",
      "Epoch 2834/20000: Train Loss = 0.447150, Test Loss = 0.228366, Learning Rate = 3.407290e-04\n",
      "Epoch 2835/20000: Train Loss = 0.444660, Test Loss = 0.240334, Learning Rate = 3.405995e-04\n",
      "Epoch 2836/20000: Train Loss = 0.444297, Test Loss = 0.259074, Learning Rate = 3.404701e-04\n",
      "Epoch 2837/20000: Train Loss = 0.445418, Test Loss = 0.243711, Learning Rate = 3.403407e-04\n",
      "Epoch 2838/20000: Train Loss = 0.445563, Test Loss = 0.232328, Learning Rate = 3.402114e-04\n",
      "Epoch 2839/20000: Train Loss = 0.445699, Test Loss = 0.234807, Learning Rate = 3.400822e-04\n",
      "Epoch 2840/20000: Train Loss = 0.444000, Test Loss = 0.239402, Learning Rate = 3.399529e-04\n",
      "Epoch 2841/20000: Train Loss = 0.444238, Test Loss = 0.246174, Learning Rate = 3.398238e-04\n",
      "Epoch 2842/20000: Train Loss = 0.444795, Test Loss = 0.244990, Learning Rate = 3.396946e-04\n",
      "Epoch 2843/20000: Train Loss = 0.444182, Test Loss = 0.253664, Learning Rate = 3.395656e-04\n",
      "Epoch 2844/20000: Train Loss = 0.446547, Test Loss = 0.235996, Learning Rate = 3.394365e-04\n",
      "Epoch 2845/20000: Train Loss = 0.445447, Test Loss = 0.261647, Learning Rate = 3.393076e-04\n",
      "Epoch 2846/20000: Train Loss = 0.444246, Test Loss = 0.241666, Learning Rate = 3.391786e-04\n",
      "Epoch 2847/20000: Train Loss = 0.444027, Test Loss = 0.249553, Learning Rate = 3.390498e-04\n",
      "Epoch 2848/20000: Train Loss = 0.443738, Test Loss = 0.250813, Learning Rate = 3.389209e-04\n",
      "Epoch 2849/20000: Train Loss = 0.444092, Test Loss = 0.245787, Learning Rate = 3.387921e-04\n",
      "Epoch 2850/20000: Train Loss = 0.444615, Test Loss = 0.247747, Learning Rate = 3.386634e-04\n",
      "Epoch 2851/20000: Train Loss = 0.443717, Test Loss = 0.243971, Learning Rate = 3.385347e-04\n",
      "Epoch 2852/20000: Train Loss = 0.444709, Test Loss = 0.248772, Learning Rate = 3.384061e-04\n",
      "Epoch 2853/20000: Train Loss = 0.443280, Test Loss = 0.240473, Learning Rate = 3.382775e-04\n",
      "Epoch 2854/20000: Train Loss = 0.444660, Test Loss = 0.242734, Learning Rate = 3.381490e-04\n",
      "Epoch 2855/20000: Train Loss = 0.445229, Test Loss = 0.233817, Learning Rate = 3.380205e-04\n",
      "Epoch 2856/20000: Train Loss = 0.443845, Test Loss = 0.243513, Learning Rate = 3.378920e-04\n",
      "Epoch 2857/20000: Train Loss = 0.443752, Test Loss = 0.249340, Learning Rate = 3.377637e-04\n",
      "Epoch 2858/20000: Train Loss = 0.444118, Test Loss = 0.249534, Learning Rate = 3.376353e-04\n",
      "Epoch 2859/20000: Train Loss = 0.444715, Test Loss = 0.234754, Learning Rate = 3.375070e-04\n",
      "Epoch 2860/20000: Train Loss = 0.444550, Test Loss = 0.236242, Learning Rate = 3.373788e-04\n",
      "Epoch 2861/20000: Train Loss = 0.443518, Test Loss = 0.252607, Learning Rate = 3.372506e-04\n",
      "Epoch 2862/20000: Train Loss = 0.446905, Test Loss = 0.251372, Learning Rate = 3.371224e-04\n",
      "Epoch 2863/20000: Train Loss = 0.444356, Test Loss = 0.246091, Learning Rate = 3.369943e-04\n",
      "Epoch 2864/20000: Train Loss = 0.443294, Test Loss = 0.240384, Learning Rate = 3.368663e-04\n",
      "Epoch 2865/20000: Train Loss = 0.445501, Test Loss = 0.233991, Learning Rate = 3.367383e-04\n",
      "Epoch 2866/20000: Train Loss = 0.445675, Test Loss = 0.244690, Learning Rate = 3.366103e-04\n",
      "Epoch 2867/20000: Train Loss = 0.443984, Test Loss = 0.250766, Learning Rate = 3.364824e-04\n",
      "Epoch 2868/20000: Train Loss = 0.444862, Test Loss = 0.243498, Learning Rate = 3.363546e-04\n",
      "Epoch 2869/20000: Train Loss = 0.443776, Test Loss = 0.249317, Learning Rate = 3.362268e-04\n",
      "Epoch 2870/20000: Train Loss = 0.444924, Test Loss = 0.251462, Learning Rate = 3.360990e-04\n",
      "Epoch 2871/20000: Train Loss = 0.443909, Test Loss = 0.241344, Learning Rate = 3.359713e-04\n",
      "Epoch 2872/20000: Train Loss = 0.443625, Test Loss = 0.250737, Learning Rate = 3.358437e-04\n",
      "Epoch 2873/20000: Train Loss = 0.444623, Test Loss = 0.254055, Learning Rate = 3.357160e-04\n",
      "Epoch 2874/20000: Train Loss = 0.444444, Test Loss = 0.255082, Learning Rate = 3.355885e-04\n",
      "Epoch 2875/20000: Train Loss = 0.443934, Test Loss = 0.252804, Learning Rate = 3.354610e-04\n",
      "Epoch 2876/20000: Train Loss = 0.444738, Test Loss = 0.252317, Learning Rate = 3.353335e-04\n",
      "Epoch 2877/20000: Train Loss = 0.444396, Test Loss = 0.248062, Learning Rate = 3.352061e-04\n",
      "Epoch 2878/20000: Train Loss = 0.443850, Test Loss = 0.244983, Learning Rate = 3.350787e-04\n",
      "Epoch 2879/20000: Train Loss = 0.445095, Test Loss = 0.247451, Learning Rate = 3.349514e-04\n",
      "Epoch 2880/20000: Train Loss = 0.444142, Test Loss = 0.254456, Learning Rate = 3.348241e-04\n",
      "Epoch 2881/20000: Train Loss = 0.445634, Test Loss = 0.260225, Learning Rate = 3.346969e-04\n",
      "Epoch 2882/20000: Train Loss = 0.447367, Test Loss = 0.229529, Learning Rate = 3.345697e-04\n",
      "Epoch 2883/20000: Train Loss = 0.445335, Test Loss = 0.229059, Learning Rate = 3.344426e-04\n",
      "Epoch 2884/20000: Train Loss = 0.446608, Test Loss = 0.238119, Learning Rate = 3.343155e-04\n",
      "Epoch 2885/20000: Train Loss = 0.444276, Test Loss = 0.255621, Learning Rate = 3.341885e-04\n",
      "Epoch 2886/20000: Train Loss = 0.444833, Test Loss = 0.255894, Learning Rate = 3.340615e-04\n",
      "Epoch 2887/20000: Train Loss = 0.443519, Test Loss = 0.243457, Learning Rate = 3.339346e-04\n",
      "Epoch 2888/20000: Train Loss = 0.443292, Test Loss = 0.247281, Learning Rate = 3.338077e-04\n",
      "Epoch 2889/20000: Train Loss = 0.443968, Test Loss = 0.247403, Learning Rate = 3.336808e-04\n",
      "Epoch 2890/20000: Train Loss = 0.443464, Test Loss = 0.248508, Learning Rate = 3.335540e-04\n",
      "Epoch 2891/20000: Train Loss = 0.443431, Test Loss = 0.246630, Learning Rate = 3.334273e-04\n",
      "Epoch 2892/20000: Train Loss = 0.442871, Test Loss = 0.233490, Learning Rate = 3.333006e-04\n",
      "Epoch 2893/20000: Train Loss = 0.444789, Test Loss = 0.241006, Learning Rate = 3.331740e-04\n",
      "Epoch 2894/20000: Train Loss = 0.444553, Test Loss = 0.249611, Learning Rate = 3.330474e-04\n",
      "Epoch 2895/20000: Train Loss = 0.444858, Test Loss = 0.247076, Learning Rate = 3.329208e-04\n",
      "Epoch 2896/20000: Train Loss = 0.444789, Test Loss = 0.238829, Learning Rate = 3.327943e-04\n",
      "Epoch 2897/20000: Train Loss = 0.444376, Test Loss = 0.240169, Learning Rate = 3.326679e-04\n",
      "Epoch 2898/20000: Train Loss = 0.443487, Test Loss = 0.256227, Learning Rate = 3.325415e-04\n",
      "Epoch 2899/20000: Train Loss = 0.443604, Test Loss = 0.247642, Learning Rate = 3.324151e-04\n",
      "Epoch 2900/20000: Train Loss = 0.443196, Test Loss = 0.253880, Learning Rate = 3.322888e-04\n",
      "Epoch 2901/20000: Train Loss = 0.445460, Test Loss = 0.250401, Learning Rate = 3.321625e-04\n",
      "Epoch 2902/20000: Train Loss = 0.445011, Test Loss = 0.245536, Learning Rate = 3.320363e-04\n",
      "Epoch 2903/20000: Train Loss = 0.444075, Test Loss = 0.242618, Learning Rate = 3.319102e-04\n",
      "Epoch 2904/20000: Train Loss = 0.443553, Test Loss = 0.251462, Learning Rate = 3.317840e-04\n",
      "Epoch 2905/20000: Train Loss = 0.443867, Test Loss = 0.244537, Learning Rate = 3.316580e-04\n",
      "Epoch 2906/20000: Train Loss = 0.444936, Test Loss = 0.243546, Learning Rate = 3.315320e-04\n",
      "Epoch 2907/20000: Train Loss = 0.443823, Test Loss = 0.249454, Learning Rate = 3.314060e-04\n",
      "Epoch 2908/20000: Train Loss = 0.444892, Test Loss = 0.257760, Learning Rate = 3.312801e-04\n",
      "Epoch 2909/20000: Train Loss = 0.444349, Test Loss = 0.257598, Learning Rate = 3.311542e-04\n",
      "Epoch 2910/20000: Train Loss = 0.444987, Test Loss = 0.242249, Learning Rate = 3.310283e-04\n",
      "Epoch 2911/20000: Train Loss = 0.445231, Test Loss = 0.253173, Learning Rate = 3.309026e-04\n",
      "Epoch 2912/20000: Train Loss = 0.444800, Test Loss = 0.247489, Learning Rate = 3.307768e-04\n",
      "Epoch 2913/20000: Train Loss = 0.443634, Test Loss = 0.247742, Learning Rate = 3.306511e-04\n",
      "Epoch 2914/20000: Train Loss = 0.443361, Test Loss = 0.242181, Learning Rate = 3.305255e-04\n",
      "Epoch 2915/20000: Train Loss = 0.445369, Test Loss = 0.244509, Learning Rate = 3.303999e-04\n",
      "Epoch 2916/20000: Train Loss = 0.444000, Test Loss = 0.239983, Learning Rate = 3.302744e-04\n",
      "Epoch 2917/20000: Train Loss = 0.443144, Test Loss = 0.244194, Learning Rate = 3.301489e-04\n",
      "Epoch 2918/20000: Train Loss = 0.443704, Test Loss = 0.247079, Learning Rate = 3.300234e-04\n",
      "Epoch 2919/20000: Train Loss = 0.444505, Test Loss = 0.253837, Learning Rate = 3.298980e-04\n",
      "Epoch 2920/20000: Train Loss = 0.444363, Test Loss = 0.245630, Learning Rate = 3.297727e-04\n",
      "Epoch 2921/20000: Train Loss = 0.444706, Test Loss = 0.246184, Learning Rate = 3.296474e-04\n",
      "Epoch 2922/20000: Train Loss = 0.444904, Test Loss = 0.228969, Learning Rate = 3.295221e-04\n",
      "Epoch 2923/20000: Train Loss = 0.443989, Test Loss = 0.247191, Learning Rate = 3.293969e-04\n",
      "Epoch 2924/20000: Train Loss = 0.444794, Test Loss = 0.241898, Learning Rate = 3.292717e-04\n",
      "Epoch 2925/20000: Train Loss = 0.444737, Test Loss = 0.253439, Learning Rate = 3.291466e-04\n",
      "Epoch 2926/20000: Train Loss = 0.444475, Test Loss = 0.251453, Learning Rate = 3.290216e-04\n",
      "Epoch 2927/20000: Train Loss = 0.444077, Test Loss = 0.252167, Learning Rate = 3.288965e-04\n",
      "Epoch 2928/20000: Train Loss = 0.443872, Test Loss = 0.260506, Learning Rate = 3.287716e-04\n",
      "Epoch 2929/20000: Train Loss = 0.443641, Test Loss = 0.244743, Learning Rate = 3.286466e-04\n",
      "Epoch 2930/20000: Train Loss = 0.444268, Test Loss = 0.245545, Learning Rate = 3.285218e-04\n",
      "Epoch 2931/20000: Train Loss = 0.443994, Test Loss = 0.242998, Learning Rate = 3.283969e-04\n",
      "Epoch 2932/20000: Train Loss = 0.443555, Test Loss = 0.251702, Learning Rate = 3.282722e-04\n",
      "Epoch 2933/20000: Train Loss = 0.445077, Test Loss = 0.261102, Learning Rate = 3.281474e-04\n",
      "Epoch 2934/20000: Train Loss = 0.444189, Test Loss = 0.232490, Learning Rate = 3.280227e-04\n",
      "Epoch 2935/20000: Train Loss = 0.444962, Test Loss = 0.256214, Learning Rate = 3.278981e-04\n",
      "Epoch 2936/20000: Train Loss = 0.444949, Test Loss = 0.241288, Learning Rate = 3.277735e-04\n",
      "Epoch 2937/20000: Train Loss = 0.445867, Test Loss = 0.240934, Learning Rate = 3.276490e-04\n",
      "Epoch 2938/20000: Train Loss = 0.445057, Test Loss = 0.247723, Learning Rate = 3.275245e-04\n",
      "Epoch 2939/20000: Train Loss = 0.444414, Test Loss = 0.249651, Learning Rate = 3.274000e-04\n",
      "Epoch 2940/20000: Train Loss = 0.444751, Test Loss = 0.246272, Learning Rate = 3.272756e-04\n",
      "Epoch 2941/20000: Train Loss = 0.445845, Test Loss = 0.244365, Learning Rate = 3.271513e-04\n",
      "Epoch 2942/20000: Train Loss = 0.444137, Test Loss = 0.251808, Learning Rate = 3.270269e-04\n",
      "Epoch 2943/20000: Train Loss = 0.444422, Test Loss = 0.245863, Learning Rate = 3.269027e-04\n",
      "Epoch 2944/20000: Train Loss = 0.444724, Test Loss = 0.237610, Learning Rate = 3.267785e-04\n",
      "Epoch 2945/20000: Train Loss = 0.444393, Test Loss = 0.255749, Learning Rate = 3.266543e-04\n",
      "Epoch 2946/20000: Train Loss = 0.444938, Test Loss = 0.250003, Learning Rate = 3.265302e-04\n",
      "Epoch 2947/20000: Train Loss = 0.444582, Test Loss = 0.243326, Learning Rate = 3.264061e-04\n",
      "Epoch 2948/20000: Train Loss = 0.444086, Test Loss = 0.242285, Learning Rate = 3.262821e-04\n",
      "Epoch 2949/20000: Train Loss = 0.443904, Test Loss = 0.248471, Learning Rate = 3.261581e-04\n",
      "Epoch 2950/20000: Train Loss = 0.445001, Test Loss = 0.254941, Learning Rate = 3.260342e-04\n",
      "Epoch 2951/20000: Train Loss = 0.443809, Test Loss = 0.245864, Learning Rate = 3.259103e-04\n",
      "Epoch 2952/20000: Train Loss = 0.447041, Test Loss = 0.243155, Learning Rate = 3.257865e-04\n",
      "Epoch 2953/20000: Train Loss = 0.444726, Test Loss = 0.258900, Learning Rate = 3.256627e-04\n",
      "Epoch 2954/20000: Train Loss = 0.444933, Test Loss = 0.234913, Learning Rate = 3.255389e-04\n",
      "Epoch 2955/20000: Train Loss = 0.445929, Test Loss = 0.230654, Learning Rate = 3.254152e-04\n",
      "Epoch 2956/20000: Train Loss = 0.444866, Test Loss = 0.245468, Learning Rate = 3.252916e-04\n",
      "Epoch 2957/20000: Train Loss = 0.443992, Test Loss = 0.252978, Learning Rate = 3.251680e-04\n",
      "Epoch 2958/20000: Train Loss = 0.445029, Test Loss = 0.243087, Learning Rate = 3.250444e-04\n",
      "Epoch 2959/20000: Train Loss = 0.443738, Test Loss = 0.255484, Learning Rate = 3.249209e-04\n",
      "Epoch 2960/20000: Train Loss = 0.444382, Test Loss = 0.235508, Learning Rate = 3.247975e-04\n",
      "Epoch 2961/20000: Train Loss = 0.443736, Test Loss = 0.250917, Learning Rate = 3.246740e-04\n",
      "Epoch 2962/20000: Train Loss = 0.446873, Test Loss = 0.234291, Learning Rate = 3.245507e-04\n",
      "Epoch 2963/20000: Train Loss = 0.444186, Test Loss = 0.238542, Learning Rate = 3.244273e-04\n",
      "Epoch 2964/20000: Train Loss = 0.445186, Test Loss = 0.254806, Learning Rate = 3.243041e-04\n",
      "Epoch 2965/20000: Train Loss = 0.446668, Test Loss = 0.255764, Learning Rate = 3.241808e-04\n",
      "Epoch 2966/20000: Train Loss = 0.443121, Test Loss = 0.236272, Learning Rate = 3.240577e-04\n",
      "Epoch 2967/20000: Train Loss = 0.444527, Test Loss = 0.260693, Learning Rate = 3.239345e-04\n",
      "Epoch 2968/20000: Train Loss = 0.444898, Test Loss = 0.243346, Learning Rate = 3.238114e-04\n",
      "Epoch 2969/20000: Train Loss = 0.443835, Test Loss = 0.250061, Learning Rate = 3.236884e-04\n",
      "Epoch 2970/20000: Train Loss = 0.444755, Test Loss = 0.254140, Learning Rate = 3.235654e-04\n",
      "Epoch 2971/20000: Train Loss = 0.445656, Test Loss = 0.253239, Learning Rate = 3.234425e-04\n",
      "Epoch 2972/20000: Train Loss = 0.443801, Test Loss = 0.244399, Learning Rate = 3.233196e-04\n",
      "Epoch 2973/20000: Train Loss = 0.444221, Test Loss = 0.246739, Learning Rate = 3.231967e-04\n",
      "Epoch 2974/20000: Train Loss = 0.443507, Test Loss = 0.255459, Learning Rate = 3.230739e-04\n",
      "Epoch 2975/20000: Train Loss = 0.443450, Test Loss = 0.246119, Learning Rate = 3.229512e-04\n",
      "Epoch 2976/20000: Train Loss = 0.443745, Test Loss = 0.246323, Learning Rate = 3.228284e-04\n",
      "Epoch 2977/20000: Train Loss = 0.444125, Test Loss = 0.243897, Learning Rate = 3.227058e-04\n",
      "Epoch 2978/20000: Train Loss = 0.445501, Test Loss = 0.254302, Learning Rate = 3.225832e-04\n",
      "Epoch 2979/20000: Train Loss = 0.444263, Test Loss = 0.251671, Learning Rate = 3.224606e-04\n",
      "Epoch 2980/20000: Train Loss = 0.445310, Test Loss = 0.258163, Learning Rate = 3.223381e-04\n",
      "Epoch 2981/20000: Train Loss = 0.443812, Test Loss = 0.248097, Learning Rate = 3.222156e-04\n",
      "Epoch 2982/20000: Train Loss = 0.443960, Test Loss = 0.248336, Learning Rate = 3.220931e-04\n",
      "Epoch 2983/20000: Train Loss = 0.444961, Test Loss = 0.243571, Learning Rate = 3.219708e-04\n",
      "Epoch 2984/20000: Train Loss = 0.444467, Test Loss = 0.246040, Learning Rate = 3.218484e-04\n",
      "Epoch 2985/20000: Train Loss = 0.444301, Test Loss = 0.254716, Learning Rate = 3.217261e-04\n",
      "Epoch 2986/20000: Train Loss = 0.446287, Test Loss = 0.241717, Learning Rate = 3.216039e-04\n",
      "Epoch 2987/20000: Train Loss = 0.443436, Test Loss = 0.241947, Learning Rate = 3.214817e-04\n",
      "Epoch 2988/20000: Train Loss = 0.445802, Test Loss = 0.255086, Learning Rate = 3.213595e-04\n",
      "Epoch 2989/20000: Train Loss = 0.443965, Test Loss = 0.254479, Learning Rate = 3.212374e-04\n",
      "Epoch 2990/20000: Train Loss = 0.444534, Test Loss = 0.248104, Learning Rate = 3.211154e-04\n",
      "Epoch 2991/20000: Train Loss = 0.444136, Test Loss = 0.245907, Learning Rate = 3.209933e-04\n",
      "Epoch 2992/20000: Train Loss = 0.444572, Test Loss = 0.243011, Learning Rate = 3.208714e-04\n",
      "Epoch 2993/20000: Train Loss = 0.444673, Test Loss = 0.247331, Learning Rate = 3.207494e-04\n",
      "Epoch 2994/20000: Train Loss = 0.444229, Test Loss = 0.254968, Learning Rate = 3.206276e-04\n",
      "Epoch 2995/20000: Train Loss = 0.446530, Test Loss = 0.252690, Learning Rate = 3.205057e-04\n",
      "Epoch 2996/20000: Train Loss = 0.445564, Test Loss = 0.232448, Learning Rate = 3.203840e-04\n",
      "Epoch 2997/20000: Train Loss = 0.444633, Test Loss = 0.239980, Learning Rate = 3.202622e-04\n",
      "Epoch 2998/20000: Train Loss = 0.445931, Test Loss = 0.240522, Learning Rate = 3.201405e-04\n",
      "Epoch 2999/20000: Train Loss = 0.444954, Test Loss = 0.256821, Learning Rate = 3.200189e-04\n",
      "Epoch 3000/20000: Train Loss = 0.444661, Test Loss = 0.249313, Learning Rate = 3.198973e-04\n",
      "Epoch 3001/20000: Train Loss = 0.444592, Test Loss = 0.250901, Learning Rate = 3.197757e-04\n",
      "Epoch 3002/20000: Train Loss = 0.445559, Test Loss = 0.244344, Learning Rate = 3.196542e-04\n",
      "Epoch 3003/20000: Train Loss = 0.444039, Test Loss = 0.246797, Learning Rate = 3.195328e-04\n",
      "Epoch 3004/20000: Train Loss = 0.443912, Test Loss = 0.259298, Learning Rate = 3.194114e-04\n",
      "Epoch 3005/20000: Train Loss = 0.444239, Test Loss = 0.241053, Learning Rate = 3.192900e-04\n",
      "Epoch 3006/20000: Train Loss = 0.444469, Test Loss = 0.243080, Learning Rate = 3.191687e-04\n",
      "Epoch 3007/20000: Train Loss = 0.443617, Test Loss = 0.239094, Learning Rate = 3.190474e-04\n",
      "Epoch 3008/20000: Train Loss = 0.445506, Test Loss = 0.244066, Learning Rate = 3.189262e-04\n",
      "Epoch 3009/20000: Train Loss = 0.444521, Test Loss = 0.245480, Learning Rate = 3.188050e-04\n",
      "Epoch 3010/20000: Train Loss = 0.444429, Test Loss = 0.239981, Learning Rate = 3.186838e-04\n",
      "Epoch 3011/20000: Train Loss = 0.443826, Test Loss = 0.242968, Learning Rate = 3.185627e-04\n",
      "Epoch 3012/20000: Train Loss = 0.445522, Test Loss = 0.252400, Learning Rate = 3.184417e-04\n",
      "Epoch 3013/20000: Train Loss = 0.443832, Test Loss = 0.246354, Learning Rate = 3.183207e-04\n",
      "Epoch 3014/20000: Train Loss = 0.444614, Test Loss = 0.259875, Learning Rate = 3.181997e-04\n",
      "Epoch 3015/20000: Train Loss = 0.445617, Test Loss = 0.250073, Learning Rate = 3.180788e-04\n",
      "Epoch 3016/20000: Train Loss = 0.444495, Test Loss = 0.237621, Learning Rate = 3.179580e-04\n",
      "Epoch 3017/20000: Train Loss = 0.446285, Test Loss = 0.258592, Learning Rate = 3.178372e-04\n",
      "Epoch 3018/20000: Train Loss = 0.443300, Test Loss = 0.230561, Learning Rate = 3.177164e-04\n",
      "Epoch 3019/20000: Train Loss = 0.446037, Test Loss = 0.256239, Learning Rate = 3.175957e-04\n",
      "Epoch 3020/20000: Train Loss = 0.444650, Test Loss = 0.245695, Learning Rate = 3.174750e-04\n",
      "Epoch 3021/20000: Train Loss = 0.445325, Test Loss = 0.240911, Learning Rate = 3.173544e-04\n",
      "Epoch 3022/20000: Train Loss = 0.444591, Test Loss = 0.233113, Learning Rate = 3.172338e-04\n",
      "Epoch 3023/20000: Train Loss = 0.443686, Test Loss = 0.249342, Learning Rate = 3.171132e-04\n",
      "Epoch 3024/20000: Train Loss = 0.443671, Test Loss = 0.234567, Learning Rate = 3.169927e-04\n",
      "Epoch 3025/20000: Train Loss = 0.443654, Test Loss = 0.250308, Learning Rate = 3.168723e-04\n",
      "Epoch 3026/20000: Train Loss = 0.444330, Test Loss = 0.246416, Learning Rate = 3.167519e-04\n",
      "Epoch 3027/20000: Train Loss = 0.444796, Test Loss = 0.259638, Learning Rate = 3.166315e-04\n",
      "Epoch 3028/20000: Train Loss = 0.445921, Test Loss = 0.243377, Learning Rate = 3.165112e-04\n",
      "Epoch 3029/20000: Train Loss = 0.444458, Test Loss = 0.251276, Learning Rate = 3.163910e-04\n",
      "Epoch 3030/20000: Train Loss = 0.443845, Test Loss = 0.249183, Learning Rate = 3.162707e-04\n",
      "Epoch 3031/20000: Train Loss = 0.445165, Test Loss = 0.260259, Learning Rate = 3.161506e-04\n",
      "Epoch 3032/20000: Train Loss = 0.443625, Test Loss = 0.248461, Learning Rate = 3.160304e-04\n",
      "Epoch 3033/20000: Train Loss = 0.444367, Test Loss = 0.239875, Learning Rate = 3.159103e-04\n",
      "Epoch 3034/20000: Train Loss = 0.443263, Test Loss = 0.250015, Learning Rate = 3.157903e-04\n",
      "Epoch 3035/20000: Train Loss = 0.444567, Test Loss = 0.248583, Learning Rate = 3.156703e-04\n",
      "Epoch 3036/20000: Train Loss = 0.444248, Test Loss = 0.255101, Learning Rate = 3.155504e-04\n",
      "Epoch 3037/20000: Train Loss = 0.443620, Test Loss = 0.244256, Learning Rate = 3.154305e-04\n",
      "Epoch 3038/20000: Train Loss = 0.444081, Test Loss = 0.254606, Learning Rate = 3.153106e-04\n",
      "Epoch 3039/20000: Train Loss = 0.444349, Test Loss = 0.234107, Learning Rate = 3.151908e-04\n",
      "Epoch 3040/20000: Train Loss = 0.444879, Test Loss = 0.248422, Learning Rate = 3.150710e-04\n",
      "Epoch 3041/20000: Train Loss = 0.446721, Test Loss = 0.238795, Learning Rate = 3.149513e-04\n",
      "Epoch 3042/20000: Train Loss = 0.445792, Test Loss = 0.250940, Learning Rate = 3.148317e-04\n",
      "Epoch 3043/20000: Train Loss = 0.444715, Test Loss = 0.239824, Learning Rate = 3.147120e-04\n",
      "Epoch 3044/20000: Train Loss = 0.445474, Test Loss = 0.227765, Learning Rate = 3.145924e-04\n",
      "Epoch 3045/20000: Train Loss = 0.446264, Test Loss = 0.257163, Learning Rate = 3.144729e-04\n",
      "Epoch 3046/20000: Train Loss = 0.445495, Test Loss = 0.252430, Learning Rate = 3.143534e-04\n",
      "Epoch 3047/20000: Train Loss = 0.443879, Test Loss = 0.243895, Learning Rate = 3.142340e-04\n",
      "Epoch 3048/20000: Train Loss = 0.444743, Test Loss = 0.241557, Learning Rate = 3.141146e-04\n",
      "Epoch 3049/20000: Train Loss = 0.444763, Test Loss = 0.262849, Learning Rate = 3.139952e-04\n",
      "Epoch 3050/20000: Train Loss = 0.443438, Test Loss = 0.240519, Learning Rate = 3.138759e-04\n",
      "Epoch 3051/20000: Train Loss = 0.447275, Test Loss = 0.259968, Learning Rate = 3.137566e-04\n",
      "Epoch 3052/20000: Train Loss = 0.443152, Test Loss = 0.240067, Learning Rate = 3.136374e-04\n",
      "Epoch 3053/20000: Train Loss = 0.443551, Test Loss = 0.250068, Learning Rate = 3.135182e-04\n",
      "Epoch 3054/20000: Train Loss = 0.444645, Test Loss = 0.254485, Learning Rate = 3.133991e-04\n",
      "Epoch 3055/20000: Train Loss = 0.443339, Test Loss = 0.240673, Learning Rate = 3.132800e-04\n",
      "Epoch 3056/20000: Train Loss = 0.444793, Test Loss = 0.242767, Learning Rate = 3.131610e-04\n",
      "Epoch 3057/20000: Train Loss = 0.443928, Test Loss = 0.245487, Learning Rate = 3.130420e-04\n",
      "Epoch 3058/20000: Train Loss = 0.444080, Test Loss = 0.248501, Learning Rate = 3.129231e-04\n",
      "Epoch 3059/20000: Train Loss = 0.444370, Test Loss = 0.246134, Learning Rate = 3.128042e-04\n",
      "Epoch 3060/20000: Train Loss = 0.443855, Test Loss = 0.235460, Learning Rate = 3.126853e-04\n",
      "Epoch 3061/20000: Train Loss = 0.443871, Test Loss = 0.243240, Learning Rate = 3.125665e-04\n",
      "Epoch 3062/20000: Train Loss = 0.443825, Test Loss = 0.247417, Learning Rate = 3.124477e-04\n",
      "Epoch 3063/20000: Train Loss = 0.444713, Test Loss = 0.247704, Learning Rate = 3.123290e-04\n",
      "Epoch 3064/20000: Train Loss = 0.443642, Test Loss = 0.252568, Learning Rate = 3.122103e-04\n",
      "Epoch 3065/20000: Train Loss = 0.444186, Test Loss = 0.252842, Learning Rate = 3.120917e-04\n",
      "Epoch 3066/20000: Train Loss = 0.443976, Test Loss = 0.245175, Learning Rate = 3.119731e-04\n",
      "Epoch 3067/20000: Train Loss = 0.443557, Test Loss = 0.250113, Learning Rate = 3.118546e-04\n",
      "Epoch 3068/20000: Train Loss = 0.443895, Test Loss = 0.244428, Learning Rate = 3.117361e-04\n",
      "Epoch 3069/20000: Train Loss = 0.444434, Test Loss = 0.234722, Learning Rate = 3.116176e-04\n",
      "Epoch 3070/20000: Train Loss = 0.443725, Test Loss = 0.242799, Learning Rate = 3.114992e-04\n",
      "Epoch 3071/20000: Train Loss = 0.444069, Test Loss = 0.243348, Learning Rate = 3.113808e-04\n",
      "Epoch 3072/20000: Train Loss = 0.443939, Test Loss = 0.239709, Learning Rate = 3.112625e-04\n",
      "Epoch 3073/20000: Train Loss = 0.443434, Test Loss = 0.242802, Learning Rate = 3.111443e-04\n",
      "Epoch 3074/20000: Train Loss = 0.443554, Test Loss = 0.255744, Learning Rate = 3.110260e-04\n",
      "Epoch 3075/20000: Train Loss = 0.444364, Test Loss = 0.254849, Learning Rate = 3.109079e-04\n",
      "Epoch 3076/20000: Train Loss = 0.445020, Test Loss = 0.248424, Learning Rate = 3.107897e-04\n",
      "Epoch 3077/20000: Train Loss = 0.445592, Test Loss = 0.240984, Learning Rate = 3.106716e-04\n",
      "Epoch 3078/20000: Train Loss = 0.443461, Test Loss = 0.243021, Learning Rate = 3.105536e-04\n",
      "Epoch 3079/20000: Train Loss = 0.445575, Test Loss = 0.255089, Learning Rate = 3.104356e-04\n",
      "Epoch 3080/20000: Train Loss = 0.445954, Test Loss = 0.249800, Learning Rate = 3.103176e-04\n",
      "Epoch 3081/20000: Train Loss = 0.444216, Test Loss = 0.251252, Learning Rate = 3.101997e-04\n",
      "Epoch 3082/20000: Train Loss = 0.443771, Test Loss = 0.248188, Learning Rate = 3.100818e-04\n",
      "Epoch 3083/20000: Train Loss = 0.443699, Test Loss = 0.246946, Learning Rate = 3.099640e-04\n",
      "Epoch 3084/20000: Train Loss = 0.443588, Test Loss = 0.248577, Learning Rate = 3.098462e-04\n",
      "Epoch 3085/20000: Train Loss = 0.444998, Test Loss = 0.253481, Learning Rate = 3.097285e-04\n",
      "Epoch 3086/20000: Train Loss = 0.445508, Test Loss = 0.244033, Learning Rate = 3.096108e-04\n",
      "Epoch 3087/20000: Train Loss = 0.445626, Test Loss = 0.255484, Learning Rate = 3.094932e-04\n",
      "Epoch 3088/20000: Train Loss = 0.444495, Test Loss = 0.242087, Learning Rate = 3.093756e-04\n",
      "Epoch 3089/20000: Train Loss = 0.446335, Test Loss = 0.259997, Learning Rate = 3.092580e-04\n",
      "Epoch 3090/20000: Train Loss = 0.444995, Test Loss = 0.284205, Learning Rate = 3.091405e-04\n",
      "Epoch 3091/20000: Train Loss = 0.443786, Test Loss = 0.227588, Learning Rate = 3.090230e-04\n",
      "Epoch 3092/20000: Train Loss = 0.447272, Test Loss = 0.251429, Learning Rate = 3.089056e-04\n",
      "Epoch 3093/20000: Train Loss = 0.444432, Test Loss = 0.247755, Learning Rate = 3.087882e-04\n",
      "Epoch 3094/20000: Train Loss = 0.444665, Test Loss = 0.251996, Learning Rate = 3.086709e-04\n",
      "Epoch 3095/20000: Train Loss = 0.443610, Test Loss = 0.242772, Learning Rate = 3.085536e-04\n",
      "Epoch 3096/20000: Train Loss = 0.443951, Test Loss = 0.238995, Learning Rate = 3.084364e-04\n",
      "Epoch 3097/20000: Train Loss = 0.443865, Test Loss = 0.248381, Learning Rate = 3.083192e-04\n",
      "Epoch 3098/20000: Train Loss = 0.445112, Test Loss = 0.235607, Learning Rate = 3.082020e-04\n",
      "Epoch 3099/20000: Train Loss = 0.443959, Test Loss = 0.241575, Learning Rate = 3.080849e-04\n",
      "Epoch 3100/20000: Train Loss = 0.443906, Test Loss = 0.241210, Learning Rate = 3.079679e-04\n",
      "Epoch 3101/20000: Train Loss = 0.443759, Test Loss = 0.236223, Learning Rate = 3.078508e-04\n",
      "Epoch 3102/20000: Train Loss = 0.444668, Test Loss = 0.246204, Learning Rate = 3.077339e-04\n",
      "Epoch 3103/20000: Train Loss = 0.445674, Test Loss = 0.242251, Learning Rate = 3.076169e-04\n",
      "Epoch 3104/20000: Train Loss = 0.445724, Test Loss = 0.246664, Learning Rate = 3.075001e-04\n",
      "Epoch 3105/20000: Train Loss = 0.443835, Test Loss = 0.243720, Learning Rate = 3.073832e-04\n",
      "Epoch 3106/20000: Train Loss = 0.444950, Test Loss = 0.243525, Learning Rate = 3.072664e-04\n",
      "Epoch 3107/20000: Train Loss = 0.445932, Test Loss = 0.242868, Learning Rate = 3.071497e-04\n",
      "Epoch 3108/20000: Train Loss = 0.444125, Test Loss = 0.251402, Learning Rate = 3.070330e-04\n",
      "Epoch 3109/20000: Train Loss = 0.444088, Test Loss = 0.245791, Learning Rate = 3.069163e-04\n",
      "Epoch 3110/20000: Train Loss = 0.445231, Test Loss = 0.251156, Learning Rate = 3.067997e-04\n",
      "Epoch 3111/20000: Train Loss = 0.446768, Test Loss = 0.249613, Learning Rate = 3.066831e-04\n",
      "Epoch 3112/20000: Train Loss = 0.444371, Test Loss = 0.251723, Learning Rate = 3.065666e-04\n",
      "Epoch 3113/20000: Train Loss = 0.445538, Test Loss = 0.261115, Learning Rate = 3.064501e-04\n",
      "Epoch 3114/20000: Train Loss = 0.446933, Test Loss = 0.237591, Learning Rate = 3.063336e-04\n",
      "Epoch 3115/20000: Train Loss = 0.444969, Test Loss = 0.243130, Learning Rate = 3.062172e-04\n",
      "Epoch 3116/20000: Train Loss = 0.444379, Test Loss = 0.229633, Learning Rate = 3.061009e-04\n",
      "Epoch 3117/20000: Train Loss = 0.447872, Test Loss = 0.244977, Learning Rate = 3.059846e-04\n",
      "Epoch 3118/20000: Train Loss = 0.444252, Test Loss = 0.253184, Learning Rate = 3.058683e-04\n",
      "Epoch 3119/20000: Train Loss = 0.444652, Test Loss = 0.237716, Learning Rate = 3.057521e-04\n",
      "Epoch 3120/20000: Train Loss = 0.445907, Test Loss = 0.255512, Learning Rate = 3.056359e-04\n",
      "Epoch 3121/20000: Train Loss = 0.444500, Test Loss = 0.248176, Learning Rate = 3.055198e-04\n",
      "Epoch 3122/20000: Train Loss = 0.444312, Test Loss = 0.250806, Learning Rate = 3.054037e-04\n",
      "Epoch 3123/20000: Train Loss = 0.444565, Test Loss = 0.241594, Learning Rate = 3.052876e-04\n",
      "Epoch 3124/20000: Train Loss = 0.444034, Test Loss = 0.233489, Learning Rate = 3.051716e-04\n",
      "Epoch 3125/20000: Train Loss = 0.443731, Test Loss = 0.247973, Learning Rate = 3.050557e-04\n",
      "Epoch 3126/20000: Train Loss = 0.444684, Test Loss = 0.247208, Learning Rate = 3.049398e-04\n",
      "Epoch 3127/20000: Train Loss = 0.444653, Test Loss = 0.247357, Learning Rate = 3.048239e-04\n",
      "Epoch 3128/20000: Train Loss = 0.444114, Test Loss = 0.248748, Learning Rate = 3.047081e-04\n",
      "Epoch 3129/20000: Train Loss = 0.444166, Test Loss = 0.247178, Learning Rate = 3.045923e-04\n",
      "Epoch 3130/20000: Train Loss = 0.444253, Test Loss = 0.243039, Learning Rate = 3.044766e-04\n",
      "Epoch 3131/20000: Train Loss = 0.444736, Test Loss = 0.250742, Learning Rate = 3.043609e-04\n",
      "Epoch 3132/20000: Train Loss = 0.444184, Test Loss = 0.256212, Learning Rate = 3.042452e-04\n",
      "Epoch 3133/20000: Train Loss = 0.445285, Test Loss = 0.236273, Learning Rate = 3.041296e-04\n",
      "Epoch 3134/20000: Train Loss = 0.444713, Test Loss = 0.240005, Learning Rate = 3.040140e-04\n",
      "Epoch 3135/20000: Train Loss = 0.444008, Test Loss = 0.246503, Learning Rate = 3.038985e-04\n",
      "Epoch 3136/20000: Train Loss = 0.443739, Test Loss = 0.245128, Learning Rate = 3.037831e-04\n",
      "Epoch 3137/20000: Train Loss = 0.444000, Test Loss = 0.242566, Learning Rate = 3.036676e-04\n",
      "Epoch 3138/20000: Train Loss = 0.443462, Test Loss = 0.245009, Learning Rate = 3.035522e-04\n",
      "Epoch 3139/20000: Train Loss = 0.443376, Test Loss = 0.254789, Learning Rate = 3.034369e-04\n",
      "Epoch 3140/20000: Train Loss = 0.443656, Test Loss = 0.247858, Learning Rate = 3.033216e-04\n",
      "Epoch 3141/20000: Train Loss = 0.444439, Test Loss = 0.248300, Learning Rate = 3.032063e-04\n",
      "Epoch 3142/20000: Train Loss = 0.443671, Test Loss = 0.243655, Learning Rate = 3.030911e-04\n",
      "Epoch 3143/20000: Train Loss = 0.443542, Test Loss = 0.241876, Learning Rate = 3.029760e-04\n",
      "Epoch 3144/20000: Train Loss = 0.443619, Test Loss = 0.248335, Learning Rate = 3.028608e-04\n",
      "Epoch 3145/20000: Train Loss = 0.444892, Test Loss = 0.248270, Learning Rate = 3.027458e-04\n",
      "Epoch 3146/20000: Train Loss = 0.443617, Test Loss = 0.244546, Learning Rate = 3.026307e-04\n",
      "Epoch 3147/20000: Train Loss = 0.445848, Test Loss = 0.248845, Learning Rate = 3.025157e-04\n",
      "Epoch 3148/20000: Train Loss = 0.444363, Test Loss = 0.254215, Learning Rate = 3.024008e-04\n",
      "Epoch 3149/20000: Train Loss = 0.447091, Test Loss = 0.253531, Learning Rate = 3.022859e-04\n",
      "Epoch 3150/20000: Train Loss = 0.443975, Test Loss = 0.242169, Learning Rate = 3.021710e-04\n",
      "Epoch 3151/20000: Train Loss = 0.443834, Test Loss = 0.256915, Learning Rate = 3.020562e-04\n",
      "Epoch 3152/20000: Train Loss = 0.445104, Test Loss = 0.250059, Learning Rate = 3.019414e-04\n",
      "Epoch 3153/20000: Train Loss = 0.444817, Test Loss = 0.252398, Learning Rate = 3.018267e-04\n",
      "Epoch 3154/20000: Train Loss = 0.444018, Test Loss = 0.251689, Learning Rate = 3.017120e-04\n",
      "Epoch 3155/20000: Train Loss = 0.444637, Test Loss = 0.242938, Learning Rate = 3.015974e-04\n",
      "Epoch 3156/20000: Train Loss = 0.443859, Test Loss = 0.249034, Learning Rate = 3.014828e-04\n",
      "Epoch 3157/20000: Train Loss = 0.443533, Test Loss = 0.246980, Learning Rate = 3.013682e-04\n",
      "Epoch 3158/20000: Train Loss = 0.443895, Test Loss = 0.241129, Learning Rate = 3.012537e-04\n",
      "Epoch 3159/20000: Train Loss = 0.443565, Test Loss = 0.244916, Learning Rate = 3.011392e-04\n",
      "Epoch 3160/20000: Train Loss = 0.444307, Test Loss = 0.254967, Learning Rate = 3.010248e-04\n",
      "Epoch 3161/20000: Train Loss = 0.443669, Test Loss = 0.249262, Learning Rate = 3.009104e-04\n",
      "Epoch 3162/20000: Train Loss = 0.443261, Test Loss = 0.245350, Learning Rate = 3.007961e-04\n",
      "Epoch 3163/20000: Train Loss = 0.443765, Test Loss = 0.243005, Learning Rate = 3.006818e-04\n",
      "Epoch 3164/20000: Train Loss = 0.445100, Test Loss = 0.248937, Learning Rate = 3.005676e-04\n",
      "Epoch 3165/20000: Train Loss = 0.445360, Test Loss = 0.248903, Learning Rate = 3.004534e-04\n",
      "Epoch 3166/20000: Train Loss = 0.443836, Test Loss = 0.245004, Learning Rate = 3.003392e-04\n",
      "Epoch 3167/20000: Train Loss = 0.444580, Test Loss = 0.243250, Learning Rate = 3.002251e-04\n",
      "Epoch 3168/20000: Train Loss = 0.444919, Test Loss = 0.244886, Learning Rate = 3.001110e-04\n",
      "Epoch 3169/20000: Train Loss = 0.444370, Test Loss = 0.243887, Learning Rate = 2.999970e-04\n",
      "Epoch 3170/20000: Train Loss = 0.444455, Test Loss = 0.246745, Learning Rate = 2.998830e-04\n",
      "Epoch 3171/20000: Train Loss = 0.444812, Test Loss = 0.228206, Learning Rate = 2.997690e-04\n",
      "Epoch 3172/20000: Train Loss = 0.444050, Test Loss = 0.240619, Learning Rate = 2.996551e-04\n",
      "Epoch 3173/20000: Train Loss = 0.443421, Test Loss = 0.243626, Learning Rate = 2.995413e-04\n",
      "Epoch 3174/20000: Train Loss = 0.445125, Test Loss = 0.243536, Learning Rate = 2.994274e-04\n",
      "Epoch 3175/20000: Train Loss = 0.444160, Test Loss = 0.245773, Learning Rate = 2.993137e-04\n",
      "Epoch 3176/20000: Train Loss = 0.443906, Test Loss = 0.249129, Learning Rate = 2.991999e-04\n",
      "Epoch 3177/20000: Train Loss = 0.443596, Test Loss = 0.241308, Learning Rate = 2.990862e-04\n",
      "Epoch 3178/20000: Train Loss = 0.443889, Test Loss = 0.239744, Learning Rate = 2.989726e-04\n",
      "Epoch 3179/20000: Train Loss = 0.446237, Test Loss = 0.241315, Learning Rate = 2.988590e-04\n",
      "Epoch 3180/20000: Train Loss = 0.444423, Test Loss = 0.242634, Learning Rate = 2.987454e-04\n",
      "Epoch 3181/20000: Train Loss = 0.444101, Test Loss = 0.232122, Learning Rate = 2.986319e-04\n",
      "Epoch 3182/20000: Train Loss = 0.444312, Test Loss = 0.247974, Learning Rate = 2.985185e-04\n",
      "Epoch 3183/20000: Train Loss = 0.445457, Test Loss = 0.252547, Learning Rate = 2.984050e-04\n",
      "Epoch 3184/20000: Train Loss = 0.445122, Test Loss = 0.254472, Learning Rate = 2.982916e-04\n",
      "Epoch 3185/20000: Train Loss = 0.444840, Test Loss = 0.244878, Learning Rate = 2.981783e-04\n",
      "Epoch 3186/20000: Train Loss = 0.444782, Test Loss = 0.263430, Learning Rate = 2.980650e-04\n",
      "Epoch 3187/20000: Train Loss = 0.445000, Test Loss = 0.245691, Learning Rate = 2.979517e-04\n",
      "Epoch 3188/20000: Train Loss = 0.443785, Test Loss = 0.244526, Learning Rate = 2.978385e-04\n",
      "Epoch 3189/20000: Train Loss = 0.443403, Test Loss = 0.253429, Learning Rate = 2.977254e-04\n",
      "Epoch 3190/20000: Train Loss = 0.444643, Test Loss = 0.247648, Learning Rate = 2.976122e-04\n",
      "Epoch 3191/20000: Train Loss = 0.444266, Test Loss = 0.254892, Learning Rate = 2.974991e-04\n",
      "Epoch 3192/20000: Train Loss = 0.446147, Test Loss = 0.229640, Learning Rate = 2.973861e-04\n",
      "Epoch 3193/20000: Train Loss = 0.445269, Test Loss = 0.233486, Learning Rate = 2.972731e-04\n",
      "Epoch 3194/20000: Train Loss = 0.444559, Test Loss = 0.243967, Learning Rate = 2.971601e-04\n",
      "Epoch 3195/20000: Train Loss = 0.445067, Test Loss = 0.243456, Learning Rate = 2.970472e-04\n",
      "Epoch 3196/20000: Train Loss = 0.445366, Test Loss = 0.231170, Learning Rate = 2.969344e-04\n",
      "Epoch 3197/20000: Train Loss = 0.444977, Test Loss = 0.251405, Learning Rate = 2.968215e-04\n",
      "Epoch 3198/20000: Train Loss = 0.443981, Test Loss = 0.238768, Learning Rate = 2.967088e-04\n",
      "Epoch 3199/20000: Train Loss = 0.444415, Test Loss = 0.258166, Learning Rate = 2.965960e-04\n",
      "Epoch 3200/20000: Train Loss = 0.445681, Test Loss = 0.250258, Learning Rate = 2.964833e-04\n",
      "Epoch 3201/20000: Train Loss = 0.444585, Test Loss = 0.251101, Learning Rate = 2.963707e-04\n",
      "Epoch 3202/20000: Train Loss = 0.444538, Test Loss = 0.256052, Learning Rate = 2.962580e-04\n",
      "Epoch 3203/20000: Train Loss = 0.446741, Test Loss = 0.248181, Learning Rate = 2.961455e-04\n",
      "Epoch 3204/20000: Train Loss = 0.445171, Test Loss = 0.262826, Learning Rate = 2.960329e-04\n",
      "Epoch 3205/20000: Train Loss = 0.444981, Test Loss = 0.250500, Learning Rate = 2.959205e-04\n",
      "Epoch 3206/20000: Train Loss = 0.444722, Test Loss = 0.239652, Learning Rate = 2.958080e-04\n",
      "Epoch 3207/20000: Train Loss = 0.444087, Test Loss = 0.242952, Learning Rate = 2.956956e-04\n",
      "Epoch 3208/20000: Train Loss = 0.444947, Test Loss = 0.242764, Learning Rate = 2.955833e-04\n",
      "Epoch 3209/20000: Train Loss = 0.444042, Test Loss = 0.248437, Learning Rate = 2.954710e-04\n",
      "Epoch 3210/20000: Train Loss = 0.443721, Test Loss = 0.243241, Learning Rate = 2.953587e-04\n",
      "Epoch 3211/20000: Train Loss = 0.444228, Test Loss = 0.249646, Learning Rate = 2.952465e-04\n",
      "Epoch 3212/20000: Train Loss = 0.446198, Test Loss = 0.241966, Learning Rate = 2.951343e-04\n",
      "Epoch 3213/20000: Train Loss = 0.443322, Test Loss = 0.248790, Learning Rate = 2.950221e-04\n",
      "Epoch 3214/20000: Train Loss = 0.443491, Test Loss = 0.242735, Learning Rate = 2.949100e-04\n",
      "Epoch 3215/20000: Train Loss = 0.444075, Test Loss = 0.242601, Learning Rate = 2.947980e-04\n",
      "Epoch 3216/20000: Train Loss = 0.443916, Test Loss = 0.250159, Learning Rate = 2.946859e-04\n",
      "Epoch 3217/20000: Train Loss = 0.445400, Test Loss = 0.255357, Learning Rate = 2.945740e-04\n",
      "Epoch 3218/20000: Train Loss = 0.443487, Test Loss = 0.240624, Learning Rate = 2.944620e-04\n",
      "Epoch 3219/20000: Train Loss = 0.444708, Test Loss = 0.243071, Learning Rate = 2.943502e-04\n",
      "Epoch 3220/20000: Train Loss = 0.443161, Test Loss = 0.242402, Learning Rate = 2.942383e-04\n",
      "Epoch 3221/20000: Train Loss = 0.444319, Test Loss = 0.242251, Learning Rate = 2.941265e-04\n",
      "Epoch 3222/20000: Train Loss = 0.444164, Test Loss = 0.252883, Learning Rate = 2.940148e-04\n",
      "Epoch 3223/20000: Train Loss = 0.443380, Test Loss = 0.247282, Learning Rate = 2.939030e-04\n",
      "Epoch 3224/20000: Train Loss = 0.444724, Test Loss = 0.236634, Learning Rate = 2.937914e-04\n",
      "Epoch 3225/20000: Train Loss = 0.446712, Test Loss = 0.245958, Learning Rate = 2.936797e-04\n",
      "Epoch 3226/20000: Train Loss = 0.444571, Test Loss = 0.241756, Learning Rate = 2.935681e-04\n",
      "Epoch 3227/20000: Train Loss = 0.445026, Test Loss = 0.250699, Learning Rate = 2.934566e-04\n",
      "Epoch 3228/20000: Train Loss = 0.443567, Test Loss = 0.247709, Learning Rate = 2.933451e-04\n",
      "Epoch 3229/20000: Train Loss = 0.443328, Test Loss = 0.242915, Learning Rate = 2.932336e-04\n",
      "Epoch 3230/20000: Train Loss = 0.446353, Test Loss = 0.248771, Learning Rate = 2.931222e-04\n",
      "Epoch 3231/20000: Train Loss = 0.445762, Test Loss = 0.254272, Learning Rate = 2.930108e-04\n",
      "Epoch 3232/20000: Train Loss = 0.444436, Test Loss = 0.240753, Learning Rate = 2.928995e-04\n",
      "Epoch 3233/20000: Train Loss = 0.445148, Test Loss = 0.253578, Learning Rate = 2.927882e-04\n",
      "Epoch 3234/20000: Train Loss = 0.444198, Test Loss = 0.239903, Learning Rate = 2.926769e-04\n",
      "Epoch 3235/20000: Train Loss = 0.445200, Test Loss = 0.241533, Learning Rate = 2.925657e-04\n",
      "Epoch 3236/20000: Train Loss = 0.444828, Test Loss = 0.244093, Learning Rate = 2.924546e-04\n",
      "Epoch 3237/20000: Train Loss = 0.443826, Test Loss = 0.250731, Learning Rate = 2.923434e-04\n",
      "Epoch 3238/20000: Train Loss = 0.446173, Test Loss = 0.235733, Learning Rate = 2.922324e-04\n",
      "Epoch 3239/20000: Train Loss = 0.444795, Test Loss = 0.246744, Learning Rate = 2.921213e-04\n",
      "Epoch 3240/20000: Train Loss = 0.443855, Test Loss = 0.249269, Learning Rate = 2.920103e-04\n",
      "Epoch 3241/20000: Train Loss = 0.444083, Test Loss = 0.238060, Learning Rate = 2.918994e-04\n",
      "Epoch 3242/20000: Train Loss = 0.444529, Test Loss = 0.240442, Learning Rate = 2.917884e-04\n",
      "Epoch 3243/20000: Train Loss = 0.444222, Test Loss = 0.237689, Learning Rate = 2.916776e-04\n",
      "Epoch 3244/20000: Train Loss = 0.443162, Test Loss = 0.253461, Learning Rate = 2.915667e-04\n",
      "Epoch 3245/20000: Train Loss = 0.443797, Test Loss = 0.229336, Learning Rate = 2.914560e-04\n",
      "Epoch 3246/20000: Train Loss = 0.443896, Test Loss = 0.241107, Learning Rate = 2.913452e-04\n",
      "Epoch 3247/20000: Train Loss = 0.443248, Test Loss = 0.252234, Learning Rate = 2.912345e-04\n",
      "Epoch 3248/20000: Train Loss = 0.444268, Test Loss = 0.244336, Learning Rate = 2.911238e-04\n",
      "Epoch 3249/20000: Train Loss = 0.444553, Test Loss = 0.245167, Learning Rate = 2.910132e-04\n",
      "Epoch 3250/20000: Train Loss = 0.444030, Test Loss = 0.248571, Learning Rate = 2.909026e-04\n",
      "Epoch 3251/20000: Train Loss = 0.443584, Test Loss = 0.241352, Learning Rate = 2.907921e-04\n",
      "Epoch 3252/20000: Train Loss = 0.444209, Test Loss = 0.245336, Learning Rate = 2.906816e-04\n",
      "Epoch 3253/20000: Train Loss = 0.443912, Test Loss = 0.246862, Learning Rate = 2.905712e-04\n",
      "Epoch 3254/20000: Train Loss = 0.443659, Test Loss = 0.248841, Learning Rate = 2.904608e-04\n",
      "Epoch 3255/20000: Train Loss = 0.444654, Test Loss = 0.242318, Learning Rate = 2.903504e-04\n",
      "Epoch 3256/20000: Train Loss = 0.443734, Test Loss = 0.255881, Learning Rate = 2.902401e-04\n",
      "Epoch 3257/20000: Train Loss = 0.444120, Test Loss = 0.238766, Learning Rate = 2.901298e-04\n",
      "Epoch 3258/20000: Train Loss = 0.444835, Test Loss = 0.244107, Learning Rate = 2.900195e-04\n",
      "Epoch 3259/20000: Train Loss = 0.445457, Test Loss = 0.238325, Learning Rate = 2.899093e-04\n",
      "Epoch 3260/20000: Train Loss = 0.443765, Test Loss = 0.249031, Learning Rate = 2.897992e-04\n",
      "Epoch 3261/20000: Train Loss = 0.444447, Test Loss = 0.241575, Learning Rate = 2.896891e-04\n",
      "Epoch 3262/20000: Train Loss = 0.444857, Test Loss = 0.239490, Learning Rate = 2.895790e-04\n",
      "Epoch 3263/20000: Train Loss = 0.444617, Test Loss = 0.236522, Learning Rate = 2.894690e-04\n",
      "Epoch 3264/20000: Train Loss = 0.444393, Test Loss = 0.254645, Learning Rate = 2.893590e-04\n",
      "Epoch 3265/20000: Train Loss = 0.443268, Test Loss = 0.237580, Learning Rate = 2.892490e-04\n",
      "Epoch 3266/20000: Train Loss = 0.444569, Test Loss = 0.238343, Learning Rate = 2.891391e-04\n",
      "Epoch 3267/20000: Train Loss = 0.445216, Test Loss = 0.246472, Learning Rate = 2.890293e-04\n",
      "Epoch 3268/20000: Train Loss = 0.445145, Test Loss = 0.245432, Learning Rate = 2.889194e-04\n",
      "Epoch 3269/20000: Train Loss = 0.443945, Test Loss = 0.247037, Learning Rate = 2.888096e-04\n",
      "Epoch 3270/20000: Train Loss = 0.443982, Test Loss = 0.246505, Learning Rate = 2.886999e-04\n",
      "Epoch 3271/20000: Train Loss = 0.443523, Test Loss = 0.247000, Learning Rate = 2.885902e-04\n",
      "Epoch 3272/20000: Train Loss = 0.443712, Test Loss = 0.245397, Learning Rate = 2.884806e-04\n",
      "Epoch 3273/20000: Train Loss = 0.444071, Test Loss = 0.240449, Learning Rate = 2.883709e-04\n",
      "Epoch 3274/20000: Train Loss = 0.443340, Test Loss = 0.252776, Learning Rate = 2.882614e-04\n",
      "Epoch 3275/20000: Train Loss = 0.443680, Test Loss = 0.256205, Learning Rate = 2.881518e-04\n",
      "Epoch 3276/20000: Train Loss = 0.443398, Test Loss = 0.241542, Learning Rate = 2.880423e-04\n",
      "Epoch 3277/20000: Train Loss = 0.443695, Test Loss = 0.245902, Learning Rate = 2.879329e-04\n",
      "Epoch 3278/20000: Train Loss = 0.444061, Test Loss = 0.244856, Learning Rate = 2.878235e-04\n",
      "Epoch 3279/20000: Train Loss = 0.443945, Test Loss = 0.246179, Learning Rate = 2.877141e-04\n",
      "Epoch 3280/20000: Train Loss = 0.444488, Test Loss = 0.260359, Learning Rate = 2.876048e-04\n",
      "Epoch 3281/20000: Train Loss = 0.443358, Test Loss = 0.246227, Learning Rate = 2.874955e-04\n",
      "Epoch 3282/20000: Train Loss = 0.443256, Test Loss = 0.251178, Learning Rate = 2.873863e-04\n",
      "Epoch 3283/20000: Train Loss = 0.443770, Test Loss = 0.247802, Learning Rate = 2.872771e-04\n",
      "Epoch 3284/20000: Train Loss = 0.443853, Test Loss = 0.249483, Learning Rate = 2.871679e-04\n",
      "Epoch 3285/20000: Train Loss = 0.443647, Test Loss = 0.244370, Learning Rate = 2.870588e-04\n",
      "Epoch 3286/20000: Train Loss = 0.443600, Test Loss = 0.245466, Learning Rate = 2.869497e-04\n",
      "Epoch 3287/20000: Train Loss = 0.444171, Test Loss = 0.240234, Learning Rate = 2.868407e-04\n",
      "Epoch 3288/20000: Train Loss = 0.443692, Test Loss = 0.244438, Learning Rate = 2.867317e-04\n",
      "Epoch 3289/20000: Train Loss = 0.443517, Test Loss = 0.244161, Learning Rate = 2.866228e-04\n",
      "Epoch 3290/20000: Train Loss = 0.444051, Test Loss = 0.248270, Learning Rate = 2.865138e-04\n",
      "Epoch 3291/20000: Train Loss = 0.444448, Test Loss = 0.248859, Learning Rate = 2.864050e-04\n",
      "Epoch 3292/20000: Train Loss = 0.443217, Test Loss = 0.251975, Learning Rate = 2.862962e-04\n",
      "Epoch 3293/20000: Train Loss = 0.445252, Test Loss = 0.243139, Learning Rate = 2.861874e-04\n",
      "Epoch 3294/20000: Train Loss = 0.443455, Test Loss = 0.255744, Learning Rate = 2.860786e-04\n",
      "Epoch 3295/20000: Train Loss = 0.444993, Test Loss = 0.242154, Learning Rate = 2.859699e-04\n",
      "Epoch 3296/20000: Train Loss = 0.445573, Test Loss = 0.240929, Learning Rate = 2.858613e-04\n",
      "Epoch 3297/20000: Train Loss = 0.444971, Test Loss = 0.245546, Learning Rate = 2.857526e-04\n",
      "Epoch 3298/20000: Train Loss = 0.443915, Test Loss = 0.246334, Learning Rate = 2.856441e-04\n",
      "Epoch 3299/20000: Train Loss = 0.443954, Test Loss = 0.240690, Learning Rate = 2.855355e-04\n",
      "Epoch 3300/20000: Train Loss = 0.444432, Test Loss = 0.248721, Learning Rate = 2.854270e-04\n",
      "Epoch 3301/20000: Train Loss = 0.443492, Test Loss = 0.242537, Learning Rate = 2.853186e-04\n",
      "Epoch 3302/20000: Train Loss = 0.444560, Test Loss = 0.239987, Learning Rate = 2.852102e-04\n",
      "Epoch 3303/20000: Train Loss = 0.444409, Test Loss = 0.245412, Learning Rate = 2.851018e-04\n",
      "Epoch 3304/20000: Train Loss = 0.443882, Test Loss = 0.246887, Learning Rate = 2.849935e-04\n",
      "Epoch 3305/20000: Train Loss = 0.443582, Test Loss = 0.238503, Learning Rate = 2.848852e-04\n",
      "Epoch 3306/20000: Train Loss = 0.446513, Test Loss = 0.255040, Learning Rate = 2.847769e-04\n",
      "Epoch 3307/20000: Train Loss = 0.445335, Test Loss = 0.226343, Learning Rate = 2.846687e-04\n",
      "Epoch 3308/20000: Train Loss = 0.445647, Test Loss = 0.248777, Learning Rate = 2.845605e-04\n",
      "Epoch 3309/20000: Train Loss = 0.445074, Test Loss = 0.244885, Learning Rate = 2.844524e-04\n",
      "Epoch 3310/20000: Train Loss = 0.443883, Test Loss = 0.235294, Learning Rate = 2.843443e-04\n",
      "Epoch 3311/20000: Train Loss = 0.444061, Test Loss = 0.259379, Learning Rate = 2.842363e-04\n",
      "Epoch 3312/20000: Train Loss = 0.444458, Test Loss = 0.251503, Learning Rate = 2.841283e-04\n",
      "Epoch 3313/20000: Train Loss = 0.444119, Test Loss = 0.250073, Learning Rate = 2.840203e-04\n",
      "Epoch 3314/20000: Train Loss = 0.444821, Test Loss = 0.239762, Learning Rate = 2.839124e-04\n",
      "Epoch 3315/20000: Train Loss = 0.444072, Test Loss = 0.251189, Learning Rate = 2.838045e-04\n",
      "Epoch 3316/20000: Train Loss = 0.445041, Test Loss = 0.239560, Learning Rate = 2.836967e-04\n",
      "Epoch 3317/20000: Train Loss = 0.443944, Test Loss = 0.247257, Learning Rate = 2.835889e-04\n",
      "Epoch 3318/20000: Train Loss = 0.443883, Test Loss = 0.251248, Learning Rate = 2.834811e-04\n",
      "Epoch 3319/20000: Train Loss = 0.444185, Test Loss = 0.241826, Learning Rate = 2.833734e-04\n",
      "Epoch 3320/20000: Train Loss = 0.445805, Test Loss = 0.257915, Learning Rate = 2.832658e-04\n",
      "Epoch 3321/20000: Train Loss = 0.445465, Test Loss = 0.247127, Learning Rate = 2.831581e-04\n",
      "Epoch 3322/20000: Train Loss = 0.444551, Test Loss = 0.254392, Learning Rate = 2.830505e-04\n",
      "Epoch 3323/20000: Train Loss = 0.445324, Test Loss = 0.263068, Learning Rate = 2.829430e-04\n",
      "Epoch 3324/20000: Train Loss = 0.445309, Test Loss = 0.243208, Learning Rate = 2.828355e-04\n",
      "Epoch 3325/20000: Train Loss = 0.445827, Test Loss = 0.241936, Learning Rate = 2.827280e-04\n",
      "Epoch 3326/20000: Train Loss = 0.443751, Test Loss = 0.248631, Learning Rate = 2.826206e-04\n",
      "Epoch 3327/20000: Train Loss = 0.444108, Test Loss = 0.238061, Learning Rate = 2.825132e-04\n",
      "Epoch 3328/20000: Train Loss = 0.443785, Test Loss = 0.245082, Learning Rate = 2.824058e-04\n",
      "Epoch 3329/20000: Train Loss = 0.443746, Test Loss = 0.247170, Learning Rate = 2.822985e-04\n",
      "Epoch 3330/20000: Train Loss = 0.443380, Test Loss = 0.245272, Learning Rate = 2.821913e-04\n",
      "Epoch 3331/20000: Train Loss = 0.443344, Test Loss = 0.239906, Learning Rate = 2.820840e-04\n",
      "Epoch 3332/20000: Train Loss = 0.443365, Test Loss = 0.240047, Learning Rate = 2.819768e-04\n",
      "Epoch 3333/20000: Train Loss = 0.443460, Test Loss = 0.242166, Learning Rate = 2.818697e-04\n",
      "Epoch 3334/20000: Train Loss = 0.443884, Test Loss = 0.240883, Learning Rate = 2.817626e-04\n",
      "Epoch 3335/20000: Train Loss = 0.444056, Test Loss = 0.246740, Learning Rate = 2.816555e-04\n",
      "Epoch 3336/20000: Train Loss = 0.444249, Test Loss = 0.240756, Learning Rate = 2.815485e-04\n",
      "Epoch 3337/20000: Train Loss = 0.443735, Test Loss = 0.250461, Learning Rate = 2.814415e-04\n",
      "Epoch 3338/20000: Train Loss = 0.443407, Test Loss = 0.233886, Learning Rate = 2.813346e-04\n",
      "Epoch 3339/20000: Train Loss = 0.443745, Test Loss = 0.242347, Learning Rate = 2.812277e-04\n",
      "Epoch 3340/20000: Train Loss = 0.443886, Test Loss = 0.244503, Learning Rate = 2.811208e-04\n",
      "Epoch 3341/20000: Train Loss = 0.444344, Test Loss = 0.237875, Learning Rate = 2.810140e-04\n",
      "Epoch 3342/20000: Train Loss = 0.443017, Test Loss = 0.247670, Learning Rate = 2.809072e-04\n",
      "Epoch 3343/20000: Train Loss = 0.443057, Test Loss = 0.245048, Learning Rate = 2.808005e-04\n",
      "Epoch 3344/20000: Train Loss = 0.443828, Test Loss = 0.241650, Learning Rate = 2.806938e-04\n",
      "Epoch 3345/20000: Train Loss = 0.444635, Test Loss = 0.253226, Learning Rate = 2.805872e-04\n",
      "Epoch 3346/20000: Train Loss = 0.443402, Test Loss = 0.241771, Learning Rate = 2.804805e-04\n",
      "Epoch 3347/20000: Train Loss = 0.444450, Test Loss = 0.237793, Learning Rate = 2.803740e-04\n",
      "Epoch 3348/20000: Train Loss = 0.442924, Test Loss = 0.249820, Learning Rate = 2.802674e-04\n",
      "Epoch 3349/20000: Train Loss = 0.443312, Test Loss = 0.249018, Learning Rate = 2.801609e-04\n",
      "Epoch 3350/20000: Train Loss = 0.443658, Test Loss = 0.241396, Learning Rate = 2.800545e-04\n",
      "Epoch 3351/20000: Train Loss = 0.444540, Test Loss = 0.252310, Learning Rate = 2.799481e-04\n",
      "Epoch 3352/20000: Train Loss = 0.443542, Test Loss = 0.248562, Learning Rate = 2.798417e-04\n",
      "Epoch 3353/20000: Train Loss = 0.443415, Test Loss = 0.239925, Learning Rate = 2.797354e-04\n",
      "Epoch 3354/20000: Train Loss = 0.443812, Test Loss = 0.242925, Learning Rate = 2.796291e-04\n",
      "Epoch 3355/20000: Train Loss = 0.443657, Test Loss = 0.243789, Learning Rate = 2.795228e-04\n",
      "Epoch 3356/20000: Train Loss = 0.443352, Test Loss = 0.250740, Learning Rate = 2.794166e-04\n",
      "Epoch 3357/20000: Train Loss = 0.443844, Test Loss = 0.242531, Learning Rate = 2.793104e-04\n",
      "Epoch 3358/20000: Train Loss = 0.444489, Test Loss = 0.238137, Learning Rate = 2.792043e-04\n",
      "Epoch 3359/20000: Train Loss = 0.444063, Test Loss = 0.246795, Learning Rate = 2.790982e-04\n",
      "Epoch 3360/20000: Train Loss = 0.443249, Test Loss = 0.243281, Learning Rate = 2.789922e-04\n",
      "Epoch 3361/20000: Train Loss = 0.443659, Test Loss = 0.241144, Learning Rate = 2.788862e-04\n",
      "Epoch 3362/20000: Train Loss = 0.444982, Test Loss = 0.245426, Learning Rate = 2.787802e-04\n",
      "Epoch 3363/20000: Train Loss = 0.443410, Test Loss = 0.243166, Learning Rate = 2.786743e-04\n",
      "Epoch 3364/20000: Train Loss = 0.443773, Test Loss = 0.243762, Learning Rate = 2.785684e-04\n",
      "Epoch 3365/20000: Train Loss = 0.443441, Test Loss = 0.250244, Learning Rate = 2.784625e-04\n",
      "Epoch 3366/20000: Train Loss = 0.444201, Test Loss = 0.251082, Learning Rate = 2.783567e-04\n",
      "Epoch 3367/20000: Train Loss = 0.445233, Test Loss = 0.243251, Learning Rate = 2.782509e-04\n",
      "Epoch 3368/20000: Train Loss = 0.444825, Test Loss = 0.235520, Learning Rate = 2.781452e-04\n",
      "Epoch 3369/20000: Train Loss = 0.444519, Test Loss = 0.247474, Learning Rate = 2.780395e-04\n",
      "Epoch 3370/20000: Train Loss = 0.443619, Test Loss = 0.231535, Learning Rate = 2.779339e-04\n",
      "Epoch 3371/20000: Train Loss = 0.444190, Test Loss = 0.241955, Learning Rate = 2.778283e-04\n",
      "Epoch 3372/20000: Train Loss = 0.443698, Test Loss = 0.244413, Learning Rate = 2.777227e-04\n",
      "Epoch 3373/20000: Train Loss = 0.446455, Test Loss = 0.244672, Learning Rate = 2.776172e-04\n",
      "Epoch 3374/20000: Train Loss = 0.445742, Test Loss = 0.232985, Learning Rate = 2.775117e-04\n",
      "Epoch 3375/20000: Train Loss = 0.443919, Test Loss = 0.247980, Learning Rate = 2.774062e-04\n",
      "Epoch 3376/20000: Train Loss = 0.443426, Test Loss = 0.247220, Learning Rate = 2.773008e-04\n",
      "Epoch 3377/20000: Train Loss = 0.445662, Test Loss = 0.242603, Learning Rate = 2.771955e-04\n",
      "Epoch 3378/20000: Train Loss = 0.445046, Test Loss = 0.233185, Learning Rate = 2.770901e-04\n",
      "Epoch 3379/20000: Train Loss = 0.445537, Test Loss = 0.249859, Learning Rate = 2.769849e-04\n",
      "Epoch 3380/20000: Train Loss = 0.445591, Test Loss = 0.246310, Learning Rate = 2.768796e-04\n",
      "Epoch 3381/20000: Train Loss = 0.443551, Test Loss = 0.239335, Learning Rate = 2.767744e-04\n",
      "Epoch 3382/20000: Train Loss = 0.445267, Test Loss = 0.235775, Learning Rate = 2.766692e-04\n",
      "Epoch 3383/20000: Train Loss = 0.443263, Test Loss = 0.249506, Learning Rate = 2.765641e-04\n",
      "Epoch 3384/20000: Train Loss = 0.444947, Test Loss = 0.241370, Learning Rate = 2.764590e-04\n",
      "Epoch 3385/20000: Train Loss = 0.443846, Test Loss = 0.241679, Learning Rate = 2.763540e-04\n",
      "Epoch 3386/20000: Train Loss = 0.444925, Test Loss = 0.249667, Learning Rate = 2.762490e-04\n",
      "Epoch 3387/20000: Train Loss = 0.443294, Test Loss = 0.255611, Learning Rate = 2.761440e-04\n",
      "Epoch 3388/20000: Train Loss = 0.445902, Test Loss = 0.257511, Learning Rate = 2.760391e-04\n",
      "Epoch 3389/20000: Train Loss = 0.444652, Test Loss = 0.242100, Learning Rate = 2.759342e-04\n",
      "Epoch 3390/20000: Train Loss = 0.444413, Test Loss = 0.247981, Learning Rate = 2.758293e-04\n",
      "Epoch 3391/20000: Train Loss = 0.444114, Test Loss = 0.249249, Learning Rate = 2.757245e-04\n",
      "Epoch 3392/20000: Train Loss = 0.443827, Test Loss = 0.243809, Learning Rate = 2.756198e-04\n",
      "Epoch 3393/20000: Train Loss = 0.443140, Test Loss = 0.246876, Learning Rate = 2.755150e-04\n",
      "Epoch 3394/20000: Train Loss = 0.444894, Test Loss = 0.248576, Learning Rate = 2.754104e-04\n",
      "Epoch 3395/20000: Train Loss = 0.444251, Test Loss = 0.248870, Learning Rate = 2.753057e-04\n",
      "Epoch 3396/20000: Train Loss = 0.444279, Test Loss = 0.244607, Learning Rate = 2.752011e-04\n",
      "Epoch 3397/20000: Train Loss = 0.444009, Test Loss = 0.244783, Learning Rate = 2.750965e-04\n",
      "Epoch 3398/20000: Train Loss = 0.444155, Test Loss = 0.246594, Learning Rate = 2.749920e-04\n",
      "Epoch 3399/20000: Train Loss = 0.443501, Test Loss = 0.248857, Learning Rate = 2.748875e-04\n",
      "Epoch 3400/20000: Train Loss = 0.443380, Test Loss = 0.249542, Learning Rate = 2.747831e-04\n",
      "Epoch 3401/20000: Train Loss = 0.443373, Test Loss = 0.243874, Learning Rate = 2.746786e-04\n",
      "Epoch 3402/20000: Train Loss = 0.445647, Test Loss = 0.250487, Learning Rate = 2.745743e-04\n",
      "Epoch 3403/20000: Train Loss = 0.444947, Test Loss = 0.247579, Learning Rate = 2.744699e-04\n",
      "Epoch 3404/20000: Train Loss = 0.443533, Test Loss = 0.242935, Learning Rate = 2.743657e-04\n",
      "Epoch 3405/20000: Train Loss = 0.443895, Test Loss = 0.238155, Learning Rate = 2.742614e-04\n",
      "Epoch 3406/20000: Train Loss = 0.443612, Test Loss = 0.242330, Learning Rate = 2.741572e-04\n",
      "Epoch 3407/20000: Train Loss = 0.443860, Test Loss = 0.235454, Learning Rate = 2.740530e-04\n",
      "Epoch 3408/20000: Train Loss = 0.444483, Test Loss = 0.243036, Learning Rate = 2.739489e-04\n",
      "Epoch 3409/20000: Train Loss = 0.443481, Test Loss = 0.241167, Learning Rate = 2.738448e-04\n",
      "Epoch 3410/20000: Train Loss = 0.443219, Test Loss = 0.251340, Learning Rate = 2.737407e-04\n",
      "Epoch 3411/20000: Train Loss = 0.443080, Test Loss = 0.237105, Learning Rate = 2.736367e-04\n",
      "Epoch 3412/20000: Train Loss = 0.443172, Test Loss = 0.245885, Learning Rate = 2.735328e-04\n",
      "Epoch 3413/20000: Train Loss = 0.444340, Test Loss = 0.244207, Learning Rate = 2.734288e-04\n",
      "Epoch 3414/20000: Train Loss = 0.442939, Test Loss = 0.254976, Learning Rate = 2.733249e-04\n",
      "Epoch 3415/20000: Train Loss = 0.444297, Test Loss = 0.255413, Learning Rate = 2.732211e-04\n",
      "Epoch 3416/20000: Train Loss = 0.444875, Test Loss = 0.241252, Learning Rate = 2.731172e-04\n",
      "Epoch 3417/20000: Train Loss = 0.443436, Test Loss = 0.244468, Learning Rate = 2.730135e-04\n",
      "Epoch 3418/20000: Train Loss = 0.443217, Test Loss = 0.242295, Learning Rate = 2.729097e-04\n",
      "Epoch 3419/20000: Train Loss = 0.443585, Test Loss = 0.249399, Learning Rate = 2.728060e-04\n",
      "Epoch 3420/20000: Train Loss = 0.443410, Test Loss = 0.248107, Learning Rate = 2.727024e-04\n",
      "Epoch 3421/20000: Train Loss = 0.443637, Test Loss = 0.253928, Learning Rate = 2.725988e-04\n",
      "Epoch 3422/20000: Train Loss = 0.443619, Test Loss = 0.244452, Learning Rate = 2.724952e-04\n",
      "Epoch 3423/20000: Train Loss = 0.445257, Test Loss = 0.237191, Learning Rate = 2.723916e-04\n",
      "Epoch 3424/20000: Train Loss = 0.443370, Test Loss = 0.261462, Learning Rate = 2.722881e-04\n",
      "Epoch 3425/20000: Train Loss = 0.446375, Test Loss = 0.250588, Learning Rate = 2.721847e-04\n",
      "Epoch 3426/20000: Train Loss = 0.444068, Test Loss = 0.244482, Learning Rate = 2.720812e-04\n",
      "Epoch 3427/20000: Train Loss = 0.444038, Test Loss = 0.235297, Learning Rate = 2.719779e-04\n",
      "Epoch 3428/20000: Train Loss = 0.444443, Test Loss = 0.252162, Learning Rate = 2.718745e-04\n",
      "Epoch 3429/20000: Train Loss = 0.444874, Test Loss = 0.243435, Learning Rate = 2.717712e-04\n",
      "Epoch 3430/20000: Train Loss = 0.443808, Test Loss = 0.258648, Learning Rate = 2.716680e-04\n",
      "Epoch 3431/20000: Train Loss = 0.443854, Test Loss = 0.245785, Learning Rate = 2.715647e-04\n",
      "Epoch 3432/20000: Train Loss = 0.443159, Test Loss = 0.246678, Learning Rate = 2.714615e-04\n",
      "Epoch 3433/20000: Train Loss = 0.443859, Test Loss = 0.251686, Learning Rate = 2.713584e-04\n",
      "Epoch 3434/20000: Train Loss = 0.443837, Test Loss = 0.247573, Learning Rate = 2.712553e-04\n",
      "Epoch 3435/20000: Train Loss = 0.444048, Test Loss = 0.241032, Learning Rate = 2.711522e-04\n",
      "Epoch 3436/20000: Train Loss = 0.444255, Test Loss = 0.243308, Learning Rate = 2.710492e-04\n",
      "Epoch 3437/20000: Train Loss = 0.444099, Test Loss = 0.242501, Learning Rate = 2.709462e-04\n",
      "Epoch 3438/20000: Train Loss = 0.445751, Test Loss = 0.250457, Learning Rate = 2.708432e-04\n",
      "Epoch 3439/20000: Train Loss = 0.443784, Test Loss = 0.242613, Learning Rate = 2.707403e-04\n",
      "Epoch 3440/20000: Train Loss = 0.444594, Test Loss = 0.238454, Learning Rate = 2.706374e-04\n",
      "Epoch 3441/20000: Train Loss = 0.445002, Test Loss = 0.242865, Learning Rate = 2.705346e-04\n",
      "Epoch 3442/20000: Train Loss = 0.443843, Test Loss = 0.241456, Learning Rate = 2.704318e-04\n",
      "Epoch 3443/20000: Train Loss = 0.444089, Test Loss = 0.249697, Learning Rate = 2.703291e-04\n",
      "Epoch 3444/20000: Train Loss = 0.444892, Test Loss = 0.244947, Learning Rate = 2.702263e-04\n",
      "Epoch 3445/20000: Train Loss = 0.443491, Test Loss = 0.246526, Learning Rate = 2.701237e-04\n",
      "Epoch 3446/20000: Train Loss = 0.444056, Test Loss = 0.244483, Learning Rate = 2.700210e-04\n",
      "Epoch 3447/20000: Train Loss = 0.446191, Test Loss = 0.251912, Learning Rate = 2.699184e-04\n",
      "Epoch 3448/20000: Train Loss = 0.443889, Test Loss = 0.250898, Learning Rate = 2.698159e-04\n",
      "Epoch 3449/20000: Train Loss = 0.444062, Test Loss = 0.238960, Learning Rate = 2.697133e-04\n",
      "Epoch 3450/20000: Train Loss = 0.444897, Test Loss = 0.242413, Learning Rate = 2.696109e-04\n",
      "Epoch 3451/20000: Train Loss = 0.444074, Test Loss = 0.241155, Learning Rate = 2.695084e-04\n",
      "Epoch 3452/20000: Train Loss = 0.444730, Test Loss = 0.253174, Learning Rate = 2.694060e-04\n",
      "Epoch 3453/20000: Train Loss = 0.443981, Test Loss = 0.245195, Learning Rate = 2.693036e-04\n",
      "Epoch 3454/20000: Train Loss = 0.443785, Test Loss = 0.245346, Learning Rate = 2.692013e-04\n",
      "Epoch 3455/20000: Train Loss = 0.447649, Test Loss = 0.246370, Learning Rate = 2.690990e-04\n",
      "Epoch 3456/20000: Train Loss = 0.443696, Test Loss = 0.246387, Learning Rate = 2.689968e-04\n",
      "Epoch 3457/20000: Train Loss = 0.443129, Test Loss = 0.244925, Learning Rate = 2.688946e-04\n",
      "Epoch 3458/20000: Train Loss = 0.444305, Test Loss = 0.248194, Learning Rate = 2.687924e-04\n",
      "Epoch 3459/20000: Train Loss = 0.443716, Test Loss = 0.243202, Learning Rate = 2.686903e-04\n",
      "Epoch 3460/20000: Train Loss = 0.444392, Test Loss = 0.244497, Learning Rate = 2.685882e-04\n",
      "Epoch 3461/20000: Train Loss = 0.444099, Test Loss = 0.254573, Learning Rate = 2.684861e-04\n",
      "Epoch 3462/20000: Train Loss = 0.444638, Test Loss = 0.250736, Learning Rate = 2.683841e-04\n",
      "Epoch 3463/20000: Train Loss = 0.445082, Test Loss = 0.251158, Learning Rate = 2.682821e-04\n",
      "Epoch 3464/20000: Train Loss = 0.444234, Test Loss = 0.251769, Learning Rate = 2.681802e-04\n",
      "Epoch 3465/20000: Train Loss = 0.444924, Test Loss = 0.254084, Learning Rate = 2.680783e-04\n",
      "Epoch 3466/20000: Train Loss = 0.445636, Test Loss = 0.242745, Learning Rate = 2.679764e-04\n",
      "Epoch 3467/20000: Train Loss = 0.444157, Test Loss = 0.251966, Learning Rate = 2.678746e-04\n",
      "Epoch 3468/20000: Train Loss = 0.443234, Test Loss = 0.248179, Learning Rate = 2.677728e-04\n",
      "Epoch 3469/20000: Train Loss = 0.443564, Test Loss = 0.253799, Learning Rate = 2.676710e-04\n",
      "Epoch 3470/20000: Train Loss = 0.444127, Test Loss = 0.247154, Learning Rate = 2.675693e-04\n",
      "Epoch 3471/20000: Train Loss = 0.443207, Test Loss = 0.249665, Learning Rate = 2.674677e-04\n",
      "Epoch 3472/20000: Train Loss = 0.443558, Test Loss = 0.247647, Learning Rate = 2.673660e-04\n",
      "Epoch 3473/20000: Train Loss = 0.443331, Test Loss = 0.242158, Learning Rate = 2.672644e-04\n",
      "Epoch 3474/20000: Train Loss = 0.444158, Test Loss = 0.255683, Learning Rate = 2.671629e-04\n",
      "Epoch 3475/20000: Train Loss = 0.443834, Test Loss = 0.239820, Learning Rate = 2.670614e-04\n",
      "Epoch 3476/20000: Train Loss = 0.443693, Test Loss = 0.245366, Learning Rate = 2.669599e-04\n",
      "Epoch 3477/20000: Train Loss = 0.443661, Test Loss = 0.247107, Learning Rate = 2.668585e-04\n",
      "Epoch 3478/20000: Train Loss = 0.443435, Test Loss = 0.247864, Learning Rate = 2.667571e-04\n",
      "Epoch 3479/20000: Train Loss = 0.444187, Test Loss = 0.240689, Learning Rate = 2.666557e-04\n",
      "Epoch 3480/20000: Train Loss = 0.444053, Test Loss = 0.252295, Learning Rate = 2.665544e-04\n",
      "Epoch 3481/20000: Train Loss = 0.444212, Test Loss = 0.240202, Learning Rate = 2.664531e-04\n",
      "Epoch 3482/20000: Train Loss = 0.443519, Test Loss = 0.246716, Learning Rate = 2.663519e-04\n",
      "Epoch 3483/20000: Train Loss = 0.443707, Test Loss = 0.251039, Learning Rate = 2.662507e-04\n",
      "Epoch 3484/20000: Train Loss = 0.443721, Test Loss = 0.248781, Learning Rate = 2.661495e-04\n",
      "Epoch 3485/20000: Train Loss = 0.443873, Test Loss = 0.244198, Learning Rate = 2.660484e-04\n",
      "Epoch 3486/20000: Train Loss = 0.444707, Test Loss = 0.244394, Learning Rate = 2.659473e-04\n",
      "Epoch 3487/20000: Train Loss = 0.443056, Test Loss = 0.250991, Learning Rate = 2.658462e-04\n",
      "Epoch 3488/20000: Train Loss = 0.443182, Test Loss = 0.248783, Learning Rate = 2.657452e-04\n",
      "Epoch 3489/20000: Train Loss = 0.444760, Test Loss = 0.250369, Learning Rate = 2.656442e-04\n",
      "Epoch 3490/20000: Train Loss = 0.444676, Test Loss = 0.253795, Learning Rate = 2.655433e-04\n",
      "Epoch 3491/20000: Train Loss = 0.445236, Test Loss = 0.245002, Learning Rate = 2.654424e-04\n",
      "Epoch 3492/20000: Train Loss = 0.443817, Test Loss = 0.253809, Learning Rate = 2.653415e-04\n",
      "Epoch 3493/20000: Train Loss = 0.444030, Test Loss = 0.250935, Learning Rate = 2.652407e-04\n",
      "Epoch 3494/20000: Train Loss = 0.444511, Test Loss = 0.251329, Learning Rate = 2.651399e-04\n",
      "Epoch 3495/20000: Train Loss = 0.445934, Test Loss = 0.235570, Learning Rate = 2.650392e-04\n",
      "Epoch 3496/20000: Train Loss = 0.444429, Test Loss = 0.245803, Learning Rate = 2.649385e-04\n",
      "Epoch 3497/20000: Train Loss = 0.443266, Test Loss = 0.250802, Learning Rate = 2.648378e-04\n",
      "Epoch 3498/20000: Train Loss = 0.443575, Test Loss = 0.251238, Learning Rate = 2.647372e-04\n",
      "Epoch 3499/20000: Train Loss = 0.443644, Test Loss = 0.247438, Learning Rate = 2.646366e-04\n",
      "Epoch 3500/20000: Train Loss = 0.444509, Test Loss = 0.246273, Learning Rate = 2.645360e-04\n",
      "Epoch 3501/20000: Train Loss = 0.444861, Test Loss = 0.251591, Learning Rate = 2.644355e-04\n",
      "Epoch 3502/20000: Train Loss = 0.443778, Test Loss = 0.249553, Learning Rate = 2.643350e-04\n",
      "Epoch 3503/20000: Train Loss = 0.443792, Test Loss = 0.243270, Learning Rate = 2.642346e-04\n",
      "Epoch 3504/20000: Train Loss = 0.445040, Test Loss = 0.240549, Learning Rate = 2.641342e-04\n",
      "Epoch 3505/20000: Train Loss = 0.446639, Test Loss = 0.248139, Learning Rate = 2.640338e-04\n",
      "Epoch 3506/20000: Train Loss = 0.444162, Test Loss = 0.249909, Learning Rate = 2.639335e-04\n",
      "Epoch 3507/20000: Train Loss = 0.443737, Test Loss = 0.246350, Learning Rate = 2.638332e-04\n",
      "Epoch 3508/20000: Train Loss = 0.443775, Test Loss = 0.233516, Learning Rate = 2.637329e-04\n",
      "Epoch 3509/20000: Train Loss = 0.443837, Test Loss = 0.248482, Learning Rate = 2.636327e-04\n",
      "Epoch 3510/20000: Train Loss = 0.443967, Test Loss = 0.241727, Learning Rate = 2.635326e-04\n",
      "Epoch 3511/20000: Train Loss = 0.445834, Test Loss = 0.242585, Learning Rate = 2.634324e-04\n",
      "Epoch 3512/20000: Train Loss = 0.443137, Test Loss = 0.260065, Learning Rate = 2.633323e-04\n",
      "Epoch 3513/20000: Train Loss = 0.445333, Test Loss = 0.252338, Learning Rate = 2.632323e-04\n",
      "Epoch 3514/20000: Train Loss = 0.443927, Test Loss = 0.242668, Learning Rate = 2.631323e-04\n",
      "Epoch 3515/20000: Train Loss = 0.444462, Test Loss = 0.249576, Learning Rate = 2.630323e-04\n",
      "Epoch 3516/20000: Train Loss = 0.445275, Test Loss = 0.246979, Learning Rate = 2.629323e-04\n",
      "Epoch 3517/20000: Train Loss = 0.444791, Test Loss = 0.257225, Learning Rate = 2.628324e-04\n",
      "Epoch 3518/20000: Train Loss = 0.443662, Test Loss = 0.248145, Learning Rate = 2.627325e-04\n",
      "Epoch 3519/20000: Train Loss = 0.444314, Test Loss = 0.243337, Learning Rate = 2.626327e-04\n",
      "Epoch 3520/20000: Train Loss = 0.444655, Test Loss = 0.244426, Learning Rate = 2.625329e-04\n",
      "Epoch 3521/20000: Train Loss = 0.444909, Test Loss = 0.247579, Learning Rate = 2.624332e-04\n",
      "Epoch 3522/20000: Train Loss = 0.443549, Test Loss = 0.243561, Learning Rate = 2.623334e-04\n",
      "Epoch 3523/20000: Train Loss = 0.443313, Test Loss = 0.247247, Learning Rate = 2.622338e-04\n",
      "Epoch 3524/20000: Train Loss = 0.444491, Test Loss = 0.236842, Learning Rate = 2.621341e-04\n",
      "Epoch 3525/20000: Train Loss = 0.443234, Test Loss = 0.249999, Learning Rate = 2.620345e-04\n",
      "Epoch 3526/20000: Train Loss = 0.444102, Test Loss = 0.242750, Learning Rate = 2.619350e-04\n",
      "Epoch 3527/20000: Train Loss = 0.444160, Test Loss = 0.249841, Learning Rate = 2.618354e-04\n",
      "Epoch 3528/20000: Train Loss = 0.444169, Test Loss = 0.242220, Learning Rate = 2.617359e-04\n",
      "Epoch 3529/20000: Train Loss = 0.443658, Test Loss = 0.260898, Learning Rate = 2.616365e-04\n",
      "Epoch 3530/20000: Train Loss = 0.443502, Test Loss = 0.252884, Learning Rate = 2.615371e-04\n",
      "Epoch 3531/20000: Train Loss = 0.443752, Test Loss = 0.254809, Learning Rate = 2.614377e-04\n",
      "Epoch 3532/20000: Train Loss = 0.445360, Test Loss = 0.236821, Learning Rate = 2.613384e-04\n",
      "Epoch 3533/20000: Train Loss = 0.445247, Test Loss = 0.246353, Learning Rate = 2.612391e-04\n",
      "Epoch 3534/20000: Train Loss = 0.443627, Test Loss = 0.245459, Learning Rate = 2.611398e-04\n",
      "Epoch 3535/20000: Train Loss = 0.443659, Test Loss = 0.251410, Learning Rate = 2.610406e-04\n",
      "Epoch 3536/20000: Train Loss = 0.443555, Test Loss = 0.245144, Learning Rate = 2.609414e-04\n",
      "Epoch 3537/20000: Train Loss = 0.445198, Test Loss = 0.248454, Learning Rate = 2.608422e-04\n",
      "Epoch 3538/20000: Train Loss = 0.444437, Test Loss = 0.233644, Learning Rate = 2.607431e-04\n",
      "Epoch 3539/20000: Train Loss = 0.445395, Test Loss = 0.239271, Learning Rate = 2.606440e-04\n",
      "Epoch 3540/20000: Train Loss = 0.443736, Test Loss = 0.242828, Learning Rate = 2.605450e-04\n",
      "Epoch 3541/20000: Train Loss = 0.443822, Test Loss = 0.243654, Learning Rate = 2.604460e-04\n",
      "Epoch 3542/20000: Train Loss = 0.443834, Test Loss = 0.247645, Learning Rate = 2.603470e-04\n",
      "Epoch 3543/20000: Train Loss = 0.443439, Test Loss = 0.240385, Learning Rate = 2.602481e-04\n",
      "Epoch 3544/20000: Train Loss = 0.443377, Test Loss = 0.255180, Learning Rate = 2.601492e-04\n",
      "Epoch 3545/20000: Train Loss = 0.444275, Test Loss = 0.247410, Learning Rate = 2.600504e-04\n",
      "Epoch 3546/20000: Train Loss = 0.444208, Test Loss = 0.244447, Learning Rate = 2.599516e-04\n",
      "Epoch 3547/20000: Train Loss = 0.443788, Test Loss = 0.241139, Learning Rate = 2.598528e-04\n",
      "Epoch 3548/20000: Train Loss = 0.443485, Test Loss = 0.251893, Learning Rate = 2.597541e-04\n",
      "Epoch 3549/20000: Train Loss = 0.443287, Test Loss = 0.247819, Learning Rate = 2.596554e-04\n",
      "Epoch 3550/20000: Train Loss = 0.443209, Test Loss = 0.239066, Learning Rate = 2.595567e-04\n",
      "Epoch 3551/20000: Train Loss = 0.443689, Test Loss = 0.247713, Learning Rate = 2.594581e-04\n",
      "Epoch 3552/20000: Train Loss = 0.444086, Test Loss = 0.251240, Learning Rate = 2.593595e-04\n",
      "Epoch 3553/20000: Train Loss = 0.444268, Test Loss = 0.249561, Learning Rate = 2.592609e-04\n",
      "Epoch 3554/20000: Train Loss = 0.443533, Test Loss = 0.247144, Learning Rate = 2.591624e-04\n",
      "Epoch 3555/20000: Train Loss = 0.443723, Test Loss = 0.245459, Learning Rate = 2.590639e-04\n",
      "Epoch 3556/20000: Train Loss = 0.444297, Test Loss = 0.239085, Learning Rate = 2.589655e-04\n",
      "Epoch 3557/20000: Train Loss = 0.444790, Test Loss = 0.243853, Learning Rate = 2.588671e-04\n",
      "Epoch 3558/20000: Train Loss = 0.444254, Test Loss = 0.249878, Learning Rate = 2.587687e-04\n",
      "Epoch 3559/20000: Train Loss = 0.443551, Test Loss = 0.250229, Learning Rate = 2.586704e-04\n",
      "Epoch 3560/20000: Train Loss = 0.444067, Test Loss = 0.247947, Learning Rate = 2.585721e-04\n",
      "Epoch 3561/20000: Train Loss = 0.443521, Test Loss = 0.236486, Learning Rate = 2.584739e-04\n",
      "Epoch 3562/20000: Train Loss = 0.443705, Test Loss = 0.257397, Learning Rate = 2.583757e-04\n",
      "Epoch 3563/20000: Train Loss = 0.444308, Test Loss = 0.244346, Learning Rate = 2.582775e-04\n",
      "Epoch 3564/20000: Train Loss = 0.443737, Test Loss = 0.237964, Learning Rate = 2.581794e-04\n",
      "Epoch 3565/20000: Train Loss = 0.444069, Test Loss = 0.259039, Learning Rate = 2.580813e-04\n",
      "Epoch 3566/20000: Train Loss = 0.443600, Test Loss = 0.243186, Learning Rate = 2.579832e-04\n",
      "Epoch 3567/20000: Train Loss = 0.444061, Test Loss = 0.253527, Learning Rate = 2.578852e-04\n",
      "Epoch 3568/20000: Train Loss = 0.445074, Test Loss = 0.254226, Learning Rate = 2.577872e-04\n",
      "Epoch 3569/20000: Train Loss = 0.443922, Test Loss = 0.251210, Learning Rate = 2.576892e-04\n",
      "Epoch 3570/20000: Train Loss = 0.444147, Test Loss = 0.244650, Learning Rate = 2.575913e-04\n",
      "Epoch 3571/20000: Train Loss = 0.444027, Test Loss = 0.244637, Learning Rate = 2.574934e-04\n",
      "Epoch 3572/20000: Train Loss = 0.444759, Test Loss = 0.248090, Learning Rate = 2.573956e-04\n",
      "Epoch 3573/20000: Train Loss = 0.444825, Test Loss = 0.250447, Learning Rate = 2.572978e-04\n",
      "Epoch 3574/20000: Train Loss = 0.444321, Test Loss = 0.246211, Learning Rate = 2.572000e-04\n",
      "Epoch 3575/20000: Train Loss = 0.443009, Test Loss = 0.258494, Learning Rate = 2.571023e-04\n",
      "Epoch 3576/20000: Train Loss = 0.443423, Test Loss = 0.250198, Learning Rate = 2.570046e-04\n",
      "Epoch 3577/20000: Train Loss = 0.443106, Test Loss = 0.250446, Learning Rate = 2.569069e-04\n",
      "Epoch 3578/20000: Train Loss = 0.444164, Test Loss = 0.247220, Learning Rate = 2.568093e-04\n",
      "Epoch 3579/20000: Train Loss = 0.443378, Test Loss = 0.250216, Learning Rate = 2.567117e-04\n",
      "Epoch 3580/20000: Train Loss = 0.443763, Test Loss = 0.240649, Learning Rate = 2.566142e-04\n",
      "Epoch 3581/20000: Train Loss = 0.443737, Test Loss = 0.252827, Learning Rate = 2.565167e-04\n",
      "Epoch 3582/20000: Train Loss = 0.443919, Test Loss = 0.237849, Learning Rate = 2.564192e-04\n",
      "Epoch 3583/20000: Train Loss = 0.443413, Test Loss = 0.252431, Learning Rate = 2.563218e-04\n",
      "Epoch 3584/20000: Train Loss = 0.443701, Test Loss = 0.249077, Learning Rate = 2.562244e-04\n",
      "Epoch 3585/20000: Train Loss = 0.443487, Test Loss = 0.261338, Learning Rate = 2.561270e-04\n",
      "Epoch 3586/20000: Train Loss = 0.443908, Test Loss = 0.247167, Learning Rate = 2.560297e-04\n",
      "Epoch 3587/20000: Train Loss = 0.443487, Test Loss = 0.244162, Learning Rate = 2.559324e-04\n",
      "Epoch 3588/20000: Train Loss = 0.443591, Test Loss = 0.247121, Learning Rate = 2.558352e-04\n",
      "Epoch 3589/20000: Train Loss = 0.443585, Test Loss = 0.239824, Learning Rate = 2.557380e-04\n",
      "Epoch 3590/20000: Train Loss = 0.444020, Test Loss = 0.241941, Learning Rate = 2.556408e-04\n",
      "Epoch 3591/20000: Train Loss = 0.443419, Test Loss = 0.249220, Learning Rate = 2.555437e-04\n",
      "Epoch 3592/20000: Train Loss = 0.443626, Test Loss = 0.240618, Learning Rate = 2.554466e-04\n",
      "Epoch 3593/20000: Train Loss = 0.443324, Test Loss = 0.242743, Learning Rate = 2.553495e-04\n",
      "Epoch 3594/20000: Train Loss = 0.444585, Test Loss = 0.250834, Learning Rate = 2.552525e-04\n",
      "Epoch 3595/20000: Train Loss = 0.443506, Test Loss = 0.245537, Learning Rate = 2.551555e-04\n",
      "Epoch 3596/20000: Train Loss = 0.443305, Test Loss = 0.246426, Learning Rate = 2.550585e-04\n",
      "Epoch 3597/20000: Train Loss = 0.443478, Test Loss = 0.246425, Learning Rate = 2.549616e-04\n",
      "Epoch 3598/20000: Train Loss = 0.445255, Test Loss = 0.239455, Learning Rate = 2.548647e-04\n",
      "Epoch 3599/20000: Train Loss = 0.443735, Test Loss = 0.244851, Learning Rate = 2.547679e-04\n",
      "Epoch 3600/20000: Train Loss = 0.443003, Test Loss = 0.240552, Learning Rate = 2.546711e-04\n",
      "Epoch 3601/20000: Train Loss = 0.443260, Test Loss = 0.247602, Learning Rate = 2.545743e-04\n",
      "Epoch 3602/20000: Train Loss = 0.443181, Test Loss = 0.245816, Learning Rate = 2.544776e-04\n",
      "Epoch 3603/20000: Train Loss = 0.443779, Test Loss = 0.234275, Learning Rate = 2.543809e-04\n",
      "Epoch 3604/20000: Train Loss = 0.444465, Test Loss = 0.237149, Learning Rate = 2.542842e-04\n",
      "Epoch 3605/20000: Train Loss = 0.443957, Test Loss = 0.245948, Learning Rate = 2.541876e-04\n",
      "Epoch 3606/20000: Train Loss = 0.444387, Test Loss = 0.239995, Learning Rate = 2.540910e-04\n",
      "Epoch 3607/20000: Train Loss = 0.443425, Test Loss = 0.249141, Learning Rate = 2.539945e-04\n",
      "Epoch 3608/20000: Train Loss = 0.444800, Test Loss = 0.256859, Learning Rate = 2.538980e-04\n",
      "Epoch 3609/20000: Train Loss = 0.444210, Test Loss = 0.242082, Learning Rate = 2.538015e-04\n",
      "Epoch 3610/20000: Train Loss = 0.443938, Test Loss = 0.255128, Learning Rate = 2.537051e-04\n",
      "Epoch 3611/20000: Train Loss = 0.444812, Test Loss = 0.256870, Learning Rate = 2.536087e-04\n",
      "Epoch 3612/20000: Train Loss = 0.443892, Test Loss = 0.251561, Learning Rate = 2.535123e-04\n",
      "Epoch 3613/20000: Train Loss = 0.444506, Test Loss = 0.247082, Learning Rate = 2.534160e-04\n",
      "Epoch 3614/20000: Train Loss = 0.443650, Test Loss = 0.255088, Learning Rate = 2.533197e-04\n",
      "Epoch 3615/20000: Train Loss = 0.444569, Test Loss = 0.247777, Learning Rate = 2.532234e-04\n",
      "Epoch 3616/20000: Train Loss = 0.445183, Test Loss = 0.237353, Learning Rate = 2.531272e-04\n",
      "Epoch 3617/20000: Train Loss = 0.444148, Test Loss = 0.253811, Learning Rate = 2.530310e-04\n",
      "Epoch 3618/20000: Train Loss = 0.444479, Test Loss = 0.258603, Learning Rate = 2.529349e-04\n",
      "Epoch 3619/20000: Train Loss = 0.443158, Test Loss = 0.244759, Learning Rate = 2.528388e-04\n",
      "Epoch 3620/20000: Train Loss = 0.444616, Test Loss = 0.245273, Learning Rate = 2.527427e-04\n",
      "Epoch 3621/20000: Train Loss = 0.444044, Test Loss = 0.237340, Learning Rate = 2.526467e-04\n",
      "Epoch 3622/20000: Train Loss = 0.444047, Test Loss = 0.239089, Learning Rate = 2.525507e-04\n",
      "Epoch 3623/20000: Train Loss = 0.443888, Test Loss = 0.239311, Learning Rate = 2.524547e-04\n",
      "Epoch 3624/20000: Train Loss = 0.445164, Test Loss = 0.245673, Learning Rate = 2.523588e-04\n",
      "Epoch 3625/20000: Train Loss = 0.443429, Test Loss = 0.243621, Learning Rate = 2.522629e-04\n",
      "Epoch 3626/20000: Train Loss = 0.444010, Test Loss = 0.241807, Learning Rate = 2.521670e-04\n",
      "Epoch 3627/20000: Train Loss = 0.443869, Test Loss = 0.242806, Learning Rate = 2.520712e-04\n",
      "Epoch 3628/20000: Train Loss = 0.443106, Test Loss = 0.246870, Learning Rate = 2.519754e-04\n",
      "Epoch 3629/20000: Train Loss = 0.444219, Test Loss = 0.249924, Learning Rate = 2.518797e-04\n",
      "Epoch 3630/20000: Train Loss = 0.444529, Test Loss = 0.246902, Learning Rate = 2.517840e-04\n",
      "Epoch 3631/20000: Train Loss = 0.443851, Test Loss = 0.250859, Learning Rate = 2.516883e-04\n",
      "Epoch 3632/20000: Train Loss = 0.443818, Test Loss = 0.254143, Learning Rate = 2.515927e-04\n",
      "Epoch 3633/20000: Train Loss = 0.444517, Test Loss = 0.248656, Learning Rate = 2.514971e-04\n",
      "Epoch 3634/20000: Train Loss = 0.443940, Test Loss = 0.241373, Learning Rate = 2.514015e-04\n",
      "Epoch 3635/20000: Train Loss = 0.444424, Test Loss = 0.251064, Learning Rate = 2.513060e-04\n",
      "Epoch 3636/20000: Train Loss = 0.443070, Test Loss = 0.248700, Learning Rate = 2.512105e-04\n",
      "Epoch 3637/20000: Train Loss = 0.444943, Test Loss = 0.254316, Learning Rate = 2.511151e-04\n",
      "Epoch 3638/20000: Train Loss = 0.443700, Test Loss = 0.254505, Learning Rate = 2.510196e-04\n",
      "Epoch 3639/20000: Train Loss = 0.443919, Test Loss = 0.248922, Learning Rate = 2.509243e-04\n",
      "Epoch 3640/20000: Train Loss = 0.444053, Test Loss = 0.258822, Learning Rate = 2.508289e-04\n",
      "Epoch 3641/20000: Train Loss = 0.443647, Test Loss = 0.249717, Learning Rate = 2.507336e-04\n",
      "Epoch 3642/20000: Train Loss = 0.444985, Test Loss = 0.249689, Learning Rate = 2.506383e-04\n",
      "Epoch 3643/20000: Train Loss = 0.443667, Test Loss = 0.245517, Learning Rate = 2.505431e-04\n",
      "Epoch 3644/20000: Train Loss = 0.443663, Test Loss = 0.244005, Learning Rate = 2.504479e-04\n",
      "Epoch 3645/20000: Train Loss = 0.443877, Test Loss = 0.250596, Learning Rate = 2.503527e-04\n",
      "Epoch 3646/20000: Train Loss = 0.445500, Test Loss = 0.238410, Learning Rate = 2.502576e-04\n",
      "Epoch 3647/20000: Train Loss = 0.444138, Test Loss = 0.234318, Learning Rate = 2.501625e-04\n",
      "Epoch 3648/20000: Train Loss = 0.446352, Test Loss = 0.247142, Learning Rate = 2.500675e-04\n",
      "Epoch 3649/20000: Train Loss = 0.445159, Test Loss = 0.238665, Learning Rate = 2.499724e-04\n",
      "Epoch 3650/20000: Train Loss = 0.443950, Test Loss = 0.252043, Learning Rate = 2.498775e-04\n",
      "Epoch 3651/20000: Train Loss = 0.444621, Test Loss = 0.244690, Learning Rate = 2.497825e-04\n",
      "Epoch 3652/20000: Train Loss = 0.443352, Test Loss = 0.241286, Learning Rate = 2.496876e-04\n",
      "Epoch 3653/20000: Train Loss = 0.443524, Test Loss = 0.241806, Learning Rate = 2.495927e-04\n",
      "Epoch 3654/20000: Train Loss = 0.443304, Test Loss = 0.246479, Learning Rate = 2.494979e-04\n",
      "Epoch 3655/20000: Train Loss = 0.443989, Test Loss = 0.242557, Learning Rate = 2.494031e-04\n",
      "Epoch 3656/20000: Train Loss = 0.443496, Test Loss = 0.244562, Learning Rate = 2.493083e-04\n",
      "Epoch 3657/20000: Train Loss = 0.443769, Test Loss = 0.247008, Learning Rate = 2.492136e-04\n",
      "Epoch 3658/20000: Train Loss = 0.444408, Test Loss = 0.251284, Learning Rate = 2.491189e-04\n",
      "Epoch 3659/20000: Train Loss = 0.443660, Test Loss = 0.241183, Learning Rate = 2.490242e-04\n",
      "Epoch 3660/20000: Train Loss = 0.444953, Test Loss = 0.249151, Learning Rate = 2.489296e-04\n",
      "Epoch 3661/20000: Train Loss = 0.444044, Test Loss = 0.242411, Learning Rate = 2.488350e-04\n",
      "Epoch 3662/20000: Train Loss = 0.445323, Test Loss = 0.242039, Learning Rate = 2.487405e-04\n",
      "Epoch 3663/20000: Train Loss = 0.443646, Test Loss = 0.242859, Learning Rate = 2.486460e-04\n",
      "Epoch 3664/20000: Train Loss = 0.443708, Test Loss = 0.254708, Learning Rate = 2.485515e-04\n",
      "Epoch 3665/20000: Train Loss = 0.444114, Test Loss = 0.244255, Learning Rate = 2.484570e-04\n",
      "Epoch 3666/20000: Train Loss = 0.443610, Test Loss = 0.251499, Learning Rate = 2.483626e-04\n",
      "Epoch 3667/20000: Train Loss = 0.443973, Test Loss = 0.244448, Learning Rate = 2.482683e-04\n",
      "Epoch 3668/20000: Train Loss = 0.443431, Test Loss = 0.247192, Learning Rate = 2.481739e-04\n",
      "Epoch 3669/20000: Train Loss = 0.443380, Test Loss = 0.249033, Learning Rate = 2.480796e-04\n",
      "Epoch 3670/20000: Train Loss = 0.446866, Test Loss = 0.245287, Learning Rate = 2.479854e-04\n",
      "Epoch 3671/20000: Train Loss = 0.443920, Test Loss = 0.255279, Learning Rate = 2.478911e-04\n",
      "Epoch 3672/20000: Train Loss = 0.443277, Test Loss = 0.243448, Learning Rate = 2.477969e-04\n",
      "Epoch 3673/20000: Train Loss = 0.443213, Test Loss = 0.245927, Learning Rate = 2.477028e-04\n",
      "Epoch 3674/20000: Train Loss = 0.445249, Test Loss = 0.243029, Learning Rate = 2.476087e-04\n",
      "Epoch 3675/20000: Train Loss = 0.443218, Test Loss = 0.247124, Learning Rate = 2.475146e-04\n",
      "Epoch 3676/20000: Train Loss = 0.443244, Test Loss = 0.245163, Learning Rate = 2.474205e-04\n",
      "Epoch 3677/20000: Train Loss = 0.443565, Test Loss = 0.243606, Learning Rate = 2.473265e-04\n",
      "Epoch 3678/20000: Train Loss = 0.443973, Test Loss = 0.248450, Learning Rate = 2.472325e-04\n",
      "Epoch 3679/20000: Train Loss = 0.444435, Test Loss = 0.242217, Learning Rate = 2.471386e-04\n",
      "Epoch 3680/20000: Train Loss = 0.444934, Test Loss = 0.253113, Learning Rate = 2.470447e-04\n",
      "Epoch 3681/20000: Train Loss = 0.445384, Test Loss = 0.241140, Learning Rate = 2.469508e-04\n",
      "Epoch 3682/20000: Train Loss = 0.443284, Test Loss = 0.251424, Learning Rate = 2.468570e-04\n",
      "Epoch 3683/20000: Train Loss = 0.443647, Test Loss = 0.247115, Learning Rate = 2.467632e-04\n",
      "Epoch 3684/20000: Train Loss = 0.444372, Test Loss = 0.241476, Learning Rate = 2.466694e-04\n",
      "Epoch 3685/20000: Train Loss = 0.443715, Test Loss = 0.254007, Learning Rate = 2.465757e-04\n",
      "Epoch 3686/20000: Train Loss = 0.444487, Test Loss = 0.241892, Learning Rate = 2.464820e-04\n",
      "Epoch 3687/20000: Train Loss = 0.443162, Test Loss = 0.240529, Learning Rate = 2.463884e-04\n",
      "Epoch 3688/20000: Train Loss = 0.445007, Test Loss = 0.244623, Learning Rate = 2.462947e-04\n",
      "Epoch 3689/20000: Train Loss = 0.444389, Test Loss = 0.248102, Learning Rate = 2.462011e-04\n",
      "Epoch 3690/20000: Train Loss = 0.443986, Test Loss = 0.237235, Learning Rate = 2.461076e-04\n",
      "Epoch 3691/20000: Train Loss = 0.443718, Test Loss = 0.244446, Learning Rate = 2.460141e-04\n",
      "Epoch 3692/20000: Train Loss = 0.443455, Test Loss = 0.249255, Learning Rate = 2.459206e-04\n",
      "Epoch 3693/20000: Train Loss = 0.443554, Test Loss = 0.253362, Learning Rate = 2.458272e-04\n",
      "Epoch 3694/20000: Train Loss = 0.444416, Test Loss = 0.239382, Learning Rate = 2.457338e-04\n",
      "Epoch 3695/20000: Train Loss = 0.442879, Test Loss = 0.252898, Learning Rate = 2.456404e-04\n",
      "Epoch 3696/20000: Train Loss = 0.443255, Test Loss = 0.242733, Learning Rate = 2.455470e-04\n",
      "Epoch 3697/20000: Train Loss = 0.444187, Test Loss = 0.244750, Learning Rate = 2.454537e-04\n",
      "Epoch 3698/20000: Train Loss = 0.445211, Test Loss = 0.235786, Learning Rate = 2.453605e-04\n",
      "Epoch 3699/20000: Train Loss = 0.445035, Test Loss = 0.244989, Learning Rate = 2.452672e-04\n",
      "Epoch 3700/20000: Train Loss = 0.444511, Test Loss = 0.236710, Learning Rate = 2.451741e-04\n",
      "Epoch 3701/20000: Train Loss = 0.443201, Test Loss = 0.247737, Learning Rate = 2.450809e-04\n",
      "Epoch 3702/20000: Train Loss = 0.443328, Test Loss = 0.245643, Learning Rate = 2.449878e-04\n",
      "Epoch 3703/20000: Train Loss = 0.444030, Test Loss = 0.238019, Learning Rate = 2.448947e-04\n",
      "Epoch 3704/20000: Train Loss = 0.443267, Test Loss = 0.251611, Learning Rate = 2.448016e-04\n",
      "Epoch 3705/20000: Train Loss = 0.443412, Test Loss = 0.238960, Learning Rate = 2.447086e-04\n",
      "Epoch 3706/20000: Train Loss = 0.444179, Test Loss = 0.243384, Learning Rate = 2.446156e-04\n",
      "Epoch 3707/20000: Train Loss = 0.443042, Test Loss = 0.241894, Learning Rate = 2.445227e-04\n",
      "Epoch 3708/20000: Train Loss = 0.444105, Test Loss = 0.242729, Learning Rate = 2.444298e-04\n",
      "Epoch 3709/20000: Train Loss = 0.443157, Test Loss = 0.245659, Learning Rate = 2.443369e-04\n",
      "Epoch 3710/20000: Train Loss = 0.443132, Test Loss = 0.243693, Learning Rate = 2.442440e-04\n",
      "Epoch 3711/20000: Train Loss = 0.443364, Test Loss = 0.246802, Learning Rate = 2.441512e-04\n",
      "Epoch 3712/20000: Train Loss = 0.443700, Test Loss = 0.251919, Learning Rate = 2.440585e-04\n",
      "Epoch 3713/20000: Train Loss = 0.443005, Test Loss = 0.243061, Learning Rate = 2.439657e-04\n",
      "Epoch 3714/20000: Train Loss = 0.443965, Test Loss = 0.248361, Learning Rate = 2.438730e-04\n",
      "Epoch 3715/20000: Train Loss = 0.443427, Test Loss = 0.254367, Learning Rate = 2.437804e-04\n",
      "Epoch 3716/20000: Train Loss = 0.443669, Test Loss = 0.247050, Learning Rate = 2.436877e-04\n",
      "Epoch 3717/20000: Train Loss = 0.443075, Test Loss = 0.247804, Learning Rate = 2.435951e-04\n",
      "Epoch 3718/20000: Train Loss = 0.444569, Test Loss = 0.241209, Learning Rate = 2.435026e-04\n",
      "Epoch 3719/20000: Train Loss = 0.444390, Test Loss = 0.243980, Learning Rate = 2.434101e-04\n",
      "Epoch 3720/20000: Train Loss = 0.444246, Test Loss = 0.251831, Learning Rate = 2.433176e-04\n",
      "Epoch 3721/20000: Train Loss = 0.444185, Test Loss = 0.255457, Learning Rate = 2.432251e-04\n",
      "Epoch 3722/20000: Train Loss = 0.444124, Test Loss = 0.248185, Learning Rate = 2.431327e-04\n",
      "Epoch 3723/20000: Train Loss = 0.443765, Test Loss = 0.235522, Learning Rate = 2.430403e-04\n",
      "Epoch 3724/20000: Train Loss = 0.443487, Test Loss = 0.250910, Learning Rate = 2.429480e-04\n",
      "Epoch 3725/20000: Train Loss = 0.443575, Test Loss = 0.247473, Learning Rate = 2.428557e-04\n",
      "Epoch 3726/20000: Train Loss = 0.443915, Test Loss = 0.254540, Learning Rate = 2.427634e-04\n",
      "Epoch 3727/20000: Train Loss = 0.443401, Test Loss = 0.244475, Learning Rate = 2.426711e-04\n",
      "Epoch 3728/20000: Train Loss = 0.444273, Test Loss = 0.246770, Learning Rate = 2.425789e-04\n",
      "Epoch 3729/20000: Train Loss = 0.443355, Test Loss = 0.243396, Learning Rate = 2.424867e-04\n",
      "Epoch 3730/20000: Train Loss = 0.443439, Test Loss = 0.257919, Learning Rate = 2.423946e-04\n",
      "Epoch 3731/20000: Train Loss = 0.444459, Test Loss = 0.247113, Learning Rate = 2.423025e-04\n",
      "Epoch 3732/20000: Train Loss = 0.442951, Test Loss = 0.249696, Learning Rate = 2.422104e-04\n",
      "Epoch 3733/20000: Train Loss = 0.443945, Test Loss = 0.240545, Learning Rate = 2.421184e-04\n",
      "Epoch 3734/20000: Train Loss = 0.444344, Test Loss = 0.243139, Learning Rate = 2.420264e-04\n",
      "Epoch 3735/20000: Train Loss = 0.445450, Test Loss = 0.248332, Learning Rate = 2.419344e-04\n",
      "Epoch 3736/20000: Train Loss = 0.444762, Test Loss = 0.241724, Learning Rate = 2.418425e-04\n",
      "Epoch 3737/20000: Train Loss = 0.444599, Test Loss = 0.247154, Learning Rate = 2.417506e-04\n",
      "Epoch 3738/20000: Train Loss = 0.445015, Test Loss = 0.244264, Learning Rate = 2.416588e-04\n",
      "Epoch 3739/20000: Train Loss = 0.446205, Test Loss = 0.252958, Learning Rate = 2.415669e-04\n",
      "Epoch 3740/20000: Train Loss = 0.444723, Test Loss = 0.234981, Learning Rate = 2.414751e-04\n",
      "Epoch 3741/20000: Train Loss = 0.444646, Test Loss = 0.244925, Learning Rate = 2.413834e-04\n",
      "Epoch 3742/20000: Train Loss = 0.444090, Test Loss = 0.242362, Learning Rate = 2.412917e-04\n",
      "Epoch 3743/20000: Train Loss = 0.443490, Test Loss = 0.258577, Learning Rate = 2.412000e-04\n",
      "Epoch 3744/20000: Train Loss = 0.443338, Test Loss = 0.238213, Learning Rate = 2.411083e-04\n",
      "Epoch 3745/20000: Train Loss = 0.443808, Test Loss = 0.249432, Learning Rate = 2.410167e-04\n",
      "Epoch 3746/20000: Train Loss = 0.443592, Test Loss = 0.246754, Learning Rate = 2.409251e-04\n",
      "Epoch 3747/20000: Train Loss = 0.443012, Test Loss = 0.252415, Learning Rate = 2.408336e-04\n",
      "Epoch 3748/20000: Train Loss = 0.443282, Test Loss = 0.244497, Learning Rate = 2.407421e-04\n",
      "Epoch 3749/20000: Train Loss = 0.443186, Test Loss = 0.239204, Learning Rate = 2.406506e-04\n",
      "Epoch 3750/20000: Train Loss = 0.443627, Test Loss = 0.245193, Learning Rate = 2.405592e-04\n",
      "Epoch 3751/20000: Train Loss = 0.443268, Test Loss = 0.240647, Learning Rate = 2.404678e-04\n",
      "Epoch 3752/20000: Train Loss = 0.443257, Test Loss = 0.242761, Learning Rate = 2.403764e-04\n",
      "Epoch 3753/20000: Train Loss = 0.443772, Test Loss = 0.244872, Learning Rate = 2.402851e-04\n",
      "Epoch 3754/20000: Train Loss = 0.443962, Test Loss = 0.244110, Learning Rate = 2.401938e-04\n",
      "Epoch 3755/20000: Train Loss = 0.444919, Test Loss = 0.249787, Learning Rate = 2.401025e-04\n",
      "Epoch 3756/20000: Train Loss = 0.444118, Test Loss = 0.243369, Learning Rate = 2.400113e-04\n",
      "Epoch 3757/20000: Train Loss = 0.443015, Test Loss = 0.243873, Learning Rate = 2.399201e-04\n",
      "Epoch 3758/20000: Train Loss = 0.443210, Test Loss = 0.243848, Learning Rate = 2.398289e-04\n",
      "Epoch 3759/20000: Train Loss = 0.443498, Test Loss = 0.244865, Learning Rate = 2.397378e-04\n",
      "Epoch 3760/20000: Train Loss = 0.443207, Test Loss = 0.242651, Learning Rate = 2.396467e-04\n",
      "Epoch 3761/20000: Train Loss = 0.443693, Test Loss = 0.244735, Learning Rate = 2.395556e-04\n",
      "Epoch 3762/20000: Train Loss = 0.444216, Test Loss = 0.245990, Learning Rate = 2.394646e-04\n",
      "Epoch 3763/20000: Train Loss = 0.443173, Test Loss = 0.246567, Learning Rate = 2.393736e-04\n",
      "Epoch 3764/20000: Train Loss = 0.443376, Test Loss = 0.244206, Learning Rate = 2.392826e-04\n",
      "Epoch 3765/20000: Train Loss = 0.445284, Test Loss = 0.259899, Learning Rate = 2.391917e-04\n",
      "Epoch 3766/20000: Train Loss = 0.445266, Test Loss = 0.242957, Learning Rate = 2.391008e-04\n",
      "Epoch 3767/20000: Train Loss = 0.443725, Test Loss = 0.256874, Learning Rate = 2.390100e-04\n",
      "Epoch 3768/20000: Train Loss = 0.444345, Test Loss = 0.256704, Learning Rate = 2.389192e-04\n",
      "Epoch 3769/20000: Train Loss = 0.443141, Test Loss = 0.242295, Learning Rate = 2.388284e-04\n",
      "Epoch 3770/20000: Train Loss = 0.443276, Test Loss = 0.252598, Learning Rate = 2.387376e-04\n",
      "Epoch 3771/20000: Train Loss = 0.443340, Test Loss = 0.245505, Learning Rate = 2.386469e-04\n",
      "Epoch 3772/20000: Train Loss = 0.443936, Test Loss = 0.245896, Learning Rate = 2.385562e-04\n",
      "Epoch 3773/20000: Train Loss = 0.443278, Test Loss = 0.244355, Learning Rate = 2.384656e-04\n",
      "Epoch 3774/20000: Train Loss = 0.443723, Test Loss = 0.254354, Learning Rate = 2.383750e-04\n",
      "Epoch 3775/20000: Train Loss = 0.444190, Test Loss = 0.254233, Learning Rate = 2.382844e-04\n",
      "Epoch 3776/20000: Train Loss = 0.443411, Test Loss = 0.251055, Learning Rate = 2.381939e-04\n",
      "Epoch 3777/20000: Train Loss = 0.443525, Test Loss = 0.237367, Learning Rate = 2.381034e-04\n",
      "Epoch 3778/20000: Train Loss = 0.443847, Test Loss = 0.247127, Learning Rate = 2.380129e-04\n",
      "Epoch 3779/20000: Train Loss = 0.443100, Test Loss = 0.244505, Learning Rate = 2.379225e-04\n",
      "Epoch 3780/20000: Train Loss = 0.443370, Test Loss = 0.250729, Learning Rate = 2.378321e-04\n",
      "Epoch 3781/20000: Train Loss = 0.443102, Test Loss = 0.241058, Learning Rate = 2.377417e-04\n",
      "Epoch 3782/20000: Train Loss = 0.444977, Test Loss = 0.248349, Learning Rate = 2.376513e-04\n",
      "Epoch 3783/20000: Train Loss = 0.445273, Test Loss = 0.242450, Learning Rate = 2.375610e-04\n",
      "Epoch 3784/20000: Train Loss = 0.445097, Test Loss = 0.244943, Learning Rate = 2.374708e-04\n",
      "Epoch 3785/20000: Train Loss = 0.443375, Test Loss = 0.247378, Learning Rate = 2.373805e-04\n",
      "Epoch 3786/20000: Train Loss = 0.443843, Test Loss = 0.240892, Learning Rate = 2.372903e-04\n",
      "Epoch 3787/20000: Train Loss = 0.445106, Test Loss = 0.246571, Learning Rate = 2.372002e-04\n",
      "Epoch 3788/20000: Train Loss = 0.443807, Test Loss = 0.247612, Learning Rate = 2.371101e-04\n",
      "Epoch 3789/20000: Train Loss = 0.445167, Test Loss = 0.255774, Learning Rate = 2.370200e-04\n",
      "Epoch 3790/20000: Train Loss = 0.443591, Test Loss = 0.238127, Learning Rate = 2.369299e-04\n",
      "Epoch 3791/20000: Train Loss = 0.443853, Test Loss = 0.246027, Learning Rate = 2.368399e-04\n",
      "Epoch 3792/20000: Train Loss = 0.444251, Test Loss = 0.241934, Learning Rate = 2.367499e-04\n",
      "Epoch 3793/20000: Train Loss = 0.444290, Test Loss = 0.254010, Learning Rate = 2.366599e-04\n",
      "Epoch 3794/20000: Train Loss = 0.444072, Test Loss = 0.239300, Learning Rate = 2.365700e-04\n",
      "Epoch 3795/20000: Train Loss = 0.443635, Test Loss = 0.238976, Learning Rate = 2.364801e-04\n",
      "Epoch 3796/20000: Train Loss = 0.443987, Test Loss = 0.250304, Learning Rate = 2.363902e-04\n",
      "Epoch 3797/20000: Train Loss = 0.443223, Test Loss = 0.248389, Learning Rate = 2.363004e-04\n",
      "Epoch 3798/20000: Train Loss = 0.445141, Test Loss = 0.249858, Learning Rate = 2.362106e-04\n",
      "Epoch 3799/20000: Train Loss = 0.444183, Test Loss = 0.250624, Learning Rate = 2.361209e-04\n",
      "Epoch 3800/20000: Train Loss = 0.443474, Test Loss = 0.245319, Learning Rate = 2.360312e-04\n",
      "Epoch 3801/20000: Train Loss = 0.443668, Test Loss = 0.249378, Learning Rate = 2.359415e-04\n",
      "Epoch 3802/20000: Train Loss = 0.445005, Test Loss = 0.249990, Learning Rate = 2.358518e-04\n",
      "Epoch 3803/20000: Train Loss = 0.443397, Test Loss = 0.238204, Learning Rate = 2.357622e-04\n",
      "Epoch 3804/20000: Train Loss = 0.443453, Test Loss = 0.255264, Learning Rate = 2.356726e-04\n",
      "Epoch 3805/20000: Train Loss = 0.443455, Test Loss = 0.244405, Learning Rate = 2.355831e-04\n",
      "Epoch 3806/20000: Train Loss = 0.445098, Test Loss = 0.243950, Learning Rate = 2.354936e-04\n",
      "Epoch 3807/20000: Train Loss = 0.444289, Test Loss = 0.260425, Learning Rate = 2.354041e-04\n",
      "Epoch 3808/20000: Train Loss = 0.444508, Test Loss = 0.252961, Learning Rate = 2.353146e-04\n",
      "Epoch 3809/20000: Train Loss = 0.443207, Test Loss = 0.246295, Learning Rate = 2.352252e-04\n",
      "Epoch 3810/20000: Train Loss = 0.443403, Test Loss = 0.244108, Learning Rate = 2.351358e-04\n",
      "Epoch 3811/20000: Train Loss = 0.443728, Test Loss = 0.241744, Learning Rate = 2.350465e-04\n",
      "Epoch 3812/20000: Train Loss = 0.443606, Test Loss = 0.253109, Learning Rate = 2.349572e-04\n",
      "Epoch 3813/20000: Train Loss = 0.444077, Test Loss = 0.252522, Learning Rate = 2.348679e-04\n",
      "Epoch 3814/20000: Train Loss = 0.443957, Test Loss = 0.245458, Learning Rate = 2.347787e-04\n",
      "Epoch 3815/20000: Train Loss = 0.443299, Test Loss = 0.250684, Learning Rate = 2.346895e-04\n",
      "Epoch 3816/20000: Train Loss = 0.443434, Test Loss = 0.249733, Learning Rate = 2.346003e-04\n",
      "Epoch 3817/20000: Train Loss = 0.444778, Test Loss = 0.252401, Learning Rate = 2.345111e-04\n",
      "Epoch 3818/20000: Train Loss = 0.443853, Test Loss = 0.255671, Learning Rate = 2.344220e-04\n",
      "Epoch 3819/20000: Train Loss = 0.443119, Test Loss = 0.246584, Learning Rate = 2.343330e-04\n",
      "Epoch 3820/20000: Train Loss = 0.443621, Test Loss = 0.254072, Learning Rate = 2.342439e-04\n",
      "Epoch 3821/20000: Train Loss = 0.443887, Test Loss = 0.248435, Learning Rate = 2.341549e-04\n",
      "Epoch 3822/20000: Train Loss = 0.444806, Test Loss = 0.252613, Learning Rate = 2.340659e-04\n",
      "Epoch 3823/20000: Train Loss = 0.444531, Test Loss = 0.249259, Learning Rate = 2.339770e-04\n",
      "Epoch 3824/20000: Train Loss = 0.443816, Test Loss = 0.251711, Learning Rate = 2.338881e-04\n",
      "Epoch 3825/20000: Train Loss = 0.442824, Test Loss = 0.239517, Learning Rate = 2.337992e-04\n",
      "Epoch 3826/20000: Train Loss = 0.443113, Test Loss = 0.244359, Learning Rate = 2.337104e-04\n",
      "Epoch 3827/20000: Train Loss = 0.443149, Test Loss = 0.250463, Learning Rate = 2.336216e-04\n",
      "Epoch 3828/20000: Train Loss = 0.443786, Test Loss = 0.238061, Learning Rate = 2.335328e-04\n",
      "Epoch 3829/20000: Train Loss = 0.444021, Test Loss = 0.247530, Learning Rate = 2.334441e-04\n",
      "Epoch 3830/20000: Train Loss = 0.443406, Test Loss = 0.243913, Learning Rate = 2.333554e-04\n",
      "Epoch 3831/20000: Train Loss = 0.443344, Test Loss = 0.252285, Learning Rate = 2.332667e-04\n",
      "Epoch 3832/20000: Train Loss = 0.443693, Test Loss = 0.250794, Learning Rate = 2.331781e-04\n",
      "Epoch 3833/20000: Train Loss = 0.443950, Test Loss = 0.238701, Learning Rate = 2.330895e-04\n",
      "Epoch 3834/20000: Train Loss = 0.445407, Test Loss = 0.251456, Learning Rate = 2.330009e-04\n",
      "Epoch 3835/20000: Train Loss = 0.443783, Test Loss = 0.251189, Learning Rate = 2.329124e-04\n",
      "Epoch 3836/20000: Train Loss = 0.443566, Test Loss = 0.243178, Learning Rate = 2.328239e-04\n",
      "Epoch 3837/20000: Train Loss = 0.444214, Test Loss = 0.238256, Learning Rate = 2.327354e-04\n",
      "Epoch 3838/20000: Train Loss = 0.443914, Test Loss = 0.241552, Learning Rate = 2.326470e-04\n",
      "Epoch 3839/20000: Train Loss = 0.443816, Test Loss = 0.245341, Learning Rate = 2.325586e-04\n",
      "Epoch 3840/20000: Train Loss = 0.443935, Test Loss = 0.245695, Learning Rate = 2.324702e-04\n",
      "Epoch 3841/20000: Train Loss = 0.443716, Test Loss = 0.234732, Learning Rate = 2.323819e-04\n",
      "Epoch 3842/20000: Train Loss = 0.443627, Test Loss = 0.243602, Learning Rate = 2.322936e-04\n",
      "Epoch 3843/20000: Train Loss = 0.444184, Test Loss = 0.247711, Learning Rate = 2.322053e-04\n",
      "Epoch 3844/20000: Train Loss = 0.443636, Test Loss = 0.246091, Learning Rate = 2.321171e-04\n",
      "Epoch 3845/20000: Train Loss = 0.443305, Test Loss = 0.237824, Learning Rate = 2.320289e-04\n",
      "Epoch 3846/20000: Train Loss = 0.444161, Test Loss = 0.238939, Learning Rate = 2.319407e-04\n",
      "Epoch 3847/20000: Train Loss = 0.443925, Test Loss = 0.244953, Learning Rate = 2.318526e-04\n",
      "Epoch 3848/20000: Train Loss = 0.444462, Test Loss = 0.253560, Learning Rate = 2.317645e-04\n",
      "Epoch 3849/20000: Train Loss = 0.443735, Test Loss = 0.253703, Learning Rate = 2.316764e-04\n",
      "Epoch 3850/20000: Train Loss = 0.443473, Test Loss = 0.249745, Learning Rate = 2.315884e-04\n",
      "Epoch 3851/20000: Train Loss = 0.444141, Test Loss = 0.234936, Learning Rate = 2.315004e-04\n",
      "Epoch 3852/20000: Train Loss = 0.445046, Test Loss = 0.254919, Learning Rate = 2.314124e-04\n",
      "Epoch 3853/20000: Train Loss = 0.444133, Test Loss = 0.246559, Learning Rate = 2.313245e-04\n",
      "Epoch 3854/20000: Train Loss = 0.443170, Test Loss = 0.244286, Learning Rate = 2.312366e-04\n",
      "Epoch 3855/20000: Train Loss = 0.443136, Test Loss = 0.252504, Learning Rate = 2.311487e-04\n",
      "Epoch 3856/20000: Train Loss = 0.444096, Test Loss = 0.240718, Learning Rate = 2.310609e-04\n",
      "Epoch 3857/20000: Train Loss = 0.442843, Test Loss = 0.251479, Learning Rate = 2.309731e-04\n",
      "Epoch 3858/20000: Train Loss = 0.444500, Test Loss = 0.246054, Learning Rate = 2.308853e-04\n",
      "Epoch 3859/20000: Train Loss = 0.444008, Test Loss = 0.248818, Learning Rate = 2.307976e-04\n",
      "Epoch 3860/20000: Train Loss = 0.443741, Test Loss = 0.248350, Learning Rate = 2.307099e-04\n",
      "Epoch 3861/20000: Train Loss = 0.443096, Test Loss = 0.248278, Learning Rate = 2.306223e-04\n",
      "Epoch 3862/20000: Train Loss = 0.443359, Test Loss = 0.250822, Learning Rate = 2.305346e-04\n",
      "Epoch 3863/20000: Train Loss = 0.443724, Test Loss = 0.236947, Learning Rate = 2.304470e-04\n",
      "Epoch 3864/20000: Train Loss = 0.443387, Test Loss = 0.245750, Learning Rate = 2.303595e-04\n",
      "Epoch 3865/20000: Train Loss = 0.444671, Test Loss = 0.254564, Learning Rate = 2.302719e-04\n",
      "Epoch 3866/20000: Train Loss = 0.444652, Test Loss = 0.254045, Learning Rate = 2.301844e-04\n",
      "Epoch 3867/20000: Train Loss = 0.445594, Test Loss = 0.261526, Learning Rate = 2.300970e-04\n",
      "Epoch 3868/20000: Train Loss = 0.443581, Test Loss = 0.249822, Learning Rate = 2.300095e-04\n",
      "Epoch 3869/20000: Train Loss = 0.443318, Test Loss = 0.250315, Learning Rate = 2.299221e-04\n",
      "Epoch 3870/20000: Train Loss = 0.443654, Test Loss = 0.242174, Learning Rate = 2.298348e-04\n",
      "Epoch 3871/20000: Train Loss = 0.443692, Test Loss = 0.253773, Learning Rate = 2.297474e-04\n",
      "Epoch 3872/20000: Train Loss = 0.443970, Test Loss = 0.256055, Learning Rate = 2.296602e-04\n",
      "Epoch 3873/20000: Train Loss = 0.443086, Test Loss = 0.239570, Learning Rate = 2.295729e-04\n",
      "Epoch 3874/20000: Train Loss = 0.443807, Test Loss = 0.243465, Learning Rate = 2.294857e-04\n",
      "Epoch 3875/20000: Train Loss = 0.444057, Test Loss = 0.237765, Learning Rate = 2.293985e-04\n",
      "Epoch 3876/20000: Train Loss = 0.443779, Test Loss = 0.244367, Learning Rate = 2.293113e-04\n",
      "Epoch 3877/20000: Train Loss = 0.443396, Test Loss = 0.251905, Learning Rate = 2.292242e-04\n",
      "Epoch 3878/20000: Train Loss = 0.443753, Test Loss = 0.244697, Learning Rate = 2.291371e-04\n",
      "Epoch 3879/20000: Train Loss = 0.443839, Test Loss = 0.243498, Learning Rate = 2.290500e-04\n",
      "Epoch 3880/20000: Train Loss = 0.443977, Test Loss = 0.253726, Learning Rate = 2.289630e-04\n",
      "Epoch 3881/20000: Train Loss = 0.444312, Test Loss = 0.253273, Learning Rate = 2.288760e-04\n",
      "Epoch 3882/20000: Train Loss = 0.445983, Test Loss = 0.249569, Learning Rate = 2.287890e-04\n",
      "Epoch 3883/20000: Train Loss = 0.444083, Test Loss = 0.246476, Learning Rate = 2.287021e-04\n",
      "Epoch 3884/20000: Train Loss = 0.444986, Test Loss = 0.258109, Learning Rate = 2.286152e-04\n",
      "Epoch 3885/20000: Train Loss = 0.443119, Test Loss = 0.244560, Learning Rate = 2.285283e-04\n",
      "Epoch 3886/20000: Train Loss = 0.444263, Test Loss = 0.242033, Learning Rate = 2.284415e-04\n",
      "Epoch 3887/20000: Train Loss = 0.443636, Test Loss = 0.249929, Learning Rate = 2.283547e-04\n",
      "Epoch 3888/20000: Train Loss = 0.443402, Test Loss = 0.247789, Learning Rate = 2.282679e-04\n",
      "Epoch 3889/20000: Train Loss = 0.443544, Test Loss = 0.233933, Learning Rate = 2.281812e-04\n",
      "Epoch 3890/20000: Train Loss = 0.443926, Test Loss = 0.248038, Learning Rate = 2.280944e-04\n",
      "Epoch 3891/20000: Train Loss = 0.442709, Test Loss = 0.237003, Learning Rate = 2.280078e-04\n",
      "Epoch 3892/20000: Train Loss = 0.443841, Test Loss = 0.246999, Learning Rate = 2.279211e-04\n",
      "Epoch 3893/20000: Train Loss = 0.444717, Test Loss = 0.243326, Learning Rate = 2.278345e-04\n",
      "Epoch 3894/20000: Train Loss = 0.444790, Test Loss = 0.241143, Learning Rate = 2.277480e-04\n",
      "Epoch 3895/20000: Train Loss = 0.443619, Test Loss = 0.248433, Learning Rate = 2.276614e-04\n",
      "Epoch 3896/20000: Train Loss = 0.443343, Test Loss = 0.239910, Learning Rate = 2.275749e-04\n",
      "Epoch 3897/20000: Train Loss = 0.443502, Test Loss = 0.247056, Learning Rate = 2.274885e-04\n",
      "Epoch 3898/20000: Train Loss = 0.443641, Test Loss = 0.248194, Learning Rate = 2.274020e-04\n",
      "Epoch 3899/20000: Train Loss = 0.443708, Test Loss = 0.247032, Learning Rate = 2.273156e-04\n",
      "Epoch 3900/20000: Train Loss = 0.443390, Test Loss = 0.240989, Learning Rate = 2.272292e-04\n",
      "Epoch 3901/20000: Train Loss = 0.443780, Test Loss = 0.245465, Learning Rate = 2.271429e-04\n",
      "Epoch 3902/20000: Train Loss = 0.443604, Test Loss = 0.246664, Learning Rate = 2.270566e-04\n",
      "Epoch 3903/20000: Train Loss = 0.443651, Test Loss = 0.252961, Learning Rate = 2.269703e-04\n",
      "Epoch 3904/20000: Train Loss = 0.444598, Test Loss = 0.252378, Learning Rate = 2.268841e-04\n",
      "Epoch 3905/20000: Train Loss = 0.443684, Test Loss = 0.252027, Learning Rate = 2.267979e-04\n",
      "Epoch 3906/20000: Train Loss = 0.443344, Test Loss = 0.249951, Learning Rate = 2.267117e-04\n",
      "Epoch 3907/20000: Train Loss = 0.444486, Test Loss = 0.256156, Learning Rate = 2.266255e-04\n",
      "Epoch 3908/20000: Train Loss = 0.443670, Test Loss = 0.249799, Learning Rate = 2.265394e-04\n",
      "Epoch 3909/20000: Train Loss = 0.443811, Test Loss = 0.248825, Learning Rate = 2.264533e-04\n",
      "Epoch 3910/20000: Train Loss = 0.443604, Test Loss = 0.251845, Learning Rate = 2.263673e-04\n",
      "Epoch 3911/20000: Train Loss = 0.443625, Test Loss = 0.244346, Learning Rate = 2.262813e-04\n",
      "Epoch 3912/20000: Train Loss = 0.443653, Test Loss = 0.253764, Learning Rate = 2.261953e-04\n",
      "Epoch 3913/20000: Train Loss = 0.443851, Test Loss = 0.243834, Learning Rate = 2.261094e-04\n",
      "Epoch 3914/20000: Train Loss = 0.443307, Test Loss = 0.245778, Learning Rate = 2.260234e-04\n",
      "Epoch 3915/20000: Train Loss = 0.443338, Test Loss = 0.245298, Learning Rate = 2.259376e-04\n",
      "Epoch 3916/20000: Train Loss = 0.443733, Test Loss = 0.251360, Learning Rate = 2.258517e-04\n",
      "Epoch 3917/20000: Train Loss = 0.443528, Test Loss = 0.247881, Learning Rate = 2.257659e-04\n",
      "Epoch 3918/20000: Train Loss = 0.443671, Test Loss = 0.247533, Learning Rate = 2.256801e-04\n",
      "Epoch 3919/20000: Train Loss = 0.443443, Test Loss = 0.244996, Learning Rate = 2.255944e-04\n",
      "Epoch 3920/20000: Train Loss = 0.445209, Test Loss = 0.238382, Learning Rate = 2.255086e-04\n",
      "Epoch 3921/20000: Train Loss = 0.444096, Test Loss = 0.252939, Learning Rate = 2.254229e-04\n",
      "Epoch 3922/20000: Train Loss = 0.445108, Test Loss = 0.253817, Learning Rate = 2.253373e-04\n",
      "Epoch 3923/20000: Train Loss = 0.442856, Test Loss = 0.238745, Learning Rate = 2.252517e-04\n",
      "Epoch 3924/20000: Train Loss = 0.444796, Test Loss = 0.248999, Learning Rate = 2.251661e-04\n",
      "Epoch 3925/20000: Train Loss = 0.444349, Test Loss = 0.254255, Learning Rate = 2.250805e-04\n",
      "Epoch 3926/20000: Train Loss = 0.443496, Test Loss = 0.246638, Learning Rate = 2.249950e-04\n",
      "Epoch 3927/20000: Train Loss = 0.443455, Test Loss = 0.242283, Learning Rate = 2.249095e-04\n",
      "Epoch 3928/20000: Train Loss = 0.443498, Test Loss = 0.246705, Learning Rate = 2.248240e-04\n",
      "Epoch 3929/20000: Train Loss = 0.443282, Test Loss = 0.239594, Learning Rate = 2.247386e-04\n",
      "Epoch 3930/20000: Train Loss = 0.443571, Test Loss = 0.241487, Learning Rate = 2.246532e-04\n",
      "Epoch 3931/20000: Train Loss = 0.443657, Test Loss = 0.249810, Learning Rate = 2.245679e-04\n",
      "Epoch 3932/20000: Train Loss = 0.444988, Test Loss = 0.246049, Learning Rate = 2.244825e-04\n",
      "Epoch 3933/20000: Train Loss = 0.444740, Test Loss = 0.243256, Learning Rate = 2.243972e-04\n",
      "Epoch 3934/20000: Train Loss = 0.444735, Test Loss = 0.250412, Learning Rate = 2.243120e-04\n",
      "Epoch 3935/20000: Train Loss = 0.443992, Test Loss = 0.244017, Learning Rate = 2.242267e-04\n",
      "Epoch 3936/20000: Train Loss = 0.444367, Test Loss = 0.250738, Learning Rate = 2.241415e-04\n",
      "Epoch 3937/20000: Train Loss = 0.444136, Test Loss = 0.248994, Learning Rate = 2.240564e-04\n",
      "Epoch 3938/20000: Train Loss = 0.443624, Test Loss = 0.244624, Learning Rate = 2.239712e-04\n",
      "Epoch 3939/20000: Train Loss = 0.444066, Test Loss = 0.243089, Learning Rate = 2.238861e-04\n",
      "Epoch 3940/20000: Train Loss = 0.443886, Test Loss = 0.254606, Learning Rate = 2.238011e-04\n",
      "Epoch 3941/20000: Train Loss = 0.443848, Test Loss = 0.247950, Learning Rate = 2.237160e-04\n",
      "Epoch 3942/20000: Train Loss = 0.443079, Test Loss = 0.246229, Learning Rate = 2.236310e-04\n",
      "Epoch 3943/20000: Train Loss = 0.443700, Test Loss = 0.240870, Learning Rate = 2.235460e-04\n",
      "Epoch 3944/20000: Train Loss = 0.444017, Test Loss = 0.252901, Learning Rate = 2.234611e-04\n",
      "Epoch 3945/20000: Train Loss = 0.443839, Test Loss = 0.248529, Learning Rate = 2.233762e-04\n",
      "Epoch 3946/20000: Train Loss = 0.444195, Test Loss = 0.250798, Learning Rate = 2.232913e-04\n",
      "Epoch 3947/20000: Train Loss = 0.443577, Test Loss = 0.243150, Learning Rate = 2.232065e-04\n",
      "Epoch 3948/20000: Train Loss = 0.443738, Test Loss = 0.241518, Learning Rate = 2.231217e-04\n",
      "Epoch 3949/20000: Train Loss = 0.443709, Test Loss = 0.245565, Learning Rate = 2.230369e-04\n",
      "Epoch 3950/20000: Train Loss = 0.444235, Test Loss = 0.255947, Learning Rate = 2.229521e-04\n",
      "Epoch 3951/20000: Train Loss = 0.443512, Test Loss = 0.248100, Learning Rate = 2.228674e-04\n",
      "Epoch 3952/20000: Train Loss = 0.443242, Test Loss = 0.243499, Learning Rate = 2.227827e-04\n",
      "Epoch 3953/20000: Train Loss = 0.443294, Test Loss = 0.242608, Learning Rate = 2.226981e-04\n",
      "Epoch 3954/20000: Train Loss = 0.444099, Test Loss = 0.251123, Learning Rate = 2.226135e-04\n",
      "Epoch 3955/20000: Train Loss = 0.443637, Test Loss = 0.250307, Learning Rate = 2.225289e-04\n",
      "Epoch 3956/20000: Train Loss = 0.443178, Test Loss = 0.246401, Learning Rate = 2.224443e-04\n",
      "Epoch 3957/20000: Train Loss = 0.443303, Test Loss = 0.251130, Learning Rate = 2.223598e-04\n",
      "Epoch 3958/20000: Train Loss = 0.443292, Test Loss = 0.242429, Learning Rate = 2.222753e-04\n",
      "Epoch 3959/20000: Train Loss = 0.442904, Test Loss = 0.248500, Learning Rate = 2.221908e-04\n",
      "Epoch 3960/20000: Train Loss = 0.443770, Test Loss = 0.245466, Learning Rate = 2.221064e-04\n",
      "Epoch 3961/20000: Train Loss = 0.443508, Test Loss = 0.242612, Learning Rate = 2.220220e-04\n",
      "Epoch 3962/20000: Train Loss = 0.443393, Test Loss = 0.247177, Learning Rate = 2.219377e-04\n",
      "Epoch 3963/20000: Train Loss = 0.443324, Test Loss = 0.249541, Learning Rate = 2.218533e-04\n",
      "Epoch 3964/20000: Train Loss = 0.443149, Test Loss = 0.240660, Learning Rate = 2.217690e-04\n",
      "Epoch 3965/20000: Train Loss = 0.444919, Test Loss = 0.253366, Learning Rate = 2.216848e-04\n",
      "Epoch 3966/20000: Train Loss = 0.443379, Test Loss = 0.250823, Learning Rate = 2.216005e-04\n",
      "Epoch 3967/20000: Train Loss = 0.445336, Test Loss = 0.239981, Learning Rate = 2.215163e-04\n",
      "Epoch 3968/20000: Train Loss = 0.443193, Test Loss = 0.249413, Learning Rate = 2.214322e-04\n",
      "Epoch 3969/20000: Train Loss = 0.443112, Test Loss = 0.245309, Learning Rate = 2.213480e-04\n",
      "Epoch 3970/20000: Train Loss = 0.443238, Test Loss = 0.249064, Learning Rate = 2.212639e-04\n",
      "Epoch 3971/20000: Train Loss = 0.444270, Test Loss = 0.244077, Learning Rate = 2.211798e-04\n",
      "Epoch 3972/20000: Train Loss = 0.444774, Test Loss = 0.244528, Learning Rate = 2.210958e-04\n",
      "Epoch 3973/20000: Train Loss = 0.443636, Test Loss = 0.252553, Learning Rate = 2.210118e-04\n",
      "Epoch 3974/20000: Train Loss = 0.443413, Test Loss = 0.256572, Learning Rate = 2.209278e-04\n",
      "Epoch 3975/20000: Train Loss = 0.443962, Test Loss = 0.247261, Learning Rate = 2.208439e-04\n",
      "Epoch 3976/20000: Train Loss = 0.443819, Test Loss = 0.244472, Learning Rate = 2.207600e-04\n",
      "Epoch 3977/20000: Train Loss = 0.444600, Test Loss = 0.247238, Learning Rate = 2.206761e-04\n",
      "Epoch 3978/20000: Train Loss = 0.443340, Test Loss = 0.251551, Learning Rate = 2.205922e-04\n",
      "Epoch 3979/20000: Train Loss = 0.444880, Test Loss = 0.247009, Learning Rate = 2.205084e-04\n",
      "Epoch 3980/20000: Train Loss = 0.444429, Test Loss = 0.247429, Learning Rate = 2.204246e-04\n",
      "Epoch 3981/20000: Train Loss = 0.443032, Test Loss = 0.240593, Learning Rate = 2.203409e-04\n",
      "Epoch 3982/20000: Train Loss = 0.443161, Test Loss = 0.244373, Learning Rate = 2.202571e-04\n",
      "Epoch 3983/20000: Train Loss = 0.443471, Test Loss = 0.248169, Learning Rate = 2.201734e-04\n",
      "Epoch 3984/20000: Train Loss = 0.443630, Test Loss = 0.248634, Learning Rate = 2.200898e-04\n",
      "Epoch 3985/20000: Train Loss = 0.443893, Test Loss = 0.249993, Learning Rate = 2.200062e-04\n",
      "Epoch 3986/20000: Train Loss = 0.443758, Test Loss = 0.251268, Learning Rate = 2.199226e-04\n",
      "Epoch 3987/20000: Train Loss = 0.443372, Test Loss = 0.245257, Learning Rate = 2.198390e-04\n",
      "Epoch 3988/20000: Train Loss = 0.444234, Test Loss = 0.244926, Learning Rate = 2.197555e-04\n",
      "Epoch 3989/20000: Train Loss = 0.443722, Test Loss = 0.242325, Learning Rate = 2.196720e-04\n",
      "Epoch 3990/20000: Train Loss = 0.443307, Test Loss = 0.255526, Learning Rate = 2.195885e-04\n",
      "Epoch 3991/20000: Train Loss = 0.443916, Test Loss = 0.248984, Learning Rate = 2.195050e-04\n",
      "Epoch 3992/20000: Train Loss = 0.444457, Test Loss = 0.243895, Learning Rate = 2.194216e-04\n",
      "Epoch 3993/20000: Train Loss = 0.443651, Test Loss = 0.238996, Learning Rate = 2.193383e-04\n",
      "Epoch 3994/20000: Train Loss = 0.443224, Test Loss = 0.244339, Learning Rate = 2.192549e-04\n",
      "Epoch 3995/20000: Train Loss = 0.444608, Test Loss = 0.253919, Learning Rate = 2.191716e-04\n",
      "Epoch 3996/20000: Train Loss = 0.443911, Test Loss = 0.244849, Learning Rate = 2.190883e-04\n",
      "Epoch 3997/20000: Train Loss = 0.443862, Test Loss = 0.238117, Learning Rate = 2.190051e-04\n",
      "Epoch 3998/20000: Train Loss = 0.443422, Test Loss = 0.248628, Learning Rate = 2.189219e-04\n",
      "Epoch 3999/20000: Train Loss = 0.443881, Test Loss = 0.239432, Learning Rate = 2.188387e-04\n",
      "Epoch 4000/20000: Train Loss = 0.443795, Test Loss = 0.247448, Learning Rate = 2.187555e-04\n",
      "Epoch 4001/20000: Train Loss = 0.444896, Test Loss = 0.240984, Learning Rate = 2.186724e-04\n",
      "Epoch 4002/20000: Train Loss = 0.444309, Test Loss = 0.247404, Learning Rate = 2.185893e-04\n",
      "Epoch 4003/20000: Train Loss = 0.443796, Test Loss = 0.242110, Learning Rate = 2.185063e-04\n",
      "Epoch 4004/20000: Train Loss = 0.443176, Test Loss = 0.247141, Learning Rate = 2.184232e-04\n",
      "Epoch 4005/20000: Train Loss = 0.444197, Test Loss = 0.246124, Learning Rate = 2.183402e-04\n",
      "Epoch 4006/20000: Train Loss = 0.444190, Test Loss = 0.245706, Learning Rate = 2.182573e-04\n",
      "Epoch 4007/20000: Train Loss = 0.444173, Test Loss = 0.242932, Learning Rate = 2.181744e-04\n",
      "Epoch 4008/20000: Train Loss = 0.444369, Test Loss = 0.252669, Learning Rate = 2.180915e-04\n",
      "Epoch 4009/20000: Train Loss = 0.443054, Test Loss = 0.240066, Learning Rate = 2.180086e-04\n",
      "Epoch 4010/20000: Train Loss = 0.443751, Test Loss = 0.242300, Learning Rate = 2.179257e-04\n",
      "Epoch 4011/20000: Train Loss = 0.443599, Test Loss = 0.237538, Learning Rate = 2.178429e-04\n",
      "Epoch 4012/20000: Train Loss = 0.443334, Test Loss = 0.238476, Learning Rate = 2.177602e-04\n",
      "Epoch 4013/20000: Train Loss = 0.444681, Test Loss = 0.244648, Learning Rate = 2.176774e-04\n",
      "Epoch 4014/20000: Train Loss = 0.443544, Test Loss = 0.246740, Learning Rate = 2.175947e-04\n",
      "Epoch 4015/20000: Train Loss = 0.443495, Test Loss = 0.242112, Learning Rate = 2.175120e-04\n",
      "Epoch 4016/20000: Train Loss = 0.443540, Test Loss = 0.247472, Learning Rate = 2.174294e-04\n",
      "Epoch 4017/20000: Train Loss = 0.443597, Test Loss = 0.242917, Learning Rate = 2.173468e-04\n",
      "Epoch 4018/20000: Train Loss = 0.443562, Test Loss = 0.241647, Learning Rate = 2.172642e-04\n",
      "Epoch 4019/20000: Train Loss = 0.443398, Test Loss = 0.248084, Learning Rate = 2.171816e-04\n",
      "Epoch 4020/20000: Train Loss = 0.444082, Test Loss = 0.248607, Learning Rate = 2.170991e-04\n",
      "Epoch 4021/20000: Train Loss = 0.443816, Test Loss = 0.245094, Learning Rate = 2.170166e-04\n",
      "Epoch 4022/20000: Train Loss = 0.444371, Test Loss = 0.251226, Learning Rate = 2.169341e-04\n",
      "Epoch 4023/20000: Train Loss = 0.444008, Test Loss = 0.240552, Learning Rate = 2.168517e-04\n",
      "Epoch 4024/20000: Train Loss = 0.444938, Test Loss = 0.239292, Learning Rate = 2.167693e-04\n",
      "Epoch 4025/20000: Train Loss = 0.444144, Test Loss = 0.231967, Learning Rate = 2.166870e-04\n",
      "Epoch 4026/20000: Train Loss = 0.444345, Test Loss = 0.248768, Learning Rate = 2.166046e-04\n",
      "Epoch 4027/20000: Train Loss = 0.443987, Test Loss = 0.252165, Learning Rate = 2.165223e-04\n",
      "Epoch 4028/20000: Train Loss = 0.443409, Test Loss = 0.247106, Learning Rate = 2.164400e-04\n",
      "Epoch 4029/20000: Train Loss = 0.443226, Test Loss = 0.249677, Learning Rate = 2.163578e-04\n",
      "Epoch 4030/20000: Train Loss = 0.443319, Test Loss = 0.242831, Learning Rate = 2.162756e-04\n",
      "Epoch 4031/20000: Train Loss = 0.443721, Test Loss = 0.242928, Learning Rate = 2.161934e-04\n",
      "Epoch 4032/20000: Train Loss = 0.444060, Test Loss = 0.248130, Learning Rate = 2.161113e-04\n",
      "Epoch 4033/20000: Train Loss = 0.443818, Test Loss = 0.241415, Learning Rate = 2.160291e-04\n",
      "Epoch 4034/20000: Train Loss = 0.443085, Test Loss = 0.248924, Learning Rate = 2.159471e-04\n",
      "Epoch 4035/20000: Train Loss = 0.444979, Test Loss = 0.244264, Learning Rate = 2.158650e-04\n",
      "Epoch 4036/20000: Train Loss = 0.444192, Test Loss = 0.242642, Learning Rate = 2.157830e-04\n",
      "Epoch 4037/20000: Train Loss = 0.444642, Test Loss = 0.251771, Learning Rate = 2.157010e-04\n",
      "Epoch 4038/20000: Train Loss = 0.443244, Test Loss = 0.246017, Learning Rate = 2.156190e-04\n",
      "Epoch 4039/20000: Train Loss = 0.443587, Test Loss = 0.243608, Learning Rate = 2.155371e-04\n",
      "Epoch 4040/20000: Train Loss = 0.444115, Test Loss = 0.243366, Learning Rate = 2.154552e-04\n",
      "Epoch 4041/20000: Train Loss = 0.443196, Test Loss = 0.245392, Learning Rate = 2.153733e-04\n",
      "Epoch 4042/20000: Train Loss = 0.443926, Test Loss = 0.239607, Learning Rate = 2.152915e-04\n",
      "Epoch 4043/20000: Train Loss = 0.442982, Test Loss = 0.250685, Learning Rate = 2.152097e-04\n",
      "Epoch 4044/20000: Train Loss = 0.443886, Test Loss = 0.253131, Learning Rate = 2.151279e-04\n",
      "Epoch 4045/20000: Train Loss = 0.443847, Test Loss = 0.242322, Learning Rate = 2.150462e-04\n",
      "Epoch 4046/20000: Train Loss = 0.443298, Test Loss = 0.251036, Learning Rate = 2.149645e-04\n",
      "Epoch 4047/20000: Train Loss = 0.443468, Test Loss = 0.249557, Learning Rate = 2.148828e-04\n",
      "Epoch 4048/20000: Train Loss = 0.444221, Test Loss = 0.245797, Learning Rate = 2.148011e-04\n",
      "Epoch 4049/20000: Train Loss = 0.443439, Test Loss = 0.239271, Learning Rate = 2.147195e-04\n",
      "Epoch 4050/20000: Train Loss = 0.443510, Test Loss = 0.245710, Learning Rate = 2.146379e-04\n",
      "Epoch 4051/20000: Train Loss = 0.443605, Test Loss = 0.249600, Learning Rate = 2.145564e-04\n",
      "Epoch 4052/20000: Train Loss = 0.443381, Test Loss = 0.246104, Learning Rate = 2.144749e-04\n",
      "Epoch 4053/20000: Train Loss = 0.443161, Test Loss = 0.250150, Learning Rate = 2.143934e-04\n",
      "Epoch 4054/20000: Train Loss = 0.443509, Test Loss = 0.252757, Learning Rate = 2.143119e-04\n",
      "Epoch 4055/20000: Train Loss = 0.443602, Test Loss = 0.244613, Learning Rate = 2.142305e-04\n",
      "Epoch 4056/20000: Train Loss = 0.444606, Test Loss = 0.256804, Learning Rate = 2.141491e-04\n",
      "Epoch 4057/20000: Train Loss = 0.443728, Test Loss = 0.244790, Learning Rate = 2.140677e-04\n",
      "Epoch 4058/20000: Train Loss = 0.444294, Test Loss = 0.244611, Learning Rate = 2.139863e-04\n",
      "Epoch 4059/20000: Train Loss = 0.444205, Test Loss = 0.246261, Learning Rate = 2.139050e-04\n",
      "Epoch 4060/20000: Train Loss = 0.444003, Test Loss = 0.254120, Learning Rate = 2.138238e-04\n",
      "Epoch 4061/20000: Train Loss = 0.443597, Test Loss = 0.243968, Learning Rate = 2.137425e-04\n",
      "Epoch 4062/20000: Train Loss = 0.443588, Test Loss = 0.242929, Learning Rate = 2.136613e-04\n",
      "Epoch 4063/20000: Train Loss = 0.443548, Test Loss = 0.249990, Learning Rate = 2.135801e-04\n",
      "Epoch 4064/20000: Train Loss = 0.443430, Test Loss = 0.248447, Learning Rate = 2.134990e-04\n",
      "Epoch 4065/20000: Train Loss = 0.443321, Test Loss = 0.233938, Learning Rate = 2.134178e-04\n",
      "Epoch 4066/20000: Train Loss = 0.444172, Test Loss = 0.246068, Learning Rate = 2.133367e-04\n",
      "Epoch 4067/20000: Train Loss = 0.444023, Test Loss = 0.252963, Learning Rate = 2.132557e-04\n",
      "Epoch 4068/20000: Train Loss = 0.443785, Test Loss = 0.239241, Learning Rate = 2.131746e-04\n",
      "Epoch 4069/20000: Train Loss = 0.443539, Test Loss = 0.253735, Learning Rate = 2.130936e-04\n",
      "Epoch 4070/20000: Train Loss = 0.443694, Test Loss = 0.245735, Learning Rate = 2.130127e-04\n",
      "Epoch 4071/20000: Train Loss = 0.443450, Test Loss = 0.242981, Learning Rate = 2.129317e-04\n",
      "Epoch 4072/20000: Train Loss = 0.443459, Test Loss = 0.248848, Learning Rate = 2.128508e-04\n",
      "Epoch 4073/20000: Train Loss = 0.443107, Test Loss = 0.241054, Learning Rate = 2.127700e-04\n",
      "Epoch 4074/20000: Train Loss = 0.443779, Test Loss = 0.250488, Learning Rate = 2.126891e-04\n",
      "Epoch 4075/20000: Train Loss = 0.444071, Test Loss = 0.246700, Learning Rate = 2.126083e-04\n",
      "Epoch 4076/20000: Train Loss = 0.443421, Test Loss = 0.251230, Learning Rate = 2.125275e-04\n",
      "Epoch 4077/20000: Train Loss = 0.442875, Test Loss = 0.250953, Learning Rate = 2.124467e-04\n",
      "Epoch 4078/20000: Train Loss = 0.443351, Test Loss = 0.246585, Learning Rate = 2.123660e-04\n",
      "Epoch 4079/20000: Train Loss = 0.444454, Test Loss = 0.252731, Learning Rate = 2.122853e-04\n",
      "Epoch 4080/20000: Train Loss = 0.445091, Test Loss = 0.256239, Learning Rate = 2.122047e-04\n",
      "Epoch 4081/20000: Train Loss = 0.445596, Test Loss = 0.249028, Learning Rate = 2.121240e-04\n",
      "Epoch 4082/20000: Train Loss = 0.443540, Test Loss = 0.244359, Learning Rate = 2.120434e-04\n",
      "Epoch 4083/20000: Train Loss = 0.443986, Test Loss = 0.250281, Learning Rate = 2.119629e-04\n",
      "Epoch 4084/20000: Train Loss = 0.443677, Test Loss = 0.252209, Learning Rate = 2.118823e-04\n",
      "Epoch 4085/20000: Train Loss = 0.443664, Test Loss = 0.252977, Learning Rate = 2.118018e-04\n",
      "Epoch 4086/20000: Train Loss = 0.443604, Test Loss = 0.246073, Learning Rate = 2.117213e-04\n",
      "Epoch 4087/20000: Train Loss = 0.443426, Test Loss = 0.244253, Learning Rate = 2.116409e-04\n",
      "Epoch 4088/20000: Train Loss = 0.443926, Test Loss = 0.248135, Learning Rate = 2.115605e-04\n",
      "Epoch 4089/20000: Train Loss = 0.443217, Test Loss = 0.238141, Learning Rate = 2.114801e-04\n",
      "Epoch 4090/20000: Train Loss = 0.443000, Test Loss = 0.254966, Learning Rate = 2.113997e-04\n",
      "Epoch 4091/20000: Train Loss = 0.444235, Test Loss = 0.251262, Learning Rate = 2.113194e-04\n",
      "Epoch 4092/20000: Train Loss = 0.444020, Test Loss = 0.249998, Learning Rate = 2.112391e-04\n",
      "Epoch 4093/20000: Train Loss = 0.443143, Test Loss = 0.254842, Learning Rate = 2.111588e-04\n",
      "Epoch 4094/20000: Train Loss = 0.443230, Test Loss = 0.251565, Learning Rate = 2.110786e-04\n",
      "Epoch 4095/20000: Train Loss = 0.444373, Test Loss = 0.250106, Learning Rate = 2.109984e-04\n",
      "Epoch 4096/20000: Train Loss = 0.443668, Test Loss = 0.247416, Learning Rate = 2.109182e-04\n",
      "Epoch 4097/20000: Train Loss = 0.444641, Test Loss = 0.246078, Learning Rate = 2.108381e-04\n",
      "Epoch 4098/20000: Train Loss = 0.444442, Test Loss = 0.243829, Learning Rate = 2.107580e-04\n",
      "Epoch 4099/20000: Train Loss = 0.445302, Test Loss = 0.253600, Learning Rate = 2.106779e-04\n",
      "Epoch 4100/20000: Train Loss = 0.443707, Test Loss = 0.248844, Learning Rate = 2.105978e-04\n",
      "Epoch 4101/20000: Train Loss = 0.443402, Test Loss = 0.249897, Learning Rate = 2.105178e-04\n",
      "Epoch 4102/20000: Train Loss = 0.444158, Test Loss = 0.245408, Learning Rate = 2.104378e-04\n",
      "Epoch 4103/20000: Train Loss = 0.443628, Test Loss = 0.250230, Learning Rate = 2.103579e-04\n",
      "Epoch 4104/20000: Train Loss = 0.443857, Test Loss = 0.252313, Learning Rate = 2.102779e-04\n",
      "Epoch 4105/20000: Train Loss = 0.443161, Test Loss = 0.242150, Learning Rate = 2.101980e-04\n",
      "Epoch 4106/20000: Train Loss = 0.443343, Test Loss = 0.254456, Learning Rate = 2.101182e-04\n",
      "Epoch 4107/20000: Train Loss = 0.443757, Test Loss = 0.250928, Learning Rate = 2.100383e-04\n",
      "Epoch 4108/20000: Train Loss = 0.443926, Test Loss = 0.240993, Learning Rate = 2.099585e-04\n",
      "Epoch 4109/20000: Train Loss = 0.443643, Test Loss = 0.242435, Learning Rate = 2.098787e-04\n",
      "Epoch 4110/20000: Train Loss = 0.444558, Test Loss = 0.250198, Learning Rate = 2.097990e-04\n",
      "Epoch 4111/20000: Train Loss = 0.443753, Test Loss = 0.239684, Learning Rate = 2.097193e-04\n",
      "Epoch 4112/20000: Train Loss = 0.443060, Test Loss = 0.250918, Learning Rate = 2.096396e-04\n",
      "Epoch 4113/20000: Train Loss = 0.443169, Test Loss = 0.245942, Learning Rate = 2.095599e-04\n",
      "Epoch 4114/20000: Train Loss = 0.443167, Test Loss = 0.250128, Learning Rate = 2.094803e-04\n",
      "Epoch 4115/20000: Train Loss = 0.443059, Test Loss = 0.246436, Learning Rate = 2.094007e-04\n",
      "Epoch 4116/20000: Train Loss = 0.443398, Test Loss = 0.251262, Learning Rate = 2.093211e-04\n",
      "Epoch 4117/20000: Train Loss = 0.443841, Test Loss = 0.242995, Learning Rate = 2.092416e-04\n",
      "Epoch 4118/20000: Train Loss = 0.444095, Test Loss = 0.246163, Learning Rate = 2.091621e-04\n",
      "Epoch 4119/20000: Train Loss = 0.444860, Test Loss = 0.245559, Learning Rate = 2.090826e-04\n",
      "Epoch 4120/20000: Train Loss = 0.443565, Test Loss = 0.248569, Learning Rate = 2.090032e-04\n",
      "Epoch 4121/20000: Train Loss = 0.444364, Test Loss = 0.237415, Learning Rate = 2.089238e-04\n",
      "Epoch 4122/20000: Train Loss = 0.443729, Test Loss = 0.245051, Learning Rate = 2.088444e-04\n",
      "Epoch 4123/20000: Train Loss = 0.443556, Test Loss = 0.246685, Learning Rate = 2.087650e-04\n",
      "Epoch 4124/20000: Train Loss = 0.443528, Test Loss = 0.257925, Learning Rate = 2.086857e-04\n",
      "Epoch 4125/20000: Train Loss = 0.443112, Test Loss = 0.239973, Learning Rate = 2.086064e-04\n",
      "Epoch 4126/20000: Train Loss = 0.444151, Test Loss = 0.248918, Learning Rate = 2.085271e-04\n",
      "Epoch 4127/20000: Train Loss = 0.445039, Test Loss = 0.243673, Learning Rate = 2.084479e-04\n",
      "Epoch 4128/20000: Train Loss = 0.443250, Test Loss = 0.248794, Learning Rate = 2.083687e-04\n",
      "Epoch 4129/20000: Train Loss = 0.443584, Test Loss = 0.251461, Learning Rate = 2.082895e-04\n",
      "Epoch 4130/20000: Train Loss = 0.443253, Test Loss = 0.247849, Learning Rate = 2.082104e-04\n",
      "Epoch 4131/20000: Train Loss = 0.443908, Test Loss = 0.248205, Learning Rate = 2.081313e-04\n",
      "Epoch 4132/20000: Train Loss = 0.443042, Test Loss = 0.244527, Learning Rate = 2.080522e-04\n",
      "Epoch 4133/20000: Train Loss = 0.443189, Test Loss = 0.255585, Learning Rate = 2.079731e-04\n",
      "Epoch 4134/20000: Train Loss = 0.443582, Test Loss = 0.248226, Learning Rate = 2.078941e-04\n",
      "Epoch 4135/20000: Train Loss = 0.442998, Test Loss = 0.245774, Learning Rate = 2.078151e-04\n",
      "Epoch 4136/20000: Train Loss = 0.443364, Test Loss = 0.243930, Learning Rate = 2.077361e-04\n",
      "Epoch 4137/20000: Train Loss = 0.444170, Test Loss = 0.255574, Learning Rate = 2.076572e-04\n",
      "Epoch 4138/20000: Train Loss = 0.445341, Test Loss = 0.235134, Learning Rate = 2.075783e-04\n",
      "Epoch 4139/20000: Train Loss = 0.443154, Test Loss = 0.256826, Learning Rate = 2.074994e-04\n",
      "Epoch 4140/20000: Train Loss = 0.443422, Test Loss = 0.245919, Learning Rate = 2.074206e-04\n",
      "Epoch 4141/20000: Train Loss = 0.443209, Test Loss = 0.255602, Learning Rate = 2.073418e-04\n",
      "Epoch 4142/20000: Train Loss = 0.443272, Test Loss = 0.251222, Learning Rate = 2.072630e-04\n",
      "Epoch 4143/20000: Train Loss = 0.443695, Test Loss = 0.246409, Learning Rate = 2.071842e-04\n",
      "Epoch 4144/20000: Train Loss = 0.443458, Test Loss = 0.246902, Learning Rate = 2.071055e-04\n",
      "Epoch 4145/20000: Train Loss = 0.443983, Test Loss = 0.241306, Learning Rate = 2.070268e-04\n",
      "Epoch 4146/20000: Train Loss = 0.444094, Test Loss = 0.245964, Learning Rate = 2.069481e-04\n",
      "Epoch 4147/20000: Train Loss = 0.444028, Test Loss = 0.248148, Learning Rate = 2.068695e-04\n",
      "Epoch 4148/20000: Train Loss = 0.443890, Test Loss = 0.248211, Learning Rate = 2.067909e-04\n",
      "Epoch 4149/20000: Train Loss = 0.443561, Test Loss = 0.241308, Learning Rate = 2.067123e-04\n",
      "Epoch 4150/20000: Train Loss = 0.443848, Test Loss = 0.244563, Learning Rate = 2.066338e-04\n",
      "Epoch 4151/20000: Train Loss = 0.443161, Test Loss = 0.242628, Learning Rate = 2.065553e-04\n",
      "Epoch 4152/20000: Train Loss = 0.443197, Test Loss = 0.246993, Learning Rate = 2.064768e-04\n",
      "Epoch 4153/20000: Train Loss = 0.445096, Test Loss = 0.231899, Learning Rate = 2.063983e-04\n",
      "Epoch 4154/20000: Train Loss = 0.444721, Test Loss = 0.246389, Learning Rate = 2.063199e-04\n",
      "Epoch 4155/20000: Train Loss = 0.443300, Test Loss = 0.243293, Learning Rate = 2.062415e-04\n",
      "Epoch 4156/20000: Train Loss = 0.443759, Test Loss = 0.247497, Learning Rate = 2.061631e-04\n",
      "Epoch 4157/20000: Train Loss = 0.443745, Test Loss = 0.245420, Learning Rate = 2.060848e-04\n",
      "Epoch 4158/20000: Train Loss = 0.443575, Test Loss = 0.245916, Learning Rate = 2.060065e-04\n",
      "Epoch 4159/20000: Train Loss = 0.442983, Test Loss = 0.241445, Learning Rate = 2.059282e-04\n",
      "Epoch 4160/20000: Train Loss = 0.443230, Test Loss = 0.246083, Learning Rate = 2.058500e-04\n",
      "Epoch 4161/20000: Train Loss = 0.443357, Test Loss = 0.251632, Learning Rate = 2.057718e-04\n",
      "Epoch 4162/20000: Train Loss = 0.443713, Test Loss = 0.252508, Learning Rate = 2.056936e-04\n",
      "Epoch 4163/20000: Train Loss = 0.443368, Test Loss = 0.253373, Learning Rate = 2.056154e-04\n",
      "Epoch 4164/20000: Train Loss = 0.443654, Test Loss = 0.245117, Learning Rate = 2.055373e-04\n",
      "Epoch 4165/20000: Train Loss = 0.443512, Test Loss = 0.240764, Learning Rate = 2.054592e-04\n",
      "Epoch 4166/20000: Train Loss = 0.442974, Test Loss = 0.254186, Learning Rate = 2.053811e-04\n",
      "Epoch 4167/20000: Train Loss = 0.443521, Test Loss = 0.259023, Learning Rate = 2.053031e-04\n",
      "Epoch 4168/20000: Train Loss = 0.443444, Test Loss = 0.244594, Learning Rate = 2.052251e-04\n",
      "Epoch 4169/20000: Train Loss = 0.443351, Test Loss = 0.241463, Learning Rate = 2.051471e-04\n",
      "Epoch 4170/20000: Train Loss = 0.443256, Test Loss = 0.250187, Learning Rate = 2.050691e-04\n",
      "Epoch 4171/20000: Train Loss = 0.443656, Test Loss = 0.246263, Learning Rate = 2.049912e-04\n",
      "Epoch 4172/20000: Train Loss = 0.444142, Test Loss = 0.246786, Learning Rate = 2.049133e-04\n",
      "Epoch 4173/20000: Train Loss = 0.444754, Test Loss = 0.243479, Learning Rate = 2.048355e-04\n",
      "Epoch 4174/20000: Train Loss = 0.443220, Test Loss = 0.240775, Learning Rate = 2.047576e-04\n",
      "Epoch 4175/20000: Train Loss = 0.442982, Test Loss = 0.252359, Learning Rate = 2.046798e-04\n",
      "Epoch 4176/20000: Train Loss = 0.443340, Test Loss = 0.249629, Learning Rate = 2.046021e-04\n",
      "Epoch 4177/20000: Train Loss = 0.443261, Test Loss = 0.244927, Learning Rate = 2.045243e-04\n",
      "Epoch 4178/20000: Train Loss = 0.444870, Test Loss = 0.242865, Learning Rate = 2.044466e-04\n",
      "Epoch 4179/20000: Train Loss = 0.443068, Test Loss = 0.252004, Learning Rate = 2.043689e-04\n",
      "Epoch 4180/20000: Train Loss = 0.443303, Test Loss = 0.247243, Learning Rate = 2.042913e-04\n",
      "Epoch 4181/20000: Train Loss = 0.443429, Test Loss = 0.237702, Learning Rate = 2.042136e-04\n",
      "Epoch 4182/20000: Train Loss = 0.443045, Test Loss = 0.245770, Learning Rate = 2.041360e-04\n",
      "Epoch 4183/20000: Train Loss = 0.443509, Test Loss = 0.254998, Learning Rate = 2.040585e-04\n",
      "Epoch 4184/20000: Train Loss = 0.445060, Test Loss = 0.248116, Learning Rate = 2.039809e-04\n",
      "Epoch 4185/20000: Train Loss = 0.443948, Test Loss = 0.248277, Learning Rate = 2.039034e-04\n",
      "Epoch 4186/20000: Train Loss = 0.444338, Test Loss = 0.247875, Learning Rate = 2.038259e-04\n",
      "Epoch 4187/20000: Train Loss = 0.443296, Test Loss = 0.245038, Learning Rate = 2.037485e-04\n",
      "Epoch 4188/20000: Train Loss = 0.443201, Test Loss = 0.247658, Learning Rate = 2.036711e-04\n",
      "Epoch 4189/20000: Train Loss = 0.443064, Test Loss = 0.242011, Learning Rate = 2.035937e-04\n",
      "Epoch 4190/20000: Train Loss = 0.444077, Test Loss = 0.244565, Learning Rate = 2.035163e-04\n",
      "Epoch 4191/20000: Train Loss = 0.443867, Test Loss = 0.249730, Learning Rate = 2.034390e-04\n",
      "Epoch 4192/20000: Train Loss = 0.443740, Test Loss = 0.246285, Learning Rate = 2.033617e-04\n",
      "Epoch 4193/20000: Train Loss = 0.443917, Test Loss = 0.245027, Learning Rate = 2.032844e-04\n",
      "Epoch 4194/20000: Train Loss = 0.443579, Test Loss = 0.244794, Learning Rate = 2.032072e-04\n",
      "Epoch 4195/20000: Train Loss = 0.443597, Test Loss = 0.245467, Learning Rate = 2.031300e-04\n",
      "Epoch 4196/20000: Train Loss = 0.443374, Test Loss = 0.249488, Learning Rate = 2.030528e-04\n",
      "Epoch 4197/20000: Train Loss = 0.444216, Test Loss = 0.246358, Learning Rate = 2.029756e-04\n",
      "Epoch 4198/20000: Train Loss = 0.443733, Test Loss = 0.247987, Learning Rate = 2.028985e-04\n",
      "Epoch 4199/20000: Train Loss = 0.443547, Test Loss = 0.247026, Learning Rate = 2.028214e-04\n",
      "Epoch 4200/20000: Train Loss = 0.443509, Test Loss = 0.253129, Learning Rate = 2.027443e-04\n",
      "Epoch 4201/20000: Train Loss = 0.443825, Test Loss = 0.251973, Learning Rate = 2.026673e-04\n",
      "Epoch 4202/20000: Train Loss = 0.443026, Test Loss = 0.240788, Learning Rate = 2.025903e-04\n",
      "Epoch 4203/20000: Train Loss = 0.443319, Test Loss = 0.242063, Learning Rate = 2.025133e-04\n",
      "Epoch 4204/20000: Train Loss = 0.443460, Test Loss = 0.245752, Learning Rate = 2.024364e-04\n",
      "Epoch 4205/20000: Train Loss = 0.443049, Test Loss = 0.245054, Learning Rate = 2.023595e-04\n",
      "Epoch 4206/20000: Train Loss = 0.443086, Test Loss = 0.249029, Learning Rate = 2.022826e-04\n",
      "Epoch 4207/20000: Train Loss = 0.443772, Test Loss = 0.248833, Learning Rate = 2.022057e-04\n",
      "Epoch 4208/20000: Train Loss = 0.443808, Test Loss = 0.247815, Learning Rate = 2.021289e-04\n",
      "Epoch 4209/20000: Train Loss = 0.443519, Test Loss = 0.243327, Learning Rate = 2.020521e-04\n",
      "Epoch 4210/20000: Train Loss = 0.443088, Test Loss = 0.249041, Learning Rate = 2.019753e-04\n",
      "Epoch 4211/20000: Train Loss = 0.443653, Test Loss = 0.242393, Learning Rate = 2.018985e-04\n",
      "Epoch 4212/20000: Train Loss = 0.443461, Test Loss = 0.256231, Learning Rate = 2.018218e-04\n",
      "Epoch 4213/20000: Train Loss = 0.444402, Test Loss = 0.243978, Learning Rate = 2.017451e-04\n",
      "Epoch 4214/20000: Train Loss = 0.443116, Test Loss = 0.250290, Learning Rate = 2.016685e-04\n",
      "Epoch 4215/20000: Train Loss = 0.444307, Test Loss = 0.249358, Learning Rate = 2.015919e-04\n",
      "Epoch 4216/20000: Train Loss = 0.442706, Test Loss = 0.237170, Learning Rate = 2.015153e-04\n",
      "Epoch 4217/20000: Train Loss = 0.443269, Test Loss = 0.243543, Learning Rate = 2.014387e-04\n",
      "Epoch 4218/20000: Train Loss = 0.443314, Test Loss = 0.242871, Learning Rate = 2.013621e-04\n",
      "Epoch 4219/20000: Train Loss = 0.443148, Test Loss = 0.243983, Learning Rate = 2.012856e-04\n",
      "Epoch 4220/20000: Train Loss = 0.442856, Test Loss = 0.244615, Learning Rate = 2.012091e-04\n",
      "Epoch 4221/20000: Train Loss = 0.444135, Test Loss = 0.253265, Learning Rate = 2.011327e-04\n",
      "Epoch 4222/20000: Train Loss = 0.443214, Test Loss = 0.243150, Learning Rate = 2.010563e-04\n",
      "Epoch 4223/20000: Train Loss = 0.443294, Test Loss = 0.249663, Learning Rate = 2.009799e-04\n",
      "Epoch 4224/20000: Train Loss = 0.444955, Test Loss = 0.247805, Learning Rate = 2.009035e-04\n",
      "Epoch 4225/20000: Train Loss = 0.444861, Test Loss = 0.245997, Learning Rate = 2.008272e-04\n",
      "Epoch 4226/20000: Train Loss = 0.448571, Test Loss = 0.247764, Learning Rate = 2.007509e-04\n",
      "Epoch 4227/20000: Train Loss = 0.444193, Test Loss = 0.244864, Learning Rate = 2.006746e-04\n",
      "Epoch 4228/20000: Train Loss = 0.444022, Test Loss = 0.252757, Learning Rate = 2.005983e-04\n",
      "Epoch 4229/20000: Train Loss = 0.443095, Test Loss = 0.239376, Learning Rate = 2.005221e-04\n",
      "Epoch 4230/20000: Train Loss = 0.443737, Test Loss = 0.256544, Learning Rate = 2.004459e-04\n",
      "Epoch 4231/20000: Train Loss = 0.443670, Test Loss = 0.246717, Learning Rate = 2.003697e-04\n",
      "Epoch 4232/20000: Train Loss = 0.443016, Test Loss = 0.243539, Learning Rate = 2.002936e-04\n",
      "Epoch 4233/20000: Train Loss = 0.443136, Test Loss = 0.249812, Learning Rate = 2.002175e-04\n",
      "Epoch 4234/20000: Train Loss = 0.443488, Test Loss = 0.247944, Learning Rate = 2.001414e-04\n",
      "Epoch 4235/20000: Train Loss = 0.444013, Test Loss = 0.236068, Learning Rate = 2.000654e-04\n",
      "Epoch 4236/20000: Train Loss = 0.443884, Test Loss = 0.251256, Learning Rate = 1.999894e-04\n",
      "Epoch 4237/20000: Train Loss = 0.443702, Test Loss = 0.243974, Learning Rate = 1.999134e-04\n",
      "Epoch 4238/20000: Train Loss = 0.443594, Test Loss = 0.245146, Learning Rate = 1.998374e-04\n",
      "Epoch 4239/20000: Train Loss = 0.443532, Test Loss = 0.249392, Learning Rate = 1.997615e-04\n",
      "Epoch 4240/20000: Train Loss = 0.443066, Test Loss = 0.248212, Learning Rate = 1.996856e-04\n",
      "Epoch 4241/20000: Train Loss = 0.444137, Test Loss = 0.234047, Learning Rate = 1.996097e-04\n",
      "Epoch 4242/20000: Train Loss = 0.443724, Test Loss = 0.250453, Learning Rate = 1.995339e-04\n",
      "Epoch 4243/20000: Train Loss = 0.442822, Test Loss = 0.242915, Learning Rate = 1.994580e-04\n",
      "Epoch 4244/20000: Train Loss = 0.443401, Test Loss = 0.245413, Learning Rate = 1.993822e-04\n",
      "Epoch 4245/20000: Train Loss = 0.444323, Test Loss = 0.243830, Learning Rate = 1.993065e-04\n",
      "Epoch 4246/20000: Train Loss = 0.443365, Test Loss = 0.247513, Learning Rate = 1.992308e-04\n",
      "Epoch 4247/20000: Train Loss = 0.443489, Test Loss = 0.244672, Learning Rate = 1.991551e-04\n",
      "Epoch 4248/20000: Train Loss = 0.443709, Test Loss = 0.244336, Learning Rate = 1.990794e-04\n",
      "Epoch 4249/20000: Train Loss = 0.443337, Test Loss = 0.246473, Learning Rate = 1.990037e-04\n",
      "Epoch 4250/20000: Train Loss = 0.442955, Test Loss = 0.250434, Learning Rate = 1.989281e-04\n",
      "Epoch 4251/20000: Train Loss = 0.443083, Test Loss = 0.245080, Learning Rate = 1.988525e-04\n",
      "Epoch 4252/20000: Train Loss = 0.444418, Test Loss = 0.246977, Learning Rate = 1.987770e-04\n",
      "Epoch 4253/20000: Train Loss = 0.443841, Test Loss = 0.246903, Learning Rate = 1.987014e-04\n",
      "Epoch 4254/20000: Train Loss = 0.443890, Test Loss = 0.245555, Learning Rate = 1.986259e-04\n",
      "Epoch 4255/20000: Train Loss = 0.442995, Test Loss = 0.250020, Learning Rate = 1.985505e-04\n",
      "Epoch 4256/20000: Train Loss = 0.443982, Test Loss = 0.246950, Learning Rate = 1.984750e-04\n",
      "Epoch 4257/20000: Train Loss = 0.443978, Test Loss = 0.247997, Learning Rate = 1.983996e-04\n",
      "Epoch 4258/20000: Train Loss = 0.443423, Test Loss = 0.243972, Learning Rate = 1.983242e-04\n",
      "Epoch 4259/20000: Train Loss = 0.444608, Test Loss = 0.253817, Learning Rate = 1.982489e-04\n",
      "Epoch 4260/20000: Train Loss = 0.444606, Test Loss = 0.240358, Learning Rate = 1.981735e-04\n",
      "Epoch 4261/20000: Train Loss = 0.446083, Test Loss = 0.236769, Learning Rate = 1.980982e-04\n",
      "Epoch 4262/20000: Train Loss = 0.443679, Test Loss = 0.249918, Learning Rate = 1.980230e-04\n",
      "Epoch 4263/20000: Train Loss = 0.444030, Test Loss = 0.248123, Learning Rate = 1.979477e-04\n",
      "Epoch 4264/20000: Train Loss = 0.444765, Test Loss = 0.241974, Learning Rate = 1.978725e-04\n",
      "Epoch 4265/20000: Train Loss = 0.444371, Test Loss = 0.252721, Learning Rate = 1.977973e-04\n",
      "Epoch 4266/20000: Train Loss = 0.443444, Test Loss = 0.232366, Learning Rate = 1.977222e-04\n",
      "Epoch 4267/20000: Train Loss = 0.444231, Test Loss = 0.240364, Learning Rate = 1.976470e-04\n",
      "Epoch 4268/20000: Train Loss = 0.444863, Test Loss = 0.247719, Learning Rate = 1.975719e-04\n",
      "Epoch 4269/20000: Train Loss = 0.443541, Test Loss = 0.245984, Learning Rate = 1.974969e-04\n",
      "Epoch 4270/20000: Train Loss = 0.443899, Test Loss = 0.254471, Learning Rate = 1.974218e-04\n",
      "Epoch 4271/20000: Train Loss = 0.443458, Test Loss = 0.252660, Learning Rate = 1.973468e-04\n",
      "Epoch 4272/20000: Train Loss = 0.443163, Test Loss = 0.250207, Learning Rate = 1.972718e-04\n",
      "Epoch 4273/20000: Train Loss = 0.443636, Test Loss = 0.241222, Learning Rate = 1.971969e-04\n",
      "Epoch 4274/20000: Train Loss = 0.443554, Test Loss = 0.248285, Learning Rate = 1.971219e-04\n",
      "Epoch 4275/20000: Train Loss = 0.443379, Test Loss = 0.243319, Learning Rate = 1.970470e-04\n",
      "Epoch 4276/20000: Train Loss = 0.443902, Test Loss = 0.242572, Learning Rate = 1.969722e-04\n",
      "Epoch 4277/20000: Train Loss = 0.443174, Test Loss = 0.244865, Learning Rate = 1.968973e-04\n",
      "Epoch 4278/20000: Train Loss = 0.443222, Test Loss = 0.247079, Learning Rate = 1.968225e-04\n",
      "Epoch 4279/20000: Train Loss = 0.443734, Test Loss = 0.245710, Learning Rate = 1.967477e-04\n",
      "Epoch 4280/20000: Train Loss = 0.443849, Test Loss = 0.245174, Learning Rate = 1.966730e-04\n",
      "Epoch 4281/20000: Train Loss = 0.443298, Test Loss = 0.252119, Learning Rate = 1.965982e-04\n",
      "Epoch 4282/20000: Train Loss = 0.444268, Test Loss = 0.242273, Learning Rate = 1.965235e-04\n",
      "Epoch 4283/20000: Train Loss = 0.443668, Test Loss = 0.247959, Learning Rate = 1.964488e-04\n",
      "Epoch 4284/20000: Train Loss = 0.443438, Test Loss = 0.237127, Learning Rate = 1.963742e-04\n",
      "Epoch 4285/20000: Train Loss = 0.444166, Test Loss = 0.237527, Learning Rate = 1.962996e-04\n",
      "Epoch 4286/20000: Train Loss = 0.443923, Test Loss = 0.245791, Learning Rate = 1.962250e-04\n",
      "Epoch 4287/20000: Train Loss = 0.443108, Test Loss = 0.248935, Learning Rate = 1.961504e-04\n",
      "Epoch 4288/20000: Train Loss = 0.443390, Test Loss = 0.245624, Learning Rate = 1.960759e-04\n",
      "Epoch 4289/20000: Train Loss = 0.443911, Test Loss = 0.238569, Learning Rate = 1.960014e-04\n",
      "Epoch 4290/20000: Train Loss = 0.443429, Test Loss = 0.245106, Learning Rate = 1.959269e-04\n",
      "Epoch 4291/20000: Train Loss = 0.444586, Test Loss = 0.240871, Learning Rate = 1.958525e-04\n",
      "Epoch 4292/20000: Train Loss = 0.444155, Test Loss = 0.258943, Learning Rate = 1.957781e-04\n",
      "Epoch 4293/20000: Train Loss = 0.443812, Test Loss = 0.251622, Learning Rate = 1.957037e-04\n",
      "Epoch 4294/20000: Train Loss = 0.443421, Test Loss = 0.241773, Learning Rate = 1.956293e-04\n",
      "Epoch 4295/20000: Train Loss = 0.443868, Test Loss = 0.250064, Learning Rate = 1.955550e-04\n",
      "Epoch 4296/20000: Train Loss = 0.443327, Test Loss = 0.251883, Learning Rate = 1.954807e-04\n",
      "Epoch 4297/20000: Train Loss = 0.442882, Test Loss = 0.247731, Learning Rate = 1.954064e-04\n",
      "Epoch 4298/20000: Train Loss = 0.442958, Test Loss = 0.246333, Learning Rate = 1.953321e-04\n",
      "Epoch 4299/20000: Train Loss = 0.443646, Test Loss = 0.253449, Learning Rate = 1.952579e-04\n",
      "Epoch 4300/20000: Train Loss = 0.443459, Test Loss = 0.247835, Learning Rate = 1.951837e-04\n",
      "Epoch 4301/20000: Train Loss = 0.443868, Test Loss = 0.256546, Learning Rate = 1.951096e-04\n",
      "Epoch 4302/20000: Train Loss = 0.443374, Test Loss = 0.248275, Learning Rate = 1.950354e-04\n",
      "Epoch 4303/20000: Train Loss = 0.443803, Test Loss = 0.248608, Learning Rate = 1.949613e-04\n",
      "Epoch 4304/20000: Train Loss = 0.443217, Test Loss = 0.245473, Learning Rate = 1.948872e-04\n",
      "Epoch 4305/20000: Train Loss = 0.443947, Test Loss = 0.235085, Learning Rate = 1.948132e-04\n",
      "Epoch 4306/20000: Train Loss = 0.444144, Test Loss = 0.250304, Learning Rate = 1.947392e-04\n",
      "Epoch 4307/20000: Train Loss = 0.443450, Test Loss = 0.248035, Learning Rate = 1.946652e-04\n",
      "Epoch 4308/20000: Train Loss = 0.443536, Test Loss = 0.248548, Learning Rate = 1.945912e-04\n",
      "Epoch 4309/20000: Train Loss = 0.443417, Test Loss = 0.249943, Learning Rate = 1.945173e-04\n",
      "Epoch 4310/20000: Train Loss = 0.444514, Test Loss = 0.248545, Learning Rate = 1.944433e-04\n",
      "Epoch 4311/20000: Train Loss = 0.443297, Test Loss = 0.241953, Learning Rate = 1.943695e-04\n",
      "Epoch 4312/20000: Train Loss = 0.443853, Test Loss = 0.242756, Learning Rate = 1.942956e-04\n",
      "Epoch 4313/20000: Train Loss = 0.443035, Test Loss = 0.245886, Learning Rate = 1.942218e-04\n",
      "Epoch 4314/20000: Train Loss = 0.443083, Test Loss = 0.245854, Learning Rate = 1.941480e-04\n",
      "Epoch 4315/20000: Train Loss = 0.443600, Test Loss = 0.250009, Learning Rate = 1.940742e-04\n",
      "Epoch 4316/20000: Train Loss = 0.443266, Test Loss = 0.247324, Learning Rate = 1.940005e-04\n",
      "Epoch 4317/20000: Train Loss = 0.444261, Test Loss = 0.246093, Learning Rate = 1.939268e-04\n",
      "Epoch 4318/20000: Train Loss = 0.443445, Test Loss = 0.245370, Learning Rate = 1.938531e-04\n",
      "Epoch 4319/20000: Train Loss = 0.443318, Test Loss = 0.243735, Learning Rate = 1.937794e-04\n",
      "Epoch 4320/20000: Train Loss = 0.443757, Test Loss = 0.245286, Learning Rate = 1.937058e-04\n",
      "Epoch 4321/20000: Train Loss = 0.444421, Test Loss = 0.247300, Learning Rate = 1.936322e-04\n",
      "Epoch 4322/20000: Train Loss = 0.443468, Test Loss = 0.245867, Learning Rate = 1.935586e-04\n",
      "Epoch 4323/20000: Train Loss = 0.443274, Test Loss = 0.242769, Learning Rate = 1.934851e-04\n",
      "Epoch 4324/20000: Train Loss = 0.443425, Test Loss = 0.252482, Learning Rate = 1.934115e-04\n",
      "Epoch 4325/20000: Train Loss = 0.443985, Test Loss = 0.244086, Learning Rate = 1.933380e-04\n",
      "Epoch 4326/20000: Train Loss = 0.443751, Test Loss = 0.239228, Learning Rate = 1.932646e-04\n",
      "Epoch 4327/20000: Train Loss = 0.443178, Test Loss = 0.254120, Learning Rate = 1.931911e-04\n",
      "Epoch 4328/20000: Train Loss = 0.443004, Test Loss = 0.239946, Learning Rate = 1.931177e-04\n",
      "Epoch 4329/20000: Train Loss = 0.443644, Test Loss = 0.242424, Learning Rate = 1.930444e-04\n",
      "Epoch 4330/20000: Train Loss = 0.444951, Test Loss = 0.244742, Learning Rate = 1.929710e-04\n",
      "Epoch 4331/20000: Train Loss = 0.443884, Test Loss = 0.244987, Learning Rate = 1.928977e-04\n",
      "Epoch 4332/20000: Train Loss = 0.443058, Test Loss = 0.245552, Learning Rate = 1.928244e-04\n",
      "Epoch 4333/20000: Train Loss = 0.444428, Test Loss = 0.238417, Learning Rate = 1.927511e-04\n",
      "Epoch 4334/20000: Train Loss = 0.442930, Test Loss = 0.250795, Learning Rate = 1.926779e-04\n",
      "Epoch 4335/20000: Train Loss = 0.443507, Test Loss = 0.246997, Learning Rate = 1.926047e-04\n",
      "Epoch 4336/20000: Train Loss = 0.443812, Test Loss = 0.238909, Learning Rate = 1.925315e-04\n",
      "Epoch 4337/20000: Train Loss = 0.445103, Test Loss = 0.241392, Learning Rate = 1.924583e-04\n",
      "Epoch 4338/20000: Train Loss = 0.443834, Test Loss = 0.244752, Learning Rate = 1.923852e-04\n",
      "Epoch 4339/20000: Train Loss = 0.443325, Test Loss = 0.250053, Learning Rate = 1.923121e-04\n",
      "Epoch 4340/20000: Train Loss = 0.443042, Test Loss = 0.253933, Learning Rate = 1.922390e-04\n",
      "Epoch 4341/20000: Train Loss = 0.443681, Test Loss = 0.249534, Learning Rate = 1.921660e-04\n",
      "Epoch 4342/20000: Train Loss = 0.443271, Test Loss = 0.247844, Learning Rate = 1.920930e-04\n",
      "Epoch 4343/20000: Train Loss = 0.444297, Test Loss = 0.238959, Learning Rate = 1.920200e-04\n",
      "Epoch 4344/20000: Train Loss = 0.443390, Test Loss = 0.246903, Learning Rate = 1.919470e-04\n",
      "Epoch 4345/20000: Train Loss = 0.443545, Test Loss = 0.253385, Learning Rate = 1.918741e-04\n",
      "Epoch 4346/20000: Train Loss = 0.443533, Test Loss = 0.240643, Learning Rate = 1.918012e-04\n",
      "Epoch 4347/20000: Train Loss = 0.444107, Test Loss = 0.240306, Learning Rate = 1.917283e-04\n",
      "Epoch 4348/20000: Train Loss = 0.443116, Test Loss = 0.242240, Learning Rate = 1.916554e-04\n",
      "Epoch 4349/20000: Train Loss = 0.443062, Test Loss = 0.244543, Learning Rate = 1.915826e-04\n",
      "Epoch 4350/20000: Train Loss = 0.443635, Test Loss = 0.247003, Learning Rate = 1.915098e-04\n",
      "Epoch 4351/20000: Train Loss = 0.443020, Test Loss = 0.244847, Learning Rate = 1.914370e-04\n",
      "Epoch 4352/20000: Train Loss = 0.443486, Test Loss = 0.251754, Learning Rate = 1.913643e-04\n",
      "Epoch 4353/20000: Train Loss = 0.443397, Test Loss = 0.246231, Learning Rate = 1.912916e-04\n",
      "Epoch 4354/20000: Train Loss = 0.444005, Test Loss = 0.247866, Learning Rate = 1.912189e-04\n",
      "Epoch 4355/20000: Train Loss = 0.443330, Test Loss = 0.251351, Learning Rate = 1.911462e-04\n",
      "Epoch 4356/20000: Train Loss = 0.443236, Test Loss = 0.244767, Learning Rate = 1.910736e-04\n",
      "Epoch 4357/20000: Train Loss = 0.443948, Test Loss = 0.235866, Learning Rate = 1.910010e-04\n",
      "Epoch 4358/20000: Train Loss = 0.443872, Test Loss = 0.253273, Learning Rate = 1.909284e-04\n",
      "Epoch 4359/20000: Train Loss = 0.443200, Test Loss = 0.249987, Learning Rate = 1.908559e-04\n",
      "Epoch 4360/20000: Train Loss = 0.443481, Test Loss = 0.239909, Learning Rate = 1.907834e-04\n",
      "Epoch 4361/20000: Train Loss = 0.443474, Test Loss = 0.250679, Learning Rate = 1.907109e-04\n",
      "Epoch 4362/20000: Train Loss = 0.443566, Test Loss = 0.248436, Learning Rate = 1.906384e-04\n",
      "Epoch 4363/20000: Train Loss = 0.443637, Test Loss = 0.236269, Learning Rate = 1.905660e-04\n",
      "Epoch 4364/20000: Train Loss = 0.444139, Test Loss = 0.247236, Learning Rate = 1.904936e-04\n",
      "Epoch 4365/20000: Train Loss = 0.443340, Test Loss = 0.246688, Learning Rate = 1.904212e-04\n",
      "Epoch 4366/20000: Train Loss = 0.443588, Test Loss = 0.252464, Learning Rate = 1.903488e-04\n",
      "Epoch 4367/20000: Train Loss = 0.442871, Test Loss = 0.239068, Learning Rate = 1.902765e-04\n",
      "Epoch 4368/20000: Train Loss = 0.443247, Test Loss = 0.246243, Learning Rate = 1.902042e-04\n",
      "Epoch 4369/20000: Train Loss = 0.443587, Test Loss = 0.253028, Learning Rate = 1.901319e-04\n",
      "Epoch 4370/20000: Train Loss = 0.444977, Test Loss = 0.247395, Learning Rate = 1.900597e-04\n",
      "Epoch 4371/20000: Train Loss = 0.443091, Test Loss = 0.247420, Learning Rate = 1.899875e-04\n",
      "Epoch 4372/20000: Train Loss = 0.444027, Test Loss = 0.259106, Learning Rate = 1.899153e-04\n",
      "Epoch 4373/20000: Train Loss = 0.443830, Test Loss = 0.244380, Learning Rate = 1.898431e-04\n",
      "Epoch 4374/20000: Train Loss = 0.443439, Test Loss = 0.247566, Learning Rate = 1.897710e-04\n",
      "Epoch 4375/20000: Train Loss = 0.443568, Test Loss = 0.258640, Learning Rate = 1.896989e-04\n",
      "Epoch 4376/20000: Train Loss = 0.443852, Test Loss = 0.247344, Learning Rate = 1.896268e-04\n",
      "Epoch 4377/20000: Train Loss = 0.443597, Test Loss = 0.239219, Learning Rate = 1.895547e-04\n",
      "Epoch 4378/20000: Train Loss = 0.443266, Test Loss = 0.252676, Learning Rate = 1.894827e-04\n",
      "Epoch 4379/20000: Train Loss = 0.443042, Test Loss = 0.249364, Learning Rate = 1.894107e-04\n",
      "Epoch 4380/20000: Train Loss = 0.443207, Test Loss = 0.247286, Learning Rate = 1.893387e-04\n",
      "Epoch 4381/20000: Train Loss = 0.444112, Test Loss = 0.253572, Learning Rate = 1.892668e-04\n",
      "Epoch 4382/20000: Train Loss = 0.443275, Test Loss = 0.242265, Learning Rate = 1.891949e-04\n",
      "Epoch 4383/20000: Train Loss = 0.443240, Test Loss = 0.247652, Learning Rate = 1.891230e-04\n",
      "Epoch 4384/20000: Train Loss = 0.443412, Test Loss = 0.251767, Learning Rate = 1.890511e-04\n",
      "Epoch 4385/20000: Train Loss = 0.443476, Test Loss = 0.242452, Learning Rate = 1.889793e-04\n",
      "Epoch 4386/20000: Train Loss = 0.444907, Test Loss = 0.247499, Learning Rate = 1.889075e-04\n",
      "Epoch 4387/20000: Train Loss = 0.443562, Test Loss = 0.241318, Learning Rate = 1.888357e-04\n",
      "Epoch 4388/20000: Train Loss = 0.443268, Test Loss = 0.256204, Learning Rate = 1.887640e-04\n",
      "Epoch 4389/20000: Train Loss = 0.443976, Test Loss = 0.251289, Learning Rate = 1.886922e-04\n",
      "Epoch 4390/20000: Train Loss = 0.443014, Test Loss = 0.247532, Learning Rate = 1.886205e-04\n",
      "Epoch 4391/20000: Train Loss = 0.443726, Test Loss = 0.257367, Learning Rate = 1.885489e-04\n",
      "Epoch 4392/20000: Train Loss = 0.443267, Test Loss = 0.244096, Learning Rate = 1.884772e-04\n",
      "Epoch 4393/20000: Train Loss = 0.444228, Test Loss = 0.239201, Learning Rate = 1.884056e-04\n",
      "Epoch 4394/20000: Train Loss = 0.443600, Test Loss = 0.250470, Learning Rate = 1.883340e-04\n",
      "Epoch 4395/20000: Train Loss = 0.445212, Test Loss = 0.248978, Learning Rate = 1.882625e-04\n",
      "Epoch 4396/20000: Train Loss = 0.444620, Test Loss = 0.242450, Learning Rate = 1.881909e-04\n",
      "Epoch 4397/20000: Train Loss = 0.444106, Test Loss = 0.251444, Learning Rate = 1.881194e-04\n",
      "Epoch 4398/20000: Train Loss = 0.443415, Test Loss = 0.247652, Learning Rate = 1.880479e-04\n",
      "Epoch 4399/20000: Train Loss = 0.443653, Test Loss = 0.242500, Learning Rate = 1.879765e-04\n",
      "Epoch 4400/20000: Train Loss = 0.443417, Test Loss = 0.255300, Learning Rate = 1.879051e-04\n",
      "Epoch 4401/20000: Train Loss = 0.444173, Test Loss = 0.246504, Learning Rate = 1.878337e-04\n",
      "Epoch 4402/20000: Train Loss = 0.443148, Test Loss = 0.243893, Learning Rate = 1.877623e-04\n",
      "Epoch 4403/20000: Train Loss = 0.443164, Test Loss = 0.242949, Learning Rate = 1.876909e-04\n",
      "Epoch 4404/20000: Train Loss = 0.444437, Test Loss = 0.255341, Learning Rate = 1.876196e-04\n",
      "Epoch 4405/20000: Train Loss = 0.444682, Test Loss = 0.247173, Learning Rate = 1.875483e-04\n",
      "Epoch 4406/20000: Train Loss = 0.443756, Test Loss = 0.242474, Learning Rate = 1.874771e-04\n",
      "Epoch 4407/20000: Train Loss = 0.443015, Test Loss = 0.243543, Learning Rate = 1.874058e-04\n",
      "Epoch 4408/20000: Train Loss = 0.443029, Test Loss = 0.249589, Learning Rate = 1.873346e-04\n",
      "Epoch 4409/20000: Train Loss = 0.443558, Test Loss = 0.239336, Learning Rate = 1.872634e-04\n",
      "Epoch 4410/20000: Train Loss = 0.443173, Test Loss = 0.257442, Learning Rate = 1.871923e-04\n",
      "Epoch 4411/20000: Train Loss = 0.443337, Test Loss = 0.244039, Learning Rate = 1.871212e-04\n",
      "Epoch 4412/20000: Train Loss = 0.443221, Test Loss = 0.241826, Learning Rate = 1.870501e-04\n",
      "Epoch 4413/20000: Train Loss = 0.443822, Test Loss = 0.249502, Learning Rate = 1.869790e-04\n",
      "Epoch 4414/20000: Train Loss = 0.443272, Test Loss = 0.245670, Learning Rate = 1.869079e-04\n",
      "Epoch 4415/20000: Train Loss = 0.443149, Test Loss = 0.246343, Learning Rate = 1.868369e-04\n",
      "Epoch 4416/20000: Train Loss = 0.443214, Test Loss = 0.256916, Learning Rate = 1.867659e-04\n",
      "Epoch 4417/20000: Train Loss = 0.443556, Test Loss = 0.244637, Learning Rate = 1.866950e-04\n",
      "Epoch 4418/20000: Train Loss = 0.443365, Test Loss = 0.248431, Learning Rate = 1.866240e-04\n",
      "Epoch 4419/20000: Train Loss = 0.443819, Test Loss = 0.246859, Learning Rate = 1.865531e-04\n",
      "Epoch 4420/20000: Train Loss = 0.443422, Test Loss = 0.251714, Learning Rate = 1.864822e-04\n",
      "Epoch 4421/20000: Train Loss = 0.444009, Test Loss = 0.245794, Learning Rate = 1.864114e-04\n",
      "Epoch 4422/20000: Train Loss = 0.443182, Test Loss = 0.243003, Learning Rate = 1.863405e-04\n",
      "Epoch 4423/20000: Train Loss = 0.444114, Test Loss = 0.244484, Learning Rate = 1.862697e-04\n",
      "Epoch 4424/20000: Train Loss = 0.443968, Test Loss = 0.247761, Learning Rate = 1.861989e-04\n",
      "Epoch 4425/20000: Train Loss = 0.444125, Test Loss = 0.242770, Learning Rate = 1.861282e-04\n",
      "Epoch 4426/20000: Train Loss = 0.443083, Test Loss = 0.241246, Learning Rate = 1.860575e-04\n",
      "Epoch 4427/20000: Train Loss = 0.443027, Test Loss = 0.244900, Learning Rate = 1.859868e-04\n",
      "Epoch 4428/20000: Train Loss = 0.443201, Test Loss = 0.250478, Learning Rate = 1.859161e-04\n",
      "Epoch 4429/20000: Train Loss = 0.444218, Test Loss = 0.246606, Learning Rate = 1.858455e-04\n",
      "Epoch 4430/20000: Train Loss = 0.444408, Test Loss = 0.254349, Learning Rate = 1.857748e-04\n",
      "Epoch 4431/20000: Train Loss = 0.443223, Test Loss = 0.247258, Learning Rate = 1.857043e-04\n",
      "Epoch 4432/20000: Train Loss = 0.443628, Test Loss = 0.242387, Learning Rate = 1.856337e-04\n",
      "Epoch 4433/20000: Train Loss = 0.443491, Test Loss = 0.243743, Learning Rate = 1.855632e-04\n",
      "Epoch 4434/20000: Train Loss = 0.443812, Test Loss = 0.251470, Learning Rate = 1.854926e-04\n",
      "Epoch 4435/20000: Train Loss = 0.444213, Test Loss = 0.250027, Learning Rate = 1.854222e-04\n",
      "Epoch 4436/20000: Train Loss = 0.444291, Test Loss = 0.248479, Learning Rate = 1.853517e-04\n",
      "Epoch 4437/20000: Train Loss = 0.443680, Test Loss = 0.247837, Learning Rate = 1.852813e-04\n",
      "Epoch 4438/20000: Train Loss = 0.443642, Test Loss = 0.233757, Learning Rate = 1.852109e-04\n",
      "Epoch 4439/20000: Train Loss = 0.443400, Test Loss = 0.242866, Learning Rate = 1.851405e-04\n",
      "Epoch 4440/20000: Train Loss = 0.443863, Test Loss = 0.249475, Learning Rate = 1.850702e-04\n",
      "Epoch 4441/20000: Train Loss = 0.442890, Test Loss = 0.240335, Learning Rate = 1.849998e-04\n",
      "Epoch 4442/20000: Train Loss = 0.442959, Test Loss = 0.242237, Learning Rate = 1.849295e-04\n",
      "Epoch 4443/20000: Train Loss = 0.443652, Test Loss = 0.246905, Learning Rate = 1.848593e-04\n",
      "Epoch 4444/20000: Train Loss = 0.443520, Test Loss = 0.245950, Learning Rate = 1.847890e-04\n",
      "Epoch 4445/20000: Train Loss = 0.443793, Test Loss = 0.244170, Learning Rate = 1.847188e-04\n",
      "Epoch 4446/20000: Train Loss = 0.443521, Test Loss = 0.243243, Learning Rate = 1.846486e-04\n",
      "Epoch 4447/20000: Train Loss = 0.443024, Test Loss = 0.247561, Learning Rate = 1.845785e-04\n",
      "Epoch 4448/20000: Train Loss = 0.443359, Test Loss = 0.249203, Learning Rate = 1.845083e-04\n",
      "Epoch 4449/20000: Train Loss = 0.443091, Test Loss = 0.244558, Learning Rate = 1.844382e-04\n",
      "Epoch 4450/20000: Train Loss = 0.443539, Test Loss = 0.256150, Learning Rate = 1.843681e-04\n",
      "Epoch 4451/20000: Train Loss = 0.443379, Test Loss = 0.247175, Learning Rate = 1.842981e-04\n",
      "Epoch 4452/20000: Train Loss = 0.443472, Test Loss = 0.253129, Learning Rate = 1.842281e-04\n",
      "Epoch 4453/20000: Train Loss = 0.442877, Test Loss = 0.245574, Learning Rate = 1.841581e-04\n",
      "Epoch 4454/20000: Train Loss = 0.443342, Test Loss = 0.246129, Learning Rate = 1.840881e-04\n",
      "Epoch 4455/20000: Train Loss = 0.442896, Test Loss = 0.248216, Learning Rate = 1.840181e-04\n",
      "Epoch 4456/20000: Train Loss = 0.443937, Test Loss = 0.248166, Learning Rate = 1.839482e-04\n",
      "Epoch 4457/20000: Train Loss = 0.443639, Test Loss = 0.251837, Learning Rate = 1.838783e-04\n",
      "Epoch 4458/20000: Train Loss = 0.444541, Test Loss = 0.245931, Learning Rate = 1.838084e-04\n",
      "Epoch 4459/20000: Train Loss = 0.443639, Test Loss = 0.240495, Learning Rate = 1.837386e-04\n",
      "Epoch 4460/20000: Train Loss = 0.443803, Test Loss = 0.248128, Learning Rate = 1.836688e-04\n",
      "Epoch 4461/20000: Train Loss = 0.443093, Test Loss = 0.253673, Learning Rate = 1.835990e-04\n",
      "Epoch 4462/20000: Train Loss = 0.443459, Test Loss = 0.249639, Learning Rate = 1.835292e-04\n",
      "Epoch 4463/20000: Train Loss = 0.443332, Test Loss = 0.245548, Learning Rate = 1.834595e-04\n",
      "Epoch 4464/20000: Train Loss = 0.443342, Test Loss = 0.244212, Learning Rate = 1.833898e-04\n",
      "Epoch 4465/20000: Train Loss = 0.443485, Test Loss = 0.237477, Learning Rate = 1.833201e-04\n",
      "Epoch 4466/20000: Train Loss = 0.443405, Test Loss = 0.245660, Learning Rate = 1.832505e-04\n",
      "Epoch 4467/20000: Train Loss = 0.443688, Test Loss = 0.252821, Learning Rate = 1.831808e-04\n",
      "Epoch 4468/20000: Train Loss = 0.443410, Test Loss = 0.242334, Learning Rate = 1.831112e-04\n",
      "Epoch 4469/20000: Train Loss = 0.443192, Test Loss = 0.250064, Learning Rate = 1.830416e-04\n",
      "Epoch 4470/20000: Train Loss = 0.443214, Test Loss = 0.246694, Learning Rate = 1.829721e-04\n",
      "Epoch 4471/20000: Train Loss = 0.443838, Test Loss = 0.256929, Learning Rate = 1.829026e-04\n",
      "Epoch 4472/20000: Train Loss = 0.444049, Test Loss = 0.243914, Learning Rate = 1.828331e-04\n",
      "Epoch 4473/20000: Train Loss = 0.443128, Test Loss = 0.245421, Learning Rate = 1.827636e-04\n",
      "Epoch 4474/20000: Train Loss = 0.443855, Test Loss = 0.241858, Learning Rate = 1.826942e-04\n",
      "Epoch 4475/20000: Train Loss = 0.444195, Test Loss = 0.244119, Learning Rate = 1.826247e-04\n",
      "Epoch 4476/20000: Train Loss = 0.443256, Test Loss = 0.244190, Learning Rate = 1.825553e-04\n",
      "Epoch 4477/20000: Train Loss = 0.444177, Test Loss = 0.252278, Learning Rate = 1.824860e-04\n",
      "Epoch 4478/20000: Train Loss = 0.445584, Test Loss = 0.250412, Learning Rate = 1.824166e-04\n",
      "Epoch 4479/20000: Train Loss = 0.444055, Test Loss = 0.251266, Learning Rate = 1.823473e-04\n",
      "Epoch 4480/20000: Train Loss = 0.442895, Test Loss = 0.245404, Learning Rate = 1.822780e-04\n",
      "Epoch 4481/20000: Train Loss = 0.444383, Test Loss = 0.244534, Learning Rate = 1.822088e-04\n",
      "Epoch 4482/20000: Train Loss = 0.443730, Test Loss = 0.245533, Learning Rate = 1.821395e-04\n",
      "Epoch 4483/20000: Train Loss = 0.443842, Test Loss = 0.243434, Learning Rate = 1.820703e-04\n",
      "Epoch 4484/20000: Train Loss = 0.443234, Test Loss = 0.241517, Learning Rate = 1.820011e-04\n",
      "Epoch 4485/20000: Train Loss = 0.444509, Test Loss = 0.251502, Learning Rate = 1.819320e-04\n",
      "Epoch 4486/20000: Train Loss = 0.443473, Test Loss = 0.234161, Learning Rate = 1.818629e-04\n",
      "Epoch 4487/20000: Train Loss = 0.443365, Test Loss = 0.249647, Learning Rate = 1.817938e-04\n",
      "Epoch 4488/20000: Train Loss = 0.443188, Test Loss = 0.247462, Learning Rate = 1.817247e-04\n",
      "Epoch 4489/20000: Train Loss = 0.443505, Test Loss = 0.247505, Learning Rate = 1.816556e-04\n",
      "Epoch 4490/20000: Train Loss = 0.442799, Test Loss = 0.242739, Learning Rate = 1.815866e-04\n",
      "Epoch 4491/20000: Train Loss = 0.443276, Test Loss = 0.244766, Learning Rate = 1.815176e-04\n",
      "Epoch 4492/20000: Train Loss = 0.443618, Test Loss = 0.253209, Learning Rate = 1.814486e-04\n",
      "Epoch 4493/20000: Train Loss = 0.443197, Test Loss = 0.240701, Learning Rate = 1.813797e-04\n",
      "Epoch 4494/20000: Train Loss = 0.444058, Test Loss = 0.244031, Learning Rate = 1.813108e-04\n",
      "Epoch 4495/20000: Train Loss = 0.443032, Test Loss = 0.244277, Learning Rate = 1.812419e-04\n",
      "Epoch 4496/20000: Train Loss = 0.443332, Test Loss = 0.249213, Learning Rate = 1.811730e-04\n",
      "Epoch 4497/20000: Train Loss = 0.443384, Test Loss = 0.240055, Learning Rate = 1.811042e-04\n",
      "Epoch 4498/20000: Train Loss = 0.442985, Test Loss = 0.241086, Learning Rate = 1.810354e-04\n",
      "Epoch 4499/20000: Train Loss = 0.443633, Test Loss = 0.256041, Learning Rate = 1.809666e-04\n",
      "Epoch 4500/20000: Train Loss = 0.443724, Test Loss = 0.238792, Learning Rate = 1.808978e-04\n",
      "Epoch 4501/20000: Train Loss = 0.444826, Test Loss = 0.256727, Learning Rate = 1.808291e-04\n",
      "Epoch 4502/20000: Train Loss = 0.445582, Test Loss = 0.242321, Learning Rate = 1.807604e-04\n",
      "Epoch 4503/20000: Train Loss = 0.443411, Test Loss = 0.252759, Learning Rate = 1.806917e-04\n",
      "Epoch 4504/20000: Train Loss = 0.443215, Test Loss = 0.251466, Learning Rate = 1.806230e-04\n",
      "Epoch 4505/20000: Train Loss = 0.443290, Test Loss = 0.245327, Learning Rate = 1.805544e-04\n",
      "Epoch 4506/20000: Train Loss = 0.443815, Test Loss = 0.248914, Learning Rate = 1.804858e-04\n",
      "Epoch 4507/20000: Train Loss = 0.443417, Test Loss = 0.254380, Learning Rate = 1.804172e-04\n",
      "Epoch 4508/20000: Train Loss = 0.442850, Test Loss = 0.241605, Learning Rate = 1.803486e-04\n",
      "Epoch 4509/20000: Train Loss = 0.443026, Test Loss = 0.245895, Learning Rate = 1.802801e-04\n",
      "Epoch 4510/20000: Train Loss = 0.442901, Test Loss = 0.246353, Learning Rate = 1.802116e-04\n",
      "Epoch 4511/20000: Train Loss = 0.442904, Test Loss = 0.243370, Learning Rate = 1.801431e-04\n",
      "Epoch 4512/20000: Train Loss = 0.443511, Test Loss = 0.240751, Learning Rate = 1.800747e-04\n",
      "Epoch 4513/20000: Train Loss = 0.443638, Test Loss = 0.240089, Learning Rate = 1.800063e-04\n",
      "Epoch 4514/20000: Train Loss = 0.443862, Test Loss = 0.249184, Learning Rate = 1.799379e-04\n",
      "Epoch 4515/20000: Train Loss = 0.443317, Test Loss = 0.243568, Learning Rate = 1.798695e-04\n",
      "Epoch 4516/20000: Train Loss = 0.443759, Test Loss = 0.242011, Learning Rate = 1.798012e-04\n",
      "Epoch 4517/20000: Train Loss = 0.443245, Test Loss = 0.251135, Learning Rate = 1.797328e-04\n",
      "Epoch 4518/20000: Train Loss = 0.443325, Test Loss = 0.246353, Learning Rate = 1.796645e-04\n",
      "Epoch 4519/20000: Train Loss = 0.442925, Test Loss = 0.249670, Learning Rate = 1.795963e-04\n",
      "Epoch 4520/20000: Train Loss = 0.443357, Test Loss = 0.248630, Learning Rate = 1.795280e-04\n",
      "Epoch 4521/20000: Train Loss = 0.443970, Test Loss = 0.247749, Learning Rate = 1.794598e-04\n",
      "Epoch 4522/20000: Train Loss = 0.444072, Test Loss = 0.241178, Learning Rate = 1.793916e-04\n",
      "Epoch 4523/20000: Train Loss = 0.443310, Test Loss = 0.246679, Learning Rate = 1.793235e-04\n",
      "Epoch 4524/20000: Train Loss = 0.443047, Test Loss = 0.250348, Learning Rate = 1.792553e-04\n",
      "Epoch 4525/20000: Train Loss = 0.444633, Test Loss = 0.247188, Learning Rate = 1.791872e-04\n",
      "Epoch 4526/20000: Train Loss = 0.443402, Test Loss = 0.237508, Learning Rate = 1.791191e-04\n",
      "Epoch 4527/20000: Train Loss = 0.443860, Test Loss = 0.243578, Learning Rate = 1.790511e-04\n",
      "Epoch 4528/20000: Train Loss = 0.443796, Test Loss = 0.246717, Learning Rate = 1.789830e-04\n",
      "Epoch 4529/20000: Train Loss = 0.443487, Test Loss = 0.244632, Learning Rate = 1.789150e-04\n",
      "Epoch 4530/20000: Train Loss = 0.443198, Test Loss = 0.244042, Learning Rate = 1.788470e-04\n",
      "Epoch 4531/20000: Train Loss = 0.443437, Test Loss = 0.248219, Learning Rate = 1.787791e-04\n",
      "Epoch 4532/20000: Train Loss = 0.442998, Test Loss = 0.244706, Learning Rate = 1.787112e-04\n",
      "Epoch 4533/20000: Train Loss = 0.443998, Test Loss = 0.242242, Learning Rate = 1.786432e-04\n",
      "Epoch 4534/20000: Train Loss = 0.443556, Test Loss = 0.244602, Learning Rate = 1.785754e-04\n",
      "Epoch 4535/20000: Train Loss = 0.443968, Test Loss = 0.247201, Learning Rate = 1.785075e-04\n",
      "Epoch 4536/20000: Train Loss = 0.444093, Test Loss = 0.253272, Learning Rate = 1.784397e-04\n",
      "Epoch 4537/20000: Train Loss = 0.443761, Test Loss = 0.245950, Learning Rate = 1.783719e-04\n",
      "Epoch 4538/20000: Train Loss = 0.443835, Test Loss = 0.243598, Learning Rate = 1.783041e-04\n",
      "Epoch 4539/20000: Train Loss = 0.443226, Test Loss = 0.246967, Learning Rate = 1.782364e-04\n",
      "Epoch 4540/20000: Train Loss = 0.443048, Test Loss = 0.242368, Learning Rate = 1.781686e-04\n",
      "Epoch 4541/20000: Train Loss = 0.443506, Test Loss = 0.248553, Learning Rate = 1.781009e-04\n",
      "Epoch 4542/20000: Train Loss = 0.443683, Test Loss = 0.244815, Learning Rate = 1.780333e-04\n",
      "Epoch 4543/20000: Train Loss = 0.443319, Test Loss = 0.249586, Learning Rate = 1.779656e-04\n",
      "Epoch 4544/20000: Train Loss = 0.443945, Test Loss = 0.243030, Learning Rate = 1.778980e-04\n",
      "Epoch 4545/20000: Train Loss = 0.445365, Test Loss = 0.252612, Learning Rate = 1.778304e-04\n",
      "Epoch 4546/20000: Train Loss = 0.443844, Test Loss = 0.244707, Learning Rate = 1.777628e-04\n",
      "Epoch 4547/20000: Train Loss = 0.443433, Test Loss = 0.249608, Learning Rate = 1.776953e-04\n",
      "Epoch 4548/20000: Train Loss = 0.443033, Test Loss = 0.248899, Learning Rate = 1.776278e-04\n",
      "Epoch 4549/20000: Train Loss = 0.443698, Test Loss = 0.247669, Learning Rate = 1.775603e-04\n",
      "Epoch 4550/20000: Train Loss = 0.443773, Test Loss = 0.255046, Learning Rate = 1.774928e-04\n",
      "Epoch 4551/20000: Train Loss = 0.444359, Test Loss = 0.243271, Learning Rate = 1.774254e-04\n",
      "Epoch 4552/20000: Train Loss = 0.443328, Test Loss = 0.249280, Learning Rate = 1.773579e-04\n",
      "Epoch 4553/20000: Train Loss = 0.443045, Test Loss = 0.248418, Learning Rate = 1.772905e-04\n",
      "Epoch 4554/20000: Train Loss = 0.443177, Test Loss = 0.245637, Learning Rate = 1.772232e-04\n",
      "Epoch 4555/20000: Train Loss = 0.443440, Test Loss = 0.248653, Learning Rate = 1.771558e-04\n",
      "Epoch 4556/20000: Train Loss = 0.443486, Test Loss = 0.251797, Learning Rate = 1.770885e-04\n",
      "Epoch 4557/20000: Train Loss = 0.444247, Test Loss = 0.243980, Learning Rate = 1.770212e-04\n",
      "Epoch 4558/20000: Train Loss = 0.443245, Test Loss = 0.243375, Learning Rate = 1.769540e-04\n",
      "Epoch 4559/20000: Train Loss = 0.443043, Test Loss = 0.248694, Learning Rate = 1.768867e-04\n",
      "Epoch 4560/20000: Train Loss = 0.443721, Test Loss = 0.243103, Learning Rate = 1.768195e-04\n",
      "Epoch 4561/20000: Train Loss = 0.443598, Test Loss = 0.248402, Learning Rate = 1.767523e-04\n",
      "Epoch 4562/20000: Train Loss = 0.442991, Test Loss = 0.242842, Learning Rate = 1.766852e-04\n",
      "Epoch 4563/20000: Train Loss = 0.444238, Test Loss = 0.252561, Learning Rate = 1.766180e-04\n",
      "Epoch 4564/20000: Train Loss = 0.443809, Test Loss = 0.237389, Learning Rate = 1.765509e-04\n",
      "Epoch 4565/20000: Train Loss = 0.443041, Test Loss = 0.248685, Learning Rate = 1.764838e-04\n",
      "Epoch 4566/20000: Train Loss = 0.443063, Test Loss = 0.246404, Learning Rate = 1.764168e-04\n",
      "Epoch 4567/20000: Train Loss = 0.444094, Test Loss = 0.241869, Learning Rate = 1.763498e-04\n",
      "Epoch 4568/20000: Train Loss = 0.444397, Test Loss = 0.247935, Learning Rate = 1.762827e-04\n",
      "Epoch 4569/20000: Train Loss = 0.443385, Test Loss = 0.244738, Learning Rate = 1.762158e-04\n",
      "Epoch 4570/20000: Train Loss = 0.443161, Test Loss = 0.245130, Learning Rate = 1.761488e-04\n",
      "Epoch 4571/20000: Train Loss = 0.443118, Test Loss = 0.247912, Learning Rate = 1.760819e-04\n",
      "Epoch 4572/20000: Train Loss = 0.443469, Test Loss = 0.245060, Learning Rate = 1.760150e-04\n",
      "Epoch 4573/20000: Train Loss = 0.443722, Test Loss = 0.250261, Learning Rate = 1.759481e-04\n",
      "Epoch 4574/20000: Train Loss = 0.442984, Test Loss = 0.239196, Learning Rate = 1.758812e-04\n",
      "Epoch 4575/20000: Train Loss = 0.443865, Test Loss = 0.244784, Learning Rate = 1.758144e-04\n",
      "Epoch 4576/20000: Train Loss = 0.444389, Test Loss = 0.243972, Learning Rate = 1.757476e-04\n",
      "Epoch 4577/20000: Train Loss = 0.443450, Test Loss = 0.247129, Learning Rate = 1.756808e-04\n",
      "Epoch 4578/20000: Train Loss = 0.442843, Test Loss = 0.241275, Learning Rate = 1.756141e-04\n",
      "Epoch 4579/20000: Train Loss = 0.443093, Test Loss = 0.246862, Learning Rate = 1.755473e-04\n",
      "Epoch 4580/20000: Train Loss = 0.442755, Test Loss = 0.241743, Learning Rate = 1.754806e-04\n",
      "Epoch 4581/20000: Train Loss = 0.442917, Test Loss = 0.244747, Learning Rate = 1.754140e-04\n",
      "Epoch 4582/20000: Train Loss = 0.444549, Test Loss = 0.250662, Learning Rate = 1.753473e-04\n",
      "Epoch 4583/20000: Train Loss = 0.444226, Test Loss = 0.239421, Learning Rate = 1.752807e-04\n",
      "Epoch 4584/20000: Train Loss = 0.443520, Test Loss = 0.249196, Learning Rate = 1.752141e-04\n",
      "Epoch 4585/20000: Train Loss = 0.443446, Test Loss = 0.245121, Learning Rate = 1.751475e-04\n",
      "Epoch 4586/20000: Train Loss = 0.443079, Test Loss = 0.247394, Learning Rate = 1.750809e-04\n",
      "Epoch 4587/20000: Train Loss = 0.443372, Test Loss = 0.244875, Learning Rate = 1.750144e-04\n",
      "Epoch 4588/20000: Train Loss = 0.443335, Test Loss = 0.244752, Learning Rate = 1.749479e-04\n",
      "Epoch 4589/20000: Train Loss = 0.443043, Test Loss = 0.239840, Learning Rate = 1.748814e-04\n",
      "Epoch 4590/20000: Train Loss = 0.443207, Test Loss = 0.240560, Learning Rate = 1.748150e-04\n",
      "Epoch 4591/20000: Train Loss = 0.442919, Test Loss = 0.244772, Learning Rate = 1.747486e-04\n",
      "Epoch 4592/20000: Train Loss = 0.443261, Test Loss = 0.251015, Learning Rate = 1.746822e-04\n",
      "Epoch 4593/20000: Train Loss = 0.443969, Test Loss = 0.243561, Learning Rate = 1.746158e-04\n",
      "Epoch 4594/20000: Train Loss = 0.443557, Test Loss = 0.248170, Learning Rate = 1.745494e-04\n",
      "Epoch 4595/20000: Train Loss = 0.443531, Test Loss = 0.246092, Learning Rate = 1.744831e-04\n",
      "Epoch 4596/20000: Train Loss = 0.443669, Test Loss = 0.243040, Learning Rate = 1.744168e-04\n",
      "Epoch 4597/20000: Train Loss = 0.443335, Test Loss = 0.250303, Learning Rate = 1.743505e-04\n",
      "Epoch 4598/20000: Train Loss = 0.443136, Test Loss = 0.241814, Learning Rate = 1.742843e-04\n",
      "Epoch 4599/20000: Train Loss = 0.443318, Test Loss = 0.242365, Learning Rate = 1.742181e-04\n",
      "Epoch 4600/20000: Train Loss = 0.443537, Test Loss = 0.242371, Learning Rate = 1.741519e-04\n",
      "Epoch 4601/20000: Train Loss = 0.444400, Test Loss = 0.241477, Learning Rate = 1.740857e-04\n",
      "Epoch 4602/20000: Train Loss = 0.443115, Test Loss = 0.253323, Learning Rate = 1.740196e-04\n",
      "Epoch 4603/20000: Train Loss = 0.443238, Test Loss = 0.243342, Learning Rate = 1.739534e-04\n",
      "Epoch 4604/20000: Train Loss = 0.443543, Test Loss = 0.242638, Learning Rate = 1.738873e-04\n",
      "Epoch 4605/20000: Train Loss = 0.443060, Test Loss = 0.243756, Learning Rate = 1.738213e-04\n",
      "Epoch 4606/20000: Train Loss = 0.442784, Test Loss = 0.244907, Learning Rate = 1.737552e-04\n",
      "Epoch 4607/20000: Train Loss = 0.443128, Test Loss = 0.248857, Learning Rate = 1.736892e-04\n",
      "Epoch 4608/20000: Train Loss = 0.444389, Test Loss = 0.253859, Learning Rate = 1.736232e-04\n",
      "Epoch 4609/20000: Train Loss = 0.444110, Test Loss = 0.248793, Learning Rate = 1.735572e-04\n",
      "Epoch 4610/20000: Train Loss = 0.443024, Test Loss = 0.247221, Learning Rate = 1.734913e-04\n",
      "Epoch 4611/20000: Train Loss = 0.443060, Test Loss = 0.246372, Learning Rate = 1.734254e-04\n",
      "Epoch 4612/20000: Train Loss = 0.443084, Test Loss = 0.248689, Learning Rate = 1.733595e-04\n",
      "Epoch 4613/20000: Train Loss = 0.443328, Test Loss = 0.253186, Learning Rate = 1.732936e-04\n",
      "Epoch 4614/20000: Train Loss = 0.443067, Test Loss = 0.246940, Learning Rate = 1.732277e-04\n",
      "Epoch 4615/20000: Train Loss = 0.443364, Test Loss = 0.246698, Learning Rate = 1.731619e-04\n",
      "Epoch 4616/20000: Train Loss = 0.443461, Test Loss = 0.247060, Learning Rate = 1.730961e-04\n",
      "Epoch 4617/20000: Train Loss = 0.444014, Test Loss = 0.241024, Learning Rate = 1.730303e-04\n",
      "Epoch 4618/20000: Train Loss = 0.443097, Test Loss = 0.242730, Learning Rate = 1.729646e-04\n",
      "Epoch 4619/20000: Train Loss = 0.443461, Test Loss = 0.245413, Learning Rate = 1.728989e-04\n",
      "Epoch 4620/20000: Train Loss = 0.443735, Test Loss = 0.237839, Learning Rate = 1.728332e-04\n",
      "Epoch 4621/20000: Train Loss = 0.443401, Test Loss = 0.235179, Learning Rate = 1.727675e-04\n",
      "Epoch 4622/20000: Train Loss = 0.443932, Test Loss = 0.243197, Learning Rate = 1.727019e-04\n",
      "Epoch 4623/20000: Train Loss = 0.443705, Test Loss = 0.242279, Learning Rate = 1.726362e-04\n",
      "Epoch 4624/20000: Train Loss = 0.443698, Test Loss = 0.238216, Learning Rate = 1.725706e-04\n",
      "Epoch 4625/20000: Train Loss = 0.443136, Test Loss = 0.250488, Learning Rate = 1.725051e-04\n",
      "Epoch 4626/20000: Train Loss = 0.443959, Test Loss = 0.242865, Learning Rate = 1.724395e-04\n",
      "Epoch 4627/20000: Train Loss = 0.443059, Test Loss = 0.248239, Learning Rate = 1.723740e-04\n",
      "Epoch 4628/20000: Train Loss = 0.443563, Test Loss = 0.250485, Learning Rate = 1.723085e-04\n",
      "Epoch 4629/20000: Train Loss = 0.444031, Test Loss = 0.254589, Learning Rate = 1.722430e-04\n",
      "Epoch 4630/20000: Train Loss = 0.444230, Test Loss = 0.239752, Learning Rate = 1.721776e-04\n",
      "Epoch 4631/20000: Train Loss = 0.444222, Test Loss = 0.246220, Learning Rate = 1.721122e-04\n",
      "Epoch 4632/20000: Train Loss = 0.443615, Test Loss = 0.246070, Learning Rate = 1.720468e-04\n",
      "Epoch 4633/20000: Train Loss = 0.443205, Test Loss = 0.247491, Learning Rate = 1.719814e-04\n",
      "Epoch 4634/20000: Train Loss = 0.443298, Test Loss = 0.248044, Learning Rate = 1.719160e-04\n",
      "Epoch 4635/20000: Train Loss = 0.443292, Test Loss = 0.239834, Learning Rate = 1.718507e-04\n",
      "Epoch 4636/20000: Train Loss = 0.443071, Test Loss = 0.245783, Learning Rate = 1.717854e-04\n",
      "Epoch 4637/20000: Train Loss = 0.443175, Test Loss = 0.245809, Learning Rate = 1.717201e-04\n",
      "Epoch 4638/20000: Train Loss = 0.443210, Test Loss = 0.243476, Learning Rate = 1.716549e-04\n",
      "Epoch 4639/20000: Train Loss = 0.443302, Test Loss = 0.239172, Learning Rate = 1.715897e-04\n",
      "Epoch 4640/20000: Train Loss = 0.442715, Test Loss = 0.244444, Learning Rate = 1.715245e-04\n",
      "Epoch 4641/20000: Train Loss = 0.442795, Test Loss = 0.251805, Learning Rate = 1.714593e-04\n",
      "Epoch 4642/20000: Train Loss = 0.443348, Test Loss = 0.242976, Learning Rate = 1.713941e-04\n",
      "Epoch 4643/20000: Train Loss = 0.444065, Test Loss = 0.252222, Learning Rate = 1.713290e-04\n",
      "Epoch 4644/20000: Train Loss = 0.442933, Test Loss = 0.241212, Learning Rate = 1.712639e-04\n",
      "Epoch 4645/20000: Train Loss = 0.443533, Test Loss = 0.247163, Learning Rate = 1.711988e-04\n",
      "Epoch 4646/20000: Train Loss = 0.443779, Test Loss = 0.246756, Learning Rate = 1.711338e-04\n",
      "Epoch 4647/20000: Train Loss = 0.443524, Test Loss = 0.248508, Learning Rate = 1.710688e-04\n",
      "Epoch 4648/20000: Train Loss = 0.443300, Test Loss = 0.252384, Learning Rate = 1.710038e-04\n",
      "Epoch 4649/20000: Train Loss = 0.442921, Test Loss = 0.246948, Learning Rate = 1.709388e-04\n",
      "Epoch 4650/20000: Train Loss = 0.443254, Test Loss = 0.242492, Learning Rate = 1.708738e-04\n",
      "Epoch 4651/20000: Train Loss = 0.443408, Test Loss = 0.249880, Learning Rate = 1.708089e-04\n",
      "Epoch 4652/20000: Train Loss = 0.442703, Test Loss = 0.247872, Learning Rate = 1.707440e-04\n",
      "Epoch 4653/20000: Train Loss = 0.443137, Test Loss = 0.246521, Learning Rate = 1.706791e-04\n",
      "Epoch 4654/20000: Train Loss = 0.443154, Test Loss = 0.245187, Learning Rate = 1.706143e-04\n",
      "Epoch 4655/20000: Train Loss = 0.442850, Test Loss = 0.251666, Learning Rate = 1.705494e-04\n",
      "Epoch 4656/20000: Train Loss = 0.443499, Test Loss = 0.249838, Learning Rate = 1.704846e-04\n",
      "Epoch 4657/20000: Train Loss = 0.443531, Test Loss = 0.256167, Learning Rate = 1.704199e-04\n",
      "Epoch 4658/20000: Train Loss = 0.442969, Test Loss = 0.243762, Learning Rate = 1.703551e-04\n",
      "Epoch 4659/20000: Train Loss = 0.444025, Test Loss = 0.241436, Learning Rate = 1.702904e-04\n",
      "Epoch 4660/20000: Train Loss = 0.443455, Test Loss = 0.251273, Learning Rate = 1.702257e-04\n",
      "Epoch 4661/20000: Train Loss = 0.443797, Test Loss = 0.247609, Learning Rate = 1.701610e-04\n",
      "Epoch 4662/20000: Train Loss = 0.445549, Test Loss = 0.239963, Learning Rate = 1.700963e-04\n",
      "Epoch 4663/20000: Train Loss = 0.444148, Test Loss = 0.238157, Learning Rate = 1.700317e-04\n",
      "Epoch 4664/20000: Train Loss = 0.443554, Test Loss = 0.241985, Learning Rate = 1.699671e-04\n",
      "Epoch 4665/20000: Train Loss = 0.443788, Test Loss = 0.241278, Learning Rate = 1.699025e-04\n",
      "Epoch 4666/20000: Train Loss = 0.443200, Test Loss = 0.245019, Learning Rate = 1.698380e-04\n",
      "Epoch 4667/20000: Train Loss = 0.444358, Test Loss = 0.248157, Learning Rate = 1.697734e-04\n",
      "Epoch 4668/20000: Train Loss = 0.443234, Test Loss = 0.240957, Learning Rate = 1.697089e-04\n",
      "Epoch 4669/20000: Train Loss = 0.443457, Test Loss = 0.241181, Learning Rate = 1.696444e-04\n",
      "Epoch 4670/20000: Train Loss = 0.443212, Test Loss = 0.241629, Learning Rate = 1.695800e-04\n",
      "Epoch 4671/20000: Train Loss = 0.443686, Test Loss = 0.248176, Learning Rate = 1.695155e-04\n",
      "Epoch 4672/20000: Train Loss = 0.442926, Test Loss = 0.240850, Learning Rate = 1.694511e-04\n",
      "Epoch 4673/20000: Train Loss = 0.443159, Test Loss = 0.241909, Learning Rate = 1.693867e-04\n",
      "Epoch 4674/20000: Train Loss = 0.443829, Test Loss = 0.237346, Learning Rate = 1.693224e-04\n",
      "Epoch 4675/20000: Train Loss = 0.443589, Test Loss = 0.249987, Learning Rate = 1.692580e-04\n",
      "Epoch 4676/20000: Train Loss = 0.443193, Test Loss = 0.246420, Learning Rate = 1.691937e-04\n",
      "Epoch 4677/20000: Train Loss = 0.442969, Test Loss = 0.246601, Learning Rate = 1.691294e-04\n",
      "Epoch 4678/20000: Train Loss = 0.444439, Test Loss = 0.255308, Learning Rate = 1.690652e-04\n",
      "Epoch 4679/20000: Train Loss = 0.442831, Test Loss = 0.245804, Learning Rate = 1.690009e-04\n",
      "Epoch 4680/20000: Train Loss = 0.445032, Test Loss = 0.239870, Learning Rate = 1.689367e-04\n",
      "Epoch 4681/20000: Train Loss = 0.442960, Test Loss = 0.249673, Learning Rate = 1.688725e-04\n",
      "Epoch 4682/20000: Train Loss = 0.443047, Test Loss = 0.246794, Learning Rate = 1.688084e-04\n",
      "Epoch 4683/20000: Train Loss = 0.443533, Test Loss = 0.248789, Learning Rate = 1.687442e-04\n",
      "Epoch 4684/20000: Train Loss = 0.443250, Test Loss = 0.247674, Learning Rate = 1.686801e-04\n",
      "Epoch 4685/20000: Train Loss = 0.443558, Test Loss = 0.249324, Learning Rate = 1.686160e-04\n",
      "Epoch 4686/20000: Train Loss = 0.443267, Test Loss = 0.245977, Learning Rate = 1.685519e-04\n",
      "Epoch 4687/20000: Train Loss = 0.443302, Test Loss = 0.246108, Learning Rate = 1.684879e-04\n",
      "Epoch 4688/20000: Train Loss = 0.443382, Test Loss = 0.247059, Learning Rate = 1.684239e-04\n",
      "Epoch 4689/20000: Train Loss = 0.444099, Test Loss = 0.247837, Learning Rate = 1.683599e-04\n",
      "Epoch 4690/20000: Train Loss = 0.443585, Test Loss = 0.248135, Learning Rate = 1.682959e-04\n",
      "Epoch 4691/20000: Train Loss = 0.443452, Test Loss = 0.247147, Learning Rate = 1.682319e-04\n",
      "Epoch 4692/20000: Train Loss = 0.443591, Test Loss = 0.246885, Learning Rate = 1.681680e-04\n",
      "Epoch 4693/20000: Train Loss = 0.443131, Test Loss = 0.249683, Learning Rate = 1.681041e-04\n",
      "Epoch 4694/20000: Train Loss = 0.444047, Test Loss = 0.252690, Learning Rate = 1.680402e-04\n",
      "Epoch 4695/20000: Train Loss = 0.443182, Test Loss = 0.251550, Learning Rate = 1.679764e-04\n",
      "Epoch 4696/20000: Train Loss = 0.443407, Test Loss = 0.249684, Learning Rate = 1.679126e-04\n",
      "Epoch 4697/20000: Train Loss = 0.443328, Test Loss = 0.249686, Learning Rate = 1.678488e-04\n",
      "Epoch 4698/20000: Train Loss = 0.442923, Test Loss = 0.245666, Learning Rate = 1.677850e-04\n",
      "Epoch 4699/20000: Train Loss = 0.443244, Test Loss = 0.242346, Learning Rate = 1.677212e-04\n",
      "Epoch 4700/20000: Train Loss = 0.443546, Test Loss = 0.243888, Learning Rate = 1.676575e-04\n",
      "Epoch 4701/20000: Train Loss = 0.443401, Test Loss = 0.241188, Learning Rate = 1.675938e-04\n",
      "Epoch 4702/20000: Train Loss = 0.443061, Test Loss = 0.243570, Learning Rate = 1.675301e-04\n",
      "Epoch 4703/20000: Train Loss = 0.443064, Test Loss = 0.248954, Learning Rate = 1.674665e-04\n",
      "Epoch 4704/20000: Train Loss = 0.444351, Test Loss = 0.249584, Learning Rate = 1.674028e-04\n",
      "Epoch 4705/20000: Train Loss = 0.443525, Test Loss = 0.256906, Learning Rate = 1.673392e-04\n",
      "Epoch 4706/20000: Train Loss = 0.443809, Test Loss = 0.257040, Learning Rate = 1.672756e-04\n",
      "Epoch 4707/20000: Train Loss = 0.442787, Test Loss = 0.243937, Learning Rate = 1.672121e-04\n",
      "Epoch 4708/20000: Train Loss = 0.443000, Test Loss = 0.248169, Learning Rate = 1.671485e-04\n",
      "Epoch 4709/20000: Train Loss = 0.443212, Test Loss = 0.240407, Learning Rate = 1.670850e-04\n",
      "Epoch 4710/20000: Train Loss = 0.443454, Test Loss = 0.247035, Learning Rate = 1.670215e-04\n",
      "Epoch 4711/20000: Train Loss = 0.443323, Test Loss = 0.246644, Learning Rate = 1.669581e-04\n",
      "Epoch 4712/20000: Train Loss = 0.443228, Test Loss = 0.247830, Learning Rate = 1.668946e-04\n",
      "Epoch 4713/20000: Train Loss = 0.443154, Test Loss = 0.244664, Learning Rate = 1.668312e-04\n",
      "Epoch 4714/20000: Train Loss = 0.443283, Test Loss = 0.248790, Learning Rate = 1.667678e-04\n",
      "Epoch 4715/20000: Train Loss = 0.443641, Test Loss = 0.251694, Learning Rate = 1.667045e-04\n",
      "Epoch 4716/20000: Train Loss = 0.442971, Test Loss = 0.247576, Learning Rate = 1.666411e-04\n",
      "Epoch 4717/20000: Train Loss = 0.443008, Test Loss = 0.250484, Learning Rate = 1.665778e-04\n",
      "Epoch 4718/20000: Train Loss = 0.443345, Test Loss = 0.248332, Learning Rate = 1.665145e-04\n",
      "Epoch 4719/20000: Train Loss = 0.442850, Test Loss = 0.244116, Learning Rate = 1.664512e-04\n",
      "Epoch 4720/20000: Train Loss = 0.443232, Test Loss = 0.250811, Learning Rate = 1.663880e-04\n",
      "Epoch 4721/20000: Train Loss = 0.443526, Test Loss = 0.242564, Learning Rate = 1.663248e-04\n",
      "Epoch 4722/20000: Train Loss = 0.443820, Test Loss = 0.254900, Learning Rate = 1.662616e-04\n",
      "Epoch 4723/20000: Train Loss = 0.444977, Test Loss = 0.249903, Learning Rate = 1.661984e-04\n",
      "Epoch 4724/20000: Train Loss = 0.443773, Test Loss = 0.252669, Learning Rate = 1.661352e-04\n",
      "Epoch 4725/20000: Train Loss = 0.443450, Test Loss = 0.244697, Learning Rate = 1.660721e-04\n",
      "Epoch 4726/20000: Train Loss = 0.443879, Test Loss = 0.252962, Learning Rate = 1.660090e-04\n",
      "Epoch 4727/20000: Train Loss = 0.443513, Test Loss = 0.242301, Learning Rate = 1.659459e-04\n",
      "Epoch 4728/20000: Train Loss = 0.443241, Test Loss = 0.249130, Learning Rate = 1.658829e-04\n",
      "Epoch 4729/20000: Train Loss = 0.443076, Test Loss = 0.254214, Learning Rate = 1.658198e-04\n",
      "Epoch 4730/20000: Train Loss = 0.443480, Test Loss = 0.256556, Learning Rate = 1.657568e-04\n",
      "Epoch 4731/20000: Train Loss = 0.442901, Test Loss = 0.245348, Learning Rate = 1.656939e-04\n",
      "Epoch 4732/20000: Train Loss = 0.443807, Test Loss = 0.244325, Learning Rate = 1.656309e-04\n",
      "Epoch 4733/20000: Train Loss = 0.443102, Test Loss = 0.255739, Learning Rate = 1.655680e-04\n",
      "Epoch 4734/20000: Train Loss = 0.445186, Test Loss = 0.249085, Learning Rate = 1.655051e-04\n",
      "Epoch 4735/20000: Train Loss = 0.443043, Test Loss = 0.247861, Learning Rate = 1.654422e-04\n",
      "Epoch 4736/20000: Train Loss = 0.443004, Test Loss = 0.245873, Learning Rate = 1.653793e-04\n",
      "Epoch 4737/20000: Train Loss = 0.443510, Test Loss = 0.249990, Learning Rate = 1.653165e-04\n",
      "Epoch 4738/20000: Train Loss = 0.443190, Test Loss = 0.257973, Learning Rate = 1.652536e-04\n",
      "Epoch 4739/20000: Train Loss = 0.443353, Test Loss = 0.246467, Learning Rate = 1.651909e-04\n",
      "Epoch 4740/20000: Train Loss = 0.443544, Test Loss = 0.251120, Learning Rate = 1.651281e-04\n",
      "Epoch 4741/20000: Train Loss = 0.442888, Test Loss = 0.247908, Learning Rate = 1.650653e-04\n",
      "Epoch 4742/20000: Train Loss = 0.443986, Test Loss = 0.247363, Learning Rate = 1.650026e-04\n",
      "Epoch 4743/20000: Train Loss = 0.443074, Test Loss = 0.244168, Learning Rate = 1.649399e-04\n",
      "Epoch 4744/20000: Train Loss = 0.443598, Test Loss = 0.251174, Learning Rate = 1.648773e-04\n",
      "Epoch 4745/20000: Train Loss = 0.443322, Test Loss = 0.248532, Learning Rate = 1.648146e-04\n",
      "Epoch 4746/20000: Train Loss = 0.444123, Test Loss = 0.252482, Learning Rate = 1.647520e-04\n",
      "Epoch 4747/20000: Train Loss = 0.443220, Test Loss = 0.240914, Learning Rate = 1.646894e-04\n",
      "Epoch 4748/20000: Train Loss = 0.442896, Test Loss = 0.249216, Learning Rate = 1.646268e-04\n",
      "Epoch 4749/20000: Train Loss = 0.443036, Test Loss = 0.246327, Learning Rate = 1.645642e-04\n",
      "Epoch 4750/20000: Train Loss = 0.444219, Test Loss = 0.251910, Learning Rate = 1.645017e-04\n",
      "Epoch 4751/20000: Train Loss = 0.443408, Test Loss = 0.239606, Learning Rate = 1.644392e-04\n",
      "Epoch 4752/20000: Train Loss = 0.443874, Test Loss = 0.249852, Learning Rate = 1.643767e-04\n",
      "Epoch 4753/20000: Train Loss = 0.443391, Test Loss = 0.243521, Learning Rate = 1.643143e-04\n",
      "Epoch 4754/20000: Train Loss = 0.443254, Test Loss = 0.245410, Learning Rate = 1.642518e-04\n",
      "Epoch 4755/20000: Train Loss = 0.442835, Test Loss = 0.249699, Learning Rate = 1.641894e-04\n",
      "Epoch 4756/20000: Train Loss = 0.443258, Test Loss = 0.251054, Learning Rate = 1.641270e-04\n",
      "Epoch 4757/20000: Train Loss = 0.442783, Test Loss = 0.250229, Learning Rate = 1.640647e-04\n",
      "Epoch 4758/20000: Train Loss = 0.443218, Test Loss = 0.244534, Learning Rate = 1.640023e-04\n",
      "Epoch 4759/20000: Train Loss = 0.442941, Test Loss = 0.244833, Learning Rate = 1.639400e-04\n",
      "Epoch 4760/20000: Train Loss = 0.443050, Test Loss = 0.249236, Learning Rate = 1.638777e-04\n",
      "Epoch 4761/20000: Train Loss = 0.442894, Test Loss = 0.254964, Learning Rate = 1.638155e-04\n",
      "Epoch 4762/20000: Train Loss = 0.444072, Test Loss = 0.256599, Learning Rate = 1.637532e-04\n",
      "Epoch 4763/20000: Train Loss = 0.443272, Test Loss = 0.245500, Learning Rate = 1.636910e-04\n",
      "Epoch 4764/20000: Train Loss = 0.443396, Test Loss = 0.249100, Learning Rate = 1.636288e-04\n",
      "Epoch 4765/20000: Train Loss = 0.443768, Test Loss = 0.258814, Learning Rate = 1.635666e-04\n",
      "Epoch 4766/20000: Train Loss = 0.442845, Test Loss = 0.245152, Learning Rate = 1.635045e-04\n",
      "Epoch 4767/20000: Train Loss = 0.444973, Test Loss = 0.245475, Learning Rate = 1.634423e-04\n",
      "Epoch 4768/20000: Train Loss = 0.443309, Test Loss = 0.247720, Learning Rate = 1.633802e-04\n",
      "Epoch 4769/20000: Train Loss = 0.442980, Test Loss = 0.243930, Learning Rate = 1.633181e-04\n",
      "Epoch 4770/20000: Train Loss = 0.443282, Test Loss = 0.244882, Learning Rate = 1.632561e-04\n",
      "Epoch 4771/20000: Train Loss = 0.443045, Test Loss = 0.248141, Learning Rate = 1.631941e-04\n",
      "Epoch 4772/20000: Train Loss = 0.442959, Test Loss = 0.246131, Learning Rate = 1.631321e-04\n",
      "Epoch 4773/20000: Train Loss = 0.443274, Test Loss = 0.245259, Learning Rate = 1.630701e-04\n",
      "Epoch 4774/20000: Train Loss = 0.443235, Test Loss = 0.250586, Learning Rate = 1.630081e-04\n",
      "Epoch 4775/20000: Train Loss = 0.443141, Test Loss = 0.247556, Learning Rate = 1.629462e-04\n",
      "Epoch 4776/20000: Train Loss = 0.443025, Test Loss = 0.243223, Learning Rate = 1.628842e-04\n",
      "Epoch 4777/20000: Train Loss = 0.443071, Test Loss = 0.242578, Learning Rate = 1.628224e-04\n",
      "Epoch 4778/20000: Train Loss = 0.443542, Test Loss = 0.248562, Learning Rate = 1.627605e-04\n",
      "Epoch 4779/20000: Train Loss = 0.443067, Test Loss = 0.246874, Learning Rate = 1.626986e-04\n",
      "Epoch 4780/20000: Train Loss = 0.443636, Test Loss = 0.244585, Learning Rate = 1.626368e-04\n",
      "Epoch 4781/20000: Train Loss = 0.443235, Test Loss = 0.246969, Learning Rate = 1.625750e-04\n",
      "Epoch 4782/20000: Train Loss = 0.442826, Test Loss = 0.250260, Learning Rate = 1.625133e-04\n",
      "Epoch 4783/20000: Train Loss = 0.443322, Test Loss = 0.248211, Learning Rate = 1.624515e-04\n",
      "Epoch 4784/20000: Train Loss = 0.443404, Test Loss = 0.247813, Learning Rate = 1.623898e-04\n",
      "Epoch 4785/20000: Train Loss = 0.443798, Test Loss = 0.252216, Learning Rate = 1.623281e-04\n",
      "Epoch 4786/20000: Train Loss = 0.443856, Test Loss = 0.244020, Learning Rate = 1.622664e-04\n",
      "Epoch 4787/20000: Train Loss = 0.443849, Test Loss = 0.252985, Learning Rate = 1.622047e-04\n",
      "Epoch 4788/20000: Train Loss = 0.443943, Test Loss = 0.244506, Learning Rate = 1.621431e-04\n",
      "Epoch 4789/20000: Train Loss = 0.443083, Test Loss = 0.246041, Learning Rate = 1.620815e-04\n",
      "Epoch 4790/20000: Train Loss = 0.443290, Test Loss = 0.247860, Learning Rate = 1.620199e-04\n",
      "Epoch 4791/20000: Train Loss = 0.443811, Test Loss = 0.246801, Learning Rate = 1.619583e-04\n",
      "Epoch 4792/20000: Train Loss = 0.443415, Test Loss = 0.246938, Learning Rate = 1.618968e-04\n",
      "Epoch 4793/20000: Train Loss = 0.444203, Test Loss = 0.249732, Learning Rate = 1.618353e-04\n",
      "Epoch 4794/20000: Train Loss = 0.443789, Test Loss = 0.244896, Learning Rate = 1.617738e-04\n",
      "Epoch 4795/20000: Train Loss = 0.443140, Test Loss = 0.247649, Learning Rate = 1.617123e-04\n",
      "Epoch 4796/20000: Train Loss = 0.443419, Test Loss = 0.248126, Learning Rate = 1.616509e-04\n",
      "Epoch 4797/20000: Train Loss = 0.443330, Test Loss = 0.246458, Learning Rate = 1.615895e-04\n",
      "Epoch 4798/20000: Train Loss = 0.443697, Test Loss = 0.252493, Learning Rate = 1.615281e-04\n",
      "Epoch 4799/20000: Train Loss = 0.444336, Test Loss = 0.248128, Learning Rate = 1.614667e-04\n",
      "Epoch 4800/20000: Train Loss = 0.443753, Test Loss = 0.247195, Learning Rate = 1.614053e-04\n",
      "Epoch 4801/20000: Train Loss = 0.443180, Test Loss = 0.251384, Learning Rate = 1.613440e-04\n",
      "Epoch 4802/20000: Train Loss = 0.443284, Test Loss = 0.248069, Learning Rate = 1.612827e-04\n",
      "Epoch 4803/20000: Train Loss = 0.443182, Test Loss = 0.247118, Learning Rate = 1.612214e-04\n",
      "Epoch 4804/20000: Train Loss = 0.442920, Test Loss = 0.250253, Learning Rate = 1.611601e-04\n",
      "Epoch 4805/20000: Train Loss = 0.443144, Test Loss = 0.250860, Learning Rate = 1.610989e-04\n",
      "Epoch 4806/20000: Train Loss = 0.442926, Test Loss = 0.243546, Learning Rate = 1.610377e-04\n",
      "Epoch 4807/20000: Train Loss = 0.443384, Test Loss = 0.243359, Learning Rate = 1.609765e-04\n",
      "Epoch 4808/20000: Train Loss = 0.443511, Test Loss = 0.249820, Learning Rate = 1.609153e-04\n",
      "Epoch 4809/20000: Train Loss = 0.443013, Test Loss = 0.241203, Learning Rate = 1.608542e-04\n",
      "Epoch 4810/20000: Train Loss = 0.443576, Test Loss = 0.240919, Learning Rate = 1.607931e-04\n",
      "Epoch 4811/20000: Train Loss = 0.442985, Test Loss = 0.241760, Learning Rate = 1.607320e-04\n",
      "Epoch 4812/20000: Train Loss = 0.443022, Test Loss = 0.244528, Learning Rate = 1.606709e-04\n",
      "Epoch 4813/20000: Train Loss = 0.443020, Test Loss = 0.255061, Learning Rate = 1.606099e-04\n",
      "Epoch 4814/20000: Train Loss = 0.443850, Test Loss = 0.245319, Learning Rate = 1.605488e-04\n",
      "Epoch 4815/20000: Train Loss = 0.442847, Test Loss = 0.252629, Learning Rate = 1.604878e-04\n",
      "Epoch 4816/20000: Train Loss = 0.443937, Test Loss = 0.245706, Learning Rate = 1.604268e-04\n",
      "Epoch 4817/20000: Train Loss = 0.443386, Test Loss = 0.252080, Learning Rate = 1.603659e-04\n",
      "Epoch 4818/20000: Train Loss = 0.443940, Test Loss = 0.248955, Learning Rate = 1.603049e-04\n",
      "Epoch 4819/20000: Train Loss = 0.443782, Test Loss = 0.250408, Learning Rate = 1.602440e-04\n",
      "Epoch 4820/20000: Train Loss = 0.443175, Test Loss = 0.252023, Learning Rate = 1.601831e-04\n",
      "Epoch 4821/20000: Train Loss = 0.443114, Test Loss = 0.253513, Learning Rate = 1.601223e-04\n",
      "Epoch 4822/20000: Train Loss = 0.443388, Test Loss = 0.244890, Learning Rate = 1.600614e-04\n",
      "Epoch 4823/20000: Train Loss = 0.443424, Test Loss = 0.250856, Learning Rate = 1.600006e-04\n",
      "Epoch 4824/20000: Train Loss = 0.443267, Test Loss = 0.250957, Learning Rate = 1.599398e-04\n",
      "Epoch 4825/20000: Train Loss = 0.443342, Test Loss = 0.249211, Learning Rate = 1.598791e-04\n",
      "Epoch 4826/20000: Train Loss = 0.443148, Test Loss = 0.247638, Learning Rate = 1.598183e-04\n",
      "Epoch 4827/20000: Train Loss = 0.442936, Test Loss = 0.254552, Learning Rate = 1.597576e-04\n",
      "Epoch 4828/20000: Train Loss = 0.442953, Test Loss = 0.246918, Learning Rate = 1.596969e-04\n",
      "Epoch 4829/20000: Train Loss = 0.443666, Test Loss = 0.244782, Learning Rate = 1.596362e-04\n",
      "Epoch 4830/20000: Train Loss = 0.442882, Test Loss = 0.248329, Learning Rate = 1.595755e-04\n",
      "Epoch 4831/20000: Train Loss = 0.443282, Test Loss = 0.251504, Learning Rate = 1.595149e-04\n",
      "Epoch 4832/20000: Train Loss = 0.443502, Test Loss = 0.249603, Learning Rate = 1.594543e-04\n",
      "Epoch 4833/20000: Train Loss = 0.443114, Test Loss = 0.249248, Learning Rate = 1.593937e-04\n",
      "Epoch 4834/20000: Train Loss = 0.443654, Test Loss = 0.245075, Learning Rate = 1.593331e-04\n",
      "Epoch 4835/20000: Train Loss = 0.443548, Test Loss = 0.242627, Learning Rate = 1.592726e-04\n",
      "Epoch 4836/20000: Train Loss = 0.445287, Test Loss = 0.249056, Learning Rate = 1.592121e-04\n",
      "Epoch 4837/20000: Train Loss = 0.443181, Test Loss = 0.247456, Learning Rate = 1.591516e-04\n",
      "Epoch 4838/20000: Train Loss = 0.444224, Test Loss = 0.249851, Learning Rate = 1.590911e-04\n",
      "Epoch 4839/20000: Train Loss = 0.444650, Test Loss = 0.243588, Learning Rate = 1.590307e-04\n",
      "Epoch 4840/20000: Train Loss = 0.444228, Test Loss = 0.250308, Learning Rate = 1.589702e-04\n",
      "Epoch 4841/20000: Train Loss = 0.443129, Test Loss = 0.245550, Learning Rate = 1.589098e-04\n",
      "Epoch 4842/20000: Train Loss = 0.442826, Test Loss = 0.246120, Learning Rate = 1.588494e-04\n",
      "Epoch 4843/20000: Train Loss = 0.443089, Test Loss = 0.241008, Learning Rate = 1.587891e-04\n",
      "Epoch 4844/20000: Train Loss = 0.444177, Test Loss = 0.242097, Learning Rate = 1.587287e-04\n",
      "Epoch 4845/20000: Train Loss = 0.442972, Test Loss = 0.251376, Learning Rate = 1.586684e-04\n",
      "Epoch 4846/20000: Train Loss = 0.443176, Test Loss = 0.245264, Learning Rate = 1.586081e-04\n",
      "Epoch 4847/20000: Train Loss = 0.442865, Test Loss = 0.253990, Learning Rate = 1.585479e-04\n",
      "Epoch 4848/20000: Train Loss = 0.443433, Test Loss = 0.250562, Learning Rate = 1.584876e-04\n",
      "Epoch 4849/20000: Train Loss = 0.442876, Test Loss = 0.249729, Learning Rate = 1.584274e-04\n",
      "Epoch 4850/20000: Train Loss = 0.443684, Test Loss = 0.242306, Learning Rate = 1.583672e-04\n",
      "Epoch 4851/20000: Train Loss = 0.443386, Test Loss = 0.256849, Learning Rate = 1.583070e-04\n",
      "Epoch 4852/20000: Train Loss = 0.443690, Test Loss = 0.249229, Learning Rate = 1.582469e-04\n",
      "Epoch 4853/20000: Train Loss = 0.443237, Test Loss = 0.247759, Learning Rate = 1.581868e-04\n",
      "Epoch 4854/20000: Train Loss = 0.443453, Test Loss = 0.252266, Learning Rate = 1.581267e-04\n",
      "Epoch 4855/20000: Train Loss = 0.443289, Test Loss = 0.250307, Learning Rate = 1.580666e-04\n",
      "Epoch 4856/20000: Train Loss = 0.443818, Test Loss = 0.241663, Learning Rate = 1.580065e-04\n",
      "Epoch 4857/20000: Train Loss = 0.442888, Test Loss = 0.252802, Learning Rate = 1.579465e-04\n",
      "Epoch 4858/20000: Train Loss = 0.443573, Test Loss = 0.247260, Learning Rate = 1.578865e-04\n",
      "Epoch 4859/20000: Train Loss = 0.442915, Test Loss = 0.245003, Learning Rate = 1.578265e-04\n",
      "Epoch 4860/20000: Train Loss = 0.443359, Test Loss = 0.239868, Learning Rate = 1.577665e-04\n",
      "Epoch 4861/20000: Train Loss = 0.443189, Test Loss = 0.248964, Learning Rate = 1.577065e-04\n",
      "Epoch 4862/20000: Train Loss = 0.443673, Test Loss = 0.248716, Learning Rate = 1.576466e-04\n",
      "Epoch 4863/20000: Train Loss = 0.443623, Test Loss = 0.251173, Learning Rate = 1.575867e-04\n",
      "Epoch 4864/20000: Train Loss = 0.443737, Test Loss = 0.246962, Learning Rate = 1.575268e-04\n",
      "Epoch 4865/20000: Train Loss = 0.442971, Test Loss = 0.246661, Learning Rate = 1.574670e-04\n",
      "Epoch 4866/20000: Train Loss = 0.443022, Test Loss = 0.248772, Learning Rate = 1.574071e-04\n",
      "Epoch 4867/20000: Train Loss = 0.443271, Test Loss = 0.248419, Learning Rate = 1.573473e-04\n",
      "Epoch 4868/20000: Train Loss = 0.442882, Test Loss = 0.245852, Learning Rate = 1.572876e-04\n",
      "Epoch 4869/20000: Train Loss = 0.442873, Test Loss = 0.245993, Learning Rate = 1.572278e-04\n",
      "Epoch 4870/20000: Train Loss = 0.442921, Test Loss = 0.249269, Learning Rate = 1.571680e-04\n",
      "Epoch 4871/20000: Train Loss = 0.443289, Test Loss = 0.245388, Learning Rate = 1.571083e-04\n",
      "Epoch 4872/20000: Train Loss = 0.443913, Test Loss = 0.256330, Learning Rate = 1.570486e-04\n",
      "Epoch 4873/20000: Train Loss = 0.445686, Test Loss = 0.239306, Learning Rate = 1.569890e-04\n",
      "Epoch 4874/20000: Train Loss = 0.442462, Test Loss = 0.253811, Learning Rate = 1.569293e-04\n",
      "Epoch 4875/20000: Train Loss = 0.444215, Test Loss = 0.250343, Learning Rate = 1.568697e-04\n",
      "Epoch 4876/20000: Train Loss = 0.443352, Test Loss = 0.246368, Learning Rate = 1.568101e-04\n",
      "Epoch 4877/20000: Train Loss = 0.443670, Test Loss = 0.251940, Learning Rate = 1.567505e-04\n",
      "Epoch 4878/20000: Train Loss = 0.443999, Test Loss = 0.249744, Learning Rate = 1.566909e-04\n",
      "Epoch 4879/20000: Train Loss = 0.443355, Test Loss = 0.252099, Learning Rate = 1.566314e-04\n",
      "Epoch 4880/20000: Train Loss = 0.443210, Test Loss = 0.250476, Learning Rate = 1.565719e-04\n",
      "Epoch 4881/20000: Train Loss = 0.443914, Test Loss = 0.243008, Learning Rate = 1.565124e-04\n",
      "Epoch 4882/20000: Train Loss = 0.443519, Test Loss = 0.240856, Learning Rate = 1.564529e-04\n",
      "Epoch 4883/20000: Train Loss = 0.442738, Test Loss = 0.252190, Learning Rate = 1.563935e-04\n",
      "Epoch 4884/20000: Train Loss = 0.443694, Test Loss = 0.254339, Learning Rate = 1.563340e-04\n",
      "Epoch 4885/20000: Train Loss = 0.443492, Test Loss = 0.252263, Learning Rate = 1.562746e-04\n",
      "Epoch 4886/20000: Train Loss = 0.442682, Test Loss = 0.241183, Learning Rate = 1.562152e-04\n",
      "Epoch 4887/20000: Train Loss = 0.443557, Test Loss = 0.243385, Learning Rate = 1.561559e-04\n",
      "Epoch 4888/20000: Train Loss = 0.443590, Test Loss = 0.248392, Learning Rate = 1.560966e-04\n",
      "Epoch 4889/20000: Train Loss = 0.443145, Test Loss = 0.237756, Learning Rate = 1.560372e-04\n",
      "Epoch 4890/20000: Train Loss = 0.443308, Test Loss = 0.242746, Learning Rate = 1.559780e-04\n",
      "Epoch 4891/20000: Train Loss = 0.443071, Test Loss = 0.242320, Learning Rate = 1.559187e-04\n",
      "Epoch 4892/20000: Train Loss = 0.444018, Test Loss = 0.243802, Learning Rate = 1.558594e-04\n",
      "Epoch 4893/20000: Train Loss = 0.443494, Test Loss = 0.239154, Learning Rate = 1.558002e-04\n",
      "Epoch 4894/20000: Train Loss = 0.444027, Test Loss = 0.253414, Learning Rate = 1.557410e-04\n",
      "Epoch 4895/20000: Train Loss = 0.443381, Test Loss = 0.250694, Learning Rate = 1.556818e-04\n",
      "Epoch 4896/20000: Train Loss = 0.443697, Test Loss = 0.245562, Learning Rate = 1.556227e-04\n",
      "Epoch 4897/20000: Train Loss = 0.445224, Test Loss = 0.243030, Learning Rate = 1.555636e-04\n",
      "Epoch 4898/20000: Train Loss = 0.443773, Test Loss = 0.252804, Learning Rate = 1.555044e-04\n",
      "Epoch 4899/20000: Train Loss = 0.443734, Test Loss = 0.244826, Learning Rate = 1.554454e-04\n",
      "Epoch 4900/20000: Train Loss = 0.443216, Test Loss = 0.243103, Learning Rate = 1.553863e-04\n",
      "Epoch 4901/20000: Train Loss = 0.443540, Test Loss = 0.246282, Learning Rate = 1.553272e-04\n",
      "Epoch 4902/20000: Train Loss = 0.443887, Test Loss = 0.248200, Learning Rate = 1.552682e-04\n",
      "Epoch 4903/20000: Train Loss = 0.443069, Test Loss = 0.243049, Learning Rate = 1.552092e-04\n",
      "Epoch 4904/20000: Train Loss = 0.443525, Test Loss = 0.242348, Learning Rate = 1.551503e-04\n",
      "Epoch 4905/20000: Train Loss = 0.443070, Test Loss = 0.250272, Learning Rate = 1.550913e-04\n",
      "Epoch 4906/20000: Train Loss = 0.443564, Test Loss = 0.244678, Learning Rate = 1.550324e-04\n",
      "Epoch 4907/20000: Train Loss = 0.442887, Test Loss = 0.243616, Learning Rate = 1.549735e-04\n",
      "Epoch 4908/20000: Train Loss = 0.444393, Test Loss = 0.248816, Learning Rate = 1.549146e-04\n",
      "Epoch 4909/20000: Train Loss = 0.442888, Test Loss = 0.248072, Learning Rate = 1.548557e-04\n",
      "Epoch 4910/20000: Train Loss = 0.443040, Test Loss = 0.250473, Learning Rate = 1.547969e-04\n",
      "Epoch 4911/20000: Train Loss = 0.443244, Test Loss = 0.248254, Learning Rate = 1.547381e-04\n",
      "Epoch 4912/20000: Train Loss = 0.443214, Test Loss = 0.246255, Learning Rate = 1.546793e-04\n",
      "Epoch 4913/20000: Train Loss = 0.443043, Test Loss = 0.242524, Learning Rate = 1.546205e-04\n",
      "Epoch 4914/20000: Train Loss = 0.443605, Test Loss = 0.237124, Learning Rate = 1.545617e-04\n",
      "Epoch 4915/20000: Train Loss = 0.443689, Test Loss = 0.254068, Learning Rate = 1.545030e-04\n",
      "Epoch 4916/20000: Train Loss = 0.443104, Test Loss = 0.242343, Learning Rate = 1.544443e-04\n",
      "Epoch 4917/20000: Train Loss = 0.444076, Test Loss = 0.245187, Learning Rate = 1.543856e-04\n",
      "Epoch 4918/20000: Train Loss = 0.443097, Test Loss = 0.250617, Learning Rate = 1.543270e-04\n",
      "Epoch 4919/20000: Train Loss = 0.443056, Test Loss = 0.244997, Learning Rate = 1.542683e-04\n",
      "Epoch 4920/20000: Train Loss = 0.443630, Test Loss = 0.245818, Learning Rate = 1.542097e-04\n",
      "Epoch 4921/20000: Train Loss = 0.443276, Test Loss = 0.240499, Learning Rate = 1.541511e-04\n",
      "Epoch 4922/20000: Train Loss = 0.443930, Test Loss = 0.251011, Learning Rate = 1.540925e-04\n",
      "Epoch 4923/20000: Train Loss = 0.443781, Test Loss = 0.248196, Learning Rate = 1.540340e-04\n",
      "Epoch 4924/20000: Train Loss = 0.443891, Test Loss = 0.247210, Learning Rate = 1.539754e-04\n",
      "Epoch 4925/20000: Train Loss = 0.443278, Test Loss = 0.252705, Learning Rate = 1.539169e-04\n",
      "Epoch 4926/20000: Train Loss = 0.443269, Test Loss = 0.249713, Learning Rate = 1.538585e-04\n",
      "Epoch 4927/20000: Train Loss = 0.442863, Test Loss = 0.246990, Learning Rate = 1.538000e-04\n",
      "Epoch 4928/20000: Train Loss = 0.443056, Test Loss = 0.242970, Learning Rate = 1.537416e-04\n",
      "Epoch 4929/20000: Train Loss = 0.443298, Test Loss = 0.242322, Learning Rate = 1.536831e-04\n",
      "Epoch 4930/20000: Train Loss = 0.442794, Test Loss = 0.245981, Learning Rate = 1.536247e-04\n",
      "Epoch 4931/20000: Train Loss = 0.442793, Test Loss = 0.248709, Learning Rate = 1.535664e-04\n",
      "Epoch 4932/20000: Train Loss = 0.443095, Test Loss = 0.247721, Learning Rate = 1.535080e-04\n",
      "Epoch 4933/20000: Train Loss = 0.443019, Test Loss = 0.242791, Learning Rate = 1.534497e-04\n",
      "Epoch 4934/20000: Train Loss = 0.443224, Test Loss = 0.247510, Learning Rate = 1.533914e-04\n",
      "Epoch 4935/20000: Train Loss = 0.443957, Test Loss = 0.249742, Learning Rate = 1.533331e-04\n",
      "Epoch 4936/20000: Train Loss = 0.443740, Test Loss = 0.251005, Learning Rate = 1.532748e-04\n",
      "Epoch 4937/20000: Train Loss = 0.443032, Test Loss = 0.247110, Learning Rate = 1.532166e-04\n",
      "Epoch 4938/20000: Train Loss = 0.443978, Test Loss = 0.247292, Learning Rate = 1.531584e-04\n",
      "Epoch 4939/20000: Train Loss = 0.443096, Test Loss = 0.244510, Learning Rate = 1.531002e-04\n",
      "Epoch 4940/20000: Train Loss = 0.443014, Test Loss = 0.247602, Learning Rate = 1.530420e-04\n",
      "Epoch 4941/20000: Train Loss = 0.443155, Test Loss = 0.247199, Learning Rate = 1.529839e-04\n",
      "Epoch 4942/20000: Train Loss = 0.443390, Test Loss = 0.242256, Learning Rate = 1.529257e-04\n",
      "Epoch 4943/20000: Train Loss = 0.443400, Test Loss = 0.255943, Learning Rate = 1.528676e-04\n",
      "Epoch 4944/20000: Train Loss = 0.443635, Test Loss = 0.249405, Learning Rate = 1.528095e-04\n",
      "Epoch 4945/20000: Train Loss = 0.443478, Test Loss = 0.248907, Learning Rate = 1.527515e-04\n",
      "Epoch 4946/20000: Train Loss = 0.443119, Test Loss = 0.240516, Learning Rate = 1.526934e-04\n",
      "Epoch 4947/20000: Train Loss = 0.443177, Test Loss = 0.241895, Learning Rate = 1.526354e-04\n",
      "Epoch 4948/20000: Train Loss = 0.443815, Test Loss = 0.243844, Learning Rate = 1.525774e-04\n",
      "Epoch 4949/20000: Train Loss = 0.443119, Test Loss = 0.253915, Learning Rate = 1.525194e-04\n",
      "Epoch 4950/20000: Train Loss = 0.443831, Test Loss = 0.246863, Learning Rate = 1.524615e-04\n",
      "Epoch 4951/20000: Train Loss = 0.443910, Test Loss = 0.256215, Learning Rate = 1.524035e-04\n",
      "Epoch 4952/20000: Train Loss = 0.443173, Test Loss = 0.246290, Learning Rate = 1.523456e-04\n",
      "Epoch 4953/20000: Train Loss = 0.442920, Test Loss = 0.239733, Learning Rate = 1.522878e-04\n",
      "Epoch 4954/20000: Train Loss = 0.443608, Test Loss = 0.241078, Learning Rate = 1.522299e-04\n",
      "Epoch 4955/20000: Train Loss = 0.443652, Test Loss = 0.243147, Learning Rate = 1.521720e-04\n",
      "Epoch 4956/20000: Train Loss = 0.443137, Test Loss = 0.245518, Learning Rate = 1.521142e-04\n",
      "Epoch 4957/20000: Train Loss = 0.443472, Test Loss = 0.245086, Learning Rate = 1.520564e-04\n",
      "Epoch 4958/20000: Train Loss = 0.443468, Test Loss = 0.249651, Learning Rate = 1.519986e-04\n",
      "Epoch 4959/20000: Train Loss = 0.445239, Test Loss = 0.242315, Learning Rate = 1.519409e-04\n",
      "Epoch 4960/20000: Train Loss = 0.443547, Test Loss = 0.246665, Learning Rate = 1.518832e-04\n",
      "Epoch 4961/20000: Train Loss = 0.443082, Test Loss = 0.237874, Learning Rate = 1.518254e-04\n",
      "Epoch 4962/20000: Train Loss = 0.444018, Test Loss = 0.249713, Learning Rate = 1.517678e-04\n",
      "Epoch 4963/20000: Train Loss = 0.443327, Test Loss = 0.240044, Learning Rate = 1.517101e-04\n",
      "Epoch 4964/20000: Train Loss = 0.443500, Test Loss = 0.244673, Learning Rate = 1.516524e-04\n",
      "Epoch 4965/20000: Train Loss = 0.442908, Test Loss = 0.245887, Learning Rate = 1.515948e-04\n",
      "Epoch 4966/20000: Train Loss = 0.443598, Test Loss = 0.247377, Learning Rate = 1.515372e-04\n",
      "Epoch 4967/20000: Train Loss = 0.443578, Test Loss = 0.242777, Learning Rate = 1.514796e-04\n",
      "Epoch 4968/20000: Train Loss = 0.443401, Test Loss = 0.245809, Learning Rate = 1.514221e-04\n",
      "Epoch 4969/20000: Train Loss = 0.442968, Test Loss = 0.237547, Learning Rate = 1.513645e-04\n",
      "Epoch 4970/20000: Train Loss = 0.443829, Test Loss = 0.237013, Learning Rate = 1.513070e-04\n",
      "Epoch 4971/20000: Train Loss = 0.443182, Test Loss = 0.244363, Learning Rate = 1.512495e-04\n",
      "Epoch 4972/20000: Train Loss = 0.443497, Test Loss = 0.239945, Learning Rate = 1.511921e-04\n",
      "Epoch 4973/20000: Train Loss = 0.443387, Test Loss = 0.239010, Learning Rate = 1.511346e-04\n",
      "Epoch 4974/20000: Train Loss = 0.442949, Test Loss = 0.241682, Learning Rate = 1.510772e-04\n",
      "Epoch 4975/20000: Train Loss = 0.442925, Test Loss = 0.246309, Learning Rate = 1.510198e-04\n",
      "Epoch 4976/20000: Train Loss = 0.443204, Test Loss = 0.242586, Learning Rate = 1.509624e-04\n",
      "Epoch 4977/20000: Train Loss = 0.443146, Test Loss = 0.243717, Learning Rate = 1.509050e-04\n",
      "Epoch 4978/20000: Train Loss = 0.443114, Test Loss = 0.245290, Learning Rate = 1.508477e-04\n",
      "Epoch 4979/20000: Train Loss = 0.443497, Test Loss = 0.244507, Learning Rate = 1.507904e-04\n",
      "Epoch 4980/20000: Train Loss = 0.442972, Test Loss = 0.244429, Learning Rate = 1.507331e-04\n",
      "Epoch 4981/20000: Train Loss = 0.443033, Test Loss = 0.248367, Learning Rate = 1.506758e-04\n",
      "Epoch 4982/20000: Train Loss = 0.443089, Test Loss = 0.249005, Learning Rate = 1.506186e-04\n",
      "Epoch 4983/20000: Train Loss = 0.442920, Test Loss = 0.246748, Learning Rate = 1.505613e-04\n",
      "Epoch 4984/20000: Train Loss = 0.443880, Test Loss = 0.245972, Learning Rate = 1.505041e-04\n",
      "Epoch 4985/20000: Train Loss = 0.443006, Test Loss = 0.248246, Learning Rate = 1.504469e-04\n",
      "Epoch 4986/20000: Train Loss = 0.443769, Test Loss = 0.245521, Learning Rate = 1.503898e-04\n",
      "Epoch 4987/20000: Train Loss = 0.443433, Test Loss = 0.254468, Learning Rate = 1.503326e-04\n",
      "Epoch 4988/20000: Train Loss = 0.442976, Test Loss = 0.241217, Learning Rate = 1.502755e-04\n",
      "Epoch 4989/20000: Train Loss = 0.443037, Test Loss = 0.241722, Learning Rate = 1.502184e-04\n",
      "Epoch 4990/20000: Train Loss = 0.443303, Test Loss = 0.245574, Learning Rate = 1.501613e-04\n",
      "Epoch 4991/20000: Train Loss = 0.443193, Test Loss = 0.248010, Learning Rate = 1.501043e-04\n",
      "Epoch 4992/20000: Train Loss = 0.443318, Test Loss = 0.254565, Learning Rate = 1.500472e-04\n",
      "Epoch 4993/20000: Train Loss = 0.444388, Test Loss = 0.245507, Learning Rate = 1.499902e-04\n",
      "Epoch 4994/20000: Train Loss = 0.443251, Test Loss = 0.247644, Learning Rate = 1.499332e-04\n",
      "Epoch 4995/20000: Train Loss = 0.443973, Test Loss = 0.247473, Learning Rate = 1.498762e-04\n",
      "Epoch 4996/20000: Train Loss = 0.442831, Test Loss = 0.245973, Learning Rate = 1.498193e-04\n",
      "Epoch 4997/20000: Train Loss = 0.443078, Test Loss = 0.247949, Learning Rate = 1.497624e-04\n",
      "Epoch 4998/20000: Train Loss = 0.443075, Test Loss = 0.249819, Learning Rate = 1.497055e-04\n",
      "Epoch 4999/20000: Train Loss = 0.443186, Test Loss = 0.249503, Learning Rate = 1.496486e-04\n",
      "Epoch 5000/20000: Train Loss = 0.443935, Test Loss = 0.251238, Learning Rate = 1.495917e-04\n",
      "Epoch 5001/20000: Train Loss = 0.442832, Test Loss = 0.245517, Learning Rate = 1.495349e-04\n",
      "Epoch 5002/20000: Train Loss = 0.442738, Test Loss = 0.245919, Learning Rate = 1.494781e-04\n",
      "Epoch 5003/20000: Train Loss = 0.443323, Test Loss = 0.248227, Learning Rate = 1.494213e-04\n",
      "Epoch 5004/20000: Train Loss = 0.443091, Test Loss = 0.243721, Learning Rate = 1.493645e-04\n",
      "Epoch 5005/20000: Train Loss = 0.443203, Test Loss = 0.252779, Learning Rate = 1.493077e-04\n",
      "Epoch 5006/20000: Train Loss = 0.443495, Test Loss = 0.256767, Learning Rate = 1.492510e-04\n",
      "Epoch 5007/20000: Train Loss = 0.443487, Test Loss = 0.243457, Learning Rate = 1.491943e-04\n",
      "Epoch 5008/20000: Train Loss = 0.443069, Test Loss = 0.246564, Learning Rate = 1.491376e-04\n",
      "Epoch 5009/20000: Train Loss = 0.443204, Test Loss = 0.246458, Learning Rate = 1.490809e-04\n",
      "Epoch 5010/20000: Train Loss = 0.443254, Test Loss = 0.249291, Learning Rate = 1.490243e-04\n",
      "Epoch 5011/20000: Train Loss = 0.443296, Test Loss = 0.243369, Learning Rate = 1.489677e-04\n",
      "Epoch 5012/20000: Train Loss = 0.443732, Test Loss = 0.247546, Learning Rate = 1.489111e-04\n",
      "Epoch 5013/20000: Train Loss = 0.442939, Test Loss = 0.246010, Learning Rate = 1.488545e-04\n",
      "Epoch 5014/20000: Train Loss = 0.442765, Test Loss = 0.249255, Learning Rate = 1.487979e-04\n",
      "Epoch 5015/20000: Train Loss = 0.443166, Test Loss = 0.247632, Learning Rate = 1.487414e-04\n",
      "Epoch 5016/20000: Train Loss = 0.443489, Test Loss = 0.246742, Learning Rate = 1.486849e-04\n",
      "Epoch 5017/20000: Train Loss = 0.443179, Test Loss = 0.247966, Learning Rate = 1.486284e-04\n",
      "Epoch 5018/20000: Train Loss = 0.443038, Test Loss = 0.241505, Learning Rate = 1.485719e-04\n",
      "Epoch 5019/20000: Train Loss = 0.443631, Test Loss = 0.247805, Learning Rate = 1.485154e-04\n",
      "Epoch 5020/20000: Train Loss = 0.443217, Test Loss = 0.243399, Learning Rate = 1.484590e-04\n",
      "Epoch 5021/20000: Train Loss = 0.443035, Test Loss = 0.250266, Learning Rate = 1.484026e-04\n",
      "Epoch 5022/20000: Train Loss = 0.443146, Test Loss = 0.251705, Learning Rate = 1.483462e-04\n",
      "Epoch 5023/20000: Train Loss = 0.443705, Test Loss = 0.251527, Learning Rate = 1.482898e-04\n",
      "Epoch 5024/20000: Train Loss = 0.443711, Test Loss = 0.262225, Learning Rate = 1.482335e-04\n",
      "Epoch 5025/20000: Train Loss = 0.444269, Test Loss = 0.246587, Learning Rate = 1.481772e-04\n",
      "Epoch 5026/20000: Train Loss = 0.443288, Test Loss = 0.254942, Learning Rate = 1.481209e-04\n",
      "Epoch 5027/20000: Train Loss = 0.444247, Test Loss = 0.257106, Learning Rate = 1.480646e-04\n",
      "Epoch 5028/20000: Train Loss = 0.443294, Test Loss = 0.257124, Learning Rate = 1.480083e-04\n",
      "Epoch 5029/20000: Train Loss = 0.443340, Test Loss = 0.247663, Learning Rate = 1.479521e-04\n",
      "Epoch 5030/20000: Train Loss = 0.443191, Test Loss = 0.243117, Learning Rate = 1.478959e-04\n",
      "Epoch 5031/20000: Train Loss = 0.442923, Test Loss = 0.251710, Learning Rate = 1.478397e-04\n",
      "Epoch 5032/20000: Train Loss = 0.444332, Test Loss = 0.254294, Learning Rate = 1.477835e-04\n",
      "Epoch 5033/20000: Train Loss = 0.442967, Test Loss = 0.254318, Learning Rate = 1.477273e-04\n",
      "Epoch 5034/20000: Train Loss = 0.443745, Test Loss = 0.246980, Learning Rate = 1.476712e-04\n",
      "Epoch 5035/20000: Train Loss = 0.443395, Test Loss = 0.255660, Learning Rate = 1.476151e-04\n",
      "Epoch 5036/20000: Train Loss = 0.443255, Test Loss = 0.252283, Learning Rate = 1.475590e-04\n",
      "Epoch 5037/20000: Train Loss = 0.442832, Test Loss = 0.247557, Learning Rate = 1.475029e-04\n",
      "Epoch 5038/20000: Train Loss = 0.444529, Test Loss = 0.244333, Learning Rate = 1.474469e-04\n",
      "Epoch 5039/20000: Train Loss = 0.442933, Test Loss = 0.248717, Learning Rate = 1.473909e-04\n",
      "Epoch 5040/20000: Train Loss = 0.442936, Test Loss = 0.252002, Learning Rate = 1.473349e-04\n",
      "Epoch 5041/20000: Train Loss = 0.443291, Test Loss = 0.245205, Learning Rate = 1.472789e-04\n",
      "Epoch 5042/20000: Train Loss = 0.443042, Test Loss = 0.245466, Learning Rate = 1.472229e-04\n",
      "Epoch 5043/20000: Train Loss = 0.443175, Test Loss = 0.244572, Learning Rate = 1.471670e-04\n",
      "Epoch 5044/20000: Train Loss = 0.443174, Test Loss = 0.247576, Learning Rate = 1.471110e-04\n",
      "Epoch 5045/20000: Train Loss = 0.443176, Test Loss = 0.242869, Learning Rate = 1.470551e-04\n",
      "Epoch 5046/20000: Train Loss = 0.442826, Test Loss = 0.249138, Learning Rate = 1.469993e-04\n",
      "Epoch 5047/20000: Train Loss = 0.443217, Test Loss = 0.250168, Learning Rate = 1.469434e-04\n",
      "Epoch 5048/20000: Train Loss = 0.443019, Test Loss = 0.247964, Learning Rate = 1.468876e-04\n",
      "Epoch 5049/20000: Train Loss = 0.443443, Test Loss = 0.248076, Learning Rate = 1.468318e-04\n",
      "Epoch 5050/20000: Train Loss = 0.443010, Test Loss = 0.245193, Learning Rate = 1.467760e-04\n",
      "Epoch 5051/20000: Train Loss = 0.443078, Test Loss = 0.251009, Learning Rate = 1.467202e-04\n",
      "Epoch 5052/20000: Train Loss = 0.443206, Test Loss = 0.248020, Learning Rate = 1.466645e-04\n",
      "Epoch 5053/20000: Train Loss = 0.442847, Test Loss = 0.248642, Learning Rate = 1.466087e-04\n",
      "Epoch 5054/20000: Train Loss = 0.443006, Test Loss = 0.249905, Learning Rate = 1.465530e-04\n",
      "Epoch 5055/20000: Train Loss = 0.442889, Test Loss = 0.251335, Learning Rate = 1.464973e-04\n",
      "Epoch 5056/20000: Train Loss = 0.442896, Test Loss = 0.247670, Learning Rate = 1.464417e-04\n",
      "Epoch 5057/20000: Train Loss = 0.443566, Test Loss = 0.242884, Learning Rate = 1.463860e-04\n",
      "Epoch 5058/20000: Train Loss = 0.443058, Test Loss = 0.244034, Learning Rate = 1.463304e-04\n",
      "Epoch 5059/20000: Train Loss = 0.443077, Test Loss = 0.249164, Learning Rate = 1.462748e-04\n",
      "Epoch 5060/20000: Train Loss = 0.442836, Test Loss = 0.248426, Learning Rate = 1.462192e-04\n",
      "Epoch 5061/20000: Train Loss = 0.443073, Test Loss = 0.248733, Learning Rate = 1.461637e-04\n",
      "Epoch 5062/20000: Train Loss = 0.443015, Test Loss = 0.251393, Learning Rate = 1.461081e-04\n",
      "Epoch 5063/20000: Train Loss = 0.444018, Test Loss = 0.245182, Learning Rate = 1.460526e-04\n",
      "Epoch 5064/20000: Train Loss = 0.442974, Test Loss = 0.247281, Learning Rate = 1.459971e-04\n",
      "Epoch 5065/20000: Train Loss = 0.443024, Test Loss = 0.245672, Learning Rate = 1.459416e-04\n",
      "Epoch 5066/20000: Train Loss = 0.443305, Test Loss = 0.249983, Learning Rate = 1.458862e-04\n",
      "Epoch 5067/20000: Train Loss = 0.442893, Test Loss = 0.243293, Learning Rate = 1.458307e-04\n",
      "Epoch 5068/20000: Train Loss = 0.443160, Test Loss = 0.244767, Learning Rate = 1.457753e-04\n",
      "Epoch 5069/20000: Train Loss = 0.443083, Test Loss = 0.244591, Learning Rate = 1.457199e-04\n",
      "Epoch 5070/20000: Train Loss = 0.444020, Test Loss = 0.248463, Learning Rate = 1.456646e-04\n",
      "Epoch 5071/20000: Train Loss = 0.443152, Test Loss = 0.246559, Learning Rate = 1.456092e-04\n",
      "Epoch 5072/20000: Train Loss = 0.443214, Test Loss = 0.244618, Learning Rate = 1.455539e-04\n",
      "Epoch 5073/20000: Train Loss = 0.443049, Test Loss = 0.251126, Learning Rate = 1.454986e-04\n",
      "Epoch 5074/20000: Train Loss = 0.443211, Test Loss = 0.247370, Learning Rate = 1.454433e-04\n",
      "Epoch 5075/20000: Train Loss = 0.443239, Test Loss = 0.248860, Learning Rate = 1.453880e-04\n",
      "Epoch 5076/20000: Train Loss = 0.443627, Test Loss = 0.241808, Learning Rate = 1.453328e-04\n",
      "Epoch 5077/20000: Train Loss = 0.443189, Test Loss = 0.250437, Learning Rate = 1.452776e-04\n",
      "Epoch 5078/20000: Train Loss = 0.443458, Test Loss = 0.247051, Learning Rate = 1.452224e-04\n",
      "Epoch 5079/20000: Train Loss = 0.443012, Test Loss = 0.246531, Learning Rate = 1.451672e-04\n",
      "Epoch 5080/20000: Train Loss = 0.443983, Test Loss = 0.250504, Learning Rate = 1.451120e-04\n",
      "Epoch 5081/20000: Train Loss = 0.442978, Test Loss = 0.244221, Learning Rate = 1.450569e-04\n",
      "Epoch 5082/20000: Train Loss = 0.443401, Test Loss = 0.249248, Learning Rate = 1.450018e-04\n",
      "Epoch 5083/20000: Train Loss = 0.443562, Test Loss = 0.249273, Learning Rate = 1.449467e-04\n",
      "Epoch 5084/20000: Train Loss = 0.442725, Test Loss = 0.242268, Learning Rate = 1.448916e-04\n",
      "Epoch 5085/20000: Train Loss = 0.442997, Test Loss = 0.243905, Learning Rate = 1.448366e-04\n",
      "Epoch 5086/20000: Train Loss = 0.443028, Test Loss = 0.244418, Learning Rate = 1.447815e-04\n",
      "Epoch 5087/20000: Train Loss = 0.443298, Test Loss = 0.244594, Learning Rate = 1.447265e-04\n",
      "Epoch 5088/20000: Train Loss = 0.444657, Test Loss = 0.242974, Learning Rate = 1.446715e-04\n",
      "Epoch 5089/20000: Train Loss = 0.443268, Test Loss = 0.253428, Learning Rate = 1.446165e-04\n",
      "Epoch 5090/20000: Train Loss = 0.442883, Test Loss = 0.250376, Learning Rate = 1.445616e-04\n",
      "Epoch 5091/20000: Train Loss = 0.444890, Test Loss = 0.244862, Learning Rate = 1.445067e-04\n",
      "Epoch 5092/20000: Train Loss = 0.443229, Test Loss = 0.244433, Learning Rate = 1.444518e-04\n",
      "Epoch 5093/20000: Train Loss = 0.442845, Test Loss = 0.246140, Learning Rate = 1.443969e-04\n",
      "Epoch 5094/20000: Train Loss = 0.443229, Test Loss = 0.242453, Learning Rate = 1.443420e-04\n",
      "Epoch 5095/20000: Train Loss = 0.442962, Test Loss = 0.251092, Learning Rate = 1.442872e-04\n",
      "Epoch 5096/20000: Train Loss = 0.443361, Test Loss = 0.240918, Learning Rate = 1.442323e-04\n",
      "Epoch 5097/20000: Train Loss = 0.443716, Test Loss = 0.247321, Learning Rate = 1.441775e-04\n",
      "Epoch 5098/20000: Train Loss = 0.442928, Test Loss = 0.248436, Learning Rate = 1.441227e-04\n",
      "Epoch 5099/20000: Train Loss = 0.442819, Test Loss = 0.243807, Learning Rate = 1.440680e-04\n",
      "Epoch 5100/20000: Train Loss = 0.442945, Test Loss = 0.246195, Learning Rate = 1.440132e-04\n",
      "Epoch 5101/20000: Train Loss = 0.443365, Test Loss = 0.244943, Learning Rate = 1.439585e-04\n",
      "Epoch 5102/20000: Train Loss = 0.442834, Test Loss = 0.249511, Learning Rate = 1.439038e-04\n",
      "Epoch 5103/20000: Train Loss = 0.443319, Test Loss = 0.249346, Learning Rate = 1.438491e-04\n",
      "Epoch 5104/20000: Train Loss = 0.443085, Test Loss = 0.246226, Learning Rate = 1.437945e-04\n",
      "Epoch 5105/20000: Train Loss = 0.442914, Test Loss = 0.247694, Learning Rate = 1.437398e-04\n",
      "Epoch 5106/20000: Train Loss = 0.443313, Test Loss = 0.248041, Learning Rate = 1.436852e-04\n",
      "Epoch 5107/20000: Train Loss = 0.443128, Test Loss = 0.249431, Learning Rate = 1.436306e-04\n",
      "Epoch 5108/20000: Train Loss = 0.443568, Test Loss = 0.256200, Learning Rate = 1.435760e-04\n",
      "Epoch 5109/20000: Train Loss = 0.442697, Test Loss = 0.243377, Learning Rate = 1.435215e-04\n",
      "Epoch 5110/20000: Train Loss = 0.442814, Test Loss = 0.247088, Learning Rate = 1.434670e-04\n",
      "Epoch 5111/20000: Train Loss = 0.443868, Test Loss = 0.258929, Learning Rate = 1.434124e-04\n",
      "Epoch 5112/20000: Train Loss = 0.443462, Test Loss = 0.249422, Learning Rate = 1.433580e-04\n",
      "Epoch 5113/20000: Train Loss = 0.444006, Test Loss = 0.249931, Learning Rate = 1.433035e-04\n",
      "Epoch 5114/20000: Train Loss = 0.442866, Test Loss = 0.248175, Learning Rate = 1.432490e-04\n",
      "Epoch 5115/20000: Train Loss = 0.442973, Test Loss = 0.247734, Learning Rate = 1.431946e-04\n",
      "Epoch 5116/20000: Train Loss = 0.443420, Test Loss = 0.253410, Learning Rate = 1.431402e-04\n",
      "Epoch 5117/20000: Train Loss = 0.443396, Test Loss = 0.245431, Learning Rate = 1.430858e-04\n",
      "Epoch 5118/20000: Train Loss = 0.443567, Test Loss = 0.246694, Learning Rate = 1.430314e-04\n",
      "Epoch 5119/20000: Train Loss = 0.442900, Test Loss = 0.246625, Learning Rate = 1.429771e-04\n",
      "Epoch 5120/20000: Train Loss = 0.443089, Test Loss = 0.251051, Learning Rate = 1.429228e-04\n",
      "Epoch 5121/20000: Train Loss = 0.443554, Test Loss = 0.250100, Learning Rate = 1.428684e-04\n",
      "Epoch 5122/20000: Train Loss = 0.442868, Test Loss = 0.247892, Learning Rate = 1.428142e-04\n",
      "Epoch 5123/20000: Train Loss = 0.443886, Test Loss = 0.246063, Learning Rate = 1.427599e-04\n",
      "Epoch 5124/20000: Train Loss = 0.443265, Test Loss = 0.247899, Learning Rate = 1.427056e-04\n",
      "Epoch 5125/20000: Train Loss = 0.443089, Test Loss = 0.253145, Learning Rate = 1.426514e-04\n",
      "Epoch 5126/20000: Train Loss = 0.443219, Test Loss = 0.248110, Learning Rate = 1.425972e-04\n",
      "Epoch 5127/20000: Train Loss = 0.442873, Test Loss = 0.247904, Learning Rate = 1.425430e-04\n",
      "Epoch 5128/20000: Train Loss = 0.443178, Test Loss = 0.248029, Learning Rate = 1.424889e-04\n",
      "Epoch 5129/20000: Train Loss = 0.443091, Test Loss = 0.245069, Learning Rate = 1.424347e-04\n",
      "Epoch 5130/20000: Train Loss = 0.443311, Test Loss = 0.245449, Learning Rate = 1.423806e-04\n",
      "Epoch 5131/20000: Train Loss = 0.443383, Test Loss = 0.249659, Learning Rate = 1.423265e-04\n",
      "Epoch 5132/20000: Train Loss = 0.443274, Test Loss = 0.253078, Learning Rate = 1.422724e-04\n",
      "Epoch 5133/20000: Train Loss = 0.443518, Test Loss = 0.242262, Learning Rate = 1.422184e-04\n",
      "Epoch 5134/20000: Train Loss = 0.443205, Test Loss = 0.244830, Learning Rate = 1.421643e-04\n",
      "Epoch 5135/20000: Train Loss = 0.443406, Test Loss = 0.251455, Learning Rate = 1.421103e-04\n",
      "Epoch 5136/20000: Train Loss = 0.443187, Test Loss = 0.247016, Learning Rate = 1.420563e-04\n",
      "Epoch 5137/20000: Train Loss = 0.443275, Test Loss = 0.247594, Learning Rate = 1.420023e-04\n",
      "Epoch 5138/20000: Train Loss = 0.443951, Test Loss = 0.249326, Learning Rate = 1.419484e-04\n",
      "Epoch 5139/20000: Train Loss = 0.443263, Test Loss = 0.246571, Learning Rate = 1.418944e-04\n",
      "Epoch 5140/20000: Train Loss = 0.443301, Test Loss = 0.253671, Learning Rate = 1.418405e-04\n",
      "Epoch 5141/20000: Train Loss = 0.443220, Test Loss = 0.253313, Learning Rate = 1.417866e-04\n",
      "Epoch 5142/20000: Train Loss = 0.443013, Test Loss = 0.250490, Learning Rate = 1.417328e-04\n",
      "Epoch 5143/20000: Train Loss = 0.443281, Test Loss = 0.259585, Learning Rate = 1.416789e-04\n",
      "Epoch 5144/20000: Train Loss = 0.443206, Test Loss = 0.250201, Learning Rate = 1.416251e-04\n",
      "Epoch 5145/20000: Train Loss = 0.443187, Test Loss = 0.247615, Learning Rate = 1.415713e-04\n",
      "Epoch 5146/20000: Train Loss = 0.443278, Test Loss = 0.249931, Learning Rate = 1.415175e-04\n",
      "Epoch 5147/20000: Train Loss = 0.442964, Test Loss = 0.253078, Learning Rate = 1.414637e-04\n",
      "Epoch 5148/20000: Train Loss = 0.443148, Test Loss = 0.250911, Learning Rate = 1.414099e-04\n",
      "Epoch 5149/20000: Train Loss = 0.443268, Test Loss = 0.250493, Learning Rate = 1.413562e-04\n",
      "Epoch 5150/20000: Train Loss = 0.443390, Test Loss = 0.245839, Learning Rate = 1.413025e-04\n",
      "Epoch 5151/20000: Train Loss = 0.442982, Test Loss = 0.245239, Learning Rate = 1.412488e-04\n",
      "Epoch 5152/20000: Train Loss = 0.443891, Test Loss = 0.239726, Learning Rate = 1.411951e-04\n",
      "Epoch 5153/20000: Train Loss = 0.443349, Test Loss = 0.253912, Learning Rate = 1.411415e-04\n",
      "Epoch 5154/20000: Train Loss = 0.442753, Test Loss = 0.246819, Learning Rate = 1.410879e-04\n",
      "Epoch 5155/20000: Train Loss = 0.443515, Test Loss = 0.239897, Learning Rate = 1.410342e-04\n",
      "Epoch 5156/20000: Train Loss = 0.443447, Test Loss = 0.251833, Learning Rate = 1.409807e-04\n",
      "Epoch 5157/20000: Train Loss = 0.443075, Test Loss = 0.250434, Learning Rate = 1.409271e-04\n",
      "Epoch 5158/20000: Train Loss = 0.443810, Test Loss = 0.253634, Learning Rate = 1.408735e-04\n",
      "Epoch 5159/20000: Train Loss = 0.442898, Test Loss = 0.243092, Learning Rate = 1.408200e-04\n",
      "Epoch 5160/20000: Train Loss = 0.443684, Test Loss = 0.242658, Learning Rate = 1.407665e-04\n",
      "Epoch 5161/20000: Train Loss = 0.443363, Test Loss = 0.251368, Learning Rate = 1.407130e-04\n",
      "Epoch 5162/20000: Train Loss = 0.443038, Test Loss = 0.247195, Learning Rate = 1.406595e-04\n",
      "Epoch 5163/20000: Train Loss = 0.443239, Test Loss = 0.243218, Learning Rate = 1.406061e-04\n",
      "Epoch 5164/20000: Train Loss = 0.442872, Test Loss = 0.246951, Learning Rate = 1.405527e-04\n",
      "Epoch 5165/20000: Train Loss = 0.442983, Test Loss = 0.245646, Learning Rate = 1.404993e-04\n",
      "Epoch 5166/20000: Train Loss = 0.443076, Test Loss = 0.248833, Learning Rate = 1.404459e-04\n",
      "Epoch 5167/20000: Train Loss = 0.443014, Test Loss = 0.248507, Learning Rate = 1.403925e-04\n",
      "Epoch 5168/20000: Train Loss = 0.443033, Test Loss = 0.250120, Learning Rate = 1.403392e-04\n",
      "Epoch 5169/20000: Train Loss = 0.442732, Test Loss = 0.244691, Learning Rate = 1.402858e-04\n",
      "Epoch 5170/20000: Train Loss = 0.443175, Test Loss = 0.240853, Learning Rate = 1.402325e-04\n",
      "Epoch 5171/20000: Train Loss = 0.442876, Test Loss = 0.247812, Learning Rate = 1.401793e-04\n",
      "Epoch 5172/20000: Train Loss = 0.443276, Test Loss = 0.246742, Learning Rate = 1.401260e-04\n",
      "Epoch 5173/20000: Train Loss = 0.442934, Test Loss = 0.242687, Learning Rate = 1.400727e-04\n",
      "Epoch 5174/20000: Train Loss = 0.443057, Test Loss = 0.243487, Learning Rate = 1.400195e-04\n",
      "Epoch 5175/20000: Train Loss = 0.443438, Test Loss = 0.248552, Learning Rate = 1.399663e-04\n",
      "Epoch 5176/20000: Train Loss = 0.442693, Test Loss = 0.242683, Learning Rate = 1.399131e-04\n",
      "Epoch 5177/20000: Train Loss = 0.443163, Test Loss = 0.241750, Learning Rate = 1.398600e-04\n",
      "Epoch 5178/20000: Train Loss = 0.443317, Test Loss = 0.247371, Learning Rate = 1.398068e-04\n",
      "Epoch 5179/20000: Train Loss = 0.443196, Test Loss = 0.245407, Learning Rate = 1.397537e-04\n",
      "Epoch 5180/20000: Train Loss = 0.443193, Test Loss = 0.243754, Learning Rate = 1.397006e-04\n",
      "Epoch 5181/20000: Train Loss = 0.442851, Test Loss = 0.243860, Learning Rate = 1.396475e-04\n",
      "Epoch 5182/20000: Train Loss = 0.443431, Test Loss = 0.241315, Learning Rate = 1.395945e-04\n",
      "Epoch 5183/20000: Train Loss = 0.443222, Test Loss = 0.245449, Learning Rate = 1.395414e-04\n",
      "Epoch 5184/20000: Train Loss = 0.443194, Test Loss = 0.242488, Learning Rate = 1.394884e-04\n",
      "Epoch 5185/20000: Train Loss = 0.442865, Test Loss = 0.245228, Learning Rate = 1.394354e-04\n",
      "Epoch 5186/20000: Train Loss = 0.443156, Test Loss = 0.249909, Learning Rate = 1.393824e-04\n",
      "Epoch 5187/20000: Train Loss = 0.443610, Test Loss = 0.246482, Learning Rate = 1.393294e-04\n",
      "Epoch 5188/20000: Train Loss = 0.443000, Test Loss = 0.251450, Learning Rate = 1.392765e-04\n",
      "Epoch 5189/20000: Train Loss = 0.442959, Test Loss = 0.246946, Learning Rate = 1.392236e-04\n",
      "Epoch 5190/20000: Train Loss = 0.443054, Test Loss = 0.244462, Learning Rate = 1.391707e-04\n",
      "Epoch 5191/20000: Train Loss = 0.442960, Test Loss = 0.243564, Learning Rate = 1.391178e-04\n",
      "Epoch 5192/20000: Train Loss = 0.442907, Test Loss = 0.250183, Learning Rate = 1.390649e-04\n",
      "Epoch 5193/20000: Train Loss = 0.443309, Test Loss = 0.249745, Learning Rate = 1.390121e-04\n",
      "Epoch 5194/20000: Train Loss = 0.442870, Test Loss = 0.242895, Learning Rate = 1.389593e-04\n",
      "Epoch 5195/20000: Train Loss = 0.443614, Test Loss = 0.248716, Learning Rate = 1.389065e-04\n",
      "Epoch 5196/20000: Train Loss = 0.444026, Test Loss = 0.248135, Learning Rate = 1.388537e-04\n",
      "Epoch 5197/20000: Train Loss = 0.444362, Test Loss = 0.241666, Learning Rate = 1.388009e-04\n",
      "Epoch 5198/20000: Train Loss = 0.443572, Test Loss = 0.251601, Learning Rate = 1.387482e-04\n",
      "Epoch 5199/20000: Train Loss = 0.443077, Test Loss = 0.242424, Learning Rate = 1.386955e-04\n",
      "Epoch 5200/20000: Train Loss = 0.442967, Test Loss = 0.245822, Learning Rate = 1.386428e-04\n",
      "Epoch 5201/20000: Train Loss = 0.443021, Test Loss = 0.245113, Learning Rate = 1.385901e-04\n",
      "Epoch 5202/20000: Train Loss = 0.443051, Test Loss = 0.246343, Learning Rate = 1.385374e-04\n",
      "Epoch 5203/20000: Train Loss = 0.443617, Test Loss = 0.243240, Learning Rate = 1.384848e-04\n",
      "Epoch 5204/20000: Train Loss = 0.442464, Test Loss = 0.247180, Learning Rate = 1.384322e-04\n",
      "Epoch 5205/20000: Train Loss = 0.443238, Test Loss = 0.246065, Learning Rate = 1.383796e-04\n",
      "Epoch 5206/20000: Train Loss = 0.443100, Test Loss = 0.244261, Learning Rate = 1.383270e-04\n",
      "Epoch 5207/20000: Train Loss = 0.443381, Test Loss = 0.250132, Learning Rate = 1.382744e-04\n",
      "Epoch 5208/20000: Train Loss = 0.443499, Test Loss = 0.240235, Learning Rate = 1.382219e-04\n",
      "Epoch 5209/20000: Train Loss = 0.443715, Test Loss = 0.248359, Learning Rate = 1.381694e-04\n",
      "Epoch 5210/20000: Train Loss = 0.443973, Test Loss = 0.246569, Learning Rate = 1.381169e-04\n",
      "Epoch 5211/20000: Train Loss = 0.443921, Test Loss = 0.252723, Learning Rate = 1.380644e-04\n",
      "Epoch 5212/20000: Train Loss = 0.443282, Test Loss = 0.245222, Learning Rate = 1.380119e-04\n",
      "Epoch 5213/20000: Train Loss = 0.443583, Test Loss = 0.244676, Learning Rate = 1.379595e-04\n",
      "Epoch 5214/20000: Train Loss = 0.442833, Test Loss = 0.244643, Learning Rate = 1.379071e-04\n",
      "Epoch 5215/20000: Train Loss = 0.443107, Test Loss = 0.249528, Learning Rate = 1.378547e-04\n",
      "Epoch 5216/20000: Train Loss = 0.442915, Test Loss = 0.244455, Learning Rate = 1.378023e-04\n",
      "Epoch 5217/20000: Train Loss = 0.442858, Test Loss = 0.252107, Learning Rate = 1.377499e-04\n",
      "Epoch 5218/20000: Train Loss = 0.443193, Test Loss = 0.247276, Learning Rate = 1.376976e-04\n",
      "Epoch 5219/20000: Train Loss = 0.443120, Test Loss = 0.247351, Learning Rate = 1.376453e-04\n",
      "Epoch 5220/20000: Train Loss = 0.443671, Test Loss = 0.254926, Learning Rate = 1.375930e-04\n",
      "Epoch 5221/20000: Train Loss = 0.442895, Test Loss = 0.241035, Learning Rate = 1.375407e-04\n",
      "Epoch 5222/20000: Train Loss = 0.443814, Test Loss = 0.244515, Learning Rate = 1.374884e-04\n",
      "Epoch 5223/20000: Train Loss = 0.443266, Test Loss = 0.244201, Learning Rate = 1.374362e-04\n",
      "Epoch 5224/20000: Train Loss = 0.442818, Test Loss = 0.245195, Learning Rate = 1.373840e-04\n",
      "Epoch 5225/20000: Train Loss = 0.443547, Test Loss = 0.249522, Learning Rate = 1.373318e-04\n",
      "Epoch 5226/20000: Train Loss = 0.442988, Test Loss = 0.247198, Learning Rate = 1.372796e-04\n",
      "Epoch 5227/20000: Train Loss = 0.443568, Test Loss = 0.241613, Learning Rate = 1.372274e-04\n",
      "Epoch 5228/20000: Train Loss = 0.443492, Test Loss = 0.250684, Learning Rate = 1.371753e-04\n",
      "Epoch 5229/20000: Train Loss = 0.443022, Test Loss = 0.246080, Learning Rate = 1.371231e-04\n",
      "Epoch 5230/20000: Train Loss = 0.443466, Test Loss = 0.249825, Learning Rate = 1.370710e-04\n",
      "Epoch 5231/20000: Train Loss = 0.442954, Test Loss = 0.247612, Learning Rate = 1.370190e-04\n",
      "Epoch 5232/20000: Train Loss = 0.442898, Test Loss = 0.248627, Learning Rate = 1.369669e-04\n",
      "Epoch 5233/20000: Train Loss = 0.442606, Test Loss = 0.244382, Learning Rate = 1.369148e-04\n",
      "Epoch 5234/20000: Train Loss = 0.443286, Test Loss = 0.241136, Learning Rate = 1.368628e-04\n",
      "Epoch 5235/20000: Train Loss = 0.443059, Test Loss = 0.251127, Learning Rate = 1.368108e-04\n",
      "Epoch 5236/20000: Train Loss = 0.443011, Test Loss = 0.252836, Learning Rate = 1.367588e-04\n",
      "Epoch 5237/20000: Train Loss = 0.443063, Test Loss = 0.247173, Learning Rate = 1.367069e-04\n",
      "Epoch 5238/20000: Train Loss = 0.442795, Test Loss = 0.249482, Learning Rate = 1.366549e-04\n",
      "Epoch 5239/20000: Train Loss = 0.443617, Test Loss = 0.252676, Learning Rate = 1.366030e-04\n",
      "Epoch 5240/20000: Train Loss = 0.443253, Test Loss = 0.246866, Learning Rate = 1.365511e-04\n",
      "Epoch 5241/20000: Train Loss = 0.443084, Test Loss = 0.243277, Learning Rate = 1.364992e-04\n",
      "Epoch 5242/20000: Train Loss = 0.442851, Test Loss = 0.244744, Learning Rate = 1.364473e-04\n",
      "Epoch 5243/20000: Train Loss = 0.442869, Test Loss = 0.246779, Learning Rate = 1.363955e-04\n",
      "Epoch 5244/20000: Train Loss = 0.443080, Test Loss = 0.242735, Learning Rate = 1.363437e-04\n",
      "Epoch 5245/20000: Train Loss = 0.443911, Test Loss = 0.245716, Learning Rate = 1.362919e-04\n",
      "Epoch 5246/20000: Train Loss = 0.443061, Test Loss = 0.249029, Learning Rate = 1.362401e-04\n",
      "Epoch 5247/20000: Train Loss = 0.443278, Test Loss = 0.246365, Learning Rate = 1.361883e-04\n",
      "Epoch 5248/20000: Train Loss = 0.443294, Test Loss = 0.254714, Learning Rate = 1.361366e-04\n",
      "Epoch 5249/20000: Train Loss = 0.443019, Test Loss = 0.250455, Learning Rate = 1.360848e-04\n",
      "Epoch 5250/20000: Train Loss = 0.442993, Test Loss = 0.249844, Learning Rate = 1.360331e-04\n",
      "Epoch 5251/20000: Train Loss = 0.443699, Test Loss = 0.245816, Learning Rate = 1.359814e-04\n",
      "Epoch 5252/20000: Train Loss = 0.443801, Test Loss = 0.247257, Learning Rate = 1.359298e-04\n",
      "Epoch 5253/20000: Train Loss = 0.443009, Test Loss = 0.245276, Learning Rate = 1.358781e-04\n",
      "Epoch 5254/20000: Train Loss = 0.443410, Test Loss = 0.239852, Learning Rate = 1.358265e-04\n",
      "Epoch 5255/20000: Train Loss = 0.443135, Test Loss = 0.247639, Learning Rate = 1.357749e-04\n",
      "Epoch 5256/20000: Train Loss = 0.443309, Test Loss = 0.249238, Learning Rate = 1.357233e-04\n",
      "Epoch 5257/20000: Train Loss = 0.443240, Test Loss = 0.251308, Learning Rate = 1.356717e-04\n",
      "Epoch 5258/20000: Train Loss = 0.443381, Test Loss = 0.243931, Learning Rate = 1.356202e-04\n",
      "Epoch 5259/20000: Train Loss = 0.442895, Test Loss = 0.246206, Learning Rate = 1.355686e-04\n",
      "Epoch 5260/20000: Train Loss = 0.443011, Test Loss = 0.250888, Learning Rate = 1.355171e-04\n",
      "Epoch 5261/20000: Train Loss = 0.444029, Test Loss = 0.246907, Learning Rate = 1.354656e-04\n",
      "Epoch 5262/20000: Train Loss = 0.442871, Test Loss = 0.252249, Learning Rate = 1.354142e-04\n",
      "Epoch 5263/20000: Train Loss = 0.443789, Test Loss = 0.249383, Learning Rate = 1.353627e-04\n",
      "Epoch 5264/20000: Train Loss = 0.443078, Test Loss = 0.239855, Learning Rate = 1.353113e-04\n",
      "Epoch 5265/20000: Train Loss = 0.443439, Test Loss = 0.243180, Learning Rate = 1.352599e-04\n",
      "Epoch 5266/20000: Train Loss = 0.443114, Test Loss = 0.246849, Learning Rate = 1.352085e-04\n",
      "Epoch 5267/20000: Train Loss = 0.443004, Test Loss = 0.247987, Learning Rate = 1.351571e-04\n",
      "Epoch 5268/20000: Train Loss = 0.442859, Test Loss = 0.249365, Learning Rate = 1.351057e-04\n",
      "Epoch 5269/20000: Train Loss = 0.443182, Test Loss = 0.254179, Learning Rate = 1.350544e-04\n",
      "Epoch 5270/20000: Train Loss = 0.442561, Test Loss = 0.244636, Learning Rate = 1.350031e-04\n",
      "Epoch 5271/20000: Train Loss = 0.443141, Test Loss = 0.244078, Learning Rate = 1.349518e-04\n",
      "Epoch 5272/20000: Train Loss = 0.443402, Test Loss = 0.242912, Learning Rate = 1.349005e-04\n",
      "Epoch 5273/20000: Train Loss = 0.443050, Test Loss = 0.248510, Learning Rate = 1.348492e-04\n",
      "Epoch 5274/20000: Train Loss = 0.443337, Test Loss = 0.247916, Learning Rate = 1.347980e-04\n",
      "Epoch 5275/20000: Train Loss = 0.443009, Test Loss = 0.251010, Learning Rate = 1.347468e-04\n",
      "Epoch 5276/20000: Train Loss = 0.442942, Test Loss = 0.251660, Learning Rate = 1.346956e-04\n",
      "Epoch 5277/20000: Train Loss = 0.443000, Test Loss = 0.253836, Learning Rate = 1.346444e-04\n",
      "Epoch 5278/20000: Train Loss = 0.443111, Test Loss = 0.246155, Learning Rate = 1.345932e-04\n",
      "Epoch 5279/20000: Train Loss = 0.443513, Test Loss = 0.247271, Learning Rate = 1.345421e-04\n",
      "Epoch 5280/20000: Train Loss = 0.443389, Test Loss = 0.250601, Learning Rate = 1.344910e-04\n",
      "Epoch 5281/20000: Train Loss = 0.442812, Test Loss = 0.244968, Learning Rate = 1.344399e-04\n",
      "Epoch 5282/20000: Train Loss = 0.443694, Test Loss = 0.243878, Learning Rate = 1.343888e-04\n",
      "Epoch 5283/20000: Train Loss = 0.443557, Test Loss = 0.251648, Learning Rate = 1.343377e-04\n",
      "Epoch 5284/20000: Train Loss = 0.443274, Test Loss = 0.243935, Learning Rate = 1.342867e-04\n",
      "Epoch 5285/20000: Train Loss = 0.443432, Test Loss = 0.239561, Learning Rate = 1.342357e-04\n",
      "Epoch 5286/20000: Train Loss = 0.444216, Test Loss = 0.247443, Learning Rate = 1.341846e-04\n",
      "Epoch 5287/20000: Train Loss = 0.444119, Test Loss = 0.247033, Learning Rate = 1.341337e-04\n",
      "Epoch 5288/20000: Train Loss = 0.444001, Test Loss = 0.246993, Learning Rate = 1.340827e-04\n",
      "Epoch 5289/20000: Train Loss = 0.443888, Test Loss = 0.246158, Learning Rate = 1.340317e-04\n",
      "Epoch 5290/20000: Train Loss = 0.444303, Test Loss = 0.245779, Learning Rate = 1.339808e-04\n",
      "Epoch 5291/20000: Train Loss = 0.443629, Test Loss = 0.246153, Learning Rate = 1.339299e-04\n",
      "Epoch 5292/20000: Train Loss = 0.443193, Test Loss = 0.245883, Learning Rate = 1.338790e-04\n",
      "Epoch 5293/20000: Train Loss = 0.443077, Test Loss = 0.248717, Learning Rate = 1.338281e-04\n",
      "Epoch 5294/20000: Train Loss = 0.442737, Test Loss = 0.241755, Learning Rate = 1.337773e-04\n",
      "Epoch 5295/20000: Train Loss = 0.443250, Test Loss = 0.246229, Learning Rate = 1.337265e-04\n",
      "Epoch 5296/20000: Train Loss = 0.443670, Test Loss = 0.248689, Learning Rate = 1.336757e-04\n",
      "Epoch 5297/20000: Train Loss = 0.443658, Test Loss = 0.248028, Learning Rate = 1.336249e-04\n",
      "Epoch 5298/20000: Train Loss = 0.442861, Test Loss = 0.245173, Learning Rate = 1.335741e-04\n",
      "Epoch 5299/20000: Train Loss = 0.443262, Test Loss = 0.240937, Learning Rate = 1.335233e-04\n",
      "Epoch 5300/20000: Train Loss = 0.443839, Test Loss = 0.251427, Learning Rate = 1.334726e-04\n",
      "Epoch 5301/20000: Train Loss = 0.442868, Test Loss = 0.246085, Learning Rate = 1.334219e-04\n",
      "Epoch 5302/20000: Train Loss = 0.442891, Test Loss = 0.245161, Learning Rate = 1.333712e-04\n",
      "Epoch 5303/20000: Train Loss = 0.443619, Test Loss = 0.246638, Learning Rate = 1.333205e-04\n",
      "Epoch 5304/20000: Train Loss = 0.443356, Test Loss = 0.244848, Learning Rate = 1.332698e-04\n",
      "Epoch 5305/20000: Train Loss = 0.443025, Test Loss = 0.246824, Learning Rate = 1.332192e-04\n",
      "Epoch 5306/20000: Train Loss = 0.443020, Test Loss = 0.242934, Learning Rate = 1.331686e-04\n",
      "Epoch 5307/20000: Train Loss = 0.442850, Test Loss = 0.250212, Learning Rate = 1.331180e-04\n",
      "Epoch 5308/20000: Train Loss = 0.443462, Test Loss = 0.250479, Learning Rate = 1.330674e-04\n",
      "Epoch 5309/20000: Train Loss = 0.442940, Test Loss = 0.244058, Learning Rate = 1.330168e-04\n",
      "Epoch 5310/20000: Train Loss = 0.443502, Test Loss = 0.246668, Learning Rate = 1.329663e-04\n",
      "Epoch 5311/20000: Train Loss = 0.443739, Test Loss = 0.249930, Learning Rate = 1.329158e-04\n",
      "Epoch 5312/20000: Train Loss = 0.443493, Test Loss = 0.246609, Learning Rate = 1.328653e-04\n",
      "Epoch 5313/20000: Train Loss = 0.443236, Test Loss = 0.248880, Learning Rate = 1.328148e-04\n",
      "Epoch 5314/20000: Train Loss = 0.443546, Test Loss = 0.245489, Learning Rate = 1.327643e-04\n",
      "Epoch 5315/20000: Train Loss = 0.443212, Test Loss = 0.253608, Learning Rate = 1.327139e-04\n",
      "Epoch 5316/20000: Train Loss = 0.443367, Test Loss = 0.246466, Learning Rate = 1.326634e-04\n",
      "Epoch 5317/20000: Train Loss = 0.443272, Test Loss = 0.254287, Learning Rate = 1.326130e-04\n",
      "Epoch 5318/20000: Train Loss = 0.443540, Test Loss = 0.246579, Learning Rate = 1.325626e-04\n",
      "Epoch 5319/20000: Train Loss = 0.442970, Test Loss = 0.246857, Learning Rate = 1.325123e-04\n",
      "Epoch 5320/20000: Train Loss = 0.443329, Test Loss = 0.248704, Learning Rate = 1.324619e-04\n",
      "Epoch 5321/20000: Train Loss = 0.442978, Test Loss = 0.249563, Learning Rate = 1.324116e-04\n",
      "Epoch 5322/20000: Train Loss = 0.442892, Test Loss = 0.253611, Learning Rate = 1.323613e-04\n",
      "Epoch 5323/20000: Train Loss = 0.443194, Test Loss = 0.247552, Learning Rate = 1.323110e-04\n",
      "Epoch 5324/20000: Train Loss = 0.443340, Test Loss = 0.252396, Learning Rate = 1.322607e-04\n",
      "Epoch 5325/20000: Train Loss = 0.442790, Test Loss = 0.244186, Learning Rate = 1.322105e-04\n",
      "Epoch 5326/20000: Train Loss = 0.442892, Test Loss = 0.249193, Learning Rate = 1.321602e-04\n",
      "Epoch 5327/20000: Train Loss = 0.443162, Test Loss = 0.250897, Learning Rate = 1.321100e-04\n",
      "Epoch 5328/20000: Train Loss = 0.443283, Test Loss = 0.249099, Learning Rate = 1.320598e-04\n",
      "Epoch 5329/20000: Train Loss = 0.443370, Test Loss = 0.238580, Learning Rate = 1.320096e-04\n",
      "Epoch 5330/20000: Train Loss = 0.443514, Test Loss = 0.255416, Learning Rate = 1.319595e-04\n",
      "Epoch 5331/20000: Train Loss = 0.443498, Test Loss = 0.239537, Learning Rate = 1.319093e-04\n",
      "Epoch 5332/20000: Train Loss = 0.442704, Test Loss = 0.249082, Learning Rate = 1.318592e-04\n",
      "Epoch 5333/20000: Train Loss = 0.442747, Test Loss = 0.249562, Learning Rate = 1.318091e-04\n",
      "Epoch 5334/20000: Train Loss = 0.443185, Test Loss = 0.246587, Learning Rate = 1.317590e-04\n",
      "Epoch 5335/20000: Train Loss = 0.442877, Test Loss = 0.244679, Learning Rate = 1.317090e-04\n",
      "Epoch 5336/20000: Train Loss = 0.442766, Test Loss = 0.248349, Learning Rate = 1.316589e-04\n",
      "Epoch 5337/20000: Train Loss = 0.442689, Test Loss = 0.246302, Learning Rate = 1.316089e-04\n",
      "Epoch 5338/20000: Train Loss = 0.442897, Test Loss = 0.246008, Learning Rate = 1.315589e-04\n",
      "Epoch 5339/20000: Train Loss = 0.443555, Test Loss = 0.247339, Learning Rate = 1.315089e-04\n",
      "Epoch 5340/20000: Train Loss = 0.442947, Test Loss = 0.248792, Learning Rate = 1.314589e-04\n",
      "Epoch 5341/20000: Train Loss = 0.442820, Test Loss = 0.248328, Learning Rate = 1.314090e-04\n",
      "Epoch 5342/20000: Train Loss = 0.442983, Test Loss = 0.248411, Learning Rate = 1.313590e-04\n",
      "Epoch 5343/20000: Train Loss = 0.442905, Test Loss = 0.249680, Learning Rate = 1.313091e-04\n",
      "Epoch 5344/20000: Train Loss = 0.443066, Test Loss = 0.245290, Learning Rate = 1.312592e-04\n",
      "Epoch 5345/20000: Train Loss = 0.443633, Test Loss = 0.254339, Learning Rate = 1.312094e-04\n",
      "Epoch 5346/20000: Train Loss = 0.442945, Test Loss = 0.243634, Learning Rate = 1.311595e-04\n",
      "Epoch 5347/20000: Train Loss = 0.443057, Test Loss = 0.247475, Learning Rate = 1.311097e-04\n",
      "Epoch 5348/20000: Train Loss = 0.443030, Test Loss = 0.248079, Learning Rate = 1.310598e-04\n",
      "Epoch 5349/20000: Train Loss = 0.442950, Test Loss = 0.240143, Learning Rate = 1.310100e-04\n",
      "Epoch 5350/20000: Train Loss = 0.443135, Test Loss = 0.240943, Learning Rate = 1.309603e-04\n",
      "Epoch 5351/20000: Train Loss = 0.443205, Test Loss = 0.245989, Learning Rate = 1.309105e-04\n",
      "Epoch 5352/20000: Train Loss = 0.443259, Test Loss = 0.247507, Learning Rate = 1.308608e-04\n",
      "Epoch 5353/20000: Train Loss = 0.442913, Test Loss = 0.245934, Learning Rate = 1.308110e-04\n",
      "Epoch 5354/20000: Train Loss = 0.443656, Test Loss = 0.245472, Learning Rate = 1.307613e-04\n",
      "Epoch 5355/20000: Train Loss = 0.444131, Test Loss = 0.248328, Learning Rate = 1.307116e-04\n",
      "Epoch 5356/20000: Train Loss = 0.443398, Test Loss = 0.244228, Learning Rate = 1.306620e-04\n",
      "Epoch 5357/20000: Train Loss = 0.443008, Test Loss = 0.245995, Learning Rate = 1.306123e-04\n",
      "Epoch 5358/20000: Train Loss = 0.443002, Test Loss = 0.251012, Learning Rate = 1.305627e-04\n",
      "Epoch 5359/20000: Train Loss = 0.443315, Test Loss = 0.247371, Learning Rate = 1.305131e-04\n",
      "Epoch 5360/20000: Train Loss = 0.442994, Test Loss = 0.250233, Learning Rate = 1.304635e-04\n",
      "Epoch 5361/20000: Train Loss = 0.443456, Test Loss = 0.248836, Learning Rate = 1.304139e-04\n",
      "Epoch 5362/20000: Train Loss = 0.442761, Test Loss = 0.249051, Learning Rate = 1.303644e-04\n",
      "Epoch 5363/20000: Train Loss = 0.443061, Test Loss = 0.242725, Learning Rate = 1.303148e-04\n",
      "Epoch 5364/20000: Train Loss = 0.443383, Test Loss = 0.250174, Learning Rate = 1.302653e-04\n",
      "Epoch 5365/20000: Train Loss = 0.443380, Test Loss = 0.245329, Learning Rate = 1.302158e-04\n",
      "Epoch 5366/20000: Train Loss = 0.444197, Test Loss = 0.244450, Learning Rate = 1.301663e-04\n",
      "Epoch 5367/20000: Train Loss = 0.443384, Test Loss = 0.248962, Learning Rate = 1.301169e-04\n",
      "Epoch 5368/20000: Train Loss = 0.442883, Test Loss = 0.245221, Learning Rate = 1.300674e-04\n",
      "Epoch 5369/20000: Train Loss = 0.443837, Test Loss = 0.246596, Learning Rate = 1.300180e-04\n",
      "Epoch 5370/20000: Train Loss = 0.443478, Test Loss = 0.247659, Learning Rate = 1.299686e-04\n",
      "Epoch 5371/20000: Train Loss = 0.443825, Test Loss = 0.249512, Learning Rate = 1.299192e-04\n",
      "Epoch 5372/20000: Train Loss = 0.443776, Test Loss = 0.256819, Learning Rate = 1.298699e-04\n",
      "Epoch 5373/20000: Train Loss = 0.442763, Test Loss = 0.247748, Learning Rate = 1.298205e-04\n",
      "Epoch 5374/20000: Train Loss = 0.442940, Test Loss = 0.248379, Learning Rate = 1.297712e-04\n",
      "Epoch 5375/20000: Train Loss = 0.443078, Test Loss = 0.247984, Learning Rate = 1.297219e-04\n",
      "Epoch 5376/20000: Train Loss = 0.442899, Test Loss = 0.253134, Learning Rate = 1.296726e-04\n",
      "Epoch 5377/20000: Train Loss = 0.443006, Test Loss = 0.253932, Learning Rate = 1.296233e-04\n",
      "Epoch 5378/20000: Train Loss = 0.442686, Test Loss = 0.251598, Learning Rate = 1.295741e-04\n",
      "Epoch 5379/20000: Train Loss = 0.443374, Test Loss = 0.253493, Learning Rate = 1.295248e-04\n",
      "Epoch 5380/20000: Train Loss = 0.442709, Test Loss = 0.250209, Learning Rate = 1.294756e-04\n",
      "Epoch 5381/20000: Train Loss = 0.442798, Test Loss = 0.244920, Learning Rate = 1.294264e-04\n",
      "Epoch 5382/20000: Train Loss = 0.442973, Test Loss = 0.247742, Learning Rate = 1.293772e-04\n",
      "Epoch 5383/20000: Train Loss = 0.442892, Test Loss = 0.249258, Learning Rate = 1.293281e-04\n",
      "Epoch 5384/20000: Train Loss = 0.443178, Test Loss = 0.244179, Learning Rate = 1.292789e-04\n",
      "Epoch 5385/20000: Train Loss = 0.442556, Test Loss = 0.249641, Learning Rate = 1.292298e-04\n",
      "Epoch 5386/20000: Train Loss = 0.443058, Test Loss = 0.257700, Learning Rate = 1.291807e-04\n",
      "Epoch 5387/20000: Train Loss = 0.443552, Test Loss = 0.251892, Learning Rate = 1.291316e-04\n",
      "Epoch 5388/20000: Train Loss = 0.443214, Test Loss = 0.249028, Learning Rate = 1.290826e-04\n",
      "Epoch 5389/20000: Train Loss = 0.442792, Test Loss = 0.250893, Learning Rate = 1.290335e-04\n",
      "Epoch 5390/20000: Train Loss = 0.442930, Test Loss = 0.252743, Learning Rate = 1.289845e-04\n",
      "Epoch 5391/20000: Train Loss = 0.443227, Test Loss = 0.246938, Learning Rate = 1.289355e-04\n",
      "Epoch 5392/20000: Train Loss = 0.443143, Test Loss = 0.252035, Learning Rate = 1.288865e-04\n",
      "Epoch 5393/20000: Train Loss = 0.443606, Test Loss = 0.251817, Learning Rate = 1.288375e-04\n",
      "Epoch 5394/20000: Train Loss = 0.442987, Test Loss = 0.243546, Learning Rate = 1.287886e-04\n",
      "Epoch 5395/20000: Train Loss = 0.442807, Test Loss = 0.247326, Learning Rate = 1.287396e-04\n",
      "Epoch 5396/20000: Train Loss = 0.442968, Test Loss = 0.243362, Learning Rate = 1.286907e-04\n",
      "Epoch 5397/20000: Train Loss = 0.443381, Test Loss = 0.244665, Learning Rate = 1.286418e-04\n",
      "Epoch 5398/20000: Train Loss = 0.443191, Test Loss = 0.250555, Learning Rate = 1.285929e-04\n",
      "Epoch 5399/20000: Train Loss = 0.443113, Test Loss = 0.243025, Learning Rate = 1.285441e-04\n",
      "Epoch 5400/20000: Train Loss = 0.443004, Test Loss = 0.251349, Learning Rate = 1.284952e-04\n",
      "Epoch 5401/20000: Train Loss = 0.443059, Test Loss = 0.247827, Learning Rate = 1.284464e-04\n",
      "Epoch 5402/20000: Train Loss = 0.442888, Test Loss = 0.249873, Learning Rate = 1.283976e-04\n",
      "Epoch 5403/20000: Train Loss = 0.443188, Test Loss = 0.253042, Learning Rate = 1.283488e-04\n",
      "Epoch 5404/20000: Train Loss = 0.442786, Test Loss = 0.246514, Learning Rate = 1.283000e-04\n",
      "Epoch 5405/20000: Train Loss = 0.443221, Test Loss = 0.243552, Learning Rate = 1.282513e-04\n",
      "Epoch 5406/20000: Train Loss = 0.442860, Test Loss = 0.244405, Learning Rate = 1.282025e-04\n",
      "Epoch 5407/20000: Train Loss = 0.443105, Test Loss = 0.236777, Learning Rate = 1.281538e-04\n",
      "Epoch 5408/20000: Train Loss = 0.443068, Test Loss = 0.245184, Learning Rate = 1.281051e-04\n",
      "Epoch 5409/20000: Train Loss = 0.443408, Test Loss = 0.247558, Learning Rate = 1.280565e-04\n",
      "Epoch 5410/20000: Train Loss = 0.442908, Test Loss = 0.249178, Learning Rate = 1.280078e-04\n",
      "Epoch 5411/20000: Train Loss = 0.442813, Test Loss = 0.245889, Learning Rate = 1.279592e-04\n",
      "Epoch 5412/20000: Train Loss = 0.443141, Test Loss = 0.239707, Learning Rate = 1.279105e-04\n",
      "Epoch 5413/20000: Train Loss = 0.443014, Test Loss = 0.246088, Learning Rate = 1.278619e-04\n",
      "Epoch 5414/20000: Train Loss = 0.443885, Test Loss = 0.240494, Learning Rate = 1.278134e-04\n",
      "Epoch 5415/20000: Train Loss = 0.443984, Test Loss = 0.247520, Learning Rate = 1.277648e-04\n",
      "Epoch 5416/20000: Train Loss = 0.443519, Test Loss = 0.247427, Learning Rate = 1.277162e-04\n",
      "Epoch 5417/20000: Train Loss = 0.443052, Test Loss = 0.244305, Learning Rate = 1.276677e-04\n",
      "Epoch 5418/20000: Train Loss = 0.443265, Test Loss = 0.245015, Learning Rate = 1.276192e-04\n",
      "Epoch 5419/20000: Train Loss = 0.443270, Test Loss = 0.253542, Learning Rate = 1.275707e-04\n",
      "Epoch 5420/20000: Train Loss = 0.443483, Test Loss = 0.252954, Learning Rate = 1.275222e-04\n",
      "Epoch 5421/20000: Train Loss = 0.443420, Test Loss = 0.248593, Learning Rate = 1.274738e-04\n",
      "Epoch 5422/20000: Train Loss = 0.443295, Test Loss = 0.246948, Learning Rate = 1.274253e-04\n",
      "Epoch 5423/20000: Train Loss = 0.442905, Test Loss = 0.252181, Learning Rate = 1.273769e-04\n",
      "Epoch 5424/20000: Train Loss = 0.442981, Test Loss = 0.243701, Learning Rate = 1.273285e-04\n",
      "Epoch 5425/20000: Train Loss = 0.443008, Test Loss = 0.244263, Learning Rate = 1.272801e-04\n",
      "Epoch 5426/20000: Train Loss = 0.442867, Test Loss = 0.250588, Learning Rate = 1.272318e-04\n",
      "Epoch 5427/20000: Train Loss = 0.443083, Test Loss = 0.252300, Learning Rate = 1.271834e-04\n",
      "Epoch 5428/20000: Train Loss = 0.442765, Test Loss = 0.250026, Learning Rate = 1.271351e-04\n",
      "Epoch 5429/20000: Train Loss = 0.443053, Test Loss = 0.242038, Learning Rate = 1.270868e-04\n",
      "Epoch 5430/20000: Train Loss = 0.442606, Test Loss = 0.248701, Learning Rate = 1.270385e-04\n",
      "Epoch 5431/20000: Train Loss = 0.443316, Test Loss = 0.254824, Learning Rate = 1.269902e-04\n",
      "Epoch 5432/20000: Train Loss = 0.442857, Test Loss = 0.249325, Learning Rate = 1.269420e-04\n",
      "Epoch 5433/20000: Train Loss = 0.443116, Test Loss = 0.244179, Learning Rate = 1.268938e-04\n",
      "Epoch 5434/20000: Train Loss = 0.444500, Test Loss = 0.256373, Learning Rate = 1.268455e-04\n",
      "Epoch 5435/20000: Train Loss = 0.443962, Test Loss = 0.246312, Learning Rate = 1.267973e-04\n",
      "Epoch 5436/20000: Train Loss = 0.443293, Test Loss = 0.247742, Learning Rate = 1.267492e-04\n",
      "Epoch 5437/20000: Train Loss = 0.443312, Test Loss = 0.249248, Learning Rate = 1.267010e-04\n",
      "Epoch 5438/20000: Train Loss = 0.442703, Test Loss = 0.242820, Learning Rate = 1.266529e-04\n",
      "Epoch 5439/20000: Train Loss = 0.443501, Test Loss = 0.246843, Learning Rate = 1.266047e-04\n",
      "Epoch 5440/20000: Train Loss = 0.443310, Test Loss = 0.249305, Learning Rate = 1.265566e-04\n",
      "Epoch 5441/20000: Train Loss = 0.443896, Test Loss = 0.249651, Learning Rate = 1.265085e-04\n",
      "Epoch 5442/20000: Train Loss = 0.443556, Test Loss = 0.244186, Learning Rate = 1.264605e-04\n",
      "Epoch 5443/20000: Train Loss = 0.443068, Test Loss = 0.251596, Learning Rate = 1.264124e-04\n",
      "Epoch 5444/20000: Train Loss = 0.443205, Test Loss = 0.251899, Learning Rate = 1.263644e-04\n",
      "Epoch 5445/20000: Train Loss = 0.443569, Test Loss = 0.253798, Learning Rate = 1.263164e-04\n",
      "Epoch 5446/20000: Train Loss = 0.443233, Test Loss = 0.255575, Learning Rate = 1.262684e-04\n",
      "Epoch 5447/20000: Train Loss = 0.443111, Test Loss = 0.254958, Learning Rate = 1.262204e-04\n",
      "Epoch 5448/20000: Train Loss = 0.443314, Test Loss = 0.247711, Learning Rate = 1.261724e-04\n",
      "Epoch 5449/20000: Train Loss = 0.443049, Test Loss = 0.256502, Learning Rate = 1.261245e-04\n",
      "Epoch 5450/20000: Train Loss = 0.442766, Test Loss = 0.252226, Learning Rate = 1.260766e-04\n",
      "Epoch 5451/20000: Train Loss = 0.443390, Test Loss = 0.244414, Learning Rate = 1.260287e-04\n",
      "Epoch 5452/20000: Train Loss = 0.443313, Test Loss = 0.252821, Learning Rate = 1.259808e-04\n",
      "Epoch 5453/20000: Train Loss = 0.443263, Test Loss = 0.244613, Learning Rate = 1.259329e-04\n",
      "Epoch 5454/20000: Train Loss = 0.443397, Test Loss = 0.247873, Learning Rate = 1.258851e-04\n",
      "Epoch 5455/20000: Train Loss = 0.443080, Test Loss = 0.244253, Learning Rate = 1.258372e-04\n",
      "Epoch 5456/20000: Train Loss = 0.443193, Test Loss = 0.245907, Learning Rate = 1.257894e-04\n",
      "Epoch 5457/20000: Train Loss = 0.444087, Test Loss = 0.251959, Learning Rate = 1.257416e-04\n",
      "Epoch 5458/20000: Train Loss = 0.443435, Test Loss = 0.241528, Learning Rate = 1.256938e-04\n",
      "Epoch 5459/20000: Train Loss = 0.442797, Test Loss = 0.249249, Learning Rate = 1.256461e-04\n",
      "Epoch 5460/20000: Train Loss = 0.443535, Test Loss = 0.249447, Learning Rate = 1.255983e-04\n",
      "Epoch 5461/20000: Train Loss = 0.443073, Test Loss = 0.248200, Learning Rate = 1.255506e-04\n",
      "Epoch 5462/20000: Train Loss = 0.442574, Test Loss = 0.251310, Learning Rate = 1.255029e-04\n",
      "Epoch 5463/20000: Train Loss = 0.443031, Test Loss = 0.249490, Learning Rate = 1.254552e-04\n",
      "Epoch 5464/20000: Train Loss = 0.443071, Test Loss = 0.251338, Learning Rate = 1.254075e-04\n",
      "Epoch 5465/20000: Train Loss = 0.443373, Test Loss = 0.246804, Learning Rate = 1.253599e-04\n",
      "Epoch 5466/20000: Train Loss = 0.442971, Test Loss = 0.248324, Learning Rate = 1.253123e-04\n",
      "Epoch 5467/20000: Train Loss = 0.443372, Test Loss = 0.247150, Learning Rate = 1.252646e-04\n",
      "Epoch 5468/20000: Train Loss = 0.443380, Test Loss = 0.250110, Learning Rate = 1.252170e-04\n",
      "Epoch 5469/20000: Train Loss = 0.443376, Test Loss = 0.249725, Learning Rate = 1.251695e-04\n",
      "Epoch 5470/20000: Train Loss = 0.442669, Test Loss = 0.246037, Learning Rate = 1.251219e-04\n",
      "Epoch 5471/20000: Train Loss = 0.442916, Test Loss = 0.245466, Learning Rate = 1.250744e-04\n",
      "Epoch 5472/20000: Train Loss = 0.443071, Test Loss = 0.249718, Learning Rate = 1.250268e-04\n",
      "Epoch 5473/20000: Train Loss = 0.443003, Test Loss = 0.245146, Learning Rate = 1.249793e-04\n",
      "Epoch 5474/20000: Train Loss = 0.443557, Test Loss = 0.246947, Learning Rate = 1.249318e-04\n",
      "Epoch 5475/20000: Train Loss = 0.443262, Test Loss = 0.247914, Learning Rate = 1.248844e-04\n",
      "Epoch 5476/20000: Train Loss = 0.443805, Test Loss = 0.252803, Learning Rate = 1.248369e-04\n",
      "Epoch 5477/20000: Train Loss = 0.443143, Test Loss = 0.252574, Learning Rate = 1.247895e-04\n",
      "Epoch 5478/20000: Train Loss = 0.443207, Test Loss = 0.250956, Learning Rate = 1.247421e-04\n",
      "Epoch 5479/20000: Train Loss = 0.443145, Test Loss = 0.245513, Learning Rate = 1.246947e-04\n",
      "Epoch 5480/20000: Train Loss = 0.442916, Test Loss = 0.252212, Learning Rate = 1.246473e-04\n",
      "Epoch 5481/20000: Train Loss = 0.443377, Test Loss = 0.254085, Learning Rate = 1.245999e-04\n",
      "Epoch 5482/20000: Train Loss = 0.442785, Test Loss = 0.243405, Learning Rate = 1.245526e-04\n",
      "Epoch 5483/20000: Train Loss = 0.443100, Test Loss = 0.249373, Learning Rate = 1.245053e-04\n",
      "Epoch 5484/20000: Train Loss = 0.443101, Test Loss = 0.248311, Learning Rate = 1.244579e-04\n",
      "Epoch 5485/20000: Train Loss = 0.443184, Test Loss = 0.242050, Learning Rate = 1.244107e-04\n",
      "Epoch 5486/20000: Train Loss = 0.442724, Test Loss = 0.250726, Learning Rate = 1.243634e-04\n",
      "Epoch 5487/20000: Train Loss = 0.443175, Test Loss = 0.246721, Learning Rate = 1.243161e-04\n",
      "Epoch 5488/20000: Train Loss = 0.443166, Test Loss = 0.251420, Learning Rate = 1.242689e-04\n",
      "Epoch 5489/20000: Train Loss = 0.443518, Test Loss = 0.248873, Learning Rate = 1.242217e-04\n",
      "Epoch 5490/20000: Train Loss = 0.443209, Test Loss = 0.258288, Learning Rate = 1.241745e-04\n",
      "Epoch 5491/20000: Train Loss = 0.443413, Test Loss = 0.243598, Learning Rate = 1.241273e-04\n",
      "Epoch 5492/20000: Train Loss = 0.442751, Test Loss = 0.246567, Learning Rate = 1.240801e-04\n",
      "Epoch 5493/20000: Train Loss = 0.442895, Test Loss = 0.248519, Learning Rate = 1.240330e-04\n",
      "Epoch 5494/20000: Train Loss = 0.442954, Test Loss = 0.250558, Learning Rate = 1.239858e-04\n",
      "Epoch 5495/20000: Train Loss = 0.443072, Test Loss = 0.246540, Learning Rate = 1.239387e-04\n",
      "Epoch 5496/20000: Train Loss = 0.443199, Test Loss = 0.251954, Learning Rate = 1.238916e-04\n",
      "Epoch 5497/20000: Train Loss = 0.443243, Test Loss = 0.249645, Learning Rate = 1.238446e-04\n",
      "Epoch 5498/20000: Train Loss = 0.443245, Test Loss = 0.249553, Learning Rate = 1.237975e-04\n",
      "Epoch 5499/20000: Train Loss = 0.442987, Test Loss = 0.249389, Learning Rate = 1.237505e-04\n",
      "Epoch 5500/20000: Train Loss = 0.442720, Test Loss = 0.253897, Learning Rate = 1.237034e-04\n",
      "Epoch 5501/20000: Train Loss = 0.443331, Test Loss = 0.250065, Learning Rate = 1.236564e-04\n",
      "Epoch 5502/20000: Train Loss = 0.442717, Test Loss = 0.246839, Learning Rate = 1.236095e-04\n",
      "Epoch 5503/20000: Train Loss = 0.443383, Test Loss = 0.245294, Learning Rate = 1.235625e-04\n",
      "Epoch 5504/20000: Train Loss = 0.443056, Test Loss = 0.245374, Learning Rate = 1.235155e-04\n",
      "Epoch 5505/20000: Train Loss = 0.443805, Test Loss = 0.245464, Learning Rate = 1.234686e-04\n",
      "Epoch 5506/20000: Train Loss = 0.442916, Test Loss = 0.246082, Learning Rate = 1.234217e-04\n",
      "Epoch 5507/20000: Train Loss = 0.442985, Test Loss = 0.248873, Learning Rate = 1.233748e-04\n",
      "Epoch 5508/20000: Train Loss = 0.442455, Test Loss = 0.238730, Learning Rate = 1.233279e-04\n",
      "Epoch 5509/20000: Train Loss = 0.443352, Test Loss = 0.242413, Learning Rate = 1.232811e-04\n",
      "Epoch 5510/20000: Train Loss = 0.442798, Test Loss = 0.243513, Learning Rate = 1.232342e-04\n",
      "Epoch 5511/20000: Train Loss = 0.443580, Test Loss = 0.243562, Learning Rate = 1.231874e-04\n",
      "Epoch 5512/20000: Train Loss = 0.443181, Test Loss = 0.248325, Learning Rate = 1.231406e-04\n",
      "Epoch 5513/20000: Train Loss = 0.443612, Test Loss = 0.249440, Learning Rate = 1.230938e-04\n",
      "Epoch 5514/20000: Train Loss = 0.442949, Test Loss = 0.243628, Learning Rate = 1.230470e-04\n",
      "Epoch 5515/20000: Train Loss = 0.443625, Test Loss = 0.247430, Learning Rate = 1.230003e-04\n",
      "Epoch 5516/20000: Train Loss = 0.442855, Test Loss = 0.250917, Learning Rate = 1.229535e-04\n",
      "Epoch 5517/20000: Train Loss = 0.443092, Test Loss = 0.249506, Learning Rate = 1.229068e-04\n",
      "Epoch 5518/20000: Train Loss = 0.442957, Test Loss = 0.247485, Learning Rate = 1.228601e-04\n",
      "Epoch 5519/20000: Train Loss = 0.442985, Test Loss = 0.247742, Learning Rate = 1.228134e-04\n",
      "Epoch 5520/20000: Train Loss = 0.442881, Test Loss = 0.246665, Learning Rate = 1.227668e-04\n",
      "Epoch 5521/20000: Train Loss = 0.442872, Test Loss = 0.241503, Learning Rate = 1.227201e-04\n",
      "Epoch 5522/20000: Train Loss = 0.442692, Test Loss = 0.248074, Learning Rate = 1.226735e-04\n",
      "Epoch 5523/20000: Train Loss = 0.443207, Test Loss = 0.249048, Learning Rate = 1.226269e-04\n",
      "Epoch 5524/20000: Train Loss = 0.443153, Test Loss = 0.246601, Learning Rate = 1.225803e-04\n",
      "Epoch 5525/20000: Train Loss = 0.442991, Test Loss = 0.241190, Learning Rate = 1.225337e-04\n",
      "Epoch 5526/20000: Train Loss = 0.443159, Test Loss = 0.243353, Learning Rate = 1.224871e-04\n",
      "Epoch 5527/20000: Train Loss = 0.442859, Test Loss = 0.247613, Learning Rate = 1.224406e-04\n",
      "Epoch 5528/20000: Train Loss = 0.442899, Test Loss = 0.251627, Learning Rate = 1.223941e-04\n",
      "Epoch 5529/20000: Train Loss = 0.443131, Test Loss = 0.247921, Learning Rate = 1.223476e-04\n",
      "Epoch 5530/20000: Train Loss = 0.443466, Test Loss = 0.243272, Learning Rate = 1.223011e-04\n",
      "Epoch 5531/20000: Train Loss = 0.443188, Test Loss = 0.246626, Learning Rate = 1.222546e-04\n",
      "Epoch 5532/20000: Train Loss = 0.443339, Test Loss = 0.254697, Learning Rate = 1.222081e-04\n",
      "Epoch 5533/20000: Train Loss = 0.442921, Test Loss = 0.248362, Learning Rate = 1.221617e-04\n",
      "Epoch 5534/20000: Train Loss = 0.442807, Test Loss = 0.243509, Learning Rate = 1.221153e-04\n",
      "Epoch 5535/20000: Train Loss = 0.442668, Test Loss = 0.248884, Learning Rate = 1.220689e-04\n",
      "Epoch 5536/20000: Train Loss = 0.442953, Test Loss = 0.246269, Learning Rate = 1.220225e-04\n",
      "Epoch 5537/20000: Train Loss = 0.442802, Test Loss = 0.251704, Learning Rate = 1.219761e-04\n",
      "Epoch 5538/20000: Train Loss = 0.443261, Test Loss = 0.250500, Learning Rate = 1.219298e-04\n",
      "Epoch 5539/20000: Train Loss = 0.442996, Test Loss = 0.254400, Learning Rate = 1.218835e-04\n",
      "Epoch 5540/20000: Train Loss = 0.442867, Test Loss = 0.245034, Learning Rate = 1.218372e-04\n",
      "Epoch 5541/20000: Train Loss = 0.443131, Test Loss = 0.242796, Learning Rate = 1.217909e-04\n",
      "Epoch 5542/20000: Train Loss = 0.443076, Test Loss = 0.249928, Learning Rate = 1.217446e-04\n",
      "Epoch 5543/20000: Train Loss = 0.443036, Test Loss = 0.250357, Learning Rate = 1.216983e-04\n",
      "Epoch 5544/20000: Train Loss = 0.444209, Test Loss = 0.247306, Learning Rate = 1.216521e-04\n",
      "Epoch 5545/20000: Train Loss = 0.443554, Test Loss = 0.245488, Learning Rate = 1.216059e-04\n",
      "Epoch 5546/20000: Train Loss = 0.443263, Test Loss = 0.242242, Learning Rate = 1.215596e-04\n",
      "Epoch 5547/20000: Train Loss = 0.443761, Test Loss = 0.253543, Learning Rate = 1.215135e-04\n",
      "Epoch 5548/20000: Train Loss = 0.443345, Test Loss = 0.247574, Learning Rate = 1.214673e-04\n",
      "Epoch 5549/20000: Train Loss = 0.442959, Test Loss = 0.245491, Learning Rate = 1.214211e-04\n",
      "Epoch 5550/20000: Train Loss = 0.443102, Test Loss = 0.242327, Learning Rate = 1.213750e-04\n",
      "Epoch 5551/20000: Train Loss = 0.443510, Test Loss = 0.253095, Learning Rate = 1.213289e-04\n",
      "Epoch 5552/20000: Train Loss = 0.443586, Test Loss = 0.246779, Learning Rate = 1.212828e-04\n",
      "Epoch 5553/20000: Train Loss = 0.443048, Test Loss = 0.253647, Learning Rate = 1.212367e-04\n",
      "Epoch 5554/20000: Train Loss = 0.443010, Test Loss = 0.248899, Learning Rate = 1.211906e-04\n",
      "Epoch 5555/20000: Train Loss = 0.443431, Test Loss = 0.241982, Learning Rate = 1.211446e-04\n",
      "Epoch 5556/20000: Train Loss = 0.442844, Test Loss = 0.244840, Learning Rate = 1.210985e-04\n",
      "Epoch 5557/20000: Train Loss = 0.443072, Test Loss = 0.248686, Learning Rate = 1.210525e-04\n",
      "Epoch 5558/20000: Train Loss = 0.443437, Test Loss = 0.248501, Learning Rate = 1.210065e-04\n",
      "Epoch 5559/20000: Train Loss = 0.443532, Test Loss = 0.242772, Learning Rate = 1.209606e-04\n",
      "Epoch 5560/20000: Train Loss = 0.443258, Test Loss = 0.250397, Learning Rate = 1.209146e-04\n",
      "Epoch 5561/20000: Train Loss = 0.442817, Test Loss = 0.245049, Learning Rate = 1.208686e-04\n",
      "Epoch 5562/20000: Train Loss = 0.443412, Test Loss = 0.243987, Learning Rate = 1.208227e-04\n",
      "Epoch 5563/20000: Train Loss = 0.442726, Test Loss = 0.252828, Learning Rate = 1.207768e-04\n",
      "Epoch 5564/20000: Train Loss = 0.443134, Test Loss = 0.248331, Learning Rate = 1.207309e-04\n",
      "Epoch 5565/20000: Train Loss = 0.442906, Test Loss = 0.246411, Learning Rate = 1.206850e-04\n",
      "Epoch 5566/20000: Train Loss = 0.443713, Test Loss = 0.251367, Learning Rate = 1.206392e-04\n",
      "Epoch 5567/20000: Train Loss = 0.443180, Test Loss = 0.248337, Learning Rate = 1.205933e-04\n",
      "Epoch 5568/20000: Train Loss = 0.443210, Test Loss = 0.247076, Learning Rate = 1.205475e-04\n",
      "Epoch 5569/20000: Train Loss = 0.443411, Test Loss = 0.242662, Learning Rate = 1.205017e-04\n",
      "Epoch 5570/20000: Train Loss = 0.443983, Test Loss = 0.251377, Learning Rate = 1.204559e-04\n",
      "Epoch 5571/20000: Train Loss = 0.442914, Test Loss = 0.244313, Learning Rate = 1.204102e-04\n",
      "Epoch 5572/20000: Train Loss = 0.442953, Test Loss = 0.243278, Learning Rate = 1.203644e-04\n",
      "Epoch 5573/20000: Train Loss = 0.442958, Test Loss = 0.243966, Learning Rate = 1.203187e-04\n",
      "Epoch 5574/20000: Train Loss = 0.442688, Test Loss = 0.245424, Learning Rate = 1.202730e-04\n",
      "Epoch 5575/20000: Train Loss = 0.443719, Test Loss = 0.248677, Learning Rate = 1.202273e-04\n",
      "Epoch 5576/20000: Train Loss = 0.443031, Test Loss = 0.255118, Learning Rate = 1.201816e-04\n",
      "Epoch 5577/20000: Train Loss = 0.443684, Test Loss = 0.253577, Learning Rate = 1.201359e-04\n",
      "Epoch 5578/20000: Train Loss = 0.443067, Test Loss = 0.242142, Learning Rate = 1.200903e-04\n",
      "Epoch 5579/20000: Train Loss = 0.443394, Test Loss = 0.244374, Learning Rate = 1.200446e-04\n",
      "Epoch 5580/20000: Train Loss = 0.442752, Test Loss = 0.246453, Learning Rate = 1.199990e-04\n",
      "Epoch 5581/20000: Train Loss = 0.443140, Test Loss = 0.250430, Learning Rate = 1.199534e-04\n",
      "Epoch 5582/20000: Train Loss = 0.443723, Test Loss = 0.242844, Learning Rate = 1.199078e-04\n",
      "Epoch 5583/20000: Train Loss = 0.444068, Test Loss = 0.253212, Learning Rate = 1.198623e-04\n",
      "Epoch 5584/20000: Train Loss = 0.442982, Test Loss = 0.242717, Learning Rate = 1.198167e-04\n",
      "Epoch 5585/20000: Train Loss = 0.442985, Test Loss = 0.252018, Learning Rate = 1.197712e-04\n",
      "Epoch 5586/20000: Train Loss = 0.443028, Test Loss = 0.251433, Learning Rate = 1.197257e-04\n",
      "Epoch 5587/20000: Train Loss = 0.443490, Test Loss = 0.248071, Learning Rate = 1.196802e-04\n",
      "Epoch 5588/20000: Train Loss = 0.442946, Test Loss = 0.248233, Learning Rate = 1.196347e-04\n",
      "Epoch 5589/20000: Train Loss = 0.442792, Test Loss = 0.245595, Learning Rate = 1.195893e-04\n",
      "Epoch 5590/20000: Train Loss = 0.442712, Test Loss = 0.251025, Learning Rate = 1.195438e-04\n",
      "Epoch 5591/20000: Train Loss = 0.442960, Test Loss = 0.253695, Learning Rate = 1.194984e-04\n",
      "Epoch 5592/20000: Train Loss = 0.443382, Test Loss = 0.246343, Learning Rate = 1.194530e-04\n",
      "Epoch 5593/20000: Train Loss = 0.442897, Test Loss = 0.246275, Learning Rate = 1.194076e-04\n",
      "Epoch 5594/20000: Train Loss = 0.442874, Test Loss = 0.251808, Learning Rate = 1.193622e-04\n",
      "Epoch 5595/20000: Train Loss = 0.442902, Test Loss = 0.251379, Learning Rate = 1.193169e-04\n",
      "Epoch 5596/20000: Train Loss = 0.443030, Test Loss = 0.246448, Learning Rate = 1.192715e-04\n",
      "Epoch 5597/20000: Train Loss = 0.442900, Test Loss = 0.249107, Learning Rate = 1.192262e-04\n",
      "Epoch 5598/20000: Train Loss = 0.442756, Test Loss = 0.251842, Learning Rate = 1.191809e-04\n",
      "Epoch 5599/20000: Train Loss = 0.442875, Test Loss = 0.248530, Learning Rate = 1.191356e-04\n",
      "Epoch 5600/20000: Train Loss = 0.443201, Test Loss = 0.252903, Learning Rate = 1.190904e-04\n",
      "Epoch 5601/20000: Train Loss = 0.442751, Test Loss = 0.252297, Learning Rate = 1.190451e-04\n",
      "Epoch 5602/20000: Train Loss = 0.442949, Test Loss = 0.248818, Learning Rate = 1.189999e-04\n",
      "Epoch 5603/20000: Train Loss = 0.443232, Test Loss = 0.246244, Learning Rate = 1.189547e-04\n",
      "Epoch 5604/20000: Train Loss = 0.443262, Test Loss = 0.246009, Learning Rate = 1.189095e-04\n",
      "Epoch 5605/20000: Train Loss = 0.443263, Test Loss = 0.248004, Learning Rate = 1.188643e-04\n",
      "Epoch 5606/20000: Train Loss = 0.443321, Test Loss = 0.251237, Learning Rate = 1.188191e-04\n",
      "Epoch 5607/20000: Train Loss = 0.443184, Test Loss = 0.245497, Learning Rate = 1.187740e-04\n",
      "Epoch 5608/20000: Train Loss = 0.443161, Test Loss = 0.245578, Learning Rate = 1.187288e-04\n",
      "Epoch 5609/20000: Train Loss = 0.444043, Test Loss = 0.246138, Learning Rate = 1.186837e-04\n",
      "Epoch 5610/20000: Train Loss = 0.442747, Test Loss = 0.253352, Learning Rate = 1.186386e-04\n",
      "Epoch 5611/20000: Train Loss = 0.442751, Test Loss = 0.247907, Learning Rate = 1.185936e-04\n",
      "Epoch 5612/20000: Train Loss = 0.443045, Test Loss = 0.244219, Learning Rate = 1.185485e-04\n",
      "Epoch 5613/20000: Train Loss = 0.443035, Test Loss = 0.246672, Learning Rate = 1.185034e-04\n",
      "Epoch 5614/20000: Train Loss = 0.442987, Test Loss = 0.246110, Learning Rate = 1.184584e-04\n",
      "Epoch 5615/20000: Train Loss = 0.443181, Test Loss = 0.248085, Learning Rate = 1.184134e-04\n",
      "Epoch 5616/20000: Train Loss = 0.442873, Test Loss = 0.246903, Learning Rate = 1.183684e-04\n",
      "Epoch 5617/20000: Train Loss = 0.443811, Test Loss = 0.247768, Learning Rate = 1.183234e-04\n",
      "Epoch 5618/20000: Train Loss = 0.443110, Test Loss = 0.255462, Learning Rate = 1.182785e-04\n",
      "Epoch 5619/20000: Train Loss = 0.443156, Test Loss = 0.253541, Learning Rate = 1.182335e-04\n",
      "Epoch 5620/20000: Train Loss = 0.443063, Test Loss = 0.250700, Learning Rate = 1.181886e-04\n",
      "Epoch 5621/20000: Train Loss = 0.443018, Test Loss = 0.246515, Learning Rate = 1.181437e-04\n",
      "Epoch 5622/20000: Train Loss = 0.442933, Test Loss = 0.245908, Learning Rate = 1.180988e-04\n",
      "Epoch 5623/20000: Train Loss = 0.443322, Test Loss = 0.247647, Learning Rate = 1.180539e-04\n",
      "Epoch 5624/20000: Train Loss = 0.442774, Test Loss = 0.246751, Learning Rate = 1.180091e-04\n",
      "Epoch 5625/20000: Train Loss = 0.443390, Test Loss = 0.252108, Learning Rate = 1.179642e-04\n",
      "Epoch 5626/20000: Train Loss = 0.442989, Test Loss = 0.249826, Learning Rate = 1.179194e-04\n",
      "Epoch 5627/20000: Train Loss = 0.443191, Test Loss = 0.256928, Learning Rate = 1.178746e-04\n",
      "Epoch 5628/20000: Train Loss = 0.443174, Test Loss = 0.252596, Learning Rate = 1.178298e-04\n",
      "Epoch 5629/20000: Train Loss = 0.442744, Test Loss = 0.252162, Learning Rate = 1.177850e-04\n",
      "Epoch 5630/20000: Train Loss = 0.443318, Test Loss = 0.247300, Learning Rate = 1.177403e-04\n",
      "Epoch 5631/20000: Train Loss = 0.442902, Test Loss = 0.249054, Learning Rate = 1.176956e-04\n",
      "Epoch 5632/20000: Train Loss = 0.442793, Test Loss = 0.250500, Learning Rate = 1.176508e-04\n",
      "Epoch 5633/20000: Train Loss = 0.443267, Test Loss = 0.251908, Learning Rate = 1.176061e-04\n",
      "Epoch 5634/20000: Train Loss = 0.443100, Test Loss = 0.249436, Learning Rate = 1.175614e-04\n",
      "Epoch 5635/20000: Train Loss = 0.442980, Test Loss = 0.248341, Learning Rate = 1.175168e-04\n",
      "Epoch 5636/20000: Train Loss = 0.442907, Test Loss = 0.247860, Learning Rate = 1.174721e-04\n",
      "Epoch 5637/20000: Train Loss = 0.443464, Test Loss = 0.244121, Learning Rate = 1.174275e-04\n",
      "Epoch 5638/20000: Train Loss = 0.442806, Test Loss = 0.248592, Learning Rate = 1.173829e-04\n",
      "Epoch 5639/20000: Train Loss = 0.442972, Test Loss = 0.248639, Learning Rate = 1.173383e-04\n",
      "Epoch 5640/20000: Train Loss = 0.443628, Test Loss = 0.250620, Learning Rate = 1.172937e-04\n",
      "Epoch 5641/20000: Train Loss = 0.442808, Test Loss = 0.248819, Learning Rate = 1.172491e-04\n",
      "Epoch 5642/20000: Train Loss = 0.443271, Test Loss = 0.246503, Learning Rate = 1.172046e-04\n",
      "Epoch 5643/20000: Train Loss = 0.442974, Test Loss = 0.248046, Learning Rate = 1.171600e-04\n",
      "Epoch 5644/20000: Train Loss = 0.443111, Test Loss = 0.244299, Learning Rate = 1.171155e-04\n",
      "Epoch 5645/20000: Train Loss = 0.442822, Test Loss = 0.243439, Learning Rate = 1.170710e-04\n",
      "Epoch 5646/20000: Train Loss = 0.443311, Test Loss = 0.249645, Learning Rate = 1.170265e-04\n",
      "Epoch 5647/20000: Train Loss = 0.443200, Test Loss = 0.252460, Learning Rate = 1.169821e-04\n",
      "Epoch 5648/20000: Train Loss = 0.444031, Test Loss = 0.248701, Learning Rate = 1.169376e-04\n",
      "Epoch 5649/20000: Train Loss = 0.442851, Test Loss = 0.245819, Learning Rate = 1.168932e-04\n",
      "Epoch 5650/20000: Train Loss = 0.442787, Test Loss = 0.244327, Learning Rate = 1.168488e-04\n",
      "Epoch 5651/20000: Train Loss = 0.442954, Test Loss = 0.247935, Learning Rate = 1.168044e-04\n",
      "Epoch 5652/20000: Train Loss = 0.443451, Test Loss = 0.243231, Learning Rate = 1.167600e-04\n",
      "Epoch 5653/20000: Train Loss = 0.444320, Test Loss = 0.247801, Learning Rate = 1.167156e-04\n",
      "Epoch 5654/20000: Train Loss = 0.443702, Test Loss = 0.248116, Learning Rate = 1.166713e-04\n",
      "Epoch 5655/20000: Train Loss = 0.443594, Test Loss = 0.245900, Learning Rate = 1.166269e-04\n",
      "Epoch 5656/20000: Train Loss = 0.443073, Test Loss = 0.250290, Learning Rate = 1.165826e-04\n",
      "Epoch 5657/20000: Train Loss = 0.443190, Test Loss = 0.248256, Learning Rate = 1.165383e-04\n",
      "Epoch 5658/20000: Train Loss = 0.442876, Test Loss = 0.238773, Learning Rate = 1.164940e-04\n",
      "Epoch 5659/20000: Train Loss = 0.442778, Test Loss = 0.248457, Learning Rate = 1.164498e-04\n",
      "Epoch 5660/20000: Train Loss = 0.442908, Test Loss = 0.244028, Learning Rate = 1.164055e-04\n",
      "Epoch 5661/20000: Train Loss = 0.442706, Test Loss = 0.250000, Learning Rate = 1.163613e-04\n",
      "Epoch 5662/20000: Train Loss = 0.442766, Test Loss = 0.249586, Learning Rate = 1.163171e-04\n",
      "Epoch 5663/20000: Train Loss = 0.443101, Test Loss = 0.243424, Learning Rate = 1.162729e-04\n",
      "Epoch 5664/20000: Train Loss = 0.443135, Test Loss = 0.248373, Learning Rate = 1.162287e-04\n",
      "Epoch 5665/20000: Train Loss = 0.443993, Test Loss = 0.249244, Learning Rate = 1.161845e-04\n",
      "Epoch 5666/20000: Train Loss = 0.442991, Test Loss = 0.244481, Learning Rate = 1.161404e-04\n",
      "Epoch 5667/20000: Train Loss = 0.442844, Test Loss = 0.247435, Learning Rate = 1.160963e-04\n",
      "Epoch 5668/20000: Train Loss = 0.442734, Test Loss = 0.245077, Learning Rate = 1.160521e-04\n",
      "Epoch 5669/20000: Train Loss = 0.442710, Test Loss = 0.248912, Learning Rate = 1.160080e-04\n",
      "Epoch 5670/20000: Train Loss = 0.442768, Test Loss = 0.246159, Learning Rate = 1.159640e-04\n",
      "Epoch 5671/20000: Train Loss = 0.442800, Test Loss = 0.245708, Learning Rate = 1.159199e-04\n",
      "Epoch 5672/20000: Train Loss = 0.443354, Test Loss = 0.238925, Learning Rate = 1.158759e-04\n",
      "Epoch 5673/20000: Train Loss = 0.444070, Test Loss = 0.255714, Learning Rate = 1.158318e-04\n",
      "Epoch 5674/20000: Train Loss = 0.442788, Test Loss = 0.244097, Learning Rate = 1.157878e-04\n",
      "Epoch 5675/20000: Train Loss = 0.443169, Test Loss = 0.248353, Learning Rate = 1.157438e-04\n",
      "Epoch 5676/20000: Train Loss = 0.442996, Test Loss = 0.248202, Learning Rate = 1.156998e-04\n",
      "Epoch 5677/20000: Train Loss = 0.443064, Test Loss = 0.257621, Learning Rate = 1.156559e-04\n",
      "Epoch 5678/20000: Train Loss = 0.443116, Test Loss = 0.256664, Learning Rate = 1.156119e-04\n",
      "Epoch 5679/20000: Train Loss = 0.443043, Test Loss = 0.248823, Learning Rate = 1.155680e-04\n",
      "Epoch 5680/20000: Train Loss = 0.442999, Test Loss = 0.247710, Learning Rate = 1.155241e-04\n",
      "Epoch 5681/20000: Train Loss = 0.443301, Test Loss = 0.248316, Learning Rate = 1.154802e-04\n",
      "Epoch 5682/20000: Train Loss = 0.443418, Test Loss = 0.250925, Learning Rate = 1.154363e-04\n",
      "Epoch 5683/20000: Train Loss = 0.443161, Test Loss = 0.251694, Learning Rate = 1.153924e-04\n",
      "Epoch 5684/20000: Train Loss = 0.443327, Test Loss = 0.245770, Learning Rate = 1.153486e-04\n",
      "Epoch 5685/20000: Train Loss = 0.443102, Test Loss = 0.246616, Learning Rate = 1.153048e-04\n",
      "Epoch 5686/20000: Train Loss = 0.442985, Test Loss = 0.252546, Learning Rate = 1.152610e-04\n",
      "Epoch 5687/20000: Train Loss = 0.442936, Test Loss = 0.247608, Learning Rate = 1.152172e-04\n",
      "Epoch 5688/20000: Train Loss = 0.443352, Test Loss = 0.248609, Learning Rate = 1.151734e-04\n",
      "Epoch 5689/20000: Train Loss = 0.442882, Test Loss = 0.250780, Learning Rate = 1.151296e-04\n",
      "Epoch 5690/20000: Train Loss = 0.443001, Test Loss = 0.244251, Learning Rate = 1.150859e-04\n",
      "Epoch 5691/20000: Train Loss = 0.443135, Test Loss = 0.244151, Learning Rate = 1.150421e-04\n",
      "Epoch 5692/20000: Train Loss = 0.443283, Test Loss = 0.249144, Learning Rate = 1.149984e-04\n",
      "Epoch 5693/20000: Train Loss = 0.443062, Test Loss = 0.246302, Learning Rate = 1.149547e-04\n",
      "Epoch 5694/20000: Train Loss = 0.442845, Test Loss = 0.249403, Learning Rate = 1.149111e-04\n",
      "Epoch 5695/20000: Train Loss = 0.443713, Test Loss = 0.241943, Learning Rate = 1.148674e-04\n",
      "Epoch 5696/20000: Train Loss = 0.443113, Test Loss = 0.247320, Learning Rate = 1.148237e-04\n",
      "Epoch 5697/20000: Train Loss = 0.443084, Test Loss = 0.241404, Learning Rate = 1.147801e-04\n",
      "Epoch 5698/20000: Train Loss = 0.443101, Test Loss = 0.250224, Learning Rate = 1.147365e-04\n",
      "Epoch 5699/20000: Train Loss = 0.443313, Test Loss = 0.252549, Learning Rate = 1.146929e-04\n",
      "Epoch 5700/20000: Train Loss = 0.444061, Test Loss = 0.246614, Learning Rate = 1.146493e-04\n",
      "Epoch 5701/20000: Train Loss = 0.443405, Test Loss = 0.252892, Learning Rate = 1.146058e-04\n",
      "Epoch 5702/20000: Train Loss = 0.442984, Test Loss = 0.249828, Learning Rate = 1.145622e-04\n",
      "Epoch 5703/20000: Train Loss = 0.442896, Test Loss = 0.248450, Learning Rate = 1.145187e-04\n",
      "Epoch 5704/20000: Train Loss = 0.442764, Test Loss = 0.245525, Learning Rate = 1.144752e-04\n",
      "Epoch 5705/20000: Train Loss = 0.442825, Test Loss = 0.249259, Learning Rate = 1.144317e-04\n",
      "Epoch 5706/20000: Train Loss = 0.443693, Test Loss = 0.248956, Learning Rate = 1.143882e-04\n",
      "Epoch 5707/20000: Train Loss = 0.443260, Test Loss = 0.247578, Learning Rate = 1.143447e-04\n",
      "Epoch 5708/20000: Train Loss = 0.442991, Test Loss = 0.250094, Learning Rate = 1.143013e-04\n",
      "Epoch 5709/20000: Train Loss = 0.443013, Test Loss = 0.250915, Learning Rate = 1.142578e-04\n",
      "Epoch 5710/20000: Train Loss = 0.442916, Test Loss = 0.243662, Learning Rate = 1.142144e-04\n",
      "Epoch 5711/20000: Train Loss = 0.442742, Test Loss = 0.248129, Learning Rate = 1.141710e-04\n",
      "Epoch 5712/20000: Train Loss = 0.442818, Test Loss = 0.249535, Learning Rate = 1.141277e-04\n",
      "Epoch 5713/20000: Train Loss = 0.443056, Test Loss = 0.246556, Learning Rate = 1.140843e-04\n",
      "Epoch 5714/20000: Train Loss = 0.442918, Test Loss = 0.254939, Learning Rate = 1.140409e-04\n",
      "Epoch 5715/20000: Train Loss = 0.443089, Test Loss = 0.248043, Learning Rate = 1.139976e-04\n",
      "Epoch 5716/20000: Train Loss = 0.442948, Test Loss = 0.250832, Learning Rate = 1.139543e-04\n",
      "Epoch 5717/20000: Train Loss = 0.443364, Test Loss = 0.247999, Learning Rate = 1.139110e-04\n",
      "Epoch 5718/20000: Train Loss = 0.442951, Test Loss = 0.248136, Learning Rate = 1.138677e-04\n",
      "Epoch 5719/20000: Train Loss = 0.443004, Test Loss = 0.251143, Learning Rate = 1.138244e-04\n",
      "Epoch 5720/20000: Train Loss = 0.442774, Test Loss = 0.250516, Learning Rate = 1.137812e-04\n",
      "Epoch 5721/20000: Train Loss = 0.443248, Test Loss = 0.253381, Learning Rate = 1.137380e-04\n",
      "Epoch 5722/20000: Train Loss = 0.442917, Test Loss = 0.247925, Learning Rate = 1.136947e-04\n",
      "Epoch 5723/20000: Train Loss = 0.443390, Test Loss = 0.248792, Learning Rate = 1.136515e-04\n",
      "Epoch 5724/20000: Train Loss = 0.442712, Test Loss = 0.247000, Learning Rate = 1.136084e-04\n",
      "Epoch 5725/20000: Train Loss = 0.443055, Test Loss = 0.250916, Learning Rate = 1.135652e-04\n",
      "Epoch 5726/20000: Train Loss = 0.443449, Test Loss = 0.252344, Learning Rate = 1.135220e-04\n",
      "Epoch 5727/20000: Train Loss = 0.443498, Test Loss = 0.251005, Learning Rate = 1.134789e-04\n",
      "Epoch 5728/20000: Train Loss = 0.443521, Test Loss = 0.255530, Learning Rate = 1.134358e-04\n",
      "Epoch 5729/20000: Train Loss = 0.442616, Test Loss = 0.247106, Learning Rate = 1.133927e-04\n",
      "Epoch 5730/20000: Train Loss = 0.443113, Test Loss = 0.250019, Learning Rate = 1.133496e-04\n",
      "Epoch 5731/20000: Train Loss = 0.443375, Test Loss = 0.244864, Learning Rate = 1.133065e-04\n",
      "Epoch 5732/20000: Train Loss = 0.442776, Test Loss = 0.252771, Learning Rate = 1.132635e-04\n",
      "Epoch 5733/20000: Train Loss = 0.442894, Test Loss = 0.252875, Learning Rate = 1.132204e-04\n",
      "Epoch 5734/20000: Train Loss = 0.443564, Test Loss = 0.251845, Learning Rate = 1.131774e-04\n",
      "Epoch 5735/20000: Train Loss = 0.443150, Test Loss = 0.250154, Learning Rate = 1.131344e-04\n",
      "Epoch 5736/20000: Train Loss = 0.442892, Test Loss = 0.256193, Learning Rate = 1.130914e-04\n",
      "Epoch 5737/20000: Train Loss = 0.443732, Test Loss = 0.255546, Learning Rate = 1.130484e-04\n",
      "Epoch 5738/20000: Train Loss = 0.443680, Test Loss = 0.247987, Learning Rate = 1.130055e-04\n",
      "Epoch 5739/20000: Train Loss = 0.442950, Test Loss = 0.248399, Learning Rate = 1.129626e-04\n",
      "Epoch 5740/20000: Train Loss = 0.443178, Test Loss = 0.250935, Learning Rate = 1.129196e-04\n",
      "Epoch 5741/20000: Train Loss = 0.442781, Test Loss = 0.248663, Learning Rate = 1.128767e-04\n",
      "Epoch 5742/20000: Train Loss = 0.442803, Test Loss = 0.245112, Learning Rate = 1.128338e-04\n",
      "Epoch 5743/20000: Train Loss = 0.443232, Test Loss = 0.247916, Learning Rate = 1.127910e-04\n",
      "Epoch 5744/20000: Train Loss = 0.443199, Test Loss = 0.252387, Learning Rate = 1.127481e-04\n",
      "Epoch 5745/20000: Train Loss = 0.442919, Test Loss = 0.251466, Learning Rate = 1.127053e-04\n",
      "Epoch 5746/20000: Train Loss = 0.443410, Test Loss = 0.249722, Learning Rate = 1.126624e-04\n",
      "Epoch 5747/20000: Train Loss = 0.443048, Test Loss = 0.250821, Learning Rate = 1.126196e-04\n",
      "Epoch 5748/20000: Train Loss = 0.443310, Test Loss = 0.254528, Learning Rate = 1.125768e-04\n",
      "Epoch 5749/20000: Train Loss = 0.443387, Test Loss = 0.245159, Learning Rate = 1.125341e-04\n",
      "Epoch 5750/20000: Train Loss = 0.443051, Test Loss = 0.248490, Learning Rate = 1.124913e-04\n",
      "Epoch 5751/20000: Train Loss = 0.443346, Test Loss = 0.252230, Learning Rate = 1.124486e-04\n",
      "Epoch 5752/20000: Train Loss = 0.443279, Test Loss = 0.246182, Learning Rate = 1.124058e-04\n",
      "Epoch 5753/20000: Train Loss = 0.443683, Test Loss = 0.249140, Learning Rate = 1.123631e-04\n",
      "Epoch 5754/20000: Train Loss = 0.443591, Test Loss = 0.255722, Learning Rate = 1.123204e-04\n",
      "Epoch 5755/20000: Train Loss = 0.442990, Test Loss = 0.248856, Learning Rate = 1.122777e-04\n",
      "Epoch 5756/20000: Train Loss = 0.443328, Test Loss = 0.244035, Learning Rate = 1.122351e-04\n",
      "Epoch 5757/20000: Train Loss = 0.442847, Test Loss = 0.248110, Learning Rate = 1.121924e-04\n",
      "Epoch 5758/20000: Train Loss = 0.443809, Test Loss = 0.249383, Learning Rate = 1.121498e-04\n",
      "Epoch 5759/20000: Train Loss = 0.443462, Test Loss = 0.245114, Learning Rate = 1.121072e-04\n",
      "Epoch 5760/20000: Train Loss = 0.443475, Test Loss = 0.243905, Learning Rate = 1.120646e-04\n",
      "Epoch 5761/20000: Train Loss = 0.443683, Test Loss = 0.243063, Learning Rate = 1.120220e-04\n",
      "Epoch 5762/20000: Train Loss = 0.443429, Test Loss = 0.243835, Learning Rate = 1.119794e-04\n",
      "Epoch 5763/20000: Train Loss = 0.443340, Test Loss = 0.244373, Learning Rate = 1.119369e-04\n",
      "Epoch 5764/20000: Train Loss = 0.442745, Test Loss = 0.251068, Learning Rate = 1.118944e-04\n",
      "Epoch 5765/20000: Train Loss = 0.443233, Test Loss = 0.247703, Learning Rate = 1.118518e-04\n",
      "Epoch 5766/20000: Train Loss = 0.442832, Test Loss = 0.248429, Learning Rate = 1.118093e-04\n",
      "Epoch 5767/20000: Train Loss = 0.443360, Test Loss = 0.244518, Learning Rate = 1.117669e-04\n",
      "Epoch 5768/20000: Train Loss = 0.442957, Test Loss = 0.248114, Learning Rate = 1.117244e-04\n",
      "Epoch 5769/20000: Train Loss = 0.442805, Test Loss = 0.249631, Learning Rate = 1.116819e-04\n",
      "Epoch 5770/20000: Train Loss = 0.443156, Test Loss = 0.247106, Learning Rate = 1.116395e-04\n",
      "Epoch 5771/20000: Train Loss = 0.442979, Test Loss = 0.246014, Learning Rate = 1.115971e-04\n",
      "Epoch 5772/20000: Train Loss = 0.442975, Test Loss = 0.244991, Learning Rate = 1.115547e-04\n",
      "Epoch 5773/20000: Train Loss = 0.443017, Test Loss = 0.249477, Learning Rate = 1.115123e-04\n",
      "Epoch 5774/20000: Train Loss = 0.443227, Test Loss = 0.250371, Learning Rate = 1.114699e-04\n",
      "Epoch 5775/20000: Train Loss = 0.443081, Test Loss = 0.245755, Learning Rate = 1.114276e-04\n",
      "Epoch 5776/20000: Train Loss = 0.443489, Test Loss = 0.251142, Learning Rate = 1.113852e-04\n",
      "Epoch 5777/20000: Train Loss = 0.443983, Test Loss = 0.244786, Learning Rate = 1.113429e-04\n",
      "Epoch 5778/20000: Train Loss = 0.443295, Test Loss = 0.249652, Learning Rate = 1.113006e-04\n",
      "Epoch 5779/20000: Train Loss = 0.443118, Test Loss = 0.244686, Learning Rate = 1.112583e-04\n",
      "Epoch 5780/20000: Train Loss = 0.443065, Test Loss = 0.243520, Learning Rate = 1.112160e-04\n",
      "Epoch 5781/20000: Train Loss = 0.443284, Test Loss = 0.245191, Learning Rate = 1.111738e-04\n",
      "Epoch 5782/20000: Train Loss = 0.443101, Test Loss = 0.249505, Learning Rate = 1.111315e-04\n",
      "Epoch 5783/20000: Train Loss = 0.443255, Test Loss = 0.247984, Learning Rate = 1.110893e-04\n",
      "Epoch 5784/20000: Train Loss = 0.442930, Test Loss = 0.248140, Learning Rate = 1.110471e-04\n",
      "Epoch 5785/20000: Train Loss = 0.442837, Test Loss = 0.245886, Learning Rate = 1.110049e-04\n",
      "Epoch 5786/20000: Train Loss = 0.442671, Test Loss = 0.247041, Learning Rate = 1.109627e-04\n",
      "Epoch 5787/20000: Train Loss = 0.443102, Test Loss = 0.244746, Learning Rate = 1.109206e-04\n",
      "Epoch 5788/20000: Train Loss = 0.442766, Test Loss = 0.248883, Learning Rate = 1.108784e-04\n",
      "Epoch 5789/20000: Train Loss = 0.442959, Test Loss = 0.248648, Learning Rate = 1.108363e-04\n",
      "Epoch 5790/20000: Train Loss = 0.442794, Test Loss = 0.246020, Learning Rate = 1.107942e-04\n",
      "Epoch 5791/20000: Train Loss = 0.442800, Test Loss = 0.250413, Learning Rate = 1.107521e-04\n",
      "Epoch 5792/20000: Train Loss = 0.443011, Test Loss = 0.249796, Learning Rate = 1.107100e-04\n",
      "Epoch 5793/20000: Train Loss = 0.442698, Test Loss = 0.246778, Learning Rate = 1.106679e-04\n",
      "Epoch 5794/20000: Train Loss = 0.442782, Test Loss = 0.244607, Learning Rate = 1.106259e-04\n",
      "Epoch 5795/20000: Train Loss = 0.442953, Test Loss = 0.244061, Learning Rate = 1.105838e-04\n",
      "Epoch 5796/20000: Train Loss = 0.443014, Test Loss = 0.244879, Learning Rate = 1.105418e-04\n",
      "Epoch 5797/20000: Train Loss = 0.443025, Test Loss = 0.242745, Learning Rate = 1.104998e-04\n",
      "Epoch 5798/20000: Train Loss = 0.442988, Test Loss = 0.245175, Learning Rate = 1.104578e-04\n",
      "Epoch 5799/20000: Train Loss = 0.443173, Test Loss = 0.253625, Learning Rate = 1.104158e-04\n",
      "Epoch 5800/20000: Train Loss = 0.442937, Test Loss = 0.244827, Learning Rate = 1.103739e-04\n",
      "Epoch 5801/20000: Train Loss = 0.442762, Test Loss = 0.248924, Learning Rate = 1.103320e-04\n",
      "Epoch 5802/20000: Train Loss = 0.443382, Test Loss = 0.251047, Learning Rate = 1.102900e-04\n",
      "Epoch 5803/20000: Train Loss = 0.442686, Test Loss = 0.245846, Learning Rate = 1.102481e-04\n",
      "Epoch 5804/20000: Train Loss = 0.442847, Test Loss = 0.244805, Learning Rate = 1.102062e-04\n",
      "Epoch 5805/20000: Train Loss = 0.443045, Test Loss = 0.242032, Learning Rate = 1.101644e-04\n",
      "Epoch 5806/20000: Train Loss = 0.442937, Test Loss = 0.246227, Learning Rate = 1.101225e-04\n",
      "Epoch 5807/20000: Train Loss = 0.443822, Test Loss = 0.248520, Learning Rate = 1.100807e-04\n",
      "Epoch 5808/20000: Train Loss = 0.442898, Test Loss = 0.249857, Learning Rate = 1.100388e-04\n",
      "Epoch 5809/20000: Train Loss = 0.443513, Test Loss = 0.249517, Learning Rate = 1.099970e-04\n",
      "Epoch 5810/20000: Train Loss = 0.442972, Test Loss = 0.250146, Learning Rate = 1.099552e-04\n",
      "Epoch 5811/20000: Train Loss = 0.442739, Test Loss = 0.248021, Learning Rate = 1.099134e-04\n",
      "Epoch 5812/20000: Train Loss = 0.443323, Test Loss = 0.248735, Learning Rate = 1.098717e-04\n",
      "Epoch 5813/20000: Train Loss = 0.443419, Test Loss = 0.255268, Learning Rate = 1.098299e-04\n",
      "Epoch 5814/20000: Train Loss = 0.443012, Test Loss = 0.247055, Learning Rate = 1.097882e-04\n",
      "Epoch 5815/20000: Train Loss = 0.443291, Test Loss = 0.251247, Learning Rate = 1.097465e-04\n",
      "Epoch 5816/20000: Train Loss = 0.442793, Test Loss = 0.251962, Learning Rate = 1.097048e-04\n",
      "Epoch 5817/20000: Train Loss = 0.443274, Test Loss = 0.246985, Learning Rate = 1.096631e-04\n",
      "Epoch 5818/20000: Train Loss = 0.443214, Test Loss = 0.250964, Learning Rate = 1.096214e-04\n",
      "Epoch 5819/20000: Train Loss = 0.442824, Test Loss = 0.250962, Learning Rate = 1.095798e-04\n",
      "Epoch 5820/20000: Train Loss = 0.442994, Test Loss = 0.244606, Learning Rate = 1.095381e-04\n",
      "Epoch 5821/20000: Train Loss = 0.442854, Test Loss = 0.243289, Learning Rate = 1.094965e-04\n",
      "Epoch 5822/20000: Train Loss = 0.443386, Test Loss = 0.250200, Learning Rate = 1.094549e-04\n",
      "Epoch 5823/20000: Train Loss = 0.442831, Test Loss = 0.247131, Learning Rate = 1.094133e-04\n",
      "Epoch 5824/20000: Train Loss = 0.443268, Test Loss = 0.244305, Learning Rate = 1.093717e-04\n",
      "Epoch 5825/20000: Train Loss = 0.443147, Test Loss = 0.244784, Learning Rate = 1.093302e-04\n",
      "Epoch 5826/20000: Train Loss = 0.443235, Test Loss = 0.244668, Learning Rate = 1.092886e-04\n",
      "Epoch 5827/20000: Train Loss = 0.443202, Test Loss = 0.249030, Learning Rate = 1.092471e-04\n",
      "Epoch 5828/20000: Train Loss = 0.442953, Test Loss = 0.248509, Learning Rate = 1.092056e-04\n",
      "Epoch 5829/20000: Train Loss = 0.443095, Test Loss = 0.244519, Learning Rate = 1.091641e-04\n",
      "Epoch 5830/20000: Train Loss = 0.443061, Test Loss = 0.252131, Learning Rate = 1.091226e-04\n",
      "Epoch 5831/20000: Train Loss = 0.443222, Test Loss = 0.246700, Learning Rate = 1.090812e-04\n",
      "Epoch 5832/20000: Train Loss = 0.443224, Test Loss = 0.250672, Learning Rate = 1.090397e-04\n",
      "Epoch 5833/20000: Train Loss = 0.442759, Test Loss = 0.252185, Learning Rate = 1.089983e-04\n",
      "Epoch 5834/20000: Train Loss = 0.442983, Test Loss = 0.250690, Learning Rate = 1.089569e-04\n",
      "Epoch 5835/20000: Train Loss = 0.443052, Test Loss = 0.249304, Learning Rate = 1.089155e-04\n",
      "Epoch 5836/20000: Train Loss = 0.443168, Test Loss = 0.245765, Learning Rate = 1.088741e-04\n",
      "Epoch 5837/20000: Train Loss = 0.443547, Test Loss = 0.253024, Learning Rate = 1.088327e-04\n",
      "Epoch 5838/20000: Train Loss = 0.442841, Test Loss = 0.247689, Learning Rate = 1.087914e-04\n",
      "Epoch 5839/20000: Train Loss = 0.442833, Test Loss = 0.247438, Learning Rate = 1.087500e-04\n",
      "Epoch 5840/20000: Train Loss = 0.443368, Test Loss = 0.248889, Learning Rate = 1.087087e-04\n",
      "Epoch 5841/20000: Train Loss = 0.442804, Test Loss = 0.248669, Learning Rate = 1.086674e-04\n",
      "Epoch 5842/20000: Train Loss = 0.442752, Test Loss = 0.248655, Learning Rate = 1.086261e-04\n",
      "Epoch 5843/20000: Train Loss = 0.442592, Test Loss = 0.246103, Learning Rate = 1.085848e-04\n",
      "Epoch 5844/20000: Train Loss = 0.442845, Test Loss = 0.248919, Learning Rate = 1.085436e-04\n",
      "Epoch 5845/20000: Train Loss = 0.442797, Test Loss = 0.244693, Learning Rate = 1.085023e-04\n",
      "Epoch 5846/20000: Train Loss = 0.442917, Test Loss = 0.244918, Learning Rate = 1.084611e-04\n",
      "Epoch 5847/20000: Train Loss = 0.443190, Test Loss = 0.248140, Learning Rate = 1.084199e-04\n",
      "Epoch 5848/20000: Train Loss = 0.443092, Test Loss = 0.244793, Learning Rate = 1.083787e-04\n",
      "Epoch 5849/20000: Train Loss = 0.443255, Test Loss = 0.250943, Learning Rate = 1.083375e-04\n",
      "Epoch 5850/20000: Train Loss = 0.443019, Test Loss = 0.244744, Learning Rate = 1.082963e-04\n",
      "Epoch 5851/20000: Train Loss = 0.443551, Test Loss = 0.243636, Learning Rate = 1.082552e-04\n",
      "Epoch 5852/20000: Train Loss = 0.442701, Test Loss = 0.247422, Learning Rate = 1.082141e-04\n",
      "Epoch 5853/20000: Train Loss = 0.442845, Test Loss = 0.253183, Learning Rate = 1.081729e-04\n",
      "Epoch 5854/20000: Train Loss = 0.443474, Test Loss = 0.251580, Learning Rate = 1.081318e-04\n",
      "Epoch 5855/20000: Train Loss = 0.443162, Test Loss = 0.248729, Learning Rate = 1.080907e-04\n",
      "Epoch 5856/20000: Train Loss = 0.443134, Test Loss = 0.248033, Learning Rate = 1.080497e-04\n",
      "Epoch 5857/20000: Train Loss = 0.442800, Test Loss = 0.245116, Learning Rate = 1.080086e-04\n",
      "Epoch 5858/20000: Train Loss = 0.443203, Test Loss = 0.246076, Learning Rate = 1.079676e-04\n",
      "Epoch 5859/20000: Train Loss = 0.442698, Test Loss = 0.251070, Learning Rate = 1.079266e-04\n",
      "Epoch 5860/20000: Train Loss = 0.442961, Test Loss = 0.253569, Learning Rate = 1.078855e-04\n",
      "Epoch 5861/20000: Train Loss = 0.443275, Test Loss = 0.253363, Learning Rate = 1.078446e-04\n",
      "Epoch 5862/20000: Train Loss = 0.442762, Test Loss = 0.248126, Learning Rate = 1.078036e-04\n",
      "Epoch 5863/20000: Train Loss = 0.443035, Test Loss = 0.250716, Learning Rate = 1.077626e-04\n",
      "Epoch 5864/20000: Train Loss = 0.443046, Test Loss = 0.249861, Learning Rate = 1.077217e-04\n",
      "Epoch 5865/20000: Train Loss = 0.442958, Test Loss = 0.243609, Learning Rate = 1.076807e-04\n",
      "Epoch 5866/20000: Train Loss = 0.442838, Test Loss = 0.251455, Learning Rate = 1.076398e-04\n",
      "Epoch 5867/20000: Train Loss = 0.443428, Test Loss = 0.246475, Learning Rate = 1.075989e-04\n",
      "Epoch 5868/20000: Train Loss = 0.442940, Test Loss = 0.251223, Learning Rate = 1.075580e-04\n",
      "Epoch 5869/20000: Train Loss = 0.443020, Test Loss = 0.246943, Learning Rate = 1.075172e-04\n",
      "Epoch 5870/20000: Train Loss = 0.443058, Test Loss = 0.246042, Learning Rate = 1.074763e-04\n",
      "Epoch 5871/20000: Train Loss = 0.443436, Test Loss = 0.248771, Learning Rate = 1.074355e-04\n",
      "Epoch 5872/20000: Train Loss = 0.442894, Test Loss = 0.248111, Learning Rate = 1.073946e-04\n",
      "Epoch 5873/20000: Train Loss = 0.442852, Test Loss = 0.242395, Learning Rate = 1.073538e-04\n",
      "Epoch 5874/20000: Train Loss = 0.443282, Test Loss = 0.244745, Learning Rate = 1.073131e-04\n",
      "Epoch 5875/20000: Train Loss = 0.442874, Test Loss = 0.240096, Learning Rate = 1.072723e-04\n",
      "Epoch 5876/20000: Train Loss = 0.443356, Test Loss = 0.250431, Learning Rate = 1.072315e-04\n",
      "Epoch 5877/20000: Train Loss = 0.443209, Test Loss = 0.247593, Learning Rate = 1.071908e-04\n",
      "Epoch 5878/20000: Train Loss = 0.442958, Test Loss = 0.246970, Learning Rate = 1.071500e-04\n",
      "Epoch 5879/20000: Train Loss = 0.443121, Test Loss = 0.244208, Learning Rate = 1.071093e-04\n",
      "Epoch 5880/20000: Train Loss = 0.442882, Test Loss = 0.249386, Learning Rate = 1.070686e-04\n",
      "Epoch 5881/20000: Train Loss = 0.443237, Test Loss = 0.247317, Learning Rate = 1.070279e-04\n",
      "Epoch 5882/20000: Train Loss = 0.442681, Test Loss = 0.252454, Learning Rate = 1.069873e-04\n",
      "Epoch 5883/20000: Train Loss = 0.442889, Test Loss = 0.249779, Learning Rate = 1.069466e-04\n",
      "Epoch 5884/20000: Train Loss = 0.442664, Test Loss = 0.251610, Learning Rate = 1.069060e-04\n",
      "Epoch 5885/20000: Train Loss = 0.442967, Test Loss = 0.246001, Learning Rate = 1.068654e-04\n",
      "Epoch 5886/20000: Train Loss = 0.442858, Test Loss = 0.251097, Learning Rate = 1.068248e-04\n",
      "Epoch 5887/20000: Train Loss = 0.442860, Test Loss = 0.245603, Learning Rate = 1.067842e-04\n",
      "Epoch 5888/20000: Train Loss = 0.443006, Test Loss = 0.250811, Learning Rate = 1.067436e-04\n",
      "Epoch 5889/20000: Train Loss = 0.442975, Test Loss = 0.246192, Learning Rate = 1.067030e-04\n",
      "Epoch 5890/20000: Train Loss = 0.444328, Test Loss = 0.239305, Learning Rate = 1.066625e-04\n",
      "Epoch 5891/20000: Train Loss = 0.443231, Test Loss = 0.247100, Learning Rate = 1.066220e-04\n",
      "Epoch 5892/20000: Train Loss = 0.443213, Test Loss = 0.245817, Learning Rate = 1.065814e-04\n",
      "Epoch 5893/20000: Train Loss = 0.442669, Test Loss = 0.248531, Learning Rate = 1.065409e-04\n",
      "Epoch 5894/20000: Train Loss = 0.442948, Test Loss = 0.249873, Learning Rate = 1.065005e-04\n",
      "Epoch 5895/20000: Train Loss = 0.443029, Test Loss = 0.247220, Learning Rate = 1.064600e-04\n",
      "Epoch 5896/20000: Train Loss = 0.443841, Test Loss = 0.247834, Learning Rate = 1.064195e-04\n",
      "Epoch 5897/20000: Train Loss = 0.443110, Test Loss = 0.242086, Learning Rate = 1.063791e-04\n",
      "Epoch 5898/20000: Train Loss = 0.442852, Test Loss = 0.245920, Learning Rate = 1.063387e-04\n",
      "Epoch 5899/20000: Train Loss = 0.442991, Test Loss = 0.246045, Learning Rate = 1.062983e-04\n",
      "Epoch 5900/20000: Train Loss = 0.443135, Test Loss = 0.246608, Learning Rate = 1.062579e-04\n",
      "Epoch 5901/20000: Train Loss = 0.442880, Test Loss = 0.244489, Learning Rate = 1.062175e-04\n",
      "Epoch 5902/20000: Train Loss = 0.443343, Test Loss = 0.242269, Learning Rate = 1.061772e-04\n",
      "Epoch 5903/20000: Train Loss = 0.442886, Test Loss = 0.248304, Learning Rate = 1.061368e-04\n",
      "Epoch 5904/20000: Train Loss = 0.442868, Test Loss = 0.244237, Learning Rate = 1.060965e-04\n",
      "Epoch 5905/20000: Train Loss = 0.443302, Test Loss = 0.242465, Learning Rate = 1.060562e-04\n",
      "Epoch 5906/20000: Train Loss = 0.442924, Test Loss = 0.249502, Learning Rate = 1.060159e-04\n",
      "Epoch 5907/20000: Train Loss = 0.443257, Test Loss = 0.253275, Learning Rate = 1.059756e-04\n",
      "Epoch 5908/20000: Train Loss = 0.443082, Test Loss = 0.245994, Learning Rate = 1.059353e-04\n",
      "Epoch 5909/20000: Train Loss = 0.443312, Test Loss = 0.249538, Learning Rate = 1.058951e-04\n",
      "Epoch 5910/20000: Train Loss = 0.442685, Test Loss = 0.241357, Learning Rate = 1.058548e-04\n",
      "Epoch 5911/20000: Train Loss = 0.443330, Test Loss = 0.245696, Learning Rate = 1.058146e-04\n",
      "Epoch 5912/20000: Train Loss = 0.442910, Test Loss = 0.241693, Learning Rate = 1.057744e-04\n",
      "Epoch 5913/20000: Train Loss = 0.443903, Test Loss = 0.243705, Learning Rate = 1.057342e-04\n",
      "Epoch 5914/20000: Train Loss = 0.442878, Test Loss = 0.248187, Learning Rate = 1.056940e-04\n",
      "Epoch 5915/20000: Train Loss = 0.442953, Test Loss = 0.252644, Learning Rate = 1.056539e-04\n",
      "Epoch 5916/20000: Train Loss = 0.443250, Test Loss = 0.250239, Learning Rate = 1.056137e-04\n",
      "Epoch 5917/20000: Train Loss = 0.443473, Test Loss = 0.244318, Learning Rate = 1.055736e-04\n",
      "Epoch 5918/20000: Train Loss = 0.442944, Test Loss = 0.240360, Learning Rate = 1.055335e-04\n",
      "Epoch 5919/20000: Train Loss = 0.443118, Test Loss = 0.247895, Learning Rate = 1.054934e-04\n",
      "Epoch 5920/20000: Train Loss = 0.442982, Test Loss = 0.244987, Learning Rate = 1.054533e-04\n",
      "Epoch 5921/20000: Train Loss = 0.442953, Test Loss = 0.246234, Learning Rate = 1.054132e-04\n",
      "Epoch 5922/20000: Train Loss = 0.443139, Test Loss = 0.250563, Learning Rate = 1.053732e-04\n",
      "Epoch 5923/20000: Train Loss = 0.442892, Test Loss = 0.246495, Learning Rate = 1.053331e-04\n",
      "Epoch 5924/20000: Train Loss = 0.443239, Test Loss = 0.247217, Learning Rate = 1.052931e-04\n",
      "Epoch 5925/20000: Train Loss = 0.442942, Test Loss = 0.246273, Learning Rate = 1.052531e-04\n",
      "Epoch 5926/20000: Train Loss = 0.442911, Test Loss = 0.244701, Learning Rate = 1.052131e-04\n",
      "Epoch 5927/20000: Train Loss = 0.443390, Test Loss = 0.245396, Learning Rate = 1.051731e-04\n",
      "Epoch 5928/20000: Train Loss = 0.442890, Test Loss = 0.250665, Learning Rate = 1.051332e-04\n",
      "Epoch 5929/20000: Train Loss = 0.442774, Test Loss = 0.247958, Learning Rate = 1.050932e-04\n",
      "Epoch 5930/20000: Train Loss = 0.442930, Test Loss = 0.244553, Learning Rate = 1.050533e-04\n",
      "Epoch 5931/20000: Train Loss = 0.442679, Test Loss = 0.244649, Learning Rate = 1.050134e-04\n",
      "Epoch 5932/20000: Train Loss = 0.443641, Test Loss = 0.245139, Learning Rate = 1.049735e-04\n",
      "Epoch 5933/20000: Train Loss = 0.442937, Test Loss = 0.249055, Learning Rate = 1.049336e-04\n",
      "Epoch 5934/20000: Train Loss = 0.442650, Test Loss = 0.250047, Learning Rate = 1.048937e-04\n",
      "Epoch 5935/20000: Train Loss = 0.443155, Test Loss = 0.251575, Learning Rate = 1.048539e-04\n",
      "Epoch 5936/20000: Train Loss = 0.442906, Test Loss = 0.252235, Learning Rate = 1.048140e-04\n",
      "Epoch 5937/20000: Train Loss = 0.442863, Test Loss = 0.252445, Learning Rate = 1.047742e-04\n",
      "Epoch 5938/20000: Train Loss = 0.442887, Test Loss = 0.246093, Learning Rate = 1.047344e-04\n",
      "Epoch 5939/20000: Train Loss = 0.442877, Test Loss = 0.246300, Learning Rate = 1.046946e-04\n",
      "Epoch 5940/20000: Train Loss = 0.442675, Test Loss = 0.252834, Learning Rate = 1.046548e-04\n",
      "Epoch 5941/20000: Train Loss = 0.442636, Test Loss = 0.250967, Learning Rate = 1.046150e-04\n",
      "Epoch 5942/20000: Train Loss = 0.443124, Test Loss = 0.249599, Learning Rate = 1.045753e-04\n",
      "Epoch 5943/20000: Train Loss = 0.443224, Test Loss = 0.251021, Learning Rate = 1.045355e-04\n",
      "Epoch 5944/20000: Train Loss = 0.443164, Test Loss = 0.253336, Learning Rate = 1.044958e-04\n",
      "Epoch 5945/20000: Train Loss = 0.442877, Test Loss = 0.245013, Learning Rate = 1.044561e-04\n",
      "Epoch 5946/20000: Train Loss = 0.443263, Test Loss = 0.248939, Learning Rate = 1.044164e-04\n",
      "Epoch 5947/20000: Train Loss = 0.443001, Test Loss = 0.249295, Learning Rate = 1.043768e-04\n",
      "Epoch 5948/20000: Train Loss = 0.443279, Test Loss = 0.246534, Learning Rate = 1.043371e-04\n",
      "Epoch 5949/20000: Train Loss = 0.443052, Test Loss = 0.247148, Learning Rate = 1.042974e-04\n",
      "Epoch 5950/20000: Train Loss = 0.443683, Test Loss = 0.248297, Learning Rate = 1.042578e-04\n",
      "Epoch 5951/20000: Train Loss = 0.443229, Test Loss = 0.250838, Learning Rate = 1.042182e-04\n",
      "Epoch 5952/20000: Train Loss = 0.443113, Test Loss = 0.253957, Learning Rate = 1.041786e-04\n",
      "Epoch 5953/20000: Train Loss = 0.442548, Test Loss = 0.248176, Learning Rate = 1.041390e-04\n",
      "Epoch 5954/20000: Train Loss = 0.443352, Test Loss = 0.247277, Learning Rate = 1.040994e-04\n",
      "Epoch 5955/20000: Train Loss = 0.442792, Test Loss = 0.249684, Learning Rate = 1.040599e-04\n",
      "Epoch 5956/20000: Train Loss = 0.442809, Test Loss = 0.243316, Learning Rate = 1.040204e-04\n",
      "Epoch 5957/20000: Train Loss = 0.443026, Test Loss = 0.244867, Learning Rate = 1.039808e-04\n",
      "Epoch 5958/20000: Train Loss = 0.443053, Test Loss = 0.247834, Learning Rate = 1.039413e-04\n",
      "Epoch 5959/20000: Train Loss = 0.443056, Test Loss = 0.246843, Learning Rate = 1.039018e-04\n",
      "Epoch 5960/20000: Train Loss = 0.442802, Test Loss = 0.244427, Learning Rate = 1.038623e-04\n",
      "Epoch 5961/20000: Train Loss = 0.442837, Test Loss = 0.243941, Learning Rate = 1.038229e-04\n",
      "Epoch 5962/20000: Train Loss = 0.443013, Test Loss = 0.241828, Learning Rate = 1.037834e-04\n",
      "Epoch 5963/20000: Train Loss = 0.443254, Test Loss = 0.252830, Learning Rate = 1.037440e-04\n",
      "Epoch 5964/20000: Train Loss = 0.442970, Test Loss = 0.248104, Learning Rate = 1.037046e-04\n",
      "Epoch 5965/20000: Train Loss = 0.443260, Test Loss = 0.249108, Learning Rate = 1.036652e-04\n",
      "Epoch 5966/20000: Train Loss = 0.442736, Test Loss = 0.246378, Learning Rate = 1.036258e-04\n",
      "Epoch 5967/20000: Train Loss = 0.442639, Test Loss = 0.249382, Learning Rate = 1.035864e-04\n",
      "Epoch 5968/20000: Train Loss = 0.443353, Test Loss = 0.246739, Learning Rate = 1.035470e-04\n",
      "Epoch 5969/20000: Train Loss = 0.442821, Test Loss = 0.242881, Learning Rate = 1.035077e-04\n",
      "Epoch 5970/20000: Train Loss = 0.443301, Test Loss = 0.246461, Learning Rate = 1.034684e-04\n",
      "Epoch 5971/20000: Train Loss = 0.443204, Test Loss = 0.242678, Learning Rate = 1.034291e-04\n",
      "Epoch 5972/20000: Train Loss = 0.442788, Test Loss = 0.245521, Learning Rate = 1.033898e-04\n",
      "Epoch 5973/20000: Train Loss = 0.443026, Test Loss = 0.243946, Learning Rate = 1.033505e-04\n",
      "Epoch 5974/20000: Train Loss = 0.443158, Test Loss = 0.248423, Learning Rate = 1.033112e-04\n",
      "Epoch 5975/20000: Train Loss = 0.443104, Test Loss = 0.249394, Learning Rate = 1.032719e-04\n",
      "Epoch 5976/20000: Train Loss = 0.442764, Test Loss = 0.245551, Learning Rate = 1.032327e-04\n",
      "Epoch 5977/20000: Train Loss = 0.442825, Test Loss = 0.245750, Learning Rate = 1.031935e-04\n",
      "Epoch 5978/20000: Train Loss = 0.443051, Test Loss = 0.243669, Learning Rate = 1.031543e-04\n",
      "Epoch 5979/20000: Train Loss = 0.443024, Test Loss = 0.242563, Learning Rate = 1.031151e-04\n",
      "Epoch 5980/20000: Train Loss = 0.443018, Test Loss = 0.247054, Learning Rate = 1.030759e-04\n",
      "Epoch 5981/20000: Train Loss = 0.443137, Test Loss = 0.245928, Learning Rate = 1.030367e-04\n",
      "Epoch 5982/20000: Train Loss = 0.443233, Test Loss = 0.256420, Learning Rate = 1.029976e-04\n",
      "Epoch 5983/20000: Train Loss = 0.443423, Test Loss = 0.245367, Learning Rate = 1.029584e-04\n",
      "Epoch 5984/20000: Train Loss = 0.443359, Test Loss = 0.247637, Learning Rate = 1.029193e-04\n",
      "Epoch 5985/20000: Train Loss = 0.443013, Test Loss = 0.246101, Learning Rate = 1.028802e-04\n",
      "Epoch 5986/20000: Train Loss = 0.443633, Test Loss = 0.250927, Learning Rate = 1.028411e-04\n",
      "Epoch 5987/20000: Train Loss = 0.443052, Test Loss = 0.248897, Learning Rate = 1.028020e-04\n",
      "Epoch 5988/20000: Train Loss = 0.442806, Test Loss = 0.246451, Learning Rate = 1.027630e-04\n",
      "Epoch 5989/20000: Train Loss = 0.443238, Test Loss = 0.249918, Learning Rate = 1.027239e-04\n",
      "Epoch 5990/20000: Train Loss = 0.442992, Test Loss = 0.249138, Learning Rate = 1.026849e-04\n",
      "Epoch 5991/20000: Train Loss = 0.442827, Test Loss = 0.246712, Learning Rate = 1.026459e-04\n",
      "Epoch 5992/20000: Train Loss = 0.442765, Test Loss = 0.246911, Learning Rate = 1.026069e-04\n",
      "Epoch 5993/20000: Train Loss = 0.442627, Test Loss = 0.247622, Learning Rate = 1.025679e-04\n",
      "Epoch 5994/20000: Train Loss = 0.442747, Test Loss = 0.249592, Learning Rate = 1.025289e-04\n",
      "Epoch 5995/20000: Train Loss = 0.443940, Test Loss = 0.242020, Learning Rate = 1.024900e-04\n",
      "Epoch 5996/20000: Train Loss = 0.442890, Test Loss = 0.245553, Learning Rate = 1.024510e-04\n",
      "Epoch 5997/20000: Train Loss = 0.442885, Test Loss = 0.242384, Learning Rate = 1.024121e-04\n",
      "Epoch 5998/20000: Train Loss = 0.442759, Test Loss = 0.242993, Learning Rate = 1.023732e-04\n",
      "Epoch 5999/20000: Train Loss = 0.444514, Test Loss = 0.255336, Learning Rate = 1.023343e-04\n",
      "Epoch 6000/20000: Train Loss = 0.442487, Test Loss = 0.240728, Learning Rate = 1.022954e-04\n",
      "Epoch 6001/20000: Train Loss = 0.442936, Test Loss = 0.240651, Learning Rate = 1.022565e-04\n",
      "Epoch 6002/20000: Train Loss = 0.443039, Test Loss = 0.245747, Learning Rate = 1.022177e-04\n",
      "Epoch 6003/20000: Train Loss = 0.442983, Test Loss = 0.247116, Learning Rate = 1.021788e-04\n",
      "Epoch 6004/20000: Train Loss = 0.442730, Test Loss = 0.249299, Learning Rate = 1.021400e-04\n",
      "Epoch 6005/20000: Train Loss = 0.442954, Test Loss = 0.244376, Learning Rate = 1.021012e-04\n",
      "Epoch 6006/20000: Train Loss = 0.442695, Test Loss = 0.247927, Learning Rate = 1.020624e-04\n",
      "Epoch 6007/20000: Train Loss = 0.442635, Test Loss = 0.249890, Learning Rate = 1.020236e-04\n",
      "Epoch 6008/20000: Train Loss = 0.443394, Test Loss = 0.243990, Learning Rate = 1.019848e-04\n",
      "Epoch 6009/20000: Train Loss = 0.442980, Test Loss = 0.245870, Learning Rate = 1.019461e-04\n",
      "Epoch 6010/20000: Train Loss = 0.442953, Test Loss = 0.248465, Learning Rate = 1.019074e-04\n",
      "Epoch 6011/20000: Train Loss = 0.442788, Test Loss = 0.246488, Learning Rate = 1.018686e-04\n",
      "Epoch 6012/20000: Train Loss = 0.443074, Test Loss = 0.248654, Learning Rate = 1.018299e-04\n",
      "Epoch 6013/20000: Train Loss = 0.442991, Test Loss = 0.249452, Learning Rate = 1.017912e-04\n",
      "Epoch 6014/20000: Train Loss = 0.443279, Test Loss = 0.246317, Learning Rate = 1.017526e-04\n",
      "Epoch 6015/20000: Train Loss = 0.443049, Test Loss = 0.249374, Learning Rate = 1.017139e-04\n",
      "Epoch 6016/20000: Train Loss = 0.442993, Test Loss = 0.250531, Learning Rate = 1.016752e-04\n",
      "Epoch 6017/20000: Train Loss = 0.444316, Test Loss = 0.243151, Learning Rate = 1.016366e-04\n",
      "Epoch 6018/20000: Train Loss = 0.442873, Test Loss = 0.246497, Learning Rate = 1.015980e-04\n",
      "Epoch 6019/20000: Train Loss = 0.443259, Test Loss = 0.246182, Learning Rate = 1.015594e-04\n",
      "Epoch 6020/20000: Train Loss = 0.443694, Test Loss = 0.245242, Learning Rate = 1.015208e-04\n",
      "Epoch 6021/20000: Train Loss = 0.443489, Test Loss = 0.241583, Learning Rate = 1.014822e-04\n",
      "Epoch 6022/20000: Train Loss = 0.442668, Test Loss = 0.247694, Learning Rate = 1.014437e-04\n",
      "Epoch 6023/20000: Train Loss = 0.442985, Test Loss = 0.249745, Learning Rate = 1.014051e-04\n",
      "Epoch 6024/20000: Train Loss = 0.442696, Test Loss = 0.244454, Learning Rate = 1.013666e-04\n",
      "Epoch 6025/20000: Train Loss = 0.442861, Test Loss = 0.246043, Learning Rate = 1.013281e-04\n",
      "Epoch 6026/20000: Train Loss = 0.443079, Test Loss = 0.251159, Learning Rate = 1.012896e-04\n",
      "Epoch 6027/20000: Train Loss = 0.443131, Test Loss = 0.241324, Learning Rate = 1.012511e-04\n",
      "Epoch 6028/20000: Train Loss = 0.443094, Test Loss = 0.248637, Learning Rate = 1.012126e-04\n",
      "Epoch 6029/20000: Train Loss = 0.442881, Test Loss = 0.246217, Learning Rate = 1.011741e-04\n",
      "Epoch 6030/20000: Train Loss = 0.443038, Test Loss = 0.242066, Learning Rate = 1.011357e-04\n",
      "Epoch 6031/20000: Train Loss = 0.442878, Test Loss = 0.243337, Learning Rate = 1.010973e-04\n",
      "Epoch 6032/20000: Train Loss = 0.443016, Test Loss = 0.243741, Learning Rate = 1.010589e-04\n",
      "Epoch 6033/20000: Train Loss = 0.442607, Test Loss = 0.247996, Learning Rate = 1.010205e-04\n",
      "Epoch 6034/20000: Train Loss = 0.444068, Test Loss = 0.251640, Learning Rate = 1.009821e-04\n",
      "Epoch 6035/20000: Train Loss = 0.443187, Test Loss = 0.250640, Learning Rate = 1.009437e-04\n",
      "Epoch 6036/20000: Train Loss = 0.442752, Test Loss = 0.245898, Learning Rate = 1.009054e-04\n",
      "Epoch 6037/20000: Train Loss = 0.442632, Test Loss = 0.243776, Learning Rate = 1.008670e-04\n",
      "Epoch 6038/20000: Train Loss = 0.442974, Test Loss = 0.240061, Learning Rate = 1.008287e-04\n",
      "Epoch 6039/20000: Train Loss = 0.442825, Test Loss = 0.245682, Learning Rate = 1.007904e-04\n",
      "Epoch 6040/20000: Train Loss = 0.442795, Test Loss = 0.246493, Learning Rate = 1.007521e-04\n",
      "Epoch 6041/20000: Train Loss = 0.442815, Test Loss = 0.245517, Learning Rate = 1.007138e-04\n",
      "Epoch 6042/20000: Train Loss = 0.443113, Test Loss = 0.246051, Learning Rate = 1.006755e-04\n",
      "Epoch 6043/20000: Train Loss = 0.443132, Test Loss = 0.243992, Learning Rate = 1.006373e-04\n",
      "Epoch 6044/20000: Train Loss = 0.443270, Test Loss = 0.253393, Learning Rate = 1.005990e-04\n",
      "Epoch 6045/20000: Train Loss = 0.443098, Test Loss = 0.246107, Learning Rate = 1.005608e-04\n",
      "Epoch 6046/20000: Train Loss = 0.442806, Test Loss = 0.248113, Learning Rate = 1.005226e-04\n",
      "Epoch 6047/20000: Train Loss = 0.443560, Test Loss = 0.243777, Learning Rate = 1.004844e-04\n",
      "Epoch 6048/20000: Train Loss = 0.443305, Test Loss = 0.245146, Learning Rate = 1.004462e-04\n",
      "Epoch 6049/20000: Train Loss = 0.442827, Test Loss = 0.245007, Learning Rate = 1.004080e-04\n",
      "Epoch 6050/20000: Train Loss = 0.442762, Test Loss = 0.243340, Learning Rate = 1.003699e-04\n",
      "Epoch 6051/20000: Train Loss = 0.443038, Test Loss = 0.246320, Learning Rate = 1.003318e-04\n",
      "Epoch 6052/20000: Train Loss = 0.443524, Test Loss = 0.248783, Learning Rate = 1.002936e-04\n",
      "Epoch 6053/20000: Train Loss = 0.443172, Test Loss = 0.246151, Learning Rate = 1.002555e-04\n",
      "Epoch 6054/20000: Train Loss = 0.442742, Test Loss = 0.248796, Learning Rate = 1.002174e-04\n",
      "Epoch 6055/20000: Train Loss = 0.442829, Test Loss = 0.246106, Learning Rate = 1.001794e-04\n",
      "Epoch 6056/20000: Train Loss = 0.442725, Test Loss = 0.250088, Learning Rate = 1.001413e-04\n",
      "Epoch 6057/20000: Train Loss = 0.443018, Test Loss = 0.244885, Learning Rate = 1.001032e-04\n",
      "Epoch 6058/20000: Train Loss = 0.442803, Test Loss = 0.246146, Learning Rate = 1.000652e-04\n",
      "Epoch 6059/20000: Train Loss = 0.443193, Test Loss = 0.245358, Learning Rate = 1.000272e-04\n",
      "Epoch 6060/20000: Train Loss = 0.442813, Test Loss = 0.248627, Learning Rate = 9.998917e-05\n",
      "Epoch 6061/20000: Train Loss = 0.442962, Test Loss = 0.241823, Learning Rate = 9.995118e-05\n",
      "Epoch 6062/20000: Train Loss = 0.442967, Test Loss = 0.245519, Learning Rate = 9.991320e-05\n",
      "Epoch 6063/20000: Train Loss = 0.442802, Test Loss = 0.241512, Learning Rate = 9.987523e-05\n",
      "Epoch 6064/20000: Train Loss = 0.443213, Test Loss = 0.244951, Learning Rate = 9.983728e-05\n",
      "Epoch 6065/20000: Train Loss = 0.442818, Test Loss = 0.243916, Learning Rate = 9.979935e-05\n",
      "Epoch 6066/20000: Train Loss = 0.442995, Test Loss = 0.249564, Learning Rate = 9.976143e-05\n",
      "Epoch 6067/20000: Train Loss = 0.443093, Test Loss = 0.247930, Learning Rate = 9.972352e-05\n",
      "Epoch 6068/20000: Train Loss = 0.442987, Test Loss = 0.244960, Learning Rate = 9.968563e-05\n",
      "Epoch 6069/20000: Train Loss = 0.442915, Test Loss = 0.242118, Learning Rate = 9.964775e-05\n",
      "Epoch 6070/20000: Train Loss = 0.442973, Test Loss = 0.242255, Learning Rate = 9.960989e-05\n",
      "Epoch 6071/20000: Train Loss = 0.443009, Test Loss = 0.251130, Learning Rate = 9.957204e-05\n",
      "Epoch 6072/20000: Train Loss = 0.442903, Test Loss = 0.247783, Learning Rate = 9.953420e-05\n",
      "Epoch 6073/20000: Train Loss = 0.443233, Test Loss = 0.244813, Learning Rate = 9.949638e-05\n",
      "Epoch 6074/20000: Train Loss = 0.442879, Test Loss = 0.245355, Learning Rate = 9.945858e-05\n",
      "Epoch 6075/20000: Train Loss = 0.442816, Test Loss = 0.249833, Learning Rate = 9.942079e-05\n",
      "Epoch 6076/20000: Train Loss = 0.443478, Test Loss = 0.245158, Learning Rate = 9.938301e-05\n",
      "Epoch 6077/20000: Train Loss = 0.442978, Test Loss = 0.251829, Learning Rate = 9.934525e-05\n",
      "Epoch 6078/20000: Train Loss = 0.443043, Test Loss = 0.252798, Learning Rate = 9.930750e-05\n",
      "Epoch 6079/20000: Train Loss = 0.442817, Test Loss = 0.250162, Learning Rate = 9.926976e-05\n",
      "Epoch 6080/20000: Train Loss = 0.442901, Test Loss = 0.249129, Learning Rate = 9.923204e-05\n",
      "Epoch 6081/20000: Train Loss = 0.443167, Test Loss = 0.245515, Learning Rate = 9.919434e-05\n",
      "Epoch 6082/20000: Train Loss = 0.442947, Test Loss = 0.243350, Learning Rate = 9.915665e-05\n",
      "Epoch 6083/20000: Train Loss = 0.442838, Test Loss = 0.248768, Learning Rate = 9.911897e-05\n",
      "Epoch 6084/20000: Train Loss = 0.443111, Test Loss = 0.245613, Learning Rate = 9.908131e-05\n",
      "Epoch 6085/20000: Train Loss = 0.442738, Test Loss = 0.246480, Learning Rate = 9.904366e-05\n",
      "Epoch 6086/20000: Train Loss = 0.442655, Test Loss = 0.248811, Learning Rate = 9.900602e-05\n",
      "Epoch 6087/20000: Train Loss = 0.443108, Test Loss = 0.245970, Learning Rate = 9.896841e-05\n",
      "Epoch 6088/20000: Train Loss = 0.442998, Test Loss = 0.245230, Learning Rate = 9.893080e-05\n",
      "Epoch 6089/20000: Train Loss = 0.442729, Test Loss = 0.245847, Learning Rate = 9.889321e-05\n",
      "Epoch 6090/20000: Train Loss = 0.442730, Test Loss = 0.246727, Learning Rate = 9.885563e-05\n",
      "Epoch 6091/20000: Train Loss = 0.443618, Test Loss = 0.252526, Learning Rate = 9.881807e-05\n",
      "Epoch 6092/20000: Train Loss = 0.443037, Test Loss = 0.245718, Learning Rate = 9.878052e-05\n",
      "Epoch 6093/20000: Train Loss = 0.442977, Test Loss = 0.244759, Learning Rate = 9.874299e-05\n",
      "Epoch 6094/20000: Train Loss = 0.443191, Test Loss = 0.242510, Learning Rate = 9.870547e-05\n",
      "Epoch 6095/20000: Train Loss = 0.443161, Test Loss = 0.244916, Learning Rate = 9.866796e-05\n",
      "Epoch 6096/20000: Train Loss = 0.443086, Test Loss = 0.241492, Learning Rate = 9.863047e-05\n",
      "Epoch 6097/20000: Train Loss = 0.442987, Test Loss = 0.247315, Learning Rate = 9.859299e-05\n",
      "Epoch 6098/20000: Train Loss = 0.443108, Test Loss = 0.244670, Learning Rate = 9.855553e-05\n",
      "Epoch 6099/20000: Train Loss = 0.442607, Test Loss = 0.248837, Learning Rate = 9.851808e-05\n",
      "Epoch 6100/20000: Train Loss = 0.442718, Test Loss = 0.247922, Learning Rate = 9.848065e-05\n",
      "Epoch 6101/20000: Train Loss = 0.442760, Test Loss = 0.248018, Learning Rate = 9.844323e-05\n",
      "Epoch 6102/20000: Train Loss = 0.443478, Test Loss = 0.243292, Learning Rate = 9.840582e-05\n",
      "Epoch 6103/20000: Train Loss = 0.443235, Test Loss = 0.249738, Learning Rate = 9.836843e-05\n",
      "Epoch 6104/20000: Train Loss = 0.442914, Test Loss = 0.256530, Learning Rate = 9.833105e-05\n",
      "Epoch 6105/20000: Train Loss = 0.443071, Test Loss = 0.253199, Learning Rate = 9.829369e-05\n",
      "Epoch 6106/20000: Train Loss = 0.443204, Test Loss = 0.251836, Learning Rate = 9.825634e-05\n",
      "Epoch 6107/20000: Train Loss = 0.443045, Test Loss = 0.250719, Learning Rate = 9.821901e-05\n",
      "Epoch 6108/20000: Train Loss = 0.443389, Test Loss = 0.251814, Learning Rate = 9.818169e-05\n",
      "Epoch 6109/20000: Train Loss = 0.443528, Test Loss = 0.243328, Learning Rate = 9.814438e-05\n",
      "Epoch 6110/20000: Train Loss = 0.444251, Test Loss = 0.253471, Learning Rate = 9.810709e-05\n",
      "Epoch 6111/20000: Train Loss = 0.442281, Test Loss = 0.242779, Learning Rate = 9.806981e-05\n",
      "Epoch 6112/20000: Train Loss = 0.443041, Test Loss = 0.238283, Learning Rate = 9.803255e-05\n",
      "Epoch 6113/20000: Train Loss = 0.442725, Test Loss = 0.245489, Learning Rate = 9.799530e-05\n",
      "Epoch 6114/20000: Train Loss = 0.443167, Test Loss = 0.249441, Learning Rate = 9.795806e-05\n",
      "Epoch 6115/20000: Train Loss = 0.443372, Test Loss = 0.251817, Learning Rate = 9.792084e-05\n",
      "Epoch 6116/20000: Train Loss = 0.443134, Test Loss = 0.251579, Learning Rate = 9.788363e-05\n",
      "Epoch 6117/20000: Train Loss = 0.442738, Test Loss = 0.252158, Learning Rate = 9.784644e-05\n",
      "Epoch 6118/20000: Train Loss = 0.442947, Test Loss = 0.250209, Learning Rate = 9.780926e-05\n",
      "Epoch 6119/20000: Train Loss = 0.442721, Test Loss = 0.248724, Learning Rate = 9.777210e-05\n",
      "Epoch 6120/20000: Train Loss = 0.443000, Test Loss = 0.245476, Learning Rate = 9.773494e-05\n",
      "Epoch 6121/20000: Train Loss = 0.443693, Test Loss = 0.246596, Learning Rate = 9.769781e-05\n",
      "Epoch 6122/20000: Train Loss = 0.443294, Test Loss = 0.245281, Learning Rate = 9.766069e-05\n",
      "Epoch 6123/20000: Train Loss = 0.442876, Test Loss = 0.248195, Learning Rate = 9.762358e-05\n",
      "Epoch 6124/20000: Train Loss = 0.442877, Test Loss = 0.244635, Learning Rate = 9.758648e-05\n",
      "Epoch 6125/20000: Train Loss = 0.443136, Test Loss = 0.250564, Learning Rate = 9.754940e-05\n",
      "Epoch 6126/20000: Train Loss = 0.442596, Test Loss = 0.245333, Learning Rate = 9.751234e-05\n",
      "Epoch 6127/20000: Train Loss = 0.442917, Test Loss = 0.247593, Learning Rate = 9.747528e-05\n",
      "Epoch 6128/20000: Train Loss = 0.443643, Test Loss = 0.244179, Learning Rate = 9.743825e-05\n",
      "Epoch 6129/20000: Train Loss = 0.442574, Test Loss = 0.250197, Learning Rate = 9.740122e-05\n",
      "Epoch 6130/20000: Train Loss = 0.442931, Test Loss = 0.251446, Learning Rate = 9.736421e-05\n",
      "Epoch 6131/20000: Train Loss = 0.442852, Test Loss = 0.251749, Learning Rate = 9.732722e-05\n",
      "Epoch 6132/20000: Train Loss = 0.443311, Test Loss = 0.247948, Learning Rate = 9.729024e-05\n",
      "Epoch 6133/20000: Train Loss = 0.442995, Test Loss = 0.248179, Learning Rate = 9.725327e-05\n",
      "Epoch 6134/20000: Train Loss = 0.443274, Test Loss = 0.241748, Learning Rate = 9.721631e-05\n",
      "Epoch 6135/20000: Train Loss = 0.442969, Test Loss = 0.246219, Learning Rate = 9.717937e-05\n",
      "Epoch 6136/20000: Train Loss = 0.443244, Test Loss = 0.250452, Learning Rate = 9.714245e-05\n",
      "Epoch 6137/20000: Train Loss = 0.442965, Test Loss = 0.247494, Learning Rate = 9.710554e-05\n",
      "Epoch 6138/20000: Train Loss = 0.443000, Test Loss = 0.242077, Learning Rate = 9.706864e-05\n",
      "Epoch 6139/20000: Train Loss = 0.443071, Test Loss = 0.249085, Learning Rate = 9.703176e-05\n",
      "Epoch 6140/20000: Train Loss = 0.443018, Test Loss = 0.245741, Learning Rate = 9.699489e-05\n",
      "Epoch 6141/20000: Train Loss = 0.442960, Test Loss = 0.242857, Learning Rate = 9.695803e-05\n",
      "Epoch 6142/20000: Train Loss = 0.442915, Test Loss = 0.245821, Learning Rate = 9.692119e-05\n",
      "Epoch 6143/20000: Train Loss = 0.442872, Test Loss = 0.245830, Learning Rate = 9.688436e-05\n",
      "Epoch 6144/20000: Train Loss = 0.442780, Test Loss = 0.246563, Learning Rate = 9.684755e-05\n",
      "Epoch 6145/20000: Train Loss = 0.443450, Test Loss = 0.247273, Learning Rate = 9.681075e-05\n",
      "Epoch 6146/20000: Train Loss = 0.443044, Test Loss = 0.245640, Learning Rate = 9.677396e-05\n",
      "Epoch 6147/20000: Train Loss = 0.443263, Test Loss = 0.250460, Learning Rate = 9.673719e-05\n",
      "Epoch 6148/20000: Train Loss = 0.442773, Test Loss = 0.247411, Learning Rate = 9.670044e-05\n",
      "Epoch 6149/20000: Train Loss = 0.442829, Test Loss = 0.243975, Learning Rate = 9.666369e-05\n",
      "Epoch 6150/20000: Train Loss = 0.442959, Test Loss = 0.249861, Learning Rate = 9.662696e-05\n",
      "Epoch 6151/20000: Train Loss = 0.443275, Test Loss = 0.242942, Learning Rate = 9.659025e-05\n",
      "Epoch 6152/20000: Train Loss = 0.443323, Test Loss = 0.251348, Learning Rate = 9.655355e-05\n",
      "Epoch 6153/20000: Train Loss = 0.442763, Test Loss = 0.250451, Learning Rate = 9.651686e-05\n",
      "Epoch 6154/20000: Train Loss = 0.443360, Test Loss = 0.249540, Learning Rate = 9.648018e-05\n",
      "Epoch 6155/20000: Train Loss = 0.443302, Test Loss = 0.246820, Learning Rate = 9.644352e-05\n",
      "Epoch 6156/20000: Train Loss = 0.442913, Test Loss = 0.250304, Learning Rate = 9.640688e-05\n",
      "Epoch 6157/20000: Train Loss = 0.442806, Test Loss = 0.249150, Learning Rate = 9.637025e-05\n",
      "Epoch 6158/20000: Train Loss = 0.442714, Test Loss = 0.243321, Learning Rate = 9.633363e-05\n",
      "Epoch 6159/20000: Train Loss = 0.442974, Test Loss = 0.247618, Learning Rate = 9.629702e-05\n",
      "Epoch 6160/20000: Train Loss = 0.443101, Test Loss = 0.251320, Learning Rate = 9.626043e-05\n",
      "Epoch 6161/20000: Train Loss = 0.443020, Test Loss = 0.250502, Learning Rate = 9.622386e-05\n",
      "Epoch 6162/20000: Train Loss = 0.442973, Test Loss = 0.245439, Learning Rate = 9.618729e-05\n",
      "Epoch 6163/20000: Train Loss = 0.442916, Test Loss = 0.246251, Learning Rate = 9.615075e-05\n",
      "Epoch 6164/20000: Train Loss = 0.442725, Test Loss = 0.248285, Learning Rate = 9.611421e-05\n",
      "Epoch 6165/20000: Train Loss = 0.442611, Test Loss = 0.244790, Learning Rate = 9.607769e-05\n",
      "Epoch 6166/20000: Train Loss = 0.442871, Test Loss = 0.245897, Learning Rate = 9.604118e-05\n",
      "Epoch 6167/20000: Train Loss = 0.442905, Test Loss = 0.248337, Learning Rate = 9.600469e-05\n",
      "Epoch 6168/20000: Train Loss = 0.442998, Test Loss = 0.245730, Learning Rate = 9.596821e-05\n",
      "Epoch 6169/20000: Train Loss = 0.442816, Test Loss = 0.248922, Learning Rate = 9.593175e-05\n",
      "Epoch 6170/20000: Train Loss = 0.443222, Test Loss = 0.249510, Learning Rate = 9.589529e-05\n",
      "Epoch 6171/20000: Train Loss = 0.442797, Test Loss = 0.246791, Learning Rate = 9.585886e-05\n",
      "Epoch 6172/20000: Train Loss = 0.442810, Test Loss = 0.248785, Learning Rate = 9.582243e-05\n",
      "Epoch 6173/20000: Train Loss = 0.443111, Test Loss = 0.253994, Learning Rate = 9.578602e-05\n",
      "Epoch 6174/20000: Train Loss = 0.442835, Test Loss = 0.248032, Learning Rate = 9.574963e-05\n",
      "Epoch 6175/20000: Train Loss = 0.442921, Test Loss = 0.248938, Learning Rate = 9.571324e-05\n",
      "Epoch 6176/20000: Train Loss = 0.443201, Test Loss = 0.250815, Learning Rate = 9.567688e-05\n",
      "Epoch 6177/20000: Train Loss = 0.443186, Test Loss = 0.246572, Learning Rate = 9.564052e-05\n",
      "Epoch 6178/20000: Train Loss = 0.442832, Test Loss = 0.247859, Learning Rate = 9.560418e-05\n",
      "Epoch 6179/20000: Train Loss = 0.442919, Test Loss = 0.247503, Learning Rate = 9.556785e-05\n",
      "Epoch 6180/20000: Train Loss = 0.442690, Test Loss = 0.244444, Learning Rate = 9.553154e-05\n",
      "Epoch 6181/20000: Train Loss = 0.442918, Test Loss = 0.249513, Learning Rate = 9.549524e-05\n",
      "Epoch 6182/20000: Train Loss = 0.442976, Test Loss = 0.244975, Learning Rate = 9.545896e-05\n",
      "Epoch 6183/20000: Train Loss = 0.442884, Test Loss = 0.244082, Learning Rate = 9.542268e-05\n",
      "Epoch 6184/20000: Train Loss = 0.442952, Test Loss = 0.246031, Learning Rate = 9.538643e-05\n",
      "Epoch 6185/20000: Train Loss = 0.442946, Test Loss = 0.244324, Learning Rate = 9.535018e-05\n",
      "Epoch 6186/20000: Train Loss = 0.442805, Test Loss = 0.243850, Learning Rate = 9.531395e-05\n",
      "Epoch 6187/20000: Train Loss = 0.443260, Test Loss = 0.245711, Learning Rate = 9.527773e-05\n",
      "Epoch 6188/20000: Train Loss = 0.442946, Test Loss = 0.242651, Learning Rate = 9.524153e-05\n",
      "Epoch 6189/20000: Train Loss = 0.443097, Test Loss = 0.248258, Learning Rate = 9.520534e-05\n",
      "Epoch 6190/20000: Train Loss = 0.443531, Test Loss = 0.252109, Learning Rate = 9.516917e-05\n",
      "Epoch 6191/20000: Train Loss = 0.442963, Test Loss = 0.247607, Learning Rate = 9.513301e-05\n",
      "Epoch 6192/20000: Train Loss = 0.442791, Test Loss = 0.252607, Learning Rate = 9.509686e-05\n",
      "Epoch 6193/20000: Train Loss = 0.443387, Test Loss = 0.250227, Learning Rate = 9.506072e-05\n",
      "Epoch 6194/20000: Train Loss = 0.443183, Test Loss = 0.242129, Learning Rate = 9.502460e-05\n",
      "Epoch 6195/20000: Train Loss = 0.442923, Test Loss = 0.246278, Learning Rate = 9.498850e-05\n",
      "Epoch 6196/20000: Train Loss = 0.442606, Test Loss = 0.247986, Learning Rate = 9.495240e-05\n",
      "Epoch 6197/20000: Train Loss = 0.442915, Test Loss = 0.249584, Learning Rate = 9.491632e-05\n",
      "Epoch 6198/20000: Train Loss = 0.443021, Test Loss = 0.242421, Learning Rate = 9.488026e-05\n",
      "Epoch 6199/20000: Train Loss = 0.443017, Test Loss = 0.248795, Learning Rate = 9.484421e-05\n",
      "Epoch 6200/20000: Train Loss = 0.443255, Test Loss = 0.240911, Learning Rate = 9.480817e-05\n",
      "Epoch 6201/20000: Train Loss = 0.443163, Test Loss = 0.243675, Learning Rate = 9.477214e-05\n",
      "Epoch 6202/20000: Train Loss = 0.442851, Test Loss = 0.242145, Learning Rate = 9.473613e-05\n",
      "Epoch 6203/20000: Train Loss = 0.443053, Test Loss = 0.249363, Learning Rate = 9.470014e-05\n",
      "Epoch 6204/20000: Train Loss = 0.442643, Test Loss = 0.241903, Learning Rate = 9.466415e-05\n",
      "Epoch 6205/20000: Train Loss = 0.443082, Test Loss = 0.244879, Learning Rate = 9.462818e-05\n",
      "Epoch 6206/20000: Train Loss = 0.442800, Test Loss = 0.241797, Learning Rate = 9.459223e-05\n",
      "Epoch 6207/20000: Train Loss = 0.442815, Test Loss = 0.248506, Learning Rate = 9.455628e-05\n",
      "Epoch 6208/20000: Train Loss = 0.443143, Test Loss = 0.247746, Learning Rate = 9.452035e-05\n",
      "Epoch 6209/20000: Train Loss = 0.443109, Test Loss = 0.246829, Learning Rate = 9.448444e-05\n",
      "Epoch 6210/20000: Train Loss = 0.443119, Test Loss = 0.243486, Learning Rate = 9.444854e-05\n",
      "Epoch 6211/20000: Train Loss = 0.442867, Test Loss = 0.242835, Learning Rate = 9.441265e-05\n",
      "Epoch 6212/20000: Train Loss = 0.443096, Test Loss = 0.251995, Learning Rate = 9.437678e-05\n",
      "Epoch 6213/20000: Train Loss = 0.442779, Test Loss = 0.248639, Learning Rate = 9.434091e-05\n",
      "Epoch 6214/20000: Train Loss = 0.442869, Test Loss = 0.240424, Learning Rate = 9.430507e-05\n",
      "Epoch 6215/20000: Train Loss = 0.443543, Test Loss = 0.243693, Learning Rate = 9.426923e-05\n",
      "Epoch 6216/20000: Train Loss = 0.443237, Test Loss = 0.243559, Learning Rate = 9.423341e-05\n",
      "Epoch 6217/20000: Train Loss = 0.442918, Test Loss = 0.243406, Learning Rate = 9.419761e-05\n",
      "Epoch 6218/20000: Train Loss = 0.444251, Test Loss = 0.246657, Learning Rate = 9.416182e-05\n",
      "Epoch 6219/20000: Train Loss = 0.443058, Test Loss = 0.241918, Learning Rate = 9.412604e-05\n",
      "Epoch 6220/20000: Train Loss = 0.443609, Test Loss = 0.242871, Learning Rate = 9.409027e-05\n",
      "Epoch 6221/20000: Train Loss = 0.443856, Test Loss = 0.247351, Learning Rate = 9.405452e-05\n",
      "Epoch 6222/20000: Train Loss = 0.442988, Test Loss = 0.241314, Learning Rate = 9.401878e-05\n",
      "Epoch 6223/20000: Train Loss = 0.442991, Test Loss = 0.245972, Learning Rate = 9.398306e-05\n",
      "Epoch 6224/20000: Train Loss = 0.443198, Test Loss = 0.245136, Learning Rate = 9.394735e-05\n",
      "Epoch 6225/20000: Train Loss = 0.444343, Test Loss = 0.252364, Learning Rate = 9.391165e-05\n",
      "Epoch 6226/20000: Train Loss = 0.442880, Test Loss = 0.245043, Learning Rate = 9.387596e-05\n",
      "Epoch 6227/20000: Train Loss = 0.443039, Test Loss = 0.241668, Learning Rate = 9.384029e-05\n",
      "Epoch 6228/20000: Train Loss = 0.442719, Test Loss = 0.243752, Learning Rate = 9.380464e-05\n",
      "Epoch 6229/20000: Train Loss = 0.443100, Test Loss = 0.244445, Learning Rate = 9.376899e-05\n",
      "Epoch 6230/20000: Train Loss = 0.443276, Test Loss = 0.242859, Learning Rate = 9.373336e-05\n",
      "Epoch 6231/20000: Train Loss = 0.443103, Test Loss = 0.249274, Learning Rate = 9.369775e-05\n",
      "Epoch 6232/20000: Train Loss = 0.443424, Test Loss = 0.251754, Learning Rate = 9.366215e-05\n",
      "Epoch 6233/20000: Train Loss = 0.442527, Test Loss = 0.243177, Learning Rate = 9.362656e-05\n",
      "Epoch 6234/20000: Train Loss = 0.442749, Test Loss = 0.245347, Learning Rate = 9.359098e-05\n",
      "Epoch 6235/20000: Train Loss = 0.442926, Test Loss = 0.243912, Learning Rate = 9.355542e-05\n",
      "Epoch 6236/20000: Train Loss = 0.443056, Test Loss = 0.245479, Learning Rate = 9.351987e-05\n",
      "Epoch 6237/20000: Train Loss = 0.443134, Test Loss = 0.246793, Learning Rate = 9.348434e-05\n",
      "Epoch 6238/20000: Train Loss = 0.442830, Test Loss = 0.246149, Learning Rate = 9.344881e-05\n",
      "Epoch 6239/20000: Train Loss = 0.442690, Test Loss = 0.246347, Learning Rate = 9.341331e-05\n",
      "Epoch 6240/20000: Train Loss = 0.442822, Test Loss = 0.246288, Learning Rate = 9.337781e-05\n",
      "Epoch 6241/20000: Train Loss = 0.442929, Test Loss = 0.246952, Learning Rate = 9.334233e-05\n",
      "Epoch 6242/20000: Train Loss = 0.443181, Test Loss = 0.248474, Learning Rate = 9.330686e-05\n",
      "Epoch 6243/20000: Train Loss = 0.442783, Test Loss = 0.249976, Learning Rate = 9.327141e-05\n",
      "Epoch 6244/20000: Train Loss = 0.442882, Test Loss = 0.249242, Learning Rate = 9.323597e-05\n",
      "Epoch 6245/20000: Train Loss = 0.442955, Test Loss = 0.249369, Learning Rate = 9.320054e-05\n",
      "Epoch 6246/20000: Train Loss = 0.443274, Test Loss = 0.246134, Learning Rate = 9.316513e-05\n",
      "Epoch 6247/20000: Train Loss = 0.443297, Test Loss = 0.247329, Learning Rate = 9.312973e-05\n",
      "Epoch 6248/20000: Train Loss = 0.442919, Test Loss = 0.247214, Learning Rate = 9.309434e-05\n",
      "Epoch 6249/20000: Train Loss = 0.442862, Test Loss = 0.247564, Learning Rate = 9.305897e-05\n",
      "Epoch 6250/20000: Train Loss = 0.443021, Test Loss = 0.252857, Learning Rate = 9.302361e-05\n",
      "Epoch 6251/20000: Train Loss = 0.442741, Test Loss = 0.246002, Learning Rate = 9.298826e-05\n",
      "Epoch 6252/20000: Train Loss = 0.442800, Test Loss = 0.244812, Learning Rate = 9.295293e-05\n",
      "Epoch 6253/20000: Train Loss = 0.443079, Test Loss = 0.247262, Learning Rate = 9.291761e-05\n",
      "Epoch 6254/20000: Train Loss = 0.442762, Test Loss = 0.243664, Learning Rate = 9.288230e-05\n",
      "Epoch 6255/20000: Train Loss = 0.442837, Test Loss = 0.239131, Learning Rate = 9.284701e-05\n",
      "Epoch 6256/20000: Train Loss = 0.443747, Test Loss = 0.240600, Learning Rate = 9.281173e-05\n",
      "Epoch 6257/20000: Train Loss = 0.443587, Test Loss = 0.247360, Learning Rate = 9.277646e-05\n",
      "Epoch 6258/20000: Train Loss = 0.443424, Test Loss = 0.244563, Learning Rate = 9.274121e-05\n",
      "Epoch 6259/20000: Train Loss = 0.443214, Test Loss = 0.245227, Learning Rate = 9.270597e-05\n",
      "Epoch 6260/20000: Train Loss = 0.442725, Test Loss = 0.248611, Learning Rate = 9.267075e-05\n",
      "Epoch 6261/20000: Train Loss = 0.442752, Test Loss = 0.247752, Learning Rate = 9.263553e-05\n",
      "Epoch 6262/20000: Train Loss = 0.443049, Test Loss = 0.248812, Learning Rate = 9.260034e-05\n",
      "Epoch 6263/20000: Train Loss = 0.442714, Test Loss = 0.248073, Learning Rate = 9.256515e-05\n",
      "Epoch 6264/20000: Train Loss = 0.442965, Test Loss = 0.246345, Learning Rate = 9.252998e-05\n",
      "Epoch 6265/20000: Train Loss = 0.442930, Test Loss = 0.243105, Learning Rate = 9.249482e-05\n",
      "Epoch 6266/20000: Train Loss = 0.443023, Test Loss = 0.241141, Learning Rate = 9.245967e-05\n",
      "Epoch 6267/20000: Train Loss = 0.442961, Test Loss = 0.249724, Learning Rate = 9.242454e-05\n",
      "Epoch 6268/20000: Train Loss = 0.442871, Test Loss = 0.250528, Learning Rate = 9.238942e-05\n",
      "Epoch 6269/20000: Train Loss = 0.442809, Test Loss = 0.245143, Learning Rate = 9.235432e-05\n",
      "Epoch 6270/20000: Train Loss = 0.442870, Test Loss = 0.248015, Learning Rate = 9.231922e-05\n",
      "Epoch 6271/20000: Train Loss = 0.443043, Test Loss = 0.246285, Learning Rate = 9.228415e-05\n",
      "Epoch 6272/20000: Train Loss = 0.442902, Test Loss = 0.249309, Learning Rate = 9.224908e-05\n",
      "Epoch 6273/20000: Train Loss = 0.442826, Test Loss = 0.252821, Learning Rate = 9.221403e-05\n",
      "Epoch 6274/20000: Train Loss = 0.443038, Test Loss = 0.253484, Learning Rate = 9.217899e-05\n",
      "Epoch 6275/20000: Train Loss = 0.442659, Test Loss = 0.248987, Learning Rate = 9.214396e-05\n",
      "Epoch 6276/20000: Train Loss = 0.443013, Test Loss = 0.248313, Learning Rate = 9.210895e-05\n",
      "Epoch 6277/20000: Train Loss = 0.442703, Test Loss = 0.252856, Learning Rate = 9.207395e-05\n",
      "Epoch 6278/20000: Train Loss = 0.443310, Test Loss = 0.245164, Learning Rate = 9.203897e-05\n",
      "Epoch 6279/20000: Train Loss = 0.442901, Test Loss = 0.243846, Learning Rate = 9.200399e-05\n",
      "Epoch 6280/20000: Train Loss = 0.442893, Test Loss = 0.242714, Learning Rate = 9.196904e-05\n",
      "Epoch 6281/20000: Train Loss = 0.443293, Test Loss = 0.251187, Learning Rate = 9.193409e-05\n",
      "Epoch 6282/20000: Train Loss = 0.442836, Test Loss = 0.247018, Learning Rate = 9.189916e-05\n",
      "Epoch 6283/20000: Train Loss = 0.442848, Test Loss = 0.250889, Learning Rate = 9.186424e-05\n",
      "Epoch 6284/20000: Train Loss = 0.443081, Test Loss = 0.245899, Learning Rate = 9.182933e-05\n",
      "Epoch 6285/20000: Train Loss = 0.442745, Test Loss = 0.249262, Learning Rate = 9.179444e-05\n",
      "Epoch 6286/20000: Train Loss = 0.443022, Test Loss = 0.247890, Learning Rate = 9.175956e-05\n",
      "Epoch 6287/20000: Train Loss = 0.442625, Test Loss = 0.242617, Learning Rate = 9.172469e-05\n",
      "Epoch 6288/20000: Train Loss = 0.443086, Test Loss = 0.250810, Learning Rate = 9.168984e-05\n",
      "Epoch 6289/20000: Train Loss = 0.442943, Test Loss = 0.241331, Learning Rate = 9.165500e-05\n",
      "Epoch 6290/20000: Train Loss = 0.442682, Test Loss = 0.246762, Learning Rate = 9.162018e-05\n",
      "Epoch 6291/20000: Train Loss = 0.442871, Test Loss = 0.248308, Learning Rate = 9.158536e-05\n",
      "Epoch 6292/20000: Train Loss = 0.442921, Test Loss = 0.244765, Learning Rate = 9.155056e-05\n",
      "Epoch 6293/20000: Train Loss = 0.442828, Test Loss = 0.248845, Learning Rate = 9.151578e-05\n",
      "Epoch 6294/20000: Train Loss = 0.442584, Test Loss = 0.249508, Learning Rate = 9.148100e-05\n",
      "Epoch 6295/20000: Train Loss = 0.442656, Test Loss = 0.248707, Learning Rate = 9.144624e-05\n",
      "Epoch 6296/20000: Train Loss = 0.442986, Test Loss = 0.247054, Learning Rate = 9.141149e-05\n",
      "Epoch 6297/20000: Train Loss = 0.442681, Test Loss = 0.248156, Learning Rate = 9.137676e-05\n",
      "Epoch 6298/20000: Train Loss = 0.442748, Test Loss = 0.246381, Learning Rate = 9.134204e-05\n",
      "Epoch 6299/20000: Train Loss = 0.442878, Test Loss = 0.243065, Learning Rate = 9.130733e-05\n",
      "Epoch 6300/20000: Train Loss = 0.443338, Test Loss = 0.245981, Learning Rate = 9.127264e-05\n",
      "Epoch 6301/20000: Train Loss = 0.443109, Test Loss = 0.248435, Learning Rate = 9.123796e-05\n",
      "Epoch 6302/20000: Train Loss = 0.442835, Test Loss = 0.247631, Learning Rate = 9.120329e-05\n",
      "Epoch 6303/20000: Train Loss = 0.442716, Test Loss = 0.246387, Learning Rate = 9.116863e-05\n",
      "Epoch 6304/20000: Train Loss = 0.442745, Test Loss = 0.247472, Learning Rate = 9.113399e-05\n",
      "Epoch 6305/20000: Train Loss = 0.443245, Test Loss = 0.245151, Learning Rate = 9.109936e-05\n",
      "Epoch 6306/20000: Train Loss = 0.442952, Test Loss = 0.246977, Learning Rate = 9.106475e-05\n",
      "Epoch 6307/20000: Train Loss = 0.443315, Test Loss = 0.245881, Learning Rate = 9.103015e-05\n",
      "Epoch 6308/20000: Train Loss = 0.442718, Test Loss = 0.247956, Learning Rate = 9.099556e-05\n",
      "Epoch 6309/20000: Train Loss = 0.443038, Test Loss = 0.257454, Learning Rate = 9.096098e-05\n",
      "Epoch 6310/20000: Train Loss = 0.442837, Test Loss = 0.248193, Learning Rate = 9.092642e-05\n",
      "Epoch 6311/20000: Train Loss = 0.442850, Test Loss = 0.250569, Learning Rate = 9.089187e-05\n",
      "Epoch 6312/20000: Train Loss = 0.442960, Test Loss = 0.251079, Learning Rate = 9.085733e-05\n",
      "Epoch 6313/20000: Train Loss = 0.442870, Test Loss = 0.249148, Learning Rate = 9.082281e-05\n",
      "Epoch 6314/20000: Train Loss = 0.442739, Test Loss = 0.247616, Learning Rate = 9.078830e-05\n",
      "Epoch 6315/20000: Train Loss = 0.442846, Test Loss = 0.247229, Learning Rate = 9.075380e-05\n",
      "Epoch 6316/20000: Train Loss = 0.443041, Test Loss = 0.246320, Learning Rate = 9.071932e-05\n",
      "Epoch 6317/20000: Train Loss = 0.442760, Test Loss = 0.250578, Learning Rate = 9.068485e-05\n",
      "Epoch 6318/20000: Train Loss = 0.442814, Test Loss = 0.247233, Learning Rate = 9.065039e-05\n",
      "Epoch 6319/20000: Train Loss = 0.442792, Test Loss = 0.247325, Learning Rate = 9.061595e-05\n",
      "Epoch 6320/20000: Train Loss = 0.442828, Test Loss = 0.245290, Learning Rate = 9.058151e-05\n",
      "Epoch 6321/20000: Train Loss = 0.443212, Test Loss = 0.247023, Learning Rate = 9.054709e-05\n",
      "Epoch 6322/20000: Train Loss = 0.443095, Test Loss = 0.247621, Learning Rate = 9.051269e-05\n",
      "Epoch 6323/20000: Train Loss = 0.443027, Test Loss = 0.243707, Learning Rate = 9.047830e-05\n",
      "Epoch 6324/20000: Train Loss = 0.442949, Test Loss = 0.248187, Learning Rate = 9.044392e-05\n",
      "Epoch 6325/20000: Train Loss = 0.442614, Test Loss = 0.251602, Learning Rate = 9.040955e-05\n",
      "Epoch 6326/20000: Train Loss = 0.443017, Test Loss = 0.250398, Learning Rate = 9.037520e-05\n",
      "Epoch 6327/20000: Train Loss = 0.443045, Test Loss = 0.248140, Learning Rate = 9.034086e-05\n",
      "Epoch 6328/20000: Train Loss = 0.442772, Test Loss = 0.244774, Learning Rate = 9.030653e-05\n",
      "Epoch 6329/20000: Train Loss = 0.442806, Test Loss = 0.243775, Learning Rate = 9.027222e-05\n",
      "Epoch 6330/20000: Train Loss = 0.442867, Test Loss = 0.245550, Learning Rate = 9.023792e-05\n",
      "Epoch 6331/20000: Train Loss = 0.443133, Test Loss = 0.244418, Learning Rate = 9.020363e-05\n",
      "Epoch 6332/20000: Train Loss = 0.442716, Test Loss = 0.247894, Learning Rate = 9.016935e-05\n",
      "Epoch 6333/20000: Train Loss = 0.443055, Test Loss = 0.248000, Learning Rate = 9.013509e-05\n",
      "Epoch 6334/20000: Train Loss = 0.443953, Test Loss = 0.254892, Learning Rate = 9.010084e-05\n",
      "Epoch 6335/20000: Train Loss = 0.443308, Test Loss = 0.243950, Learning Rate = 9.006661e-05\n",
      "Epoch 6336/20000: Train Loss = 0.442877, Test Loss = 0.246151, Learning Rate = 9.003238e-05\n",
      "Epoch 6337/20000: Train Loss = 0.443089, Test Loss = 0.247329, Learning Rate = 8.999817e-05\n",
      "Epoch 6338/20000: Train Loss = 0.442901, Test Loss = 0.247069, Learning Rate = 8.996398e-05\n",
      "Epoch 6339/20000: Train Loss = 0.442843, Test Loss = 0.243646, Learning Rate = 8.992979e-05\n",
      "Epoch 6340/20000: Train Loss = 0.442846, Test Loss = 0.246208, Learning Rate = 8.989562e-05\n",
      "Epoch 6341/20000: Train Loss = 0.442920, Test Loss = 0.240980, Learning Rate = 8.986146e-05\n",
      "Epoch 6342/20000: Train Loss = 0.443259, Test Loss = 0.244662, Learning Rate = 8.982732e-05\n",
      "Epoch 6343/20000: Train Loss = 0.442872, Test Loss = 0.250558, Learning Rate = 8.979319e-05\n",
      "Epoch 6344/20000: Train Loss = 0.442844, Test Loss = 0.245022, Learning Rate = 8.975907e-05\n",
      "Epoch 6345/20000: Train Loss = 0.443376, Test Loss = 0.247164, Learning Rate = 8.972496e-05\n",
      "Epoch 6346/20000: Train Loss = 0.442730, Test Loss = 0.242216, Learning Rate = 8.969087e-05\n",
      "Epoch 6347/20000: Train Loss = 0.443025, Test Loss = 0.245650, Learning Rate = 8.965679e-05\n",
      "Epoch 6348/20000: Train Loss = 0.442704, Test Loss = 0.247985, Learning Rate = 8.962272e-05\n",
      "Epoch 6349/20000: Train Loss = 0.442794, Test Loss = 0.245619, Learning Rate = 8.958867e-05\n",
      "Epoch 6350/20000: Train Loss = 0.442918, Test Loss = 0.246592, Learning Rate = 8.955463e-05\n",
      "Epoch 6351/20000: Train Loss = 0.443083, Test Loss = 0.245376, Learning Rate = 8.952060e-05\n",
      "Epoch 6352/20000: Train Loss = 0.442877, Test Loss = 0.244541, Learning Rate = 8.948658e-05\n",
      "Epoch 6353/20000: Train Loss = 0.443115, Test Loss = 0.248570, Learning Rate = 8.945258e-05\n",
      "Epoch 6354/20000: Train Loss = 0.442698, Test Loss = 0.245953, Learning Rate = 8.941859e-05\n",
      "Epoch 6355/20000: Train Loss = 0.442834, Test Loss = 0.250559, Learning Rate = 8.938461e-05\n",
      "Epoch 6356/20000: Train Loss = 0.443078, Test Loss = 0.246913, Learning Rate = 8.935065e-05\n",
      "Epoch 6357/20000: Train Loss = 0.442916, Test Loss = 0.248871, Learning Rate = 8.931670e-05\n",
      "Epoch 6358/20000: Train Loss = 0.442884, Test Loss = 0.249283, Learning Rate = 8.928276e-05\n",
      "Epoch 6359/20000: Train Loss = 0.443182, Test Loss = 0.247303, Learning Rate = 8.924884e-05\n",
      "Epoch 6360/20000: Train Loss = 0.442694, Test Loss = 0.249032, Learning Rate = 8.921492e-05\n",
      "Epoch 6361/20000: Train Loss = 0.442606, Test Loss = 0.248228, Learning Rate = 8.918103e-05\n",
      "Epoch 6362/20000: Train Loss = 0.442833, Test Loss = 0.242938, Learning Rate = 8.914714e-05\n",
      "Epoch 6363/20000: Train Loss = 0.442694, Test Loss = 0.245050, Learning Rate = 8.911327e-05\n",
      "Epoch 6364/20000: Train Loss = 0.442917, Test Loss = 0.243560, Learning Rate = 8.907940e-05\n",
      "Epoch 6365/20000: Train Loss = 0.442679, Test Loss = 0.248780, Learning Rate = 8.904556e-05\n",
      "Epoch 6366/20000: Train Loss = 0.442751, Test Loss = 0.248217, Learning Rate = 8.901172e-05\n",
      "Epoch 6367/20000: Train Loss = 0.443143, Test Loss = 0.242506, Learning Rate = 8.897790e-05\n",
      "Epoch 6368/20000: Train Loss = 0.442442, Test Loss = 0.249675, Learning Rate = 8.894409e-05\n",
      "Epoch 6369/20000: Train Loss = 0.442810, Test Loss = 0.251856, Learning Rate = 8.891029e-05\n",
      "Epoch 6370/20000: Train Loss = 0.442862, Test Loss = 0.250029, Learning Rate = 8.887651e-05\n",
      "Epoch 6371/20000: Train Loss = 0.442843, Test Loss = 0.249465, Learning Rate = 8.884274e-05\n",
      "Epoch 6372/20000: Train Loss = 0.442855, Test Loss = 0.246178, Learning Rate = 8.880898e-05\n",
      "Epoch 6373/20000: Train Loss = 0.442638, Test Loss = 0.250288, Learning Rate = 8.877524e-05\n",
      "Epoch 6374/20000: Train Loss = 0.442876, Test Loss = 0.252155, Learning Rate = 8.874151e-05\n",
      "Epoch 6375/20000: Train Loss = 0.442660, Test Loss = 0.250025, Learning Rate = 8.870779e-05\n",
      "Epoch 6376/20000: Train Loss = 0.442740, Test Loss = 0.245489, Learning Rate = 8.867408e-05\n",
      "Epoch 6377/20000: Train Loss = 0.442985, Test Loss = 0.247391, Learning Rate = 8.864039e-05\n",
      "Epoch 6378/20000: Train Loss = 0.442873, Test Loss = 0.242208, Learning Rate = 8.860670e-05\n",
      "Epoch 6379/20000: Train Loss = 0.442958, Test Loss = 0.244900, Learning Rate = 8.857304e-05\n",
      "Epoch 6380/20000: Train Loss = 0.442644, Test Loss = 0.243221, Learning Rate = 8.853938e-05\n",
      "Epoch 6381/20000: Train Loss = 0.443281, Test Loss = 0.239533, Learning Rate = 8.850574e-05\n",
      "Epoch 6382/20000: Train Loss = 0.443148, Test Loss = 0.244907, Learning Rate = 8.847211e-05\n",
      "Epoch 6383/20000: Train Loss = 0.442861, Test Loss = 0.245234, Learning Rate = 8.843849e-05\n",
      "Epoch 6384/20000: Train Loss = 0.442978, Test Loss = 0.243897, Learning Rate = 8.840489e-05\n",
      "Epoch 6385/20000: Train Loss = 0.442972, Test Loss = 0.248925, Learning Rate = 8.837130e-05\n",
      "Epoch 6386/20000: Train Loss = 0.442961, Test Loss = 0.248820, Learning Rate = 8.833772e-05\n",
      "Epoch 6387/20000: Train Loss = 0.443460, Test Loss = 0.248956, Learning Rate = 8.830415e-05\n",
      "Epoch 6388/20000: Train Loss = 0.442601, Test Loss = 0.250249, Learning Rate = 8.827060e-05\n",
      "Epoch 6389/20000: Train Loss = 0.442837, Test Loss = 0.247487, Learning Rate = 8.823706e-05\n",
      "Epoch 6390/20000: Train Loss = 0.443171, Test Loss = 0.246967, Learning Rate = 8.820353e-05\n",
      "Epoch 6391/20000: Train Loss = 0.442695, Test Loss = 0.244050, Learning Rate = 8.817002e-05\n",
      "Epoch 6392/20000: Train Loss = 0.443016, Test Loss = 0.252007, Learning Rate = 8.813651e-05\n",
      "Epoch 6393/20000: Train Loss = 0.442851, Test Loss = 0.247577, Learning Rate = 8.810302e-05\n",
      "Epoch 6394/20000: Train Loss = 0.442947, Test Loss = 0.245525, Learning Rate = 8.806955e-05\n",
      "Epoch 6395/20000: Train Loss = 0.442729, Test Loss = 0.246714, Learning Rate = 8.803608e-05\n",
      "Epoch 6396/20000: Train Loss = 0.443293, Test Loss = 0.243752, Learning Rate = 8.800263e-05\n",
      "Epoch 6397/20000: Train Loss = 0.442533, Test Loss = 0.250117, Learning Rate = 8.796919e-05\n",
      "Epoch 6398/20000: Train Loss = 0.442869, Test Loss = 0.251785, Learning Rate = 8.793577e-05\n",
      "Epoch 6399/20000: Train Loss = 0.442648, Test Loss = 0.244855, Learning Rate = 8.790235e-05\n",
      "Epoch 6400/20000: Train Loss = 0.442735, Test Loss = 0.248744, Learning Rate = 8.786895e-05\n",
      "Epoch 6401/20000: Train Loss = 0.442908, Test Loss = 0.251326, Learning Rate = 8.783557e-05\n",
      "Epoch 6402/20000: Train Loss = 0.443291, Test Loss = 0.244903, Learning Rate = 8.780219e-05\n",
      "Epoch 6403/20000: Train Loss = 0.442778, Test Loss = 0.244255, Learning Rate = 8.776883e-05\n",
      "Epoch 6404/20000: Train Loss = 0.442908, Test Loss = 0.245882, Learning Rate = 8.773548e-05\n",
      "Epoch 6405/20000: Train Loss = 0.442694, Test Loss = 0.247504, Learning Rate = 8.770214e-05\n",
      "Epoch 6406/20000: Train Loss = 0.443477, Test Loss = 0.251642, Learning Rate = 8.766882e-05\n",
      "Epoch 6407/20000: Train Loss = 0.442887, Test Loss = 0.251252, Learning Rate = 8.763550e-05\n",
      "Epoch 6408/20000: Train Loss = 0.442816, Test Loss = 0.244302, Learning Rate = 8.760221e-05\n",
      "Epoch 6409/20000: Train Loss = 0.443169, Test Loss = 0.243309, Learning Rate = 8.756892e-05\n",
      "Epoch 6410/20000: Train Loss = 0.442724, Test Loss = 0.246160, Learning Rate = 8.753565e-05\n",
      "Epoch 6411/20000: Train Loss = 0.442851, Test Loss = 0.246717, Learning Rate = 8.750238e-05\n",
      "Epoch 6412/20000: Train Loss = 0.442959, Test Loss = 0.244571, Learning Rate = 8.746914e-05\n",
      "Epoch 6413/20000: Train Loss = 0.442842, Test Loss = 0.248191, Learning Rate = 8.743590e-05\n",
      "Epoch 6414/20000: Train Loss = 0.442920, Test Loss = 0.246992, Learning Rate = 8.740268e-05\n",
      "Epoch 6415/20000: Train Loss = 0.442753, Test Loss = 0.247739, Learning Rate = 8.736947e-05\n",
      "Epoch 6416/20000: Train Loss = 0.442776, Test Loss = 0.246294, Learning Rate = 8.733627e-05\n",
      "Epoch 6417/20000: Train Loss = 0.442906, Test Loss = 0.249518, Learning Rate = 8.730308e-05\n",
      "Epoch 6418/20000: Train Loss = 0.442872, Test Loss = 0.246908, Learning Rate = 8.726991e-05\n",
      "Epoch 6419/20000: Train Loss = 0.443383, Test Loss = 0.245419, Learning Rate = 8.723675e-05\n",
      "Epoch 6420/20000: Train Loss = 0.443012, Test Loss = 0.247154, Learning Rate = 8.720360e-05\n",
      "Epoch 6421/20000: Train Loss = 0.442964, Test Loss = 0.248932, Learning Rate = 8.717047e-05\n",
      "Epoch 6422/20000: Train Loss = 0.443383, Test Loss = 0.248331, Learning Rate = 8.713734e-05\n",
      "Epoch 6423/20000: Train Loss = 0.443130, Test Loss = 0.243472, Learning Rate = 8.710423e-05\n",
      "Epoch 6424/20000: Train Loss = 0.442649, Test Loss = 0.242334, Learning Rate = 8.707114e-05\n",
      "Epoch 6425/20000: Train Loss = 0.443404, Test Loss = 0.248689, Learning Rate = 8.703805e-05\n",
      "Epoch 6426/20000: Train Loss = 0.442811, Test Loss = 0.247673, Learning Rate = 8.700498e-05\n",
      "Epoch 6427/20000: Train Loss = 0.442659, Test Loss = 0.242517, Learning Rate = 8.697192e-05\n",
      "Epoch 6428/20000: Train Loss = 0.442916, Test Loss = 0.242287, Learning Rate = 8.693887e-05\n",
      "Epoch 6429/20000: Train Loss = 0.443027, Test Loss = 0.245678, Learning Rate = 8.690584e-05\n",
      "Epoch 6430/20000: Train Loss = 0.442535, Test Loss = 0.243715, Learning Rate = 8.687282e-05\n",
      "Epoch 6431/20000: Train Loss = 0.443157, Test Loss = 0.243309, Learning Rate = 8.683981e-05\n",
      "Epoch 6432/20000: Train Loss = 0.443038, Test Loss = 0.248947, Learning Rate = 8.680681e-05\n",
      "Epoch 6433/20000: Train Loss = 0.443110, Test Loss = 0.240925, Learning Rate = 8.677383e-05\n",
      "Epoch 6434/20000: Train Loss = 0.442819, Test Loss = 0.244477, Learning Rate = 8.674086e-05\n",
      "Epoch 6435/20000: Train Loss = 0.442723, Test Loss = 0.244709, Learning Rate = 8.670790e-05\n",
      "Epoch 6436/20000: Train Loss = 0.443193, Test Loss = 0.244418, Learning Rate = 8.667495e-05\n",
      "Epoch 6437/20000: Train Loss = 0.443201, Test Loss = 0.242854, Learning Rate = 8.664202e-05\n",
      "Epoch 6438/20000: Train Loss = 0.442819, Test Loss = 0.245887, Learning Rate = 8.660909e-05\n",
      "Epoch 6439/20000: Train Loss = 0.442901, Test Loss = 0.247289, Learning Rate = 8.657618e-05\n",
      "Epoch 6440/20000: Train Loss = 0.442950, Test Loss = 0.244428, Learning Rate = 8.654329e-05\n",
      "Epoch 6441/20000: Train Loss = 0.443001, Test Loss = 0.247560, Learning Rate = 8.651040e-05\n",
      "Epoch 6442/20000: Train Loss = 0.442864, Test Loss = 0.247737, Learning Rate = 8.647753e-05\n",
      "Epoch 6443/20000: Train Loss = 0.442573, Test Loss = 0.245861, Learning Rate = 8.644467e-05\n",
      "Epoch 6444/20000: Train Loss = 0.442820, Test Loss = 0.241778, Learning Rate = 8.641183e-05\n",
      "Epoch 6445/20000: Train Loss = 0.442957, Test Loss = 0.247053, Learning Rate = 8.637899e-05\n",
      "Epoch 6446/20000: Train Loss = 0.442988, Test Loss = 0.250987, Learning Rate = 8.634617e-05\n",
      "Epoch 6447/20000: Train Loss = 0.442973, Test Loss = 0.248377, Learning Rate = 8.631336e-05\n",
      "Epoch 6448/20000: Train Loss = 0.443049, Test Loss = 0.248539, Learning Rate = 8.628057e-05\n",
      "Epoch 6449/20000: Train Loss = 0.442904, Test Loss = 0.244902, Learning Rate = 8.624778e-05\n",
      "Epoch 6450/20000: Train Loss = 0.442939, Test Loss = 0.252671, Learning Rate = 8.621501e-05\n",
      "Epoch 6451/20000: Train Loss = 0.443150, Test Loss = 0.252754, Learning Rate = 8.618225e-05\n",
      "Epoch 6452/20000: Train Loss = 0.442827, Test Loss = 0.248542, Learning Rate = 8.614950e-05\n",
      "Epoch 6453/20000: Train Loss = 0.442675, Test Loss = 0.248213, Learning Rate = 8.611677e-05\n",
      "Epoch 6454/20000: Train Loss = 0.442918, Test Loss = 0.248478, Learning Rate = 8.608405e-05\n",
      "Epoch 6455/20000: Train Loss = 0.443447, Test Loss = 0.253763, Learning Rate = 8.605134e-05\n",
      "Epoch 6456/20000: Train Loss = 0.442813, Test Loss = 0.247166, Learning Rate = 8.601864e-05\n",
      "Epoch 6457/20000: Train Loss = 0.442708, Test Loss = 0.248067, Learning Rate = 8.598595e-05\n",
      "Epoch 6458/20000: Train Loss = 0.442691, Test Loss = 0.247867, Learning Rate = 8.595328e-05\n",
      "Epoch 6459/20000: Train Loss = 0.442750, Test Loss = 0.247274, Learning Rate = 8.592062e-05\n",
      "Epoch 6460/20000: Train Loss = 0.443157, Test Loss = 0.250134, Learning Rate = 8.588797e-05\n",
      "Epoch 6461/20000: Train Loss = 0.442924, Test Loss = 0.247618, Learning Rate = 8.585534e-05\n",
      "Epoch 6462/20000: Train Loss = 0.442870, Test Loss = 0.244588, Learning Rate = 8.582272e-05\n",
      "Epoch 6463/20000: Train Loss = 0.442784, Test Loss = 0.246236, Learning Rate = 8.579011e-05\n",
      "Epoch 6464/20000: Train Loss = 0.442821, Test Loss = 0.244885, Learning Rate = 8.575751e-05\n",
      "Epoch 6465/20000: Train Loss = 0.442800, Test Loss = 0.245455, Learning Rate = 8.572492e-05\n",
      "Epoch 6466/20000: Train Loss = 0.442623, Test Loss = 0.247744, Learning Rate = 8.569235e-05\n",
      "Epoch 6467/20000: Train Loss = 0.442771, Test Loss = 0.250027, Learning Rate = 8.565979e-05\n",
      "Epoch 6468/20000: Train Loss = 0.442766, Test Loss = 0.251831, Learning Rate = 8.562724e-05\n",
      "Epoch 6469/20000: Train Loss = 0.442739, Test Loss = 0.245678, Learning Rate = 8.559470e-05\n",
      "Epoch 6470/20000: Train Loss = 0.442708, Test Loss = 0.248327, Learning Rate = 8.556218e-05\n",
      "Epoch 6471/20000: Train Loss = 0.442790, Test Loss = 0.249642, Learning Rate = 8.552967e-05\n",
      "Epoch 6472/20000: Train Loss = 0.442903, Test Loss = 0.247048, Learning Rate = 8.549717e-05\n",
      "Epoch 6473/20000: Train Loss = 0.443018, Test Loss = 0.253980, Learning Rate = 8.546468e-05\n",
      "Epoch 6474/20000: Train Loss = 0.442736, Test Loss = 0.250887, Learning Rate = 8.543221e-05\n",
      "Epoch 6475/20000: Train Loss = 0.442758, Test Loss = 0.252570, Learning Rate = 8.539975e-05\n",
      "Epoch 6476/20000: Train Loss = 0.443073, Test Loss = 0.243724, Learning Rate = 8.536730e-05\n",
      "Epoch 6477/20000: Train Loss = 0.442696, Test Loss = 0.251909, Learning Rate = 8.533486e-05\n",
      "Epoch 6478/20000: Train Loss = 0.443108, Test Loss = 0.252218, Learning Rate = 8.530244e-05\n",
      "Epoch 6479/20000: Train Loss = 0.443035, Test Loss = 0.249135, Learning Rate = 8.527002e-05\n",
      "Epoch 6480/20000: Train Loss = 0.442868, Test Loss = 0.249668, Learning Rate = 8.523762e-05\n",
      "Epoch 6481/20000: Train Loss = 0.442830, Test Loss = 0.245219, Learning Rate = 8.520524e-05\n",
      "Epoch 6482/20000: Train Loss = 0.442565, Test Loss = 0.241343, Learning Rate = 8.517286e-05\n",
      "Epoch 6483/20000: Train Loss = 0.442880, Test Loss = 0.243805, Learning Rate = 8.514050e-05\n",
      "Epoch 6484/20000: Train Loss = 0.443175, Test Loss = 0.244699, Learning Rate = 8.510815e-05\n",
      "Epoch 6485/20000: Train Loss = 0.442631, Test Loss = 0.247980, Learning Rate = 8.507581e-05\n",
      "Epoch 6486/20000: Train Loss = 0.443049, Test Loss = 0.244714, Learning Rate = 8.504348e-05\n",
      "Epoch 6487/20000: Train Loss = 0.443082, Test Loss = 0.241969, Learning Rate = 8.501117e-05\n",
      "Epoch 6488/20000: Train Loss = 0.442658, Test Loss = 0.247133, Learning Rate = 8.497886e-05\n",
      "Epoch 6489/20000: Train Loss = 0.442702, Test Loss = 0.247866, Learning Rate = 8.494657e-05\n",
      "Epoch 6490/20000: Train Loss = 0.443523, Test Loss = 0.241925, Learning Rate = 8.491430e-05\n",
      "Epoch 6491/20000: Train Loss = 0.443436, Test Loss = 0.251241, Learning Rate = 8.488203e-05\n",
      "Epoch 6492/20000: Train Loss = 0.442727, Test Loss = 0.246029, Learning Rate = 8.484978e-05\n",
      "Epoch 6493/20000: Train Loss = 0.442702, Test Loss = 0.246829, Learning Rate = 8.481754e-05\n",
      "Epoch 6494/20000: Train Loss = 0.442589, Test Loss = 0.247403, Learning Rate = 8.478531e-05\n",
      "Epoch 6495/20000: Train Loss = 0.442965, Test Loss = 0.252396, Learning Rate = 8.475309e-05\n",
      "Epoch 6496/20000: Train Loss = 0.442890, Test Loss = 0.245208, Learning Rate = 8.472089e-05\n",
      "Epoch 6497/20000: Train Loss = 0.442966, Test Loss = 0.250269, Learning Rate = 8.468870e-05\n",
      "Epoch 6498/20000: Train Loss = 0.443041, Test Loss = 0.251430, Learning Rate = 8.465652e-05\n",
      "Epoch 6499/20000: Train Loss = 0.443113, Test Loss = 0.252351, Learning Rate = 8.462435e-05\n",
      "Epoch 6500/20000: Train Loss = 0.442858, Test Loss = 0.246634, Learning Rate = 8.459220e-05\n",
      "Epoch 6501/20000: Train Loss = 0.442860, Test Loss = 0.246076, Learning Rate = 8.456005e-05\n",
      "Epoch 6502/20000: Train Loss = 0.442577, Test Loss = 0.247981, Learning Rate = 8.452792e-05\n",
      "Epoch 6503/20000: Train Loss = 0.443036, Test Loss = 0.248657, Learning Rate = 8.449581e-05\n",
      "Epoch 6504/20000: Train Loss = 0.443691, Test Loss = 0.253674, Learning Rate = 8.446370e-05\n",
      "Epoch 6505/20000: Train Loss = 0.442927, Test Loss = 0.246681, Learning Rate = 8.443161e-05\n",
      "Epoch 6506/20000: Train Loss = 0.443029, Test Loss = 0.246301, Learning Rate = 8.439952e-05\n",
      "Epoch 6507/20000: Train Loss = 0.442790, Test Loss = 0.242875, Learning Rate = 8.436745e-05\n",
      "Epoch 6508/20000: Train Loss = 0.442629, Test Loss = 0.244753, Learning Rate = 8.433540e-05\n",
      "Epoch 6509/20000: Train Loss = 0.442845, Test Loss = 0.247895, Learning Rate = 8.430335e-05\n",
      "Epoch 6510/20000: Train Loss = 0.442796, Test Loss = 0.245861, Learning Rate = 8.427132e-05\n",
      "Epoch 6511/20000: Train Loss = 0.442785, Test Loss = 0.248514, Learning Rate = 8.423930e-05\n",
      "Epoch 6512/20000: Train Loss = 0.442746, Test Loss = 0.248759, Learning Rate = 8.420729e-05\n",
      "Epoch 6513/20000: Train Loss = 0.442836, Test Loss = 0.249251, Learning Rate = 8.417529e-05\n",
      "Epoch 6514/20000: Train Loss = 0.442650, Test Loss = 0.246559, Learning Rate = 8.414331e-05\n",
      "Epoch 6515/20000: Train Loss = 0.442667, Test Loss = 0.248719, Learning Rate = 8.411134e-05\n",
      "Epoch 6516/20000: Train Loss = 0.442676, Test Loss = 0.250698, Learning Rate = 8.407938e-05\n",
      "Epoch 6517/20000: Train Loss = 0.442772, Test Loss = 0.252001, Learning Rate = 8.404743e-05\n",
      "Epoch 6518/20000: Train Loss = 0.442934, Test Loss = 0.246146, Learning Rate = 8.401549e-05\n",
      "Epoch 6519/20000: Train Loss = 0.443178, Test Loss = 0.250058, Learning Rate = 8.398357e-05\n",
      "Epoch 6520/20000: Train Loss = 0.443123, Test Loss = 0.247387, Learning Rate = 8.395166e-05\n",
      "Epoch 6521/20000: Train Loss = 0.442934, Test Loss = 0.251430, Learning Rate = 8.391976e-05\n",
      "Epoch 6522/20000: Train Loss = 0.443180, Test Loss = 0.247945, Learning Rate = 8.388787e-05\n",
      "Epoch 6523/20000: Train Loss = 0.442867, Test Loss = 0.256317, Learning Rate = 8.385600e-05\n",
      "Epoch 6524/20000: Train Loss = 0.442848, Test Loss = 0.250505, Learning Rate = 8.382413e-05\n",
      "Epoch 6525/20000: Train Loss = 0.442845, Test Loss = 0.248783, Learning Rate = 8.379228e-05\n",
      "Epoch 6526/20000: Train Loss = 0.442735, Test Loss = 0.250848, Learning Rate = 8.376044e-05\n",
      "Epoch 6527/20000: Train Loss = 0.443347, Test Loss = 0.246846, Learning Rate = 8.372862e-05\n",
      "Epoch 6528/20000: Train Loss = 0.442778, Test Loss = 0.249852, Learning Rate = 8.369680e-05\n",
      "Epoch 6529/20000: Train Loss = 0.442583, Test Loss = 0.247864, Learning Rate = 8.366500e-05\n",
      "Epoch 6530/20000: Train Loss = 0.443017, Test Loss = 0.249423, Learning Rate = 8.363321e-05\n",
      "Epoch 6531/20000: Train Loss = 0.442898, Test Loss = 0.242703, Learning Rate = 8.360143e-05\n",
      "Epoch 6532/20000: Train Loss = 0.442856, Test Loss = 0.243691, Learning Rate = 8.356966e-05\n",
      "Epoch 6533/20000: Train Loss = 0.443389, Test Loss = 0.247249, Learning Rate = 8.353791e-05\n",
      "Epoch 6534/20000: Train Loss = 0.442837, Test Loss = 0.245381, Learning Rate = 8.350617e-05\n",
      "Epoch 6535/20000: Train Loss = 0.442925, Test Loss = 0.244572, Learning Rate = 8.347444e-05\n",
      "Epoch 6536/20000: Train Loss = 0.442834, Test Loss = 0.249730, Learning Rate = 8.344272e-05\n",
      "Epoch 6537/20000: Train Loss = 0.442897, Test Loss = 0.249852, Learning Rate = 8.341101e-05\n",
      "Epoch 6538/20000: Train Loss = 0.442843, Test Loss = 0.245872, Learning Rate = 8.337932e-05\n",
      "Epoch 6539/20000: Train Loss = 0.443061, Test Loss = 0.249312, Learning Rate = 8.334764e-05\n",
      "Epoch 6540/20000: Train Loss = 0.443135, Test Loss = 0.246024, Learning Rate = 8.331597e-05\n",
      "Epoch 6541/20000: Train Loss = 0.443204, Test Loss = 0.244296, Learning Rate = 8.328431e-05\n",
      "Epoch 6542/20000: Train Loss = 0.442754, Test Loss = 0.248163, Learning Rate = 8.325266e-05\n",
      "Epoch 6543/20000: Train Loss = 0.442833, Test Loss = 0.249062, Learning Rate = 8.322103e-05\n",
      "Epoch 6544/20000: Train Loss = 0.442846, Test Loss = 0.248052, Learning Rate = 8.318941e-05\n",
      "Epoch 6545/20000: Train Loss = 0.442857, Test Loss = 0.248478, Learning Rate = 8.315780e-05\n",
      "Epoch 6546/20000: Train Loss = 0.442846, Test Loss = 0.249057, Learning Rate = 8.312620e-05\n",
      "Epoch 6547/20000: Train Loss = 0.442828, Test Loss = 0.244080, Learning Rate = 8.309462e-05\n",
      "Epoch 6548/20000: Train Loss = 0.443025, Test Loss = 0.247253, Learning Rate = 8.306304e-05\n",
      "Epoch 6549/20000: Train Loss = 0.442859, Test Loss = 0.247309, Learning Rate = 8.303148e-05\n",
      "Epoch 6550/20000: Train Loss = 0.443147, Test Loss = 0.244430, Learning Rate = 8.299993e-05\n",
      "Epoch 6551/20000: Train Loss = 0.442782, Test Loss = 0.252842, Learning Rate = 8.296839e-05\n",
      "Epoch 6552/20000: Train Loss = 0.443291, Test Loss = 0.248355, Learning Rate = 8.293687e-05\n",
      "Epoch 6553/20000: Train Loss = 0.443193, Test Loss = 0.246598, Learning Rate = 8.290535e-05\n",
      "Epoch 6554/20000: Train Loss = 0.442636, Test Loss = 0.244712, Learning Rate = 8.287385e-05\n",
      "Epoch 6555/20000: Train Loss = 0.442799, Test Loss = 0.247926, Learning Rate = 8.284236e-05\n",
      "Epoch 6556/20000: Train Loss = 0.442661, Test Loss = 0.244184, Learning Rate = 8.281088e-05\n",
      "Epoch 6557/20000: Train Loss = 0.442956, Test Loss = 0.244388, Learning Rate = 8.277942e-05\n",
      "Epoch 6558/20000: Train Loss = 0.442997, Test Loss = 0.249744, Learning Rate = 8.274796e-05\n",
      "Epoch 6559/20000: Train Loss = 0.442873, Test Loss = 0.246833, Learning Rate = 8.271652e-05\n",
      "Epoch 6560/20000: Train Loss = 0.442744, Test Loss = 0.248820, Learning Rate = 8.268509e-05\n",
      "Epoch 6561/20000: Train Loss = 0.443736, Test Loss = 0.250405, Learning Rate = 8.265367e-05\n",
      "Epoch 6562/20000: Train Loss = 0.442648, Test Loss = 0.248630, Learning Rate = 8.262227e-05\n",
      "Epoch 6563/20000: Train Loss = 0.442765, Test Loss = 0.249397, Learning Rate = 8.259087e-05\n",
      "Epoch 6564/20000: Train Loss = 0.442806, Test Loss = 0.244129, Learning Rate = 8.255949e-05\n",
      "Epoch 6565/20000: Train Loss = 0.443143, Test Loss = 0.245147, Learning Rate = 8.252812e-05\n",
      "Epoch 6566/20000: Train Loss = 0.442695, Test Loss = 0.243235, Learning Rate = 8.249676e-05\n",
      "Epoch 6567/20000: Train Loss = 0.442744, Test Loss = 0.243859, Learning Rate = 8.246542e-05\n",
      "Epoch 6568/20000: Train Loss = 0.442617, Test Loss = 0.242995, Learning Rate = 8.243408e-05\n",
      "Epoch 6569/20000: Train Loss = 0.442910, Test Loss = 0.247633, Learning Rate = 8.240276e-05\n",
      "Epoch 6570/20000: Train Loss = 0.442647, Test Loss = 0.243432, Learning Rate = 8.237145e-05\n",
      "Epoch 6571/20000: Train Loss = 0.442988, Test Loss = 0.244721, Learning Rate = 8.234015e-05\n",
      "Epoch 6572/20000: Train Loss = 0.442735, Test Loss = 0.243353, Learning Rate = 8.230886e-05\n",
      "Epoch 6573/20000: Train Loss = 0.442871, Test Loss = 0.244429, Learning Rate = 8.227759e-05\n",
      "Epoch 6574/20000: Train Loss = 0.442982, Test Loss = 0.246166, Learning Rate = 8.224632e-05\n",
      "Epoch 6575/20000: Train Loss = 0.443007, Test Loss = 0.242013, Learning Rate = 8.221507e-05\n",
      "Epoch 6576/20000: Train Loss = 0.442789, Test Loss = 0.248962, Learning Rate = 8.218383e-05\n",
      "Epoch 6577/20000: Train Loss = 0.443334, Test Loss = 0.254475, Learning Rate = 8.215260e-05\n",
      "Epoch 6578/20000: Train Loss = 0.442932, Test Loss = 0.251207, Learning Rate = 8.212139e-05\n",
      "Epoch 6579/20000: Train Loss = 0.442669, Test Loss = 0.246993, Learning Rate = 8.209019e-05\n",
      "Epoch 6580/20000: Train Loss = 0.442796, Test Loss = 0.247129, Learning Rate = 8.205899e-05\n",
      "Epoch 6581/20000: Train Loss = 0.442747, Test Loss = 0.244454, Learning Rate = 8.202781e-05\n",
      "Epoch 6582/20000: Train Loss = 0.443214, Test Loss = 0.243132, Learning Rate = 8.199664e-05\n",
      "Epoch 6583/20000: Train Loss = 0.442982, Test Loss = 0.242558, Learning Rate = 8.196549e-05\n",
      "Epoch 6584/20000: Train Loss = 0.443104, Test Loss = 0.245001, Learning Rate = 8.193434e-05\n",
      "Epoch 6585/20000: Train Loss = 0.442774, Test Loss = 0.246255, Learning Rate = 8.190321e-05\n",
      "Epoch 6586/20000: Train Loss = 0.442612, Test Loss = 0.246650, Learning Rate = 8.187209e-05\n",
      "Epoch 6587/20000: Train Loss = 0.442761, Test Loss = 0.247161, Learning Rate = 8.184098e-05\n",
      "Epoch 6588/20000: Train Loss = 0.442839, Test Loss = 0.245508, Learning Rate = 8.180988e-05\n",
      "Epoch 6589/20000: Train Loss = 0.443635, Test Loss = 0.242771, Learning Rate = 8.177880e-05\n",
      "Epoch 6590/20000: Train Loss = 0.442647, Test Loss = 0.243768, Learning Rate = 8.174772e-05\n",
      "Epoch 6591/20000: Train Loss = 0.442844, Test Loss = 0.241943, Learning Rate = 8.171666e-05\n",
      "Epoch 6592/20000: Train Loss = 0.442788, Test Loss = 0.247165, Learning Rate = 8.168561e-05\n",
      "Epoch 6593/20000: Train Loss = 0.442952, Test Loss = 0.245894, Learning Rate = 8.165457e-05\n",
      "Epoch 6594/20000: Train Loss = 0.442835, Test Loss = 0.247065, Learning Rate = 8.162355e-05\n",
      "Epoch 6595/20000: Train Loss = 0.442829, Test Loss = 0.243637, Learning Rate = 8.159253e-05\n",
      "Epoch 6596/20000: Train Loss = 0.442839, Test Loss = 0.245296, Learning Rate = 8.156153e-05\n",
      "Epoch 6597/20000: Train Loss = 0.442752, Test Loss = 0.246049, Learning Rate = 8.153054e-05\n",
      "Epoch 6598/20000: Train Loss = 0.442765, Test Loss = 0.241641, Learning Rate = 8.149956e-05\n",
      "Epoch 6599/20000: Train Loss = 0.442711, Test Loss = 0.246401, Learning Rate = 8.146859e-05\n",
      "Epoch 6600/20000: Train Loss = 0.442987, Test Loss = 0.246106, Learning Rate = 8.143763e-05\n",
      "Epoch 6601/20000: Train Loss = 0.442703, Test Loss = 0.245877, Learning Rate = 8.140669e-05\n",
      "Epoch 6602/20000: Train Loss = 0.442825, Test Loss = 0.246803, Learning Rate = 8.137576e-05\n",
      "Epoch 6603/20000: Train Loss = 0.443108, Test Loss = 0.248681, Learning Rate = 8.134484e-05\n",
      "Epoch 6604/20000: Train Loss = 0.442876, Test Loss = 0.245175, Learning Rate = 8.131393e-05\n",
      "Epoch 6605/20000: Train Loss = 0.442940, Test Loss = 0.245865, Learning Rate = 8.128303e-05\n",
      "Epoch 6606/20000: Train Loss = 0.443707, Test Loss = 0.246457, Learning Rate = 8.125215e-05\n",
      "Epoch 6607/20000: Train Loss = 0.442643, Test Loss = 0.247330, Learning Rate = 8.122127e-05\n",
      "Epoch 6608/20000: Train Loss = 0.443075, Test Loss = 0.246942, Learning Rate = 8.119041e-05\n",
      "Epoch 6609/20000: Train Loss = 0.442702, Test Loss = 0.242066, Learning Rate = 8.115956e-05\n",
      "Epoch 6610/20000: Train Loss = 0.442826, Test Loss = 0.244468, Learning Rate = 8.112872e-05\n",
      "Epoch 6611/20000: Train Loss = 0.443070, Test Loss = 0.241250, Learning Rate = 8.109790e-05\n",
      "Epoch 6612/20000: Train Loss = 0.443282, Test Loss = 0.249969, Learning Rate = 8.106708e-05\n",
      "Epoch 6613/20000: Train Loss = 0.443112, Test Loss = 0.243908, Learning Rate = 8.103628e-05\n",
      "Epoch 6614/20000: Train Loss = 0.442840, Test Loss = 0.245526, Learning Rate = 8.100549e-05\n",
      "Epoch 6615/20000: Train Loss = 0.442744, Test Loss = 0.246069, Learning Rate = 8.097471e-05\n",
      "Epoch 6616/20000: Train Loss = 0.442980, Test Loss = 0.244348, Learning Rate = 8.094394e-05\n",
      "Epoch 6617/20000: Train Loss = 0.443050, Test Loss = 0.241208, Learning Rate = 8.091318e-05\n",
      "Epoch 6618/20000: Train Loss = 0.442835, Test Loss = 0.244779, Learning Rate = 8.088244e-05\n",
      "Epoch 6619/20000: Train Loss = 0.442840, Test Loss = 0.246244, Learning Rate = 8.085170e-05\n",
      "Epoch 6620/20000: Train Loss = 0.442789, Test Loss = 0.245468, Learning Rate = 8.082098e-05\n",
      "Epoch 6621/20000: Train Loss = 0.443259, Test Loss = 0.245212, Learning Rate = 8.079027e-05\n",
      "Epoch 6622/20000: Train Loss = 0.442729, Test Loss = 0.249202, Learning Rate = 8.075957e-05\n",
      "Epoch 6623/20000: Train Loss = 0.443054, Test Loss = 0.247686, Learning Rate = 8.072889e-05\n",
      "Epoch 6624/20000: Train Loss = 0.443073, Test Loss = 0.246207, Learning Rate = 8.069821e-05\n",
      "Epoch 6625/20000: Train Loss = 0.442896, Test Loss = 0.247473, Learning Rate = 8.066755e-05\n",
      "Epoch 6626/20000: Train Loss = 0.443030, Test Loss = 0.241953, Learning Rate = 8.063690e-05\n",
      "Epoch 6627/20000: Train Loss = 0.442549, Test Loss = 0.246675, Learning Rate = 8.060626e-05\n",
      "Epoch 6628/20000: Train Loss = 0.442996, Test Loss = 0.251142, Learning Rate = 8.057563e-05\n",
      "Epoch 6629/20000: Train Loss = 0.442655, Test Loss = 0.248692, Learning Rate = 8.054501e-05\n",
      "Epoch 6630/20000: Train Loss = 0.442758, Test Loss = 0.249127, Learning Rate = 8.051441e-05\n",
      "Epoch 6631/20000: Train Loss = 0.443097, Test Loss = 0.247320, Learning Rate = 8.048382e-05\n",
      "Epoch 6632/20000: Train Loss = 0.443036, Test Loss = 0.247286, Learning Rate = 8.045323e-05\n",
      "Epoch 6633/20000: Train Loss = 0.442728, Test Loss = 0.246680, Learning Rate = 8.042266e-05\n",
      "Epoch 6634/20000: Train Loss = 0.443395, Test Loss = 0.248535, Learning Rate = 8.039211e-05\n",
      "Epoch 6635/20000: Train Loss = 0.442736, Test Loss = 0.247232, Learning Rate = 8.036156e-05\n",
      "Epoch 6636/20000: Train Loss = 0.442600, Test Loss = 0.245652, Learning Rate = 8.033102e-05\n",
      "Epoch 6637/20000: Train Loss = 0.442975, Test Loss = 0.247831, Learning Rate = 8.030050e-05\n",
      "Epoch 6638/20000: Train Loss = 0.442834, Test Loss = 0.246787, Learning Rate = 8.026999e-05\n",
      "Epoch 6639/20000: Train Loss = 0.442613, Test Loss = 0.240534, Learning Rate = 8.023949e-05\n",
      "Epoch 6640/20000: Train Loss = 0.442746, Test Loss = 0.243659, Learning Rate = 8.020900e-05\n",
      "Epoch 6641/20000: Train Loss = 0.442827, Test Loss = 0.243306, Learning Rate = 8.017852e-05\n",
      "Epoch 6642/20000: Train Loss = 0.442893, Test Loss = 0.246535, Learning Rate = 8.014806e-05\n",
      "Epoch 6643/20000: Train Loss = 0.442976, Test Loss = 0.242667, Learning Rate = 8.011760e-05\n",
      "Epoch 6644/20000: Train Loss = 0.443029, Test Loss = 0.251074, Learning Rate = 8.008716e-05\n",
      "Epoch 6645/20000: Train Loss = 0.442800, Test Loss = 0.244581, Learning Rate = 8.005673e-05\n",
      "Epoch 6646/20000: Train Loss = 0.442745, Test Loss = 0.244556, Learning Rate = 8.002631e-05\n",
      "Epoch 6647/20000: Train Loss = 0.442855, Test Loss = 0.246541, Learning Rate = 7.999590e-05\n",
      "Epoch 6648/20000: Train Loss = 0.442740, Test Loss = 0.243641, Learning Rate = 7.996550e-05\n",
      "Epoch 6649/20000: Train Loss = 0.442917, Test Loss = 0.247595, Learning Rate = 7.993512e-05\n",
      "Epoch 6650/20000: Train Loss = 0.442762, Test Loss = 0.249338, Learning Rate = 7.990475e-05\n",
      "Epoch 6651/20000: Train Loss = 0.442721, Test Loss = 0.246063, Learning Rate = 7.987438e-05\n",
      "Epoch 6652/20000: Train Loss = 0.442725, Test Loss = 0.245195, Learning Rate = 7.984403e-05\n",
      "Epoch 6653/20000: Train Loss = 0.442638, Test Loss = 0.248837, Learning Rate = 7.981370e-05\n",
      "Epoch 6654/20000: Train Loss = 0.442890, Test Loss = 0.247223, Learning Rate = 7.978337e-05\n",
      "Epoch 6655/20000: Train Loss = 0.442848, Test Loss = 0.245991, Learning Rate = 7.975305e-05\n",
      "Epoch 6656/20000: Train Loss = 0.442693, Test Loss = 0.245522, Learning Rate = 7.972275e-05\n",
      "Epoch 6657/20000: Train Loss = 0.442970, Test Loss = 0.244115, Learning Rate = 7.969246e-05\n",
      "Epoch 6658/20000: Train Loss = 0.442985, Test Loss = 0.250174, Learning Rate = 7.966218e-05\n",
      "Epoch 6659/20000: Train Loss = 0.442769, Test Loss = 0.244806, Learning Rate = 7.963191e-05\n",
      "Epoch 6660/20000: Train Loss = 0.442697, Test Loss = 0.245501, Learning Rate = 7.960165e-05\n",
      "Epoch 6661/20000: Train Loss = 0.442830, Test Loss = 0.247861, Learning Rate = 7.957140e-05\n",
      "Epoch 6662/20000: Train Loss = 0.442908, Test Loss = 0.243240, Learning Rate = 7.954117e-05\n",
      "Epoch 6663/20000: Train Loss = 0.442963, Test Loss = 0.251281, Learning Rate = 7.951094e-05\n",
      "Epoch 6664/20000: Train Loss = 0.442771, Test Loss = 0.249307, Learning Rate = 7.948073e-05\n",
      "Epoch 6665/20000: Train Loss = 0.442696, Test Loss = 0.249358, Learning Rate = 7.945053e-05\n",
      "Epoch 6666/20000: Train Loss = 0.442724, Test Loss = 0.249262, Learning Rate = 7.942034e-05\n",
      "Epoch 6667/20000: Train Loss = 0.442953, Test Loss = 0.248781, Learning Rate = 7.939016e-05\n",
      "Epoch 6668/20000: Train Loss = 0.442666, Test Loss = 0.245765, Learning Rate = 7.936000e-05\n",
      "Epoch 6669/20000: Train Loss = 0.442596, Test Loss = 0.245866, Learning Rate = 7.932984e-05\n",
      "Epoch 6670/20000: Train Loss = 0.442951, Test Loss = 0.246995, Learning Rate = 7.929970e-05\n",
      "Epoch 6671/20000: Train Loss = 0.442891, Test Loss = 0.248942, Learning Rate = 7.926957e-05\n",
      "Epoch 6672/20000: Train Loss = 0.442518, Test Loss = 0.246304, Learning Rate = 7.923945e-05\n",
      "Epoch 6673/20000: Train Loss = 0.443145, Test Loss = 0.251746, Learning Rate = 7.920934e-05\n",
      "Epoch 6674/20000: Train Loss = 0.442941, Test Loss = 0.245342, Learning Rate = 7.917924e-05\n",
      "Epoch 6675/20000: Train Loss = 0.442964, Test Loss = 0.244036, Learning Rate = 7.914916e-05\n",
      "Epoch 6676/20000: Train Loss = 0.443185, Test Loss = 0.248560, Learning Rate = 7.911908e-05\n",
      "Epoch 6677/20000: Train Loss = 0.442822, Test Loss = 0.249590, Learning Rate = 7.908902e-05\n",
      "Epoch 6678/20000: Train Loss = 0.442614, Test Loss = 0.247968, Learning Rate = 7.905897e-05\n",
      "Epoch 6679/20000: Train Loss = 0.442848, Test Loss = 0.250121, Learning Rate = 7.902893e-05\n",
      "Epoch 6680/20000: Train Loss = 0.442642, Test Loss = 0.248388, Learning Rate = 7.899890e-05\n",
      "Epoch 6681/20000: Train Loss = 0.442796, Test Loss = 0.249671, Learning Rate = 7.896888e-05\n",
      "Epoch 6682/20000: Train Loss = 0.442638, Test Loss = 0.248848, Learning Rate = 7.893887e-05\n",
      "Epoch 6683/20000: Train Loss = 0.442609, Test Loss = 0.243746, Learning Rate = 7.890888e-05\n",
      "Epoch 6684/20000: Train Loss = 0.442645, Test Loss = 0.247429, Learning Rate = 7.887890e-05\n",
      "Epoch 6685/20000: Train Loss = 0.442671, Test Loss = 0.248262, Learning Rate = 7.884892e-05\n",
      "Epoch 6686/20000: Train Loss = 0.442855, Test Loss = 0.248061, Learning Rate = 7.881896e-05\n",
      "Epoch 6687/20000: Train Loss = 0.442906, Test Loss = 0.247078, Learning Rate = 7.878902e-05\n",
      "Epoch 6688/20000: Train Loss = 0.442781, Test Loss = 0.243934, Learning Rate = 7.875908e-05\n",
      "Epoch 6689/20000: Train Loss = 0.442881, Test Loss = 0.248163, Learning Rate = 7.872915e-05\n",
      "Epoch 6690/20000: Train Loss = 0.442865, Test Loss = 0.245103, Learning Rate = 7.869924e-05\n",
      "Epoch 6691/20000: Train Loss = 0.442680, Test Loss = 0.243848, Learning Rate = 7.866933e-05\n",
      "Epoch 6692/20000: Train Loss = 0.443157, Test Loss = 0.241399, Learning Rate = 7.863944e-05\n",
      "Epoch 6693/20000: Train Loss = 0.443072, Test Loss = 0.240330, Learning Rate = 7.860956e-05\n",
      "Epoch 6694/20000: Train Loss = 0.443062, Test Loss = 0.244479, Learning Rate = 7.857969e-05\n",
      "Epoch 6695/20000: Train Loss = 0.442742, Test Loss = 0.241174, Learning Rate = 7.854983e-05\n",
      "Epoch 6696/20000: Train Loss = 0.442671, Test Loss = 0.244537, Learning Rate = 7.851999e-05\n",
      "Epoch 6697/20000: Train Loss = 0.442549, Test Loss = 0.248642, Learning Rate = 7.849015e-05\n",
      "Epoch 6698/20000: Train Loss = 0.442703, Test Loss = 0.249911, Learning Rate = 7.846033e-05\n",
      "Epoch 6699/20000: Train Loss = 0.442915, Test Loss = 0.247672, Learning Rate = 7.843051e-05\n",
      "Epoch 6700/20000: Train Loss = 0.443172, Test Loss = 0.250499, Learning Rate = 7.840071e-05\n",
      "Epoch 6701/20000: Train Loss = 0.443008, Test Loss = 0.246541, Learning Rate = 7.837092e-05\n",
      "Epoch 6702/20000: Train Loss = 0.443148, Test Loss = 0.246716, Learning Rate = 7.834114e-05\n",
      "Epoch 6703/20000: Train Loss = 0.443775, Test Loss = 0.253504, Learning Rate = 7.831137e-05\n",
      "Epoch 6704/20000: Train Loss = 0.442993, Test Loss = 0.249087, Learning Rate = 7.828162e-05\n",
      "Epoch 6705/20000: Train Loss = 0.442856, Test Loss = 0.248984, Learning Rate = 7.825187e-05\n",
      "Epoch 6706/20000: Train Loss = 0.442656, Test Loss = 0.250738, Learning Rate = 7.822214e-05\n",
      "Epoch 6707/20000: Train Loss = 0.443189, Test Loss = 0.247599, Learning Rate = 7.819242e-05\n",
      "Epoch 6708/20000: Train Loss = 0.443121, Test Loss = 0.253314, Learning Rate = 7.816271e-05\n",
      "Epoch 6709/20000: Train Loss = 0.442879, Test Loss = 0.250615, Learning Rate = 7.813301e-05\n",
      "Epoch 6710/20000: Train Loss = 0.442740, Test Loss = 0.249660, Learning Rate = 7.810332e-05\n",
      "Epoch 6711/20000: Train Loss = 0.442739, Test Loss = 0.245893, Learning Rate = 7.807364e-05\n",
      "Epoch 6712/20000: Train Loss = 0.442784, Test Loss = 0.253347, Learning Rate = 7.804398e-05\n",
      "Epoch 6713/20000: Train Loss = 0.443125, Test Loss = 0.250720, Learning Rate = 7.801432e-05\n",
      "Epoch 6714/20000: Train Loss = 0.442675, Test Loss = 0.247305, Learning Rate = 7.798468e-05\n",
      "Epoch 6715/20000: Train Loss = 0.442739, Test Loss = 0.247787, Learning Rate = 7.795505e-05\n",
      "Epoch 6716/20000: Train Loss = 0.442690, Test Loss = 0.249488, Learning Rate = 7.792542e-05\n",
      "Epoch 6717/20000: Train Loss = 0.442920, Test Loss = 0.246808, Learning Rate = 7.789582e-05\n",
      "Epoch 6718/20000: Train Loss = 0.442708, Test Loss = 0.249622, Learning Rate = 7.786622e-05\n",
      "Epoch 6719/20000: Train Loss = 0.442684, Test Loss = 0.248927, Learning Rate = 7.783663e-05\n",
      "Epoch 6720/20000: Train Loss = 0.442786, Test Loss = 0.252143, Learning Rate = 7.780705e-05\n",
      "Epoch 6721/20000: Train Loss = 0.443028, Test Loss = 0.251077, Learning Rate = 7.777749e-05\n",
      "Epoch 6722/20000: Train Loss = 0.442792, Test Loss = 0.247104, Learning Rate = 7.774794e-05\n",
      "Epoch 6723/20000: Train Loss = 0.442892, Test Loss = 0.243002, Learning Rate = 7.771839e-05\n",
      "Epoch 6724/20000: Train Loss = 0.442937, Test Loss = 0.248518, Learning Rate = 7.768886e-05\n",
      "Epoch 6725/20000: Train Loss = 0.442837, Test Loss = 0.243783, Learning Rate = 7.765934e-05\n",
      "Epoch 6726/20000: Train Loss = 0.442680, Test Loss = 0.247618, Learning Rate = 7.762984e-05\n",
      "Epoch 6727/20000: Train Loss = 0.442585, Test Loss = 0.245823, Learning Rate = 7.760034e-05\n",
      "Epoch 6728/20000: Train Loss = 0.442953, Test Loss = 0.247248, Learning Rate = 7.757085e-05\n",
      "Epoch 6729/20000: Train Loss = 0.442676, Test Loss = 0.244909, Learning Rate = 7.754138e-05\n",
      "Epoch 6730/20000: Train Loss = 0.442652, Test Loss = 0.246679, Learning Rate = 7.751191e-05\n",
      "Epoch 6731/20000: Train Loss = 0.442824, Test Loss = 0.245562, Learning Rate = 7.748246e-05\n",
      "Epoch 6732/20000: Train Loss = 0.442604, Test Loss = 0.245822, Learning Rate = 7.745302e-05\n",
      "Epoch 6733/20000: Train Loss = 0.442678, Test Loss = 0.251052, Learning Rate = 7.742359e-05\n",
      "Epoch 6734/20000: Train Loss = 0.443099, Test Loss = 0.252216, Learning Rate = 7.739417e-05\n",
      "Epoch 6735/20000: Train Loss = 0.442624, Test Loss = 0.245159, Learning Rate = 7.736476e-05\n",
      "Epoch 6736/20000: Train Loss = 0.442862, Test Loss = 0.248274, Learning Rate = 7.733537e-05\n",
      "Epoch 6737/20000: Train Loss = 0.442822, Test Loss = 0.245312, Learning Rate = 7.730598e-05\n",
      "Epoch 6738/20000: Train Loss = 0.443449, Test Loss = 0.246093, Learning Rate = 7.727661e-05\n",
      "Epoch 6739/20000: Train Loss = 0.443582, Test Loss = 0.245005, Learning Rate = 7.724724e-05\n",
      "Epoch 6740/20000: Train Loss = 0.442872, Test Loss = 0.246197, Learning Rate = 7.721789e-05\n",
      "Epoch 6741/20000: Train Loss = 0.442826, Test Loss = 0.248331, Learning Rate = 7.718855e-05\n",
      "Epoch 6742/20000: Train Loss = 0.442776, Test Loss = 0.244303, Learning Rate = 7.715922e-05\n",
      "Epoch 6743/20000: Train Loss = 0.443032, Test Loss = 0.241302, Learning Rate = 7.712990e-05\n",
      "Epoch 6744/20000: Train Loss = 0.442878, Test Loss = 0.243231, Learning Rate = 7.710060e-05\n",
      "Epoch 6745/20000: Train Loss = 0.442799, Test Loss = 0.247532, Learning Rate = 7.707130e-05\n",
      "Epoch 6746/20000: Train Loss = 0.443238, Test Loss = 0.247183, Learning Rate = 7.704202e-05\n",
      "Epoch 6747/20000: Train Loss = 0.442821, Test Loss = 0.246534, Learning Rate = 7.701274e-05\n",
      "Epoch 6748/20000: Train Loss = 0.442821, Test Loss = 0.244291, Learning Rate = 7.698348e-05\n",
      "Epoch 6749/20000: Train Loss = 0.442792, Test Loss = 0.243099, Learning Rate = 7.695423e-05\n",
      "Epoch 6750/20000: Train Loss = 0.442902, Test Loss = 0.245749, Learning Rate = 7.692499e-05\n",
      "Epoch 6751/20000: Train Loss = 0.442751, Test Loss = 0.246123, Learning Rate = 7.689576e-05\n",
      "Epoch 6752/20000: Train Loss = 0.442752, Test Loss = 0.245719, Learning Rate = 7.686654e-05\n",
      "Epoch 6753/20000: Train Loss = 0.442671, Test Loss = 0.245889, Learning Rate = 7.683733e-05\n",
      "Epoch 6754/20000: Train Loss = 0.442675, Test Loss = 0.245854, Learning Rate = 7.680814e-05\n",
      "Epoch 6755/20000: Train Loss = 0.442978, Test Loss = 0.251064, Learning Rate = 7.677895e-05\n",
      "Epoch 6756/20000: Train Loss = 0.443165, Test Loss = 0.250234, Learning Rate = 7.674978e-05\n",
      "Epoch 6757/20000: Train Loss = 0.442861, Test Loss = 0.246209, Learning Rate = 7.672061e-05\n",
      "Epoch 6758/20000: Train Loss = 0.443179, Test Loss = 0.253305, Learning Rate = 7.669146e-05\n",
      "Epoch 6759/20000: Train Loss = 0.442901, Test Loss = 0.248608, Learning Rate = 7.666232e-05\n",
      "Epoch 6760/20000: Train Loss = 0.442781, Test Loss = 0.246915, Learning Rate = 7.663319e-05\n",
      "Epoch 6761/20000: Train Loss = 0.442757, Test Loss = 0.245198, Learning Rate = 7.660407e-05\n",
      "Epoch 6762/20000: Train Loss = 0.442616, Test Loss = 0.246942, Learning Rate = 7.657497e-05\n",
      "Epoch 6763/20000: Train Loss = 0.442658, Test Loss = 0.248271, Learning Rate = 7.654587e-05\n",
      "Epoch 6764/20000: Train Loss = 0.442986, Test Loss = 0.249146, Learning Rate = 7.651678e-05\n",
      "Epoch 6765/20000: Train Loss = 0.442593, Test Loss = 0.243044, Learning Rate = 7.648771e-05\n",
      "Epoch 6766/20000: Train Loss = 0.442654, Test Loss = 0.245747, Learning Rate = 7.645865e-05\n",
      "Epoch 6767/20000: Train Loss = 0.442852, Test Loss = 0.252735, Learning Rate = 7.642959e-05\n",
      "Epoch 6768/20000: Train Loss = 0.442425, Test Loss = 0.247115, Learning Rate = 7.640055e-05\n",
      "Epoch 6769/20000: Train Loss = 0.442809, Test Loss = 0.248488, Learning Rate = 7.637152e-05\n",
      "Epoch 6770/20000: Train Loss = 0.442835, Test Loss = 0.249079, Learning Rate = 7.634250e-05\n",
      "Epoch 6771/20000: Train Loss = 0.442795, Test Loss = 0.245190, Learning Rate = 7.631350e-05\n",
      "Epoch 6772/20000: Train Loss = 0.442857, Test Loss = 0.247582, Learning Rate = 7.628450e-05\n",
      "Epoch 6773/20000: Train Loss = 0.442688, Test Loss = 0.248237, Learning Rate = 7.625551e-05\n",
      "Epoch 6774/20000: Train Loss = 0.442811, Test Loss = 0.247649, Learning Rate = 7.622654e-05\n",
      "Epoch 6775/20000: Train Loss = 0.442845, Test Loss = 0.247552, Learning Rate = 7.619757e-05\n",
      "Epoch 6776/20000: Train Loss = 0.443197, Test Loss = 0.248345, Learning Rate = 7.616862e-05\n",
      "Epoch 6777/20000: Train Loss = 0.442481, Test Loss = 0.245750, Learning Rate = 7.613968e-05\n",
      "Epoch 6778/20000: Train Loss = 0.442722, Test Loss = 0.244366, Learning Rate = 7.611075e-05\n",
      "Epoch 6779/20000: Train Loss = 0.442939, Test Loss = 0.244827, Learning Rate = 7.608183e-05\n",
      "Epoch 6780/20000: Train Loss = 0.442573, Test Loss = 0.250457, Learning Rate = 7.605292e-05\n",
      "Epoch 6781/20000: Train Loss = 0.442841, Test Loss = 0.245921, Learning Rate = 7.602402e-05\n",
      "Epoch 6782/20000: Train Loss = 0.442749, Test Loss = 0.248724, Learning Rate = 7.599513e-05\n",
      "Epoch 6783/20000: Train Loss = 0.443066, Test Loss = 0.246233, Learning Rate = 7.596626e-05\n",
      "Epoch 6784/20000: Train Loss = 0.443038, Test Loss = 0.245043, Learning Rate = 7.593739e-05\n",
      "Epoch 6785/20000: Train Loss = 0.442881, Test Loss = 0.243841, Learning Rate = 7.590854e-05\n",
      "Epoch 6786/20000: Train Loss = 0.442772, Test Loss = 0.246190, Learning Rate = 7.587969e-05\n",
      "Epoch 6787/20000: Train Loss = 0.442796, Test Loss = 0.244887, Learning Rate = 7.585086e-05\n",
      "Epoch 6788/20000: Train Loss = 0.442851, Test Loss = 0.247965, Learning Rate = 7.582204e-05\n",
      "Epoch 6789/20000: Train Loss = 0.442659, Test Loss = 0.245992, Learning Rate = 7.579323e-05\n",
      "Epoch 6790/20000: Train Loss = 0.442739, Test Loss = 0.246778, Learning Rate = 7.576443e-05\n",
      "Epoch 6791/20000: Train Loss = 0.442725, Test Loss = 0.246489, Learning Rate = 7.573564e-05\n",
      "Epoch 6792/20000: Train Loss = 0.443259, Test Loss = 0.254715, Learning Rate = 7.570687e-05\n",
      "Epoch 6793/20000: Train Loss = 0.442425, Test Loss = 0.245086, Learning Rate = 7.567810e-05\n",
      "Epoch 6794/20000: Train Loss = 0.442746, Test Loss = 0.247823, Learning Rate = 7.564934e-05\n",
      "Epoch 6795/20000: Train Loss = 0.442682, Test Loss = 0.243379, Learning Rate = 7.562060e-05\n",
      "Epoch 6796/20000: Train Loss = 0.442981, Test Loss = 0.247361, Learning Rate = 7.559186e-05\n",
      "Epoch 6797/20000: Train Loss = 0.442752, Test Loss = 0.245218, Learning Rate = 7.556314e-05\n",
      "Epoch 6798/20000: Train Loss = 0.443077, Test Loss = 0.247433, Learning Rate = 7.553443e-05\n",
      "Epoch 6799/20000: Train Loss = 0.442801, Test Loss = 0.247338, Learning Rate = 7.550573e-05\n",
      "Epoch 6800/20000: Train Loss = 0.442921, Test Loss = 0.243988, Learning Rate = 7.547704e-05\n",
      "Epoch 6801/20000: Train Loss = 0.442921, Test Loss = 0.245430, Learning Rate = 7.544836e-05\n",
      "Epoch 6802/20000: Train Loss = 0.442898, Test Loss = 0.247657, Learning Rate = 7.541969e-05\n",
      "Epoch 6803/20000: Train Loss = 0.442681, Test Loss = 0.248174, Learning Rate = 7.539103e-05\n",
      "Epoch 6804/20000: Train Loss = 0.442769, Test Loss = 0.245116, Learning Rate = 7.536239e-05\n",
      "Epoch 6805/20000: Train Loss = 0.442813, Test Loss = 0.249448, Learning Rate = 7.533375e-05\n",
      "Epoch 6806/20000: Train Loss = 0.442886, Test Loss = 0.248633, Learning Rate = 7.530513e-05\n",
      "Epoch 6807/20000: Train Loss = 0.442551, Test Loss = 0.244498, Learning Rate = 7.527651e-05\n",
      "Epoch 6808/20000: Train Loss = 0.442795, Test Loss = 0.245893, Learning Rate = 7.524791e-05\n",
      "Epoch 6809/20000: Train Loss = 0.442568, Test Loss = 0.245187, Learning Rate = 7.521932e-05\n",
      "Epoch 6810/20000: Train Loss = 0.442870, Test Loss = 0.243885, Learning Rate = 7.519074e-05\n",
      "Epoch 6811/20000: Train Loss = 0.442919, Test Loss = 0.245292, Learning Rate = 7.516217e-05\n",
      "Epoch 6812/20000: Train Loss = 0.442828, Test Loss = 0.242134, Learning Rate = 7.513361e-05\n",
      "Epoch 6813/20000: Train Loss = 0.443010, Test Loss = 0.244628, Learning Rate = 7.510506e-05\n",
      "Epoch 6814/20000: Train Loss = 0.442955, Test Loss = 0.244964, Learning Rate = 7.507652e-05\n",
      "Epoch 6815/20000: Train Loss = 0.442662, Test Loss = 0.247276, Learning Rate = 7.504799e-05\n",
      "Epoch 6816/20000: Train Loss = 0.442859, Test Loss = 0.243059, Learning Rate = 7.501948e-05\n",
      "Epoch 6817/20000: Train Loss = 0.442609, Test Loss = 0.246507, Learning Rate = 7.499097e-05\n",
      "Epoch 6818/20000: Train Loss = 0.442788, Test Loss = 0.244329, Learning Rate = 7.496248e-05\n",
      "Epoch 6819/20000: Train Loss = 0.442657, Test Loss = 0.249799, Learning Rate = 7.493399e-05\n",
      "Epoch 6820/20000: Train Loss = 0.442631, Test Loss = 0.248068, Learning Rate = 7.490552e-05\n",
      "Epoch 6821/20000: Train Loss = 0.442828, Test Loss = 0.247649, Learning Rate = 7.487706e-05\n",
      "Epoch 6822/20000: Train Loss = 0.442916, Test Loss = 0.244183, Learning Rate = 7.484861e-05\n",
      "Epoch 6823/20000: Train Loss = 0.442757, Test Loss = 0.243850, Learning Rate = 7.482017e-05\n",
      "Epoch 6824/20000: Train Loss = 0.442615, Test Loss = 0.244880, Learning Rate = 7.479174e-05\n",
      "Epoch 6825/20000: Train Loss = 0.443108, Test Loss = 0.243648, Learning Rate = 7.476332e-05\n",
      "Epoch 6826/20000: Train Loss = 0.443626, Test Loss = 0.246158, Learning Rate = 7.473491e-05\n",
      "Epoch 6827/20000: Train Loss = 0.442971, Test Loss = 0.246797, Learning Rate = 7.470651e-05\n",
      "Epoch 6828/20000: Train Loss = 0.442871, Test Loss = 0.248554, Learning Rate = 7.467813e-05\n",
      "Epoch 6829/20000: Train Loss = 0.442744, Test Loss = 0.247916, Learning Rate = 7.464975e-05\n",
      "Epoch 6830/20000: Train Loss = 0.443028, Test Loss = 0.245941, Learning Rate = 7.462139e-05\n",
      "Epoch 6831/20000: Train Loss = 0.442987, Test Loss = 0.247379, Learning Rate = 7.459303e-05\n",
      "Epoch 6832/20000: Train Loss = 0.442672, Test Loss = 0.245407, Learning Rate = 7.456469e-05\n",
      "Epoch 6833/20000: Train Loss = 0.442665, Test Loss = 0.241583, Learning Rate = 7.453636e-05\n",
      "Epoch 6834/20000: Train Loss = 0.442931, Test Loss = 0.241887, Learning Rate = 7.450803e-05\n",
      "Epoch 6835/20000: Train Loss = 0.442703, Test Loss = 0.246315, Learning Rate = 7.447972e-05\n",
      "Epoch 6836/20000: Train Loss = 0.442773, Test Loss = 0.246522, Learning Rate = 7.445142e-05\n",
      "Epoch 6837/20000: Train Loss = 0.442615, Test Loss = 0.246834, Learning Rate = 7.442313e-05\n",
      "Epoch 6838/20000: Train Loss = 0.443247, Test Loss = 0.251754, Learning Rate = 7.439485e-05\n",
      "Epoch 6839/20000: Train Loss = 0.442842, Test Loss = 0.250053, Learning Rate = 7.436659e-05\n",
      "Epoch 6840/20000: Train Loss = 0.442702, Test Loss = 0.246901, Learning Rate = 7.433833e-05\n",
      "Epoch 6841/20000: Train Loss = 0.442697, Test Loss = 0.248742, Learning Rate = 7.431008e-05\n",
      "Epoch 6842/20000: Train Loss = 0.443103, Test Loss = 0.248339, Learning Rate = 7.428185e-05\n",
      "Epoch 6843/20000: Train Loss = 0.442741, Test Loss = 0.246466, Learning Rate = 7.425362e-05\n",
      "Epoch 6844/20000: Train Loss = 0.442991, Test Loss = 0.249764, Learning Rate = 7.422541e-05\n",
      "Epoch 6845/20000: Train Loss = 0.442586, Test Loss = 0.246178, Learning Rate = 7.419720e-05\n",
      "Epoch 6846/20000: Train Loss = 0.442979, Test Loss = 0.243217, Learning Rate = 7.416901e-05\n",
      "Epoch 6847/20000: Train Loss = 0.442930, Test Loss = 0.243629, Learning Rate = 7.414083e-05\n",
      "Epoch 6848/20000: Train Loss = 0.443082, Test Loss = 0.241833, Learning Rate = 7.411266e-05\n",
      "Epoch 6849/20000: Train Loss = 0.442736, Test Loss = 0.245901, Learning Rate = 7.408450e-05\n",
      "Epoch 6850/20000: Train Loss = 0.442600, Test Loss = 0.247785, Learning Rate = 7.405635e-05\n",
      "Epoch 6851/20000: Train Loss = 0.443177, Test Loss = 0.251291, Learning Rate = 7.402821e-05\n",
      "Epoch 6852/20000: Train Loss = 0.443211, Test Loss = 0.245591, Learning Rate = 7.400008e-05\n",
      "Epoch 6853/20000: Train Loss = 0.443177, Test Loss = 0.248572, Learning Rate = 7.397196e-05\n",
      "Epoch 6854/20000: Train Loss = 0.443268, Test Loss = 0.243504, Learning Rate = 7.394385e-05\n",
      "Epoch 6855/20000: Train Loss = 0.443256, Test Loss = 0.246539, Learning Rate = 7.391576e-05\n",
      "Epoch 6856/20000: Train Loss = 0.443048, Test Loss = 0.246512, Learning Rate = 7.388767e-05\n",
      "Epoch 6857/20000: Train Loss = 0.442946, Test Loss = 0.245826, Learning Rate = 7.385959e-05\n",
      "Epoch 6858/20000: Train Loss = 0.442952, Test Loss = 0.246476, Learning Rate = 7.383153e-05\n",
      "Epoch 6859/20000: Train Loss = 0.442863, Test Loss = 0.245638, Learning Rate = 7.380348e-05\n",
      "Epoch 6860/20000: Train Loss = 0.442679, Test Loss = 0.246964, Learning Rate = 7.377543e-05\n",
      "Epoch 6861/20000: Train Loss = 0.442678, Test Loss = 0.247521, Learning Rate = 7.374740e-05\n",
      "Epoch 6862/20000: Train Loss = 0.442696, Test Loss = 0.246763, Learning Rate = 7.371938e-05\n",
      "Epoch 6863/20000: Train Loss = 0.442730, Test Loss = 0.247590, Learning Rate = 7.369137e-05\n",
      "Epoch 6864/20000: Train Loss = 0.442730, Test Loss = 0.249335, Learning Rate = 7.366337e-05\n",
      "Epoch 6865/20000: Train Loss = 0.442833, Test Loss = 0.250555, Learning Rate = 7.363538e-05\n",
      "Epoch 6866/20000: Train Loss = 0.442583, Test Loss = 0.243407, Learning Rate = 7.360740e-05\n",
      "Epoch 6867/20000: Train Loss = 0.442727, Test Loss = 0.245560, Learning Rate = 7.357943e-05\n",
      "Epoch 6868/20000: Train Loss = 0.442805, Test Loss = 0.247250, Learning Rate = 7.355147e-05\n",
      "Epoch 6869/20000: Train Loss = 0.442853, Test Loss = 0.246253, Learning Rate = 7.352352e-05\n",
      "Epoch 6870/20000: Train Loss = 0.442934, Test Loss = 0.242913, Learning Rate = 7.349558e-05\n",
      "Epoch 6871/20000: Train Loss = 0.443027, Test Loss = 0.240205, Learning Rate = 7.346766e-05\n",
      "Epoch 6872/20000: Train Loss = 0.443165, Test Loss = 0.242927, Learning Rate = 7.343974e-05\n",
      "Epoch 6873/20000: Train Loss = 0.444081, Test Loss = 0.250834, Learning Rate = 7.341184e-05\n",
      "Epoch 6874/20000: Train Loss = 0.442872, Test Loss = 0.244776, Learning Rate = 7.338394e-05\n",
      "Epoch 6875/20000: Train Loss = 0.442690, Test Loss = 0.248879, Learning Rate = 7.335606e-05\n",
      "Epoch 6876/20000: Train Loss = 0.442831, Test Loss = 0.246862, Learning Rate = 7.332819e-05\n",
      "Epoch 6877/20000: Train Loss = 0.442882, Test Loss = 0.243766, Learning Rate = 7.330032e-05\n",
      "Epoch 6878/20000: Train Loss = 0.442662, Test Loss = 0.245519, Learning Rate = 7.327247e-05\n",
      "Epoch 6879/20000: Train Loss = 0.442895, Test Loss = 0.248725, Learning Rate = 7.324463e-05\n",
      "Epoch 6880/20000: Train Loss = 0.442729, Test Loss = 0.250589, Learning Rate = 7.321680e-05\n",
      "Epoch 6881/20000: Train Loss = 0.442818, Test Loss = 0.250199, Learning Rate = 7.318898e-05\n",
      "Epoch 6882/20000: Train Loss = 0.442718, Test Loss = 0.245557, Learning Rate = 7.316117e-05\n",
      "Epoch 6883/20000: Train Loss = 0.442698, Test Loss = 0.244506, Learning Rate = 7.313337e-05\n",
      "Epoch 6884/20000: Train Loss = 0.442726, Test Loss = 0.246910, Learning Rate = 7.310558e-05\n",
      "Epoch 6885/20000: Train Loss = 0.443076, Test Loss = 0.245117, Learning Rate = 7.307780e-05\n",
      "Epoch 6886/20000: Train Loss = 0.442921, Test Loss = 0.248913, Learning Rate = 7.305003e-05\n",
      "Epoch 6887/20000: Train Loss = 0.442841, Test Loss = 0.250936, Learning Rate = 7.302228e-05\n",
      "Epoch 6888/20000: Train Loss = 0.442836, Test Loss = 0.244056, Learning Rate = 7.299453e-05\n",
      "Epoch 6889/20000: Train Loss = 0.442645, Test Loss = 0.246433, Learning Rate = 7.296679e-05\n",
      "Epoch 6890/20000: Train Loss = 0.443088, Test Loss = 0.250197, Learning Rate = 7.293907e-05\n",
      "Epoch 6891/20000: Train Loss = 0.442631, Test Loss = 0.246510, Learning Rate = 7.291135e-05\n",
      "Epoch 6892/20000: Train Loss = 0.443265, Test Loss = 0.244671, Learning Rate = 7.288365e-05\n",
      "Epoch 6893/20000: Train Loss = 0.442749, Test Loss = 0.247027, Learning Rate = 7.285596e-05\n",
      "Epoch 6894/20000: Train Loss = 0.442973, Test Loss = 0.246458, Learning Rate = 7.282827e-05\n",
      "Epoch 6895/20000: Train Loss = 0.442716, Test Loss = 0.249757, Learning Rate = 7.280060e-05\n",
      "Epoch 6896/20000: Train Loss = 0.442575, Test Loss = 0.246060, Learning Rate = 7.277294e-05\n",
      "Epoch 6897/20000: Train Loss = 0.442855, Test Loss = 0.247298, Learning Rate = 7.274529e-05\n",
      "Epoch 6898/20000: Train Loss = 0.442748, Test Loss = 0.244000, Learning Rate = 7.271765e-05\n",
      "Epoch 6899/20000: Train Loss = 0.442857, Test Loss = 0.241255, Learning Rate = 7.269001e-05\n",
      "Epoch 6900/20000: Train Loss = 0.443320, Test Loss = 0.242668, Learning Rate = 7.266239e-05\n",
      "Epoch 6901/20000: Train Loss = 0.442881, Test Loss = 0.243125, Learning Rate = 7.263478e-05\n",
      "Epoch 6902/20000: Train Loss = 0.443014, Test Loss = 0.243429, Learning Rate = 7.260719e-05\n",
      "Epoch 6903/20000: Train Loss = 0.442632, Test Loss = 0.245642, Learning Rate = 7.257960e-05\n",
      "Epoch 6904/20000: Train Loss = 0.443322, Test Loss = 0.244070, Learning Rate = 7.255202e-05\n",
      "Epoch 6905/20000: Train Loss = 0.443018, Test Loss = 0.250954, Learning Rate = 7.252445e-05\n",
      "Epoch 6906/20000: Train Loss = 0.442897, Test Loss = 0.247842, Learning Rate = 7.249689e-05\n",
      "Epoch 6907/20000: Train Loss = 0.442718, Test Loss = 0.248715, Learning Rate = 7.246935e-05\n",
      "Epoch 6908/20000: Train Loss = 0.442788, Test Loss = 0.247961, Learning Rate = 7.244181e-05\n",
      "Epoch 6909/20000: Train Loss = 0.442957, Test Loss = 0.246510, Learning Rate = 7.241428e-05\n",
      "Epoch 6910/20000: Train Loss = 0.442594, Test Loss = 0.246733, Learning Rate = 7.238677e-05\n",
      "Epoch 6911/20000: Train Loss = 0.442885, Test Loss = 0.247482, Learning Rate = 7.235926e-05\n",
      "Epoch 6912/20000: Train Loss = 0.442723, Test Loss = 0.248074, Learning Rate = 7.233177e-05\n",
      "Epoch 6913/20000: Train Loss = 0.442653, Test Loss = 0.249055, Learning Rate = 7.230428e-05\n",
      "Epoch 6914/20000: Train Loss = 0.442616, Test Loss = 0.247733, Learning Rate = 7.227681e-05\n",
      "Epoch 6915/20000: Train Loss = 0.442851, Test Loss = 0.246397, Learning Rate = 7.224935e-05\n",
      "Epoch 6916/20000: Train Loss = 0.442579, Test Loss = 0.249470, Learning Rate = 7.222189e-05\n",
      "Epoch 6917/20000: Train Loss = 0.442851, Test Loss = 0.249738, Learning Rate = 7.219445e-05\n",
      "Epoch 6918/20000: Train Loss = 0.442869, Test Loss = 0.248903, Learning Rate = 7.216702e-05\n",
      "Epoch 6919/20000: Train Loss = 0.442620, Test Loss = 0.250554, Learning Rate = 7.213960e-05\n",
      "Epoch 6920/20000: Train Loss = 0.442840, Test Loss = 0.248725, Learning Rate = 7.211219e-05\n",
      "Epoch 6921/20000: Train Loss = 0.442635, Test Loss = 0.248416, Learning Rate = 7.208479e-05\n",
      "Epoch 6922/20000: Train Loss = 0.442812, Test Loss = 0.246130, Learning Rate = 7.205740e-05\n",
      "Epoch 6923/20000: Train Loss = 0.442940, Test Loss = 0.248724, Learning Rate = 7.203002e-05\n",
      "Epoch 6924/20000: Train Loss = 0.442731, Test Loss = 0.248086, Learning Rate = 7.200265e-05\n",
      "Epoch 6925/20000: Train Loss = 0.442697, Test Loss = 0.247556, Learning Rate = 7.197529e-05\n",
      "Epoch 6926/20000: Train Loss = 0.442733, Test Loss = 0.247296, Learning Rate = 7.194794e-05\n",
      "Epoch 6927/20000: Train Loss = 0.442653, Test Loss = 0.249008, Learning Rate = 7.192060e-05\n",
      "Epoch 6928/20000: Train Loss = 0.443546, Test Loss = 0.246442, Learning Rate = 7.189327e-05\n",
      "Epoch 6929/20000: Train Loss = 0.443140, Test Loss = 0.244085, Learning Rate = 7.186596e-05\n",
      "Epoch 6930/20000: Train Loss = 0.442963, Test Loss = 0.245515, Learning Rate = 7.183865e-05\n",
      "Epoch 6931/20000: Train Loss = 0.442742, Test Loss = 0.245009, Learning Rate = 7.181135e-05\n",
      "Epoch 6932/20000: Train Loss = 0.443126, Test Loss = 0.251126, Learning Rate = 7.178407e-05\n",
      "Epoch 6933/20000: Train Loss = 0.442820, Test Loss = 0.245321, Learning Rate = 7.175679e-05\n",
      "Epoch 6934/20000: Train Loss = 0.442832, Test Loss = 0.247127, Learning Rate = 7.172952e-05\n",
      "Epoch 6935/20000: Train Loss = 0.442893, Test Loss = 0.248049, Learning Rate = 7.170227e-05\n",
      "Epoch 6936/20000: Train Loss = 0.442686, Test Loss = 0.249170, Learning Rate = 7.167502e-05\n",
      "Epoch 6937/20000: Train Loss = 0.442562, Test Loss = 0.245945, Learning Rate = 7.164779e-05\n",
      "Epoch 6938/20000: Train Loss = 0.442653, Test Loss = 0.247144, Learning Rate = 7.162057e-05\n",
      "Epoch 6939/20000: Train Loss = 0.442932, Test Loss = 0.245390, Learning Rate = 7.159335e-05\n",
      "Epoch 6940/20000: Train Loss = 0.442715, Test Loss = 0.247349, Learning Rate = 7.156615e-05\n",
      "Epoch 6941/20000: Train Loss = 0.442648, Test Loss = 0.245929, Learning Rate = 7.153895e-05\n",
      "Epoch 6942/20000: Train Loss = 0.442919, Test Loss = 0.243774, Learning Rate = 7.151177e-05\n",
      "Epoch 6943/20000: Train Loss = 0.442778, Test Loss = 0.246858, Learning Rate = 7.148460e-05\n",
      "Epoch 6944/20000: Train Loss = 0.442612, Test Loss = 0.247927, Learning Rate = 7.145744e-05\n",
      "Epoch 6945/20000: Train Loss = 0.442727, Test Loss = 0.249624, Learning Rate = 7.143029e-05\n",
      "Epoch 6946/20000: Train Loss = 0.442809, Test Loss = 0.245420, Learning Rate = 7.140314e-05\n",
      "Epoch 6947/20000: Train Loss = 0.442683, Test Loss = 0.251436, Learning Rate = 7.137601e-05\n",
      "Epoch 6948/20000: Train Loss = 0.442744, Test Loss = 0.247462, Learning Rate = 7.134889e-05\n",
      "Epoch 6949/20000: Train Loss = 0.442992, Test Loss = 0.251002, Learning Rate = 7.132178e-05\n",
      "Epoch 6950/20000: Train Loss = 0.442832, Test Loss = 0.248186, Learning Rate = 7.129468e-05\n",
      "Epoch 6951/20000: Train Loss = 0.442733, Test Loss = 0.248768, Learning Rate = 7.126759e-05\n",
      "Epoch 6952/20000: Train Loss = 0.442999, Test Loss = 0.252839, Learning Rate = 7.124051e-05\n",
      "Epoch 6953/20000: Train Loss = 0.442756, Test Loss = 0.248081, Learning Rate = 7.121344e-05\n",
      "Epoch 6954/20000: Train Loss = 0.442693, Test Loss = 0.250547, Learning Rate = 7.118638e-05\n",
      "Epoch 6955/20000: Train Loss = 0.442700, Test Loss = 0.249975, Learning Rate = 7.115933e-05\n",
      "Epoch 6956/20000: Train Loss = 0.442920, Test Loss = 0.247101, Learning Rate = 7.113229e-05\n",
      "Epoch 6957/20000: Train Loss = 0.442612, Test Loss = 0.250848, Learning Rate = 7.110527e-05\n",
      "Epoch 6958/20000: Train Loss = 0.442745, Test Loss = 0.251265, Learning Rate = 7.107825e-05\n",
      "Epoch 6959/20000: Train Loss = 0.442564, Test Loss = 0.249272, Learning Rate = 7.105124e-05\n",
      "Epoch 6960/20000: Train Loss = 0.443010, Test Loss = 0.250861, Learning Rate = 7.102424e-05\n",
      "Epoch 6961/20000: Train Loss = 0.442796, Test Loss = 0.246012, Learning Rate = 7.099726e-05\n",
      "Epoch 6962/20000: Train Loss = 0.442676, Test Loss = 0.248240, Learning Rate = 7.097028e-05\n",
      "Epoch 6963/20000: Train Loss = 0.442734, Test Loss = 0.249418, Learning Rate = 7.094331e-05\n",
      "Epoch 6964/20000: Train Loss = 0.442707, Test Loss = 0.251161, Learning Rate = 7.091636e-05\n",
      "Epoch 6965/20000: Train Loss = 0.442793, Test Loss = 0.248777, Learning Rate = 7.088941e-05\n",
      "Epoch 6966/20000: Train Loss = 0.442894, Test Loss = 0.251681, Learning Rate = 7.086247e-05\n",
      "Epoch 6967/20000: Train Loss = 0.442992, Test Loss = 0.246472, Learning Rate = 7.083555e-05\n",
      "Epoch 6968/20000: Train Loss = 0.443650, Test Loss = 0.243518, Learning Rate = 7.080863e-05\n",
      "Epoch 6969/20000: Train Loss = 0.443352, Test Loss = 0.250958, Learning Rate = 7.078173e-05\n",
      "Epoch 6970/20000: Train Loss = 0.442859, Test Loss = 0.249956, Learning Rate = 7.075483e-05\n",
      "Epoch 6971/20000: Train Loss = 0.442942, Test Loss = 0.249383, Learning Rate = 7.072795e-05\n",
      "Epoch 6972/20000: Train Loss = 0.442714, Test Loss = 0.253003, Learning Rate = 7.070107e-05\n",
      "Epoch 6973/20000: Train Loss = 0.442743, Test Loss = 0.251117, Learning Rate = 7.067421e-05\n",
      "Epoch 6974/20000: Train Loss = 0.442916, Test Loss = 0.250136, Learning Rate = 7.064735e-05\n",
      "Epoch 6975/20000: Train Loss = 0.442935, Test Loss = 0.251017, Learning Rate = 7.062051e-05\n",
      "Epoch 6976/20000: Train Loss = 0.442553, Test Loss = 0.247996, Learning Rate = 7.059367e-05\n",
      "Epoch 6977/20000: Train Loss = 0.442590, Test Loss = 0.244069, Learning Rate = 7.056685e-05\n",
      "Epoch 6978/20000: Train Loss = 0.442879, Test Loss = 0.247813, Learning Rate = 7.054004e-05\n",
      "Epoch 6979/20000: Train Loss = 0.442665, Test Loss = 0.248701, Learning Rate = 7.051323e-05\n",
      "Epoch 6980/20000: Train Loss = 0.442602, Test Loss = 0.249906, Learning Rate = 7.048644e-05\n",
      "Epoch 6981/20000: Train Loss = 0.443343, Test Loss = 0.246502, Learning Rate = 7.045966e-05\n",
      "Epoch 6982/20000: Train Loss = 0.442932, Test Loss = 0.250225, Learning Rate = 7.043289e-05\n",
      "Epoch 6983/20000: Train Loss = 0.442824, Test Loss = 0.252977, Learning Rate = 7.040612e-05\n",
      "Epoch 6984/20000: Train Loss = 0.442574, Test Loss = 0.249678, Learning Rate = 7.037937e-05\n",
      "Epoch 6985/20000: Train Loss = 0.442655, Test Loss = 0.248201, Learning Rate = 7.035263e-05\n",
      "Epoch 6986/20000: Train Loss = 0.442662, Test Loss = 0.248786, Learning Rate = 7.032590e-05\n",
      "Epoch 6987/20000: Train Loss = 0.442837, Test Loss = 0.247267, Learning Rate = 7.029917e-05\n",
      "Epoch 6988/20000: Train Loss = 0.443214, Test Loss = 0.246893, Learning Rate = 7.027246e-05\n",
      "Epoch 6989/20000: Train Loss = 0.443082, Test Loss = 0.245658, Learning Rate = 7.024576e-05\n",
      "Epoch 6990/20000: Train Loss = 0.443022, Test Loss = 0.251484, Learning Rate = 7.021907e-05\n",
      "Epoch 6991/20000: Train Loss = 0.442489, Test Loss = 0.246524, Learning Rate = 7.019239e-05\n",
      "Epoch 6992/20000: Train Loss = 0.442753, Test Loss = 0.245689, Learning Rate = 7.016572e-05\n",
      "Epoch 6993/20000: Train Loss = 0.442949, Test Loss = 0.248837, Learning Rate = 7.013906e-05\n",
      "Epoch 6994/20000: Train Loss = 0.442633, Test Loss = 0.244786, Learning Rate = 7.011240e-05\n",
      "Epoch 6995/20000: Train Loss = 0.442618, Test Loss = 0.245148, Learning Rate = 7.008576e-05\n",
      "Epoch 6996/20000: Train Loss = 0.442831, Test Loss = 0.242533, Learning Rate = 7.005913e-05\n",
      "Epoch 6997/20000: Train Loss = 0.443041, Test Loss = 0.248191, Learning Rate = 7.003251e-05\n",
      "Epoch 6998/20000: Train Loss = 0.442642, Test Loss = 0.247558, Learning Rate = 7.000590e-05\n",
      "Epoch 6999/20000: Train Loss = 0.442733, Test Loss = 0.248388, Learning Rate = 6.997930e-05\n",
      "Epoch 7000/20000: Train Loss = 0.442957, Test Loss = 0.249840, Learning Rate = 6.995271e-05\n",
      "Epoch 7001/20000: Train Loss = 0.442585, Test Loss = 0.247802, Learning Rate = 6.992613e-05\n",
      "Epoch 7002/20000: Train Loss = 0.443464, Test Loss = 0.247097, Learning Rate = 6.989956e-05\n",
      "Epoch 7003/20000: Train Loss = 0.442881, Test Loss = 0.251140, Learning Rate = 6.987300e-05\n",
      "Epoch 7004/20000: Train Loss = 0.442798, Test Loss = 0.245420, Learning Rate = 6.984645e-05\n",
      "Epoch 7005/20000: Train Loss = 0.443084, Test Loss = 0.247239, Learning Rate = 6.981991e-05\n",
      "Epoch 7006/20000: Train Loss = 0.443027, Test Loss = 0.250357, Learning Rate = 6.979338e-05\n",
      "Epoch 7007/20000: Train Loss = 0.442888, Test Loss = 0.251362, Learning Rate = 6.976686e-05\n",
      "Epoch 7008/20000: Train Loss = 0.442899, Test Loss = 0.247615, Learning Rate = 6.974035e-05\n",
      "Epoch 7009/20000: Train Loss = 0.442837, Test Loss = 0.246122, Learning Rate = 6.971385e-05\n",
      "Epoch 7010/20000: Train Loss = 0.442656, Test Loss = 0.247081, Learning Rate = 6.968736e-05\n",
      "Epoch 7011/20000: Train Loss = 0.442785, Test Loss = 0.249334, Learning Rate = 6.966088e-05\n",
      "Epoch 7012/20000: Train Loss = 0.442687, Test Loss = 0.248052, Learning Rate = 6.963442e-05\n",
      "Epoch 7013/20000: Train Loss = 0.442898, Test Loss = 0.251685, Learning Rate = 6.960796e-05\n",
      "Epoch 7014/20000: Train Loss = 0.442790, Test Loss = 0.248235, Learning Rate = 6.958151e-05\n",
      "Epoch 7015/20000: Train Loss = 0.442781, Test Loss = 0.245704, Learning Rate = 6.955507e-05\n",
      "Epoch 7016/20000: Train Loss = 0.443032, Test Loss = 0.247192, Learning Rate = 6.952864e-05\n",
      "Epoch 7017/20000: Train Loss = 0.442714, Test Loss = 0.248175, Learning Rate = 6.950222e-05\n",
      "Epoch 7018/20000: Train Loss = 0.442783, Test Loss = 0.245467, Learning Rate = 6.947581e-05\n",
      "Epoch 7019/20000: Train Loss = 0.442941, Test Loss = 0.251195, Learning Rate = 6.944941e-05\n",
      "Epoch 7020/20000: Train Loss = 0.442789, Test Loss = 0.247000, Learning Rate = 6.942302e-05\n",
      "Epoch 7021/20000: Train Loss = 0.442957, Test Loss = 0.251337, Learning Rate = 6.939664e-05\n",
      "Epoch 7022/20000: Train Loss = 0.442805, Test Loss = 0.248972, Learning Rate = 6.937028e-05\n",
      "Epoch 7023/20000: Train Loss = 0.442826, Test Loss = 0.251929, Learning Rate = 6.934392e-05\n",
      "Epoch 7024/20000: Train Loss = 0.442768, Test Loss = 0.251820, Learning Rate = 6.931757e-05\n",
      "Epoch 7025/20000: Train Loss = 0.442743, Test Loss = 0.250469, Learning Rate = 6.929123e-05\n",
      "Epoch 7026/20000: Train Loss = 0.442692, Test Loss = 0.248325, Learning Rate = 6.926490e-05\n",
      "Epoch 7027/20000: Train Loss = 0.442909, Test Loss = 0.248370, Learning Rate = 6.923858e-05\n",
      "Epoch 7028/20000: Train Loss = 0.442585, Test Loss = 0.245980, Learning Rate = 6.921227e-05\n",
      "Epoch 7029/20000: Train Loss = 0.442925, Test Loss = 0.246981, Learning Rate = 6.918597e-05\n",
      "Epoch 7030/20000: Train Loss = 0.442801, Test Loss = 0.248426, Learning Rate = 6.915969e-05\n",
      "Epoch 7031/20000: Train Loss = 0.442662, Test Loss = 0.249819, Learning Rate = 6.913341e-05\n",
      "Epoch 7032/20000: Train Loss = 0.442827, Test Loss = 0.249543, Learning Rate = 6.910714e-05\n",
      "Epoch 7033/20000: Train Loss = 0.442909, Test Loss = 0.249519, Learning Rate = 6.908088e-05\n",
      "Epoch 7034/20000: Train Loss = 0.442754, Test Loss = 0.249209, Learning Rate = 6.905463e-05\n",
      "Epoch 7035/20000: Train Loss = 0.442764, Test Loss = 0.247664, Learning Rate = 6.902839e-05\n",
      "Epoch 7036/20000: Train Loss = 0.442751, Test Loss = 0.248642, Learning Rate = 6.900216e-05\n",
      "Epoch 7037/20000: Train Loss = 0.443034, Test Loss = 0.247692, Learning Rate = 6.897594e-05\n",
      "Epoch 7038/20000: Train Loss = 0.443608, Test Loss = 0.252418, Learning Rate = 6.894973e-05\n",
      "Epoch 7039/20000: Train Loss = 0.442827, Test Loss = 0.246995, Learning Rate = 6.892354e-05\n",
      "Epoch 7040/20000: Train Loss = 0.442679, Test Loss = 0.245539, Learning Rate = 6.889735e-05\n",
      "Epoch 7041/20000: Train Loss = 0.442615, Test Loss = 0.247430, Learning Rate = 6.887117e-05\n",
      "Epoch 7042/20000: Train Loss = 0.442771, Test Loss = 0.250430, Learning Rate = 6.884500e-05\n",
      "Epoch 7043/20000: Train Loss = 0.442775, Test Loss = 0.249270, Learning Rate = 6.881884e-05\n",
      "Epoch 7044/20000: Train Loss = 0.442939, Test Loss = 0.251002, Learning Rate = 6.879269e-05\n",
      "Epoch 7045/20000: Train Loss = 0.442820, Test Loss = 0.247900, Learning Rate = 6.876655e-05\n",
      "Epoch 7046/20000: Train Loss = 0.442715, Test Loss = 0.245220, Learning Rate = 6.874042e-05\n",
      "Epoch 7047/20000: Train Loss = 0.442774, Test Loss = 0.246246, Learning Rate = 6.871430e-05\n",
      "Epoch 7048/20000: Train Loss = 0.442765, Test Loss = 0.246477, Learning Rate = 6.868819e-05\n",
      "Epoch 7049/20000: Train Loss = 0.442735, Test Loss = 0.245143, Learning Rate = 6.866209e-05\n",
      "Epoch 7050/20000: Train Loss = 0.442805, Test Loss = 0.241687, Learning Rate = 6.863600e-05\n",
      "Epoch 7051/20000: Train Loss = 0.442539, Test Loss = 0.248341, Learning Rate = 6.860992e-05\n",
      "Epoch 7052/20000: Train Loss = 0.442716, Test Loss = 0.250856, Learning Rate = 6.858385e-05\n",
      "Epoch 7053/20000: Train Loss = 0.443097, Test Loss = 0.245786, Learning Rate = 6.855779e-05\n",
      "Epoch 7054/20000: Train Loss = 0.443265, Test Loss = 0.247584, Learning Rate = 6.853174e-05\n",
      "Epoch 7055/20000: Train Loss = 0.442766, Test Loss = 0.242987, Learning Rate = 6.850570e-05\n",
      "Epoch 7056/20000: Train Loss = 0.442810, Test Loss = 0.244690, Learning Rate = 6.847967e-05\n",
      "Epoch 7057/20000: Train Loss = 0.442868, Test Loss = 0.245560, Learning Rate = 6.845365e-05\n",
      "Epoch 7058/20000: Train Loss = 0.442669, Test Loss = 0.246488, Learning Rate = 6.842764e-05\n",
      "Epoch 7059/20000: Train Loss = 0.442602, Test Loss = 0.247571, Learning Rate = 6.840164e-05\n",
      "Epoch 7060/20000: Train Loss = 0.442890, Test Loss = 0.247914, Learning Rate = 6.837565e-05\n",
      "Epoch 7061/20000: Train Loss = 0.442643, Test Loss = 0.250400, Learning Rate = 6.834967e-05\n",
      "Epoch 7062/20000: Train Loss = 0.443088, Test Loss = 0.246541, Learning Rate = 6.832370e-05\n",
      "Epoch 7063/20000: Train Loss = 0.443222, Test Loss = 0.248186, Learning Rate = 6.829774e-05\n",
      "Epoch 7064/20000: Train Loss = 0.442730, Test Loss = 0.249101, Learning Rate = 6.827178e-05\n",
      "Epoch 7065/20000: Train Loss = 0.442939, Test Loss = 0.246021, Learning Rate = 6.824584e-05\n",
      "Epoch 7066/20000: Train Loss = 0.442866, Test Loss = 0.246748, Learning Rate = 6.821991e-05\n",
      "Epoch 7067/20000: Train Loss = 0.442970, Test Loss = 0.245037, Learning Rate = 6.819399e-05\n",
      "Epoch 7068/20000: Train Loss = 0.442739, Test Loss = 0.247278, Learning Rate = 6.816808e-05\n",
      "Epoch 7069/20000: Train Loss = 0.442525, Test Loss = 0.246003, Learning Rate = 6.814218e-05\n",
      "Epoch 7070/20000: Train Loss = 0.442877, Test Loss = 0.248378, Learning Rate = 6.811628e-05\n",
      "Epoch 7071/20000: Train Loss = 0.442627, Test Loss = 0.243951, Learning Rate = 6.809040e-05\n",
      "Epoch 7072/20000: Train Loss = 0.442838, Test Loss = 0.244704, Learning Rate = 6.806453e-05\n",
      "Epoch 7073/20000: Train Loss = 0.442542, Test Loss = 0.246382, Learning Rate = 6.803867e-05\n",
      "Epoch 7074/20000: Train Loss = 0.442588, Test Loss = 0.250843, Learning Rate = 6.801281e-05\n",
      "Epoch 7075/20000: Train Loss = 0.442761, Test Loss = 0.247450, Learning Rate = 6.798697e-05\n",
      "Epoch 7076/20000: Train Loss = 0.442650, Test Loss = 0.252169, Learning Rate = 6.796114e-05\n",
      "Epoch 7077/20000: Train Loss = 0.443279, Test Loss = 0.244783, Learning Rate = 6.793531e-05\n",
      "Epoch 7078/20000: Train Loss = 0.442666, Test Loss = 0.245304, Learning Rate = 6.790950e-05\n",
      "Epoch 7079/20000: Train Loss = 0.442657, Test Loss = 0.247446, Learning Rate = 6.788370e-05\n",
      "Epoch 7080/20000: Train Loss = 0.442617, Test Loss = 0.250517, Learning Rate = 6.785790e-05\n",
      "Epoch 7081/20000: Train Loss = 0.443186, Test Loss = 0.247109, Learning Rate = 6.783212e-05\n",
      "Epoch 7082/20000: Train Loss = 0.442628, Test Loss = 0.245241, Learning Rate = 6.780634e-05\n",
      "Epoch 7083/20000: Train Loss = 0.442841, Test Loss = 0.244976, Learning Rate = 6.778058e-05\n",
      "Epoch 7084/20000: Train Loss = 0.442841, Test Loss = 0.244238, Learning Rate = 6.775482e-05\n",
      "Epoch 7085/20000: Train Loss = 0.442797, Test Loss = 0.244744, Learning Rate = 6.772908e-05\n",
      "Epoch 7086/20000: Train Loss = 0.443151, Test Loss = 0.243979, Learning Rate = 6.770334e-05\n",
      "Epoch 7087/20000: Train Loss = 0.442782, Test Loss = 0.245270, Learning Rate = 6.767762e-05\n",
      "Epoch 7088/20000: Train Loss = 0.442661, Test Loss = 0.246833, Learning Rate = 6.765190e-05\n",
      "Epoch 7089/20000: Train Loss = 0.443020, Test Loss = 0.247862, Learning Rate = 6.762620e-05\n",
      "Epoch 7090/20000: Train Loss = 0.442686, Test Loss = 0.245921, Learning Rate = 6.760050e-05\n",
      "Epoch 7091/20000: Train Loss = 0.442763, Test Loss = 0.244682, Learning Rate = 6.757482e-05\n",
      "Epoch 7092/20000: Train Loss = 0.442625, Test Loss = 0.247048, Learning Rate = 6.754914e-05\n",
      "Epoch 7093/20000: Train Loss = 0.442611, Test Loss = 0.246620, Learning Rate = 6.752347e-05\n",
      "Epoch 7094/20000: Train Loss = 0.442737, Test Loss = 0.242662, Learning Rate = 6.749781e-05\n",
      "Epoch 7095/20000: Train Loss = 0.442773, Test Loss = 0.243807, Learning Rate = 6.747217e-05\n",
      "Epoch 7096/20000: Train Loss = 0.442745, Test Loss = 0.241673, Learning Rate = 6.744653e-05\n",
      "Epoch 7097/20000: Train Loss = 0.442613, Test Loss = 0.245147, Learning Rate = 6.742090e-05\n",
      "Epoch 7098/20000: Train Loss = 0.442922, Test Loss = 0.243131, Learning Rate = 6.739528e-05\n",
      "Epoch 7099/20000: Train Loss = 0.442856, Test Loss = 0.249226, Learning Rate = 6.736968e-05\n",
      "Epoch 7100/20000: Train Loss = 0.442593, Test Loss = 0.245893, Learning Rate = 6.734408e-05\n",
      "Epoch 7101/20000: Train Loss = 0.442911, Test Loss = 0.243932, Learning Rate = 6.731849e-05\n",
      "Epoch 7102/20000: Train Loss = 0.442878, Test Loss = 0.244684, Learning Rate = 6.729291e-05\n",
      "Epoch 7103/20000: Train Loss = 0.442849, Test Loss = 0.245747, Learning Rate = 6.726734e-05\n",
      "Epoch 7104/20000: Train Loss = 0.442633, Test Loss = 0.243703, Learning Rate = 6.724178e-05\n",
      "Epoch 7105/20000: Train Loss = 0.442620, Test Loss = 0.247309, Learning Rate = 6.721623e-05\n",
      "Epoch 7106/20000: Train Loss = 0.442823, Test Loss = 0.246764, Learning Rate = 6.719069e-05\n",
      "Epoch 7107/20000: Train Loss = 0.442630, Test Loss = 0.245703, Learning Rate = 6.716516e-05\n",
      "Epoch 7108/20000: Train Loss = 0.442879, Test Loss = 0.247620, Learning Rate = 6.713964e-05\n",
      "Epoch 7109/20000: Train Loss = 0.442864, Test Loss = 0.246496, Learning Rate = 6.711413e-05\n",
      "Epoch 7110/20000: Train Loss = 0.442661, Test Loss = 0.248930, Learning Rate = 6.708862e-05\n",
      "Epoch 7111/20000: Train Loss = 0.442889, Test Loss = 0.245496, Learning Rate = 6.706313e-05\n",
      "Epoch 7112/20000: Train Loss = 0.442743, Test Loss = 0.249674, Learning Rate = 6.703765e-05\n",
      "Epoch 7113/20000: Train Loss = 0.442659, Test Loss = 0.246636, Learning Rate = 6.701218e-05\n",
      "Epoch 7114/20000: Train Loss = 0.442861, Test Loss = 0.246364, Learning Rate = 6.698672e-05\n",
      "Epoch 7115/20000: Train Loss = 0.442675, Test Loss = 0.245880, Learning Rate = 6.696126e-05\n",
      "Epoch 7116/20000: Train Loss = 0.442762, Test Loss = 0.244416, Learning Rate = 6.693582e-05\n",
      "Epoch 7117/20000: Train Loss = 0.443018, Test Loss = 0.249519, Learning Rate = 6.691038e-05\n",
      "Epoch 7118/20000: Train Loss = 0.442685, Test Loss = 0.246280, Learning Rate = 6.688496e-05\n",
      "Epoch 7119/20000: Train Loss = 0.443087, Test Loss = 0.248525, Learning Rate = 6.685955e-05\n",
      "Epoch 7120/20000: Train Loss = 0.442700, Test Loss = 0.249756, Learning Rate = 6.683414e-05\n",
      "Epoch 7121/20000: Train Loss = 0.442655, Test Loss = 0.248060, Learning Rate = 6.680875e-05\n",
      "Epoch 7122/20000: Train Loss = 0.442813, Test Loss = 0.246168, Learning Rate = 6.678336e-05\n",
      "Epoch 7123/20000: Train Loss = 0.442914, Test Loss = 0.245242, Learning Rate = 6.675798e-05\n",
      "Epoch 7124/20000: Train Loss = 0.442852, Test Loss = 0.247309, Learning Rate = 6.673262e-05\n",
      "Epoch 7125/20000: Train Loss = 0.442701, Test Loss = 0.246173, Learning Rate = 6.670726e-05\n",
      "Epoch 7126/20000: Train Loss = 0.442654, Test Loss = 0.245330, Learning Rate = 6.668192e-05\n",
      "Epoch 7127/20000: Train Loss = 0.442955, Test Loss = 0.246903, Learning Rate = 6.665658e-05\n",
      "Epoch 7128/20000: Train Loss = 0.442625, Test Loss = 0.245671, Learning Rate = 6.663125e-05\n",
      "Epoch 7129/20000: Train Loss = 0.442834, Test Loss = 0.252124, Learning Rate = 6.660593e-05\n",
      "Epoch 7130/20000: Train Loss = 0.442864, Test Loss = 0.248315, Learning Rate = 6.658062e-05\n",
      "Epoch 7131/20000: Train Loss = 0.442989, Test Loss = 0.248824, Learning Rate = 6.655532e-05\n",
      "Epoch 7132/20000: Train Loss = 0.442866, Test Loss = 0.249692, Learning Rate = 6.653004e-05\n",
      "Epoch 7133/20000: Train Loss = 0.442771, Test Loss = 0.248516, Learning Rate = 6.650476e-05\n",
      "Epoch 7134/20000: Train Loss = 0.443217, Test Loss = 0.246024, Learning Rate = 6.647949e-05\n",
      "Epoch 7135/20000: Train Loss = 0.442410, Test Loss = 0.249848, Learning Rate = 6.645423e-05\n",
      "Epoch 7136/20000: Train Loss = 0.442948, Test Loss = 0.247585, Learning Rate = 6.642897e-05\n",
      "Epoch 7137/20000: Train Loss = 0.442624, Test Loss = 0.248621, Learning Rate = 6.640373e-05\n",
      "Epoch 7138/20000: Train Loss = 0.442807, Test Loss = 0.249858, Learning Rate = 6.637850e-05\n",
      "Epoch 7139/20000: Train Loss = 0.443040, Test Loss = 0.243260, Learning Rate = 6.635328e-05\n",
      "Epoch 7140/20000: Train Loss = 0.442801, Test Loss = 0.247813, Learning Rate = 6.632807e-05\n",
      "Epoch 7141/20000: Train Loss = 0.442745, Test Loss = 0.249425, Learning Rate = 6.630286e-05\n",
      "Epoch 7142/20000: Train Loss = 0.442664, Test Loss = 0.245807, Learning Rate = 6.627767e-05\n",
      "Epoch 7143/20000: Train Loss = 0.442827, Test Loss = 0.245456, Learning Rate = 6.625249e-05\n",
      "Epoch 7144/20000: Train Loss = 0.442874, Test Loss = 0.247349, Learning Rate = 6.622731e-05\n",
      "Epoch 7145/20000: Train Loss = 0.442625, Test Loss = 0.247547, Learning Rate = 6.620215e-05\n",
      "Epoch 7146/20000: Train Loss = 0.442876, Test Loss = 0.247951, Learning Rate = 6.617699e-05\n",
      "Epoch 7147/20000: Train Loss = 0.442770, Test Loss = 0.245990, Learning Rate = 6.615185e-05\n",
      "Epoch 7148/20000: Train Loss = 0.442756, Test Loss = 0.247154, Learning Rate = 6.612671e-05\n",
      "Epoch 7149/20000: Train Loss = 0.442696, Test Loss = 0.247906, Learning Rate = 6.610159e-05\n",
      "Epoch 7150/20000: Train Loss = 0.442895, Test Loss = 0.245713, Learning Rate = 6.607647e-05\n",
      "Epoch 7151/20000: Train Loss = 0.443026, Test Loss = 0.245698, Learning Rate = 6.605136e-05\n",
      "Epoch 7152/20000: Train Loss = 0.443242, Test Loss = 0.252674, Learning Rate = 6.602626e-05\n",
      "Epoch 7153/20000: Train Loss = 0.442921, Test Loss = 0.248290, Learning Rate = 6.600118e-05\n",
      "Epoch 7154/20000: Train Loss = 0.442611, Test Loss = 0.250279, Learning Rate = 6.597610e-05\n",
      "Epoch 7155/20000: Train Loss = 0.442690, Test Loss = 0.248538, Learning Rate = 6.595103e-05\n",
      "Epoch 7156/20000: Train Loss = 0.442667, Test Loss = 0.249414, Learning Rate = 6.592597e-05\n",
      "Epoch 7157/20000: Train Loss = 0.443073, Test Loss = 0.249857, Learning Rate = 6.590092e-05\n",
      "Epoch 7158/20000: Train Loss = 0.442883, Test Loss = 0.253651, Learning Rate = 6.587588e-05\n",
      "Epoch 7159/20000: Train Loss = 0.442886, Test Loss = 0.246953, Learning Rate = 6.585085e-05\n",
      "Epoch 7160/20000: Train Loss = 0.442822, Test Loss = 0.248874, Learning Rate = 6.582583e-05\n",
      "Epoch 7161/20000: Train Loss = 0.442714, Test Loss = 0.249238, Learning Rate = 6.580081e-05\n",
      "Epoch 7162/20000: Train Loss = 0.442806, Test Loss = 0.246989, Learning Rate = 6.577581e-05\n",
      "Epoch 7163/20000: Train Loss = 0.442822, Test Loss = 0.247163, Learning Rate = 6.575082e-05\n",
      "Epoch 7164/20000: Train Loss = 0.442765, Test Loss = 0.246633, Learning Rate = 6.572583e-05\n",
      "Epoch 7165/20000: Train Loss = 0.442879, Test Loss = 0.246153, Learning Rate = 6.570086e-05\n",
      "Epoch 7166/20000: Train Loss = 0.442861, Test Loss = 0.242991, Learning Rate = 6.567590e-05\n",
      "Epoch 7167/20000: Train Loss = 0.442645, Test Loss = 0.246601, Learning Rate = 6.565094e-05\n",
      "Epoch 7168/20000: Train Loss = 0.442684, Test Loss = 0.243289, Learning Rate = 6.562599e-05\n",
      "Epoch 7169/20000: Train Loss = 0.442932, Test Loss = 0.247947, Learning Rate = 6.560106e-05\n",
      "Epoch 7170/20000: Train Loss = 0.442812, Test Loss = 0.249716, Learning Rate = 6.557613e-05\n",
      "Epoch 7171/20000: Train Loss = 0.443033, Test Loss = 0.247508, Learning Rate = 6.555122e-05\n",
      "Epoch 7172/20000: Train Loss = 0.442854, Test Loss = 0.243189, Learning Rate = 6.552631e-05\n",
      "Epoch 7173/20000: Train Loss = 0.442847, Test Loss = 0.246275, Learning Rate = 6.550141e-05\n",
      "Epoch 7174/20000: Train Loss = 0.442488, Test Loss = 0.244419, Learning Rate = 6.547652e-05\n",
      "Epoch 7175/20000: Train Loss = 0.442891, Test Loss = 0.247272, Learning Rate = 6.545164e-05\n",
      "Epoch 7176/20000: Train Loss = 0.443010, Test Loss = 0.245604, Learning Rate = 6.542677e-05\n",
      "Epoch 7177/20000: Train Loss = 0.442821, Test Loss = 0.244462, Learning Rate = 6.540191e-05\n",
      "Epoch 7178/20000: Train Loss = 0.442700, Test Loss = 0.245980, Learning Rate = 6.537706e-05\n",
      "Epoch 7179/20000: Train Loss = 0.442805, Test Loss = 0.251759, Learning Rate = 6.535222e-05\n",
      "Epoch 7180/20000: Train Loss = 0.442851, Test Loss = 0.249449, Learning Rate = 6.532739e-05\n",
      "Epoch 7181/20000: Train Loss = 0.443088, Test Loss = 0.247927, Learning Rate = 6.530256e-05\n",
      "Epoch 7182/20000: Train Loss = 0.442817, Test Loss = 0.243695, Learning Rate = 6.527775e-05\n",
      "Epoch 7183/20000: Train Loss = 0.442735, Test Loss = 0.245903, Learning Rate = 6.525295e-05\n",
      "Epoch 7184/20000: Train Loss = 0.442657, Test Loss = 0.249713, Learning Rate = 6.522815e-05\n",
      "Epoch 7185/20000: Train Loss = 0.442839, Test Loss = 0.247784, Learning Rate = 6.520337e-05\n",
      "Epoch 7186/20000: Train Loss = 0.442651, Test Loss = 0.251686, Learning Rate = 6.517859e-05\n",
      "Epoch 7187/20000: Train Loss = 0.442825, Test Loss = 0.249078, Learning Rate = 6.515383e-05\n",
      "Epoch 7188/20000: Train Loss = 0.443150, Test Loss = 0.245092, Learning Rate = 6.512907e-05\n",
      "Epoch 7189/20000: Train Loss = 0.442640, Test Loss = 0.249099, Learning Rate = 6.510432e-05\n",
      "Epoch 7190/20000: Train Loss = 0.442620, Test Loss = 0.247396, Learning Rate = 6.507958e-05\n",
      "Epoch 7191/20000: Train Loss = 0.442657, Test Loss = 0.245824, Learning Rate = 6.505486e-05\n",
      "Epoch 7192/20000: Train Loss = 0.442713, Test Loss = 0.245688, Learning Rate = 6.503014e-05\n",
      "Epoch 7193/20000: Train Loss = 0.443053, Test Loss = 0.243834, Learning Rate = 6.500543e-05\n",
      "Epoch 7194/20000: Train Loss = 0.443813, Test Loss = 0.244543, Learning Rate = 6.498073e-05\n",
      "Epoch 7195/20000: Train Loss = 0.443093, Test Loss = 0.243646, Learning Rate = 6.495604e-05\n",
      "Epoch 7196/20000: Train Loss = 0.442886, Test Loss = 0.244647, Learning Rate = 6.493135e-05\n",
      "Epoch 7197/20000: Train Loss = 0.442758, Test Loss = 0.245074, Learning Rate = 6.490668e-05\n",
      "Epoch 7198/20000: Train Loss = 0.442919, Test Loss = 0.244199, Learning Rate = 6.488202e-05\n",
      "Epoch 7199/20000: Train Loss = 0.442575, Test Loss = 0.247129, Learning Rate = 6.485737e-05\n",
      "Epoch 7200/20000: Train Loss = 0.442640, Test Loss = 0.248504, Learning Rate = 6.483272e-05\n",
      "Epoch 7201/20000: Train Loss = 0.442668, Test Loss = 0.245122, Learning Rate = 6.480809e-05\n",
      "Epoch 7202/20000: Train Loss = 0.442581, Test Loss = 0.244104, Learning Rate = 6.478346e-05\n",
      "Epoch 7203/20000: Train Loss = 0.442994, Test Loss = 0.240978, Learning Rate = 6.475885e-05\n",
      "Epoch 7204/20000: Train Loss = 0.442705, Test Loss = 0.244904, Learning Rate = 6.473424e-05\n",
      "Epoch 7205/20000: Train Loss = 0.442506, Test Loss = 0.250288, Learning Rate = 6.470964e-05\n",
      "Epoch 7206/20000: Train Loss = 0.442665, Test Loss = 0.249512, Learning Rate = 6.468505e-05\n",
      "Epoch 7207/20000: Train Loss = 0.442717, Test Loss = 0.249384, Learning Rate = 6.466048e-05\n",
      "Epoch 7208/20000: Train Loss = 0.442910, Test Loss = 0.244932, Learning Rate = 6.463591e-05\n",
      "Epoch 7209/20000: Train Loss = 0.442596, Test Loss = 0.244153, Learning Rate = 6.461135e-05\n",
      "Epoch 7210/20000: Train Loss = 0.442643, Test Loss = 0.245640, Learning Rate = 6.458680e-05\n",
      "Epoch 7211/20000: Train Loss = 0.442565, Test Loss = 0.246368, Learning Rate = 6.456225e-05\n",
      "Epoch 7212/20000: Train Loss = 0.442884, Test Loss = 0.246552, Learning Rate = 6.453772e-05\n",
      "Epoch 7213/20000: Train Loss = 0.442847, Test Loss = 0.244266, Learning Rate = 6.451320e-05\n",
      "Epoch 7214/20000: Train Loss = 0.442812, Test Loss = 0.249615, Learning Rate = 6.448869e-05\n",
      "Epoch 7215/20000: Train Loss = 0.442810, Test Loss = 0.250317, Learning Rate = 6.446418e-05\n",
      "Epoch 7216/20000: Train Loss = 0.442604, Test Loss = 0.246905, Learning Rate = 6.443969e-05\n",
      "Epoch 7217/20000: Train Loss = 0.442671, Test Loss = 0.246634, Learning Rate = 6.441520e-05\n",
      "Epoch 7218/20000: Train Loss = 0.442792, Test Loss = 0.245335, Learning Rate = 6.439073e-05\n",
      "Epoch 7219/20000: Train Loss = 0.442634, Test Loss = 0.247404, Learning Rate = 6.436626e-05\n",
      "Epoch 7220/20000: Train Loss = 0.443099, Test Loss = 0.245687, Learning Rate = 6.434180e-05\n",
      "Epoch 7221/20000: Train Loss = 0.442859, Test Loss = 0.250137, Learning Rate = 6.431735e-05\n",
      "Epoch 7222/20000: Train Loss = 0.442743, Test Loss = 0.247103, Learning Rate = 6.429292e-05\n",
      "Epoch 7223/20000: Train Loss = 0.442686, Test Loss = 0.248175, Learning Rate = 6.426849e-05\n",
      "Epoch 7224/20000: Train Loss = 0.443459, Test Loss = 0.245130, Learning Rate = 6.424407e-05\n",
      "Epoch 7225/20000: Train Loss = 0.442660, Test Loss = 0.248840, Learning Rate = 6.421965e-05\n",
      "Epoch 7226/20000: Train Loss = 0.443024, Test Loss = 0.246759, Learning Rate = 6.419525e-05\n",
      "Epoch 7227/20000: Train Loss = 0.442710, Test Loss = 0.248235, Learning Rate = 6.417086e-05\n",
      "Epoch 7228/20000: Train Loss = 0.442753, Test Loss = 0.247876, Learning Rate = 6.414648e-05\n",
      "Epoch 7229/20000: Train Loss = 0.442833, Test Loss = 0.245805, Learning Rate = 6.412210e-05\n",
      "Epoch 7230/20000: Train Loss = 0.442735, Test Loss = 0.247141, Learning Rate = 6.409774e-05\n",
      "Epoch 7231/20000: Train Loss = 0.442861, Test Loss = 0.244913, Learning Rate = 6.407338e-05\n",
      "Epoch 7232/20000: Train Loss = 0.443539, Test Loss = 0.253206, Learning Rate = 6.404904e-05\n",
      "Epoch 7233/20000: Train Loss = 0.442865, Test Loss = 0.246498, Learning Rate = 6.402470e-05\n",
      "Epoch 7234/20000: Train Loss = 0.442922, Test Loss = 0.249411, Learning Rate = 6.400037e-05\n",
      "Epoch 7235/20000: Train Loss = 0.442773, Test Loss = 0.246186, Learning Rate = 6.397605e-05\n",
      "Epoch 7236/20000: Train Loss = 0.442665, Test Loss = 0.245042, Learning Rate = 6.395174e-05\n",
      "Epoch 7237/20000: Train Loss = 0.442778, Test Loss = 0.246751, Learning Rate = 6.392745e-05\n",
      "Epoch 7238/20000: Train Loss = 0.442799, Test Loss = 0.243979, Learning Rate = 6.390315e-05\n",
      "Epoch 7239/20000: Train Loss = 0.442658, Test Loss = 0.245933, Learning Rate = 6.387887e-05\n",
      "Epoch 7240/20000: Train Loss = 0.443059, Test Loss = 0.245163, Learning Rate = 6.385460e-05\n",
      "Epoch 7241/20000: Train Loss = 0.443057, Test Loss = 0.250852, Learning Rate = 6.383034e-05\n",
      "Epoch 7242/20000: Train Loss = 0.442716, Test Loss = 0.250023, Learning Rate = 6.380608e-05\n",
      "Epoch 7243/20000: Train Loss = 0.442672, Test Loss = 0.246373, Learning Rate = 6.378184e-05\n",
      "Epoch 7244/20000: Train Loss = 0.442907, Test Loss = 0.248077, Learning Rate = 6.375760e-05\n",
      "Epoch 7245/20000: Train Loss = 0.442915, Test Loss = 0.245422, Learning Rate = 6.373338e-05\n",
      "Epoch 7246/20000: Train Loss = 0.443204, Test Loss = 0.244825, Learning Rate = 6.370916e-05\n",
      "Epoch 7247/20000: Train Loss = 0.442861, Test Loss = 0.250853, Learning Rate = 6.368495e-05\n",
      "Epoch 7248/20000: Train Loss = 0.442639, Test Loss = 0.247536, Learning Rate = 6.366075e-05\n",
      "Epoch 7249/20000: Train Loss = 0.442977, Test Loss = 0.251777, Learning Rate = 6.363657e-05\n",
      "Epoch 7250/20000: Train Loss = 0.443187, Test Loss = 0.243988, Learning Rate = 6.361238e-05\n",
      "Epoch 7251/20000: Train Loss = 0.442941, Test Loss = 0.248455, Learning Rate = 6.358821e-05\n",
      "Epoch 7252/20000: Train Loss = 0.442860, Test Loss = 0.243831, Learning Rate = 6.356405e-05\n",
      "Epoch 7253/20000: Train Loss = 0.442632, Test Loss = 0.246009, Learning Rate = 6.353990e-05\n",
      "Epoch 7254/20000: Train Loss = 0.442740, Test Loss = 0.247337, Learning Rate = 6.351576e-05\n",
      "Epoch 7255/20000: Train Loss = 0.442938, Test Loss = 0.246953, Learning Rate = 6.349162e-05\n",
      "Epoch 7256/20000: Train Loss = 0.442805, Test Loss = 0.250654, Learning Rate = 6.346750e-05\n",
      "Epoch 7257/20000: Train Loss = 0.442598, Test Loss = 0.247446, Learning Rate = 6.344338e-05\n",
      "Epoch 7258/20000: Train Loss = 0.442550, Test Loss = 0.248833, Learning Rate = 6.341927e-05\n",
      "Epoch 7259/20000: Train Loss = 0.442861, Test Loss = 0.249520, Learning Rate = 6.339518e-05\n",
      "Epoch 7260/20000: Train Loss = 0.442624, Test Loss = 0.248886, Learning Rate = 6.337109e-05\n",
      "Epoch 7261/20000: Train Loss = 0.442813, Test Loss = 0.249877, Learning Rate = 6.334701e-05\n",
      "Epoch 7262/20000: Train Loss = 0.442669, Test Loss = 0.251530, Learning Rate = 6.332294e-05\n",
      "Epoch 7263/20000: Train Loss = 0.442836, Test Loss = 0.250279, Learning Rate = 6.329888e-05\n",
      "Epoch 7264/20000: Train Loss = 0.442587, Test Loss = 0.250736, Learning Rate = 6.327483e-05\n",
      "Epoch 7265/20000: Train Loss = 0.443147, Test Loss = 0.249893, Learning Rate = 6.325078e-05\n",
      "Epoch 7266/20000: Train Loss = 0.442646, Test Loss = 0.253162, Learning Rate = 6.322675e-05\n",
      "Epoch 7267/20000: Train Loss = 0.442643, Test Loss = 0.250914, Learning Rate = 6.320272e-05\n",
      "Epoch 7268/20000: Train Loss = 0.442657, Test Loss = 0.250776, Learning Rate = 6.317871e-05\n",
      "Epoch 7269/20000: Train Loss = 0.443075, Test Loss = 0.246163, Learning Rate = 6.315470e-05\n",
      "Epoch 7270/20000: Train Loss = 0.442708, Test Loss = 0.245328, Learning Rate = 6.313071e-05\n",
      "Epoch 7271/20000: Train Loss = 0.442886, Test Loss = 0.242298, Learning Rate = 6.310672e-05\n",
      "Epoch 7272/20000: Train Loss = 0.442943, Test Loss = 0.241767, Learning Rate = 6.308274e-05\n",
      "Epoch 7273/20000: Train Loss = 0.442631, Test Loss = 0.249775, Learning Rate = 6.305877e-05\n",
      "Epoch 7274/20000: Train Loss = 0.442736, Test Loss = 0.249528, Learning Rate = 6.303481e-05\n",
      "Epoch 7275/20000: Train Loss = 0.442712, Test Loss = 0.249341, Learning Rate = 6.301086e-05\n",
      "Epoch 7276/20000: Train Loss = 0.442785, Test Loss = 0.248620, Learning Rate = 6.298692e-05\n",
      "Epoch 7277/20000: Train Loss = 0.442875, Test Loss = 0.250002, Learning Rate = 6.296298e-05\n",
      "Epoch 7278/20000: Train Loss = 0.442578, Test Loss = 0.248922, Learning Rate = 6.293906e-05\n",
      "Epoch 7279/20000: Train Loss = 0.443063, Test Loss = 0.248737, Learning Rate = 6.291514e-05\n",
      "Epoch 7280/20000: Train Loss = 0.442909, Test Loss = 0.253529, Learning Rate = 6.289124e-05\n",
      "Epoch 7281/20000: Train Loss = 0.442805, Test Loss = 0.247429, Learning Rate = 6.286734e-05\n",
      "Epoch 7282/20000: Train Loss = 0.442663, Test Loss = 0.249618, Learning Rate = 6.284345e-05\n",
      "Epoch 7283/20000: Train Loss = 0.442648, Test Loss = 0.250845, Learning Rate = 6.281957e-05\n",
      "Epoch 7284/20000: Train Loss = 0.442666, Test Loss = 0.251813, Learning Rate = 6.279570e-05\n",
      "Epoch 7285/20000: Train Loss = 0.442687, Test Loss = 0.248878, Learning Rate = 6.277184e-05\n",
      "Epoch 7286/20000: Train Loss = 0.442630, Test Loss = 0.249535, Learning Rate = 6.274799e-05\n",
      "Epoch 7287/20000: Train Loss = 0.442805, Test Loss = 0.251866, Learning Rate = 6.272415e-05\n",
      "Epoch 7288/20000: Train Loss = 0.442927, Test Loss = 0.248394, Learning Rate = 6.270031e-05\n",
      "Epoch 7289/20000: Train Loss = 0.442638, Test Loss = 0.247791, Learning Rate = 6.267649e-05\n",
      "Epoch 7290/20000: Train Loss = 0.442934, Test Loss = 0.247792, Learning Rate = 6.265267e-05\n",
      "Epoch 7291/20000: Train Loss = 0.442651, Test Loss = 0.249917, Learning Rate = 6.262887e-05\n",
      "Epoch 7292/20000: Train Loss = 0.442978, Test Loss = 0.253377, Learning Rate = 6.260507e-05\n",
      "Epoch 7293/20000: Train Loss = 0.442678, Test Loss = 0.249834, Learning Rate = 6.258128e-05\n",
      "Epoch 7294/20000: Train Loss = 0.442627, Test Loss = 0.249185, Learning Rate = 6.255750e-05\n",
      "Epoch 7295/20000: Train Loss = 0.442694, Test Loss = 0.249822, Learning Rate = 6.253373e-05\n",
      "Epoch 7296/20000: Train Loss = 0.442652, Test Loss = 0.245108, Learning Rate = 6.250997e-05\n",
      "Epoch 7297/20000: Train Loss = 0.442658, Test Loss = 0.248286, Learning Rate = 6.248622e-05\n",
      "Epoch 7298/20000: Train Loss = 0.442866, Test Loss = 0.246893, Learning Rate = 6.246248e-05\n",
      "Epoch 7299/20000: Train Loss = 0.442864, Test Loss = 0.250081, Learning Rate = 6.243874e-05\n",
      "Epoch 7300/20000: Train Loss = 0.442834, Test Loss = 0.250149, Learning Rate = 6.241502e-05\n",
      "Epoch 7301/20000: Train Loss = 0.442706, Test Loss = 0.247537, Learning Rate = 6.239130e-05\n",
      "Epoch 7302/20000: Train Loss = 0.442705, Test Loss = 0.247678, Learning Rate = 6.236760e-05\n",
      "Epoch 7303/20000: Train Loss = 0.442585, Test Loss = 0.247112, Learning Rate = 6.234390e-05\n",
      "Epoch 7304/20000: Train Loss = 0.442642, Test Loss = 0.248144, Learning Rate = 6.232021e-05\n",
      "Epoch 7305/20000: Train Loss = 0.442599, Test Loss = 0.250270, Learning Rate = 6.229653e-05\n",
      "Epoch 7306/20000: Train Loss = 0.442711, Test Loss = 0.245428, Learning Rate = 6.227286e-05\n",
      "Epoch 7307/20000: Train Loss = 0.442621, Test Loss = 0.247257, Learning Rate = 6.224920e-05\n",
      "Epoch 7308/20000: Train Loss = 0.442586, Test Loss = 0.248750, Learning Rate = 6.222554e-05\n",
      "Epoch 7309/20000: Train Loss = 0.442788, Test Loss = 0.251829, Learning Rate = 6.220190e-05\n",
      "Epoch 7310/20000: Train Loss = 0.442759, Test Loss = 0.249081, Learning Rate = 6.217826e-05\n",
      "Epoch 7311/20000: Train Loss = 0.442677, Test Loss = 0.249486, Learning Rate = 6.215464e-05\n",
      "Epoch 7312/20000: Train Loss = 0.442704, Test Loss = 0.247254, Learning Rate = 6.213102e-05\n",
      "Epoch 7313/20000: Train Loss = 0.442740, Test Loss = 0.245314, Learning Rate = 6.210741e-05\n",
      "Epoch 7314/20000: Train Loss = 0.443488, Test Loss = 0.245651, Learning Rate = 6.208381e-05\n",
      "Epoch 7315/20000: Train Loss = 0.443061, Test Loss = 0.244379, Learning Rate = 6.206022e-05\n",
      "Epoch 7316/20000: Train Loss = 0.443263, Test Loss = 0.251321, Learning Rate = 6.203664e-05\n",
      "Epoch 7317/20000: Train Loss = 0.442576, Test Loss = 0.243496, Learning Rate = 6.201307e-05\n",
      "Epoch 7318/20000: Train Loss = 0.442719, Test Loss = 0.246067, Learning Rate = 6.198951e-05\n",
      "Epoch 7319/20000: Train Loss = 0.442903, Test Loss = 0.247924, Learning Rate = 6.196595e-05\n",
      "Epoch 7320/20000: Train Loss = 0.443496, Test Loss = 0.244365, Learning Rate = 6.194241e-05\n",
      "Epoch 7321/20000: Train Loss = 0.442643, Test Loss = 0.247632, Learning Rate = 6.191887e-05\n",
      "Epoch 7322/20000: Train Loss = 0.442791, Test Loss = 0.247602, Learning Rate = 6.189534e-05\n",
      "Epoch 7323/20000: Train Loss = 0.442547, Test Loss = 0.251409, Learning Rate = 6.187182e-05\n",
      "Epoch 7324/20000: Train Loss = 0.442581, Test Loss = 0.250749, Learning Rate = 6.184831e-05\n",
      "Epoch 7325/20000: Train Loss = 0.442670, Test Loss = 0.249580, Learning Rate = 6.182481e-05\n",
      "Epoch 7326/20000: Train Loss = 0.442520, Test Loss = 0.249500, Learning Rate = 6.180132e-05\n",
      "Epoch 7327/20000: Train Loss = 0.442715, Test Loss = 0.247306, Learning Rate = 6.177784e-05\n",
      "Epoch 7328/20000: Train Loss = 0.442485, Test Loss = 0.248228, Learning Rate = 6.175437e-05\n",
      "Epoch 7329/20000: Train Loss = 0.442569, Test Loss = 0.248199, Learning Rate = 6.173090e-05\n",
      "Epoch 7330/20000: Train Loss = 0.442708, Test Loss = 0.249386, Learning Rate = 6.170744e-05\n",
      "Epoch 7331/20000: Train Loss = 0.442697, Test Loss = 0.246982, Learning Rate = 6.168400e-05\n",
      "Epoch 7332/20000: Train Loss = 0.442650, Test Loss = 0.245181, Learning Rate = 6.166056e-05\n",
      "Epoch 7333/20000: Train Loss = 0.442586, Test Loss = 0.245504, Learning Rate = 6.163713e-05\n",
      "Epoch 7334/20000: Train Loss = 0.442913, Test Loss = 0.248197, Learning Rate = 6.161371e-05\n",
      "Epoch 7335/20000: Train Loss = 0.442701, Test Loss = 0.248146, Learning Rate = 6.159030e-05\n",
      "Epoch 7336/20000: Train Loss = 0.442653, Test Loss = 0.246347, Learning Rate = 6.156689e-05\n",
      "Epoch 7337/20000: Train Loss = 0.442844, Test Loss = 0.248359, Learning Rate = 6.154350e-05\n",
      "Epoch 7338/20000: Train Loss = 0.442764, Test Loss = 0.250249, Learning Rate = 6.152012e-05\n",
      "Epoch 7339/20000: Train Loss = 0.442479, Test Loss = 0.246137, Learning Rate = 6.149674e-05\n",
      "Epoch 7340/20000: Train Loss = 0.442945, Test Loss = 0.249072, Learning Rate = 6.147337e-05\n",
      "Epoch 7341/20000: Train Loss = 0.442672, Test Loss = 0.248484, Learning Rate = 6.145001e-05\n",
      "Epoch 7342/20000: Train Loss = 0.442633, Test Loss = 0.249541, Learning Rate = 6.142667e-05\n",
      "Epoch 7343/20000: Train Loss = 0.442889, Test Loss = 0.249052, Learning Rate = 6.140332e-05\n",
      "Epoch 7344/20000: Train Loss = 0.442855, Test Loss = 0.252187, Learning Rate = 6.137999e-05\n",
      "Epoch 7345/20000: Train Loss = 0.443025, Test Loss = 0.251201, Learning Rate = 6.135667e-05\n",
      "Epoch 7346/20000: Train Loss = 0.442720, Test Loss = 0.249928, Learning Rate = 6.133336e-05\n",
      "Epoch 7347/20000: Train Loss = 0.442658, Test Loss = 0.246411, Learning Rate = 6.131005e-05\n",
      "Epoch 7348/20000: Train Loss = 0.442726, Test Loss = 0.250559, Learning Rate = 6.128676e-05\n",
      "Epoch 7349/20000: Train Loss = 0.442649, Test Loss = 0.249730, Learning Rate = 6.126347e-05\n",
      "Epoch 7350/20000: Train Loss = 0.442694, Test Loss = 0.249634, Learning Rate = 6.124019e-05\n",
      "Epoch 7351/20000: Train Loss = 0.442744, Test Loss = 0.249665, Learning Rate = 6.121692e-05\n",
      "Epoch 7352/20000: Train Loss = 0.442586, Test Loss = 0.244659, Learning Rate = 6.119366e-05\n",
      "Epoch 7353/20000: Train Loss = 0.442703, Test Loss = 0.244363, Learning Rate = 6.117041e-05\n",
      "Epoch 7354/20000: Train Loss = 0.442697, Test Loss = 0.247969, Learning Rate = 6.114716e-05\n",
      "Epoch 7355/20000: Train Loss = 0.442614, Test Loss = 0.248262, Learning Rate = 6.112393e-05\n",
      "Epoch 7356/20000: Train Loss = 0.442936, Test Loss = 0.244196, Learning Rate = 6.110070e-05\n",
      "Epoch 7357/20000: Train Loss = 0.442739, Test Loss = 0.248905, Learning Rate = 6.107749e-05\n",
      "Epoch 7358/20000: Train Loss = 0.442665, Test Loss = 0.250431, Learning Rate = 6.105428e-05\n",
      "Epoch 7359/20000: Train Loss = 0.442718, Test Loss = 0.248149, Learning Rate = 6.103108e-05\n",
      "Epoch 7360/20000: Train Loss = 0.442614, Test Loss = 0.249783, Learning Rate = 6.100789e-05\n",
      "Epoch 7361/20000: Train Loss = 0.442556, Test Loss = 0.247405, Learning Rate = 6.098471e-05\n",
      "Epoch 7362/20000: Train Loss = 0.442585, Test Loss = 0.247096, Learning Rate = 6.096154e-05\n",
      "Epoch 7363/20000: Train Loss = 0.442713, Test Loss = 0.249560, Learning Rate = 6.093837e-05\n",
      "Epoch 7364/20000: Train Loss = 0.442776, Test Loss = 0.248081, Learning Rate = 6.091522e-05\n",
      "Epoch 7365/20000: Train Loss = 0.442667, Test Loss = 0.250228, Learning Rate = 6.089207e-05\n",
      "Epoch 7366/20000: Train Loss = 0.442834, Test Loss = 0.248669, Learning Rate = 6.086894e-05\n",
      "Epoch 7367/20000: Train Loss = 0.442639, Test Loss = 0.246939, Learning Rate = 6.084581e-05\n",
      "Epoch 7368/20000: Train Loss = 0.442892, Test Loss = 0.244651, Learning Rate = 6.082269e-05\n",
      "Epoch 7369/20000: Train Loss = 0.442541, Test Loss = 0.245554, Learning Rate = 6.079958e-05\n",
      "Epoch 7370/20000: Train Loss = 0.442648, Test Loss = 0.246500, Learning Rate = 6.077647e-05\n",
      "Epoch 7371/20000: Train Loss = 0.442676, Test Loss = 0.249826, Learning Rate = 6.075338e-05\n",
      "Epoch 7372/20000: Train Loss = 0.443141, Test Loss = 0.250453, Learning Rate = 6.073030e-05\n",
      "Epoch 7373/20000: Train Loss = 0.442836, Test Loss = 0.244090, Learning Rate = 6.070722e-05\n",
      "Epoch 7374/20000: Train Loss = 0.443067, Test Loss = 0.244025, Learning Rate = 6.068415e-05\n",
      "Epoch 7375/20000: Train Loss = 0.442956, Test Loss = 0.246208, Learning Rate = 6.066109e-05\n",
      "Epoch 7376/20000: Train Loss = 0.442627, Test Loss = 0.246447, Learning Rate = 6.063804e-05\n",
      "Epoch 7377/20000: Train Loss = 0.442692, Test Loss = 0.246247, Learning Rate = 6.061500e-05\n",
      "Epoch 7378/20000: Train Loss = 0.442623, Test Loss = 0.247403, Learning Rate = 6.059197e-05\n",
      "Epoch 7379/20000: Train Loss = 0.442713, Test Loss = 0.248768, Learning Rate = 6.056895e-05\n",
      "Epoch 7380/20000: Train Loss = 0.442516, Test Loss = 0.247889, Learning Rate = 6.054593e-05\n",
      "Epoch 7381/20000: Train Loss = 0.443357, Test Loss = 0.244417, Learning Rate = 6.052293e-05\n",
      "Epoch 7382/20000: Train Loss = 0.442565, Test Loss = 0.246414, Learning Rate = 6.049993e-05\n",
      "Epoch 7383/20000: Train Loss = 0.442887, Test Loss = 0.250677, Learning Rate = 6.047694e-05\n",
      "Epoch 7384/20000: Train Loss = 0.442679, Test Loss = 0.247295, Learning Rate = 6.045396e-05\n",
      "Epoch 7385/20000: Train Loss = 0.442832, Test Loss = 0.244511, Learning Rate = 6.043099e-05\n",
      "Epoch 7386/20000: Train Loss = 0.442667, Test Loss = 0.246686, Learning Rate = 6.040803e-05\n",
      "Epoch 7387/20000: Train Loss = 0.442576, Test Loss = 0.247158, Learning Rate = 6.038508e-05\n",
      "Epoch 7388/20000: Train Loss = 0.442735, Test Loss = 0.248771, Learning Rate = 6.036213e-05\n",
      "Epoch 7389/20000: Train Loss = 0.442815, Test Loss = 0.245169, Learning Rate = 6.033920e-05\n",
      "Epoch 7390/20000: Train Loss = 0.442435, Test Loss = 0.247142, Learning Rate = 6.031627e-05\n",
      "Epoch 7391/20000: Train Loss = 0.442594, Test Loss = 0.245906, Learning Rate = 6.029335e-05\n",
      "Epoch 7392/20000: Train Loss = 0.442749, Test Loss = 0.247271, Learning Rate = 6.027044e-05\n",
      "Epoch 7393/20000: Train Loss = 0.442928, Test Loss = 0.248620, Learning Rate = 6.024754e-05\n",
      "Epoch 7394/20000: Train Loss = 0.442587, Test Loss = 0.249326, Learning Rate = 6.022465e-05\n",
      "Epoch 7395/20000: Train Loss = 0.442872, Test Loss = 0.251924, Learning Rate = 6.020176e-05\n",
      "Epoch 7396/20000: Train Loss = 0.442720, Test Loss = 0.248239, Learning Rate = 6.017889e-05\n",
      "Epoch 7397/20000: Train Loss = 0.442735, Test Loss = 0.248641, Learning Rate = 6.015602e-05\n",
      "Epoch 7398/20000: Train Loss = 0.442795, Test Loss = 0.252468, Learning Rate = 6.013316e-05\n",
      "Epoch 7399/20000: Train Loss = 0.442499, Test Loss = 0.248815, Learning Rate = 6.011032e-05\n",
      "Epoch 7400/20000: Train Loss = 0.442659, Test Loss = 0.249119, Learning Rate = 6.008747e-05\n",
      "Epoch 7401/20000: Train Loss = 0.442785, Test Loss = 0.245832, Learning Rate = 6.006464e-05\n",
      "Epoch 7402/20000: Train Loss = 0.443052, Test Loss = 0.249571, Learning Rate = 6.004182e-05\n",
      "Epoch 7403/20000: Train Loss = 0.442888, Test Loss = 0.245186, Learning Rate = 6.001901e-05\n",
      "Epoch 7404/20000: Train Loss = 0.442565, Test Loss = 0.247320, Learning Rate = 5.999620e-05\n",
      "Epoch 7405/20000: Train Loss = 0.442713, Test Loss = 0.248812, Learning Rate = 5.997340e-05\n",
      "Epoch 7406/20000: Train Loss = 0.442838, Test Loss = 0.251905, Learning Rate = 5.995062e-05\n",
      "Epoch 7407/20000: Train Loss = 0.442428, Test Loss = 0.245552, Learning Rate = 5.992784e-05\n",
      "Epoch 7408/20000: Train Loss = 0.442622, Test Loss = 0.244818, Learning Rate = 5.990506e-05\n",
      "Epoch 7409/20000: Train Loss = 0.442786, Test Loss = 0.242804, Learning Rate = 5.988230e-05\n",
      "Epoch 7410/20000: Train Loss = 0.442885, Test Loss = 0.248224, Learning Rate = 5.985955e-05\n",
      "Epoch 7411/20000: Train Loss = 0.442559, Test Loss = 0.245805, Learning Rate = 5.983680e-05\n",
      "Epoch 7412/20000: Train Loss = 0.442494, Test Loss = 0.246971, Learning Rate = 5.981407e-05\n",
      "Epoch 7413/20000: Train Loss = 0.442600, Test Loss = 0.248961, Learning Rate = 5.979134e-05\n",
      "Epoch 7414/20000: Train Loss = 0.443064, Test Loss = 0.246452, Learning Rate = 5.976862e-05\n",
      "Epoch 7415/20000: Train Loss = 0.442605, Test Loss = 0.247425, Learning Rate = 5.974591e-05\n",
      "Epoch 7416/20000: Train Loss = 0.443159, Test Loss = 0.248700, Learning Rate = 5.972321e-05\n",
      "Epoch 7417/20000: Train Loss = 0.442867, Test Loss = 0.246290, Learning Rate = 5.970052e-05\n",
      "Epoch 7418/20000: Train Loss = 0.442692, Test Loss = 0.247149, Learning Rate = 5.967783e-05\n",
      "Epoch 7419/20000: Train Loss = 0.442774, Test Loss = 0.248066, Learning Rate = 5.965515e-05\n",
      "Epoch 7420/20000: Train Loss = 0.442733, Test Loss = 0.247276, Learning Rate = 5.963249e-05\n",
      "Epoch 7421/20000: Train Loss = 0.442964, Test Loss = 0.245334, Learning Rate = 5.960983e-05\n",
      "Epoch 7422/20000: Train Loss = 0.442876, Test Loss = 0.249787, Learning Rate = 5.958718e-05\n",
      "Epoch 7423/20000: Train Loss = 0.442730, Test Loss = 0.248505, Learning Rate = 5.956454e-05\n",
      "Epoch 7424/20000: Train Loss = 0.443048, Test Loss = 0.248111, Learning Rate = 5.954190e-05\n",
      "Epoch 7425/20000: Train Loss = 0.442846, Test Loss = 0.250009, Learning Rate = 5.951928e-05\n",
      "Epoch 7426/20000: Train Loss = 0.442507, Test Loss = 0.247425, Learning Rate = 5.949666e-05\n",
      "Epoch 7427/20000: Train Loss = 0.442876, Test Loss = 0.249950, Learning Rate = 5.947406e-05\n",
      "Epoch 7428/20000: Train Loss = 0.443064, Test Loss = 0.245571, Learning Rate = 5.945146e-05\n",
      "Epoch 7429/20000: Train Loss = 0.442660, Test Loss = 0.250042, Learning Rate = 5.942887e-05\n",
      "Epoch 7430/20000: Train Loss = 0.442703, Test Loss = 0.251028, Learning Rate = 5.940629e-05\n",
      "Epoch 7431/20000: Train Loss = 0.442609, Test Loss = 0.250485, Learning Rate = 5.938371e-05\n",
      "Epoch 7432/20000: Train Loss = 0.442932, Test Loss = 0.246831, Learning Rate = 5.936115e-05\n",
      "Epoch 7433/20000: Train Loss = 0.442752, Test Loss = 0.245200, Learning Rate = 5.933859e-05\n",
      "Epoch 7434/20000: Train Loss = 0.442740, Test Loss = 0.244954, Learning Rate = 5.931605e-05\n",
      "Epoch 7435/20000: Train Loss = 0.442818, Test Loss = 0.245597, Learning Rate = 5.929351e-05\n",
      "Epoch 7436/20000: Train Loss = 0.442687, Test Loss = 0.247840, Learning Rate = 5.927098e-05\n",
      "Epoch 7437/20000: Train Loss = 0.442631, Test Loss = 0.247058, Learning Rate = 5.924846e-05\n",
      "Epoch 7438/20000: Train Loss = 0.442687, Test Loss = 0.243672, Learning Rate = 5.922594e-05\n",
      "Epoch 7439/20000: Train Loss = 0.442736, Test Loss = 0.244448, Learning Rate = 5.920344e-05\n",
      "Epoch 7440/20000: Train Loss = 0.442619, Test Loss = 0.245911, Learning Rate = 5.918094e-05\n",
      "Epoch 7441/20000: Train Loss = 0.442728, Test Loss = 0.243595, Learning Rate = 5.915846e-05\n",
      "Epoch 7442/20000: Train Loss = 0.443067, Test Loss = 0.243918, Learning Rate = 5.913598e-05\n",
      "Epoch 7443/20000: Train Loss = 0.442876, Test Loss = 0.246975, Learning Rate = 5.911351e-05\n",
      "Epoch 7444/20000: Train Loss = 0.442744, Test Loss = 0.249416, Learning Rate = 5.909105e-05\n",
      "Epoch 7445/20000: Train Loss = 0.442580, Test Loss = 0.248308, Learning Rate = 5.906859e-05\n",
      "Epoch 7446/20000: Train Loss = 0.443204, Test Loss = 0.248767, Learning Rate = 5.904615e-05\n",
      "Epoch 7447/20000: Train Loss = 0.442722, Test Loss = 0.249598, Learning Rate = 5.902371e-05\n",
      "Epoch 7448/20000: Train Loss = 0.442712, Test Loss = 0.249675, Learning Rate = 5.900129e-05\n",
      "Epoch 7449/20000: Train Loss = 0.442812, Test Loss = 0.252024, Learning Rate = 5.897887e-05\n",
      "Epoch 7450/20000: Train Loss = 0.442871, Test Loss = 0.247086, Learning Rate = 5.895646e-05\n",
      "Epoch 7451/20000: Train Loss = 0.442623, Test Loss = 0.248926, Learning Rate = 5.893406e-05\n",
      "Epoch 7452/20000: Train Loss = 0.442873, Test Loss = 0.247336, Learning Rate = 5.891166e-05\n",
      "Epoch 7453/20000: Train Loss = 0.443452, Test Loss = 0.249374, Learning Rate = 5.888928e-05\n",
      "Epoch 7454/20000: Train Loss = 0.442517, Test Loss = 0.245647, Learning Rate = 5.886690e-05\n",
      "Epoch 7455/20000: Train Loss = 0.442914, Test Loss = 0.246273, Learning Rate = 5.884453e-05\n",
      "Epoch 7456/20000: Train Loss = 0.442911, Test Loss = 0.246484, Learning Rate = 5.882217e-05\n",
      "Epoch 7457/20000: Train Loss = 0.442593, Test Loss = 0.247182, Learning Rate = 5.879982e-05\n",
      "Epoch 7458/20000: Train Loss = 0.442513, Test Loss = 0.248819, Learning Rate = 5.877748e-05\n",
      "Epoch 7459/20000: Train Loss = 0.442680, Test Loss = 0.249476, Learning Rate = 5.875515e-05\n",
      "Epoch 7460/20000: Train Loss = 0.443143, Test Loss = 0.249567, Learning Rate = 5.873282e-05\n",
      "Epoch 7461/20000: Train Loss = 0.442761, Test Loss = 0.245793, Learning Rate = 5.871050e-05\n",
      "Epoch 7462/20000: Train Loss = 0.442612, Test Loss = 0.248482, Learning Rate = 5.868820e-05\n",
      "Epoch 7463/20000: Train Loss = 0.442729, Test Loss = 0.245587, Learning Rate = 5.866590e-05\n",
      "Epoch 7464/20000: Train Loss = 0.442622, Test Loss = 0.244567, Learning Rate = 5.864360e-05\n",
      "Epoch 7465/20000: Train Loss = 0.442603, Test Loss = 0.246528, Learning Rate = 5.862132e-05\n",
      "Epoch 7466/20000: Train Loss = 0.442673, Test Loss = 0.244088, Learning Rate = 5.859905e-05\n",
      "Epoch 7467/20000: Train Loss = 0.442765, Test Loss = 0.243941, Learning Rate = 5.857678e-05\n",
      "Epoch 7468/20000: Train Loss = 0.442585, Test Loss = 0.245242, Learning Rate = 5.855452e-05\n",
      "Epoch 7469/20000: Train Loss = 0.443085, Test Loss = 0.248075, Learning Rate = 5.853227e-05\n",
      "Epoch 7470/20000: Train Loss = 0.442653, Test Loss = 0.246824, Learning Rate = 5.851003e-05\n",
      "Epoch 7471/20000: Train Loss = 0.442738, Test Loss = 0.244833, Learning Rate = 5.848780e-05\n",
      "Epoch 7472/20000: Train Loss = 0.442746, Test Loss = 0.245621, Learning Rate = 5.846558e-05\n",
      "Epoch 7473/20000: Train Loss = 0.442764, Test Loss = 0.246143, Learning Rate = 5.844336e-05\n",
      "Epoch 7474/20000: Train Loss = 0.442802, Test Loss = 0.248601, Learning Rate = 5.842116e-05\n",
      "Epoch 7475/20000: Train Loss = 0.442620, Test Loss = 0.247232, Learning Rate = 5.839896e-05\n",
      "Epoch 7476/20000: Train Loss = 0.442813, Test Loss = 0.247263, Learning Rate = 5.837677e-05\n",
      "Epoch 7477/20000: Train Loss = 0.443043, Test Loss = 0.246369, Learning Rate = 5.835459e-05\n",
      "Epoch 7478/20000: Train Loss = 0.442625, Test Loss = 0.250997, Learning Rate = 5.833241e-05\n",
      "Epoch 7479/20000: Train Loss = 0.442825, Test Loss = 0.248573, Learning Rate = 5.831025e-05\n",
      "Epoch 7480/20000: Train Loss = 0.442887, Test Loss = 0.245374, Learning Rate = 5.828809e-05\n",
      "Epoch 7481/20000: Train Loss = 0.442719, Test Loss = 0.246846, Learning Rate = 5.826594e-05\n",
      "Epoch 7482/20000: Train Loss = 0.442724, Test Loss = 0.251375, Learning Rate = 5.824380e-05\n",
      "Epoch 7483/20000: Train Loss = 0.442628, Test Loss = 0.247562, Learning Rate = 5.822167e-05\n",
      "Epoch 7484/20000: Train Loss = 0.442748, Test Loss = 0.249998, Learning Rate = 5.819955e-05\n",
      "Epoch 7485/20000: Train Loss = 0.442628, Test Loss = 0.246072, Learning Rate = 5.817744e-05\n",
      "Epoch 7486/20000: Train Loss = 0.442770, Test Loss = 0.243319, Learning Rate = 5.815533e-05\n",
      "Epoch 7487/20000: Train Loss = 0.442690, Test Loss = 0.246390, Learning Rate = 5.813323e-05\n",
      "Epoch 7488/20000: Train Loss = 0.442913, Test Loss = 0.251169, Learning Rate = 5.811114e-05\n",
      "Epoch 7489/20000: Train Loss = 0.442571, Test Loss = 0.251255, Learning Rate = 5.808906e-05\n",
      "Epoch 7490/20000: Train Loss = 0.442758, Test Loss = 0.248382, Learning Rate = 5.806699e-05\n",
      "Epoch 7491/20000: Train Loss = 0.442927, Test Loss = 0.248770, Learning Rate = 5.804493e-05\n",
      "Epoch 7492/20000: Train Loss = 0.442705, Test Loss = 0.246831, Learning Rate = 5.802287e-05\n",
      "Epoch 7493/20000: Train Loss = 0.442828, Test Loss = 0.248447, Learning Rate = 5.800082e-05\n",
      "Epoch 7494/20000: Train Loss = 0.442782, Test Loss = 0.245143, Learning Rate = 5.797879e-05\n",
      "Epoch 7495/20000: Train Loss = 0.442500, Test Loss = 0.248058, Learning Rate = 5.795675e-05\n",
      "Epoch 7496/20000: Train Loss = 0.442668, Test Loss = 0.248064, Learning Rate = 5.793473e-05\n",
      "Epoch 7497/20000: Train Loss = 0.442768, Test Loss = 0.246967, Learning Rate = 5.791272e-05\n",
      "Epoch 7498/20000: Train Loss = 0.443054, Test Loss = 0.245805, Learning Rate = 5.789071e-05\n",
      "Epoch 7499/20000: Train Loss = 0.442690, Test Loss = 0.246508, Learning Rate = 5.786872e-05\n",
      "Epoch 7500/20000: Train Loss = 0.442893, Test Loss = 0.248523, Learning Rate = 5.784673e-05\n",
      "Epoch 7501/20000: Train Loss = 0.442591, Test Loss = 0.248656, Learning Rate = 5.782475e-05\n",
      "Epoch 7502/20000: Train Loss = 0.442924, Test Loss = 0.249465, Learning Rate = 5.780278e-05\n",
      "Epoch 7503/20000: Train Loss = 0.442585, Test Loss = 0.246340, Learning Rate = 5.778081e-05\n",
      "Epoch 7504/20000: Train Loss = 0.442701, Test Loss = 0.247671, Learning Rate = 5.775886e-05\n",
      "Epoch 7505/20000: Train Loss = 0.442836, Test Loss = 0.249492, Learning Rate = 5.773691e-05\n",
      "Epoch 7506/20000: Train Loss = 0.442672, Test Loss = 0.251005, Learning Rate = 5.771497e-05\n",
      "Epoch 7507/20000: Train Loss = 0.442763, Test Loss = 0.251681, Learning Rate = 5.769304e-05\n",
      "Epoch 7508/20000: Train Loss = 0.442718, Test Loss = 0.251025, Learning Rate = 5.767112e-05\n",
      "Epoch 7509/20000: Train Loss = 0.442655, Test Loss = 0.247054, Learning Rate = 5.764921e-05\n",
      "Epoch 7510/20000: Train Loss = 0.443041, Test Loss = 0.244729, Learning Rate = 5.762730e-05\n",
      "Epoch 7511/20000: Train Loss = 0.443161, Test Loss = 0.247054, Learning Rate = 5.760541e-05\n",
      "Epoch 7512/20000: Train Loss = 0.442563, Test Loss = 0.247087, Learning Rate = 5.758352e-05\n",
      "Epoch 7513/20000: Train Loss = 0.442602, Test Loss = 0.246072, Learning Rate = 5.756164e-05\n",
      "Epoch 7514/20000: Train Loss = 0.442591, Test Loss = 0.245348, Learning Rate = 5.753976e-05\n",
      "Epoch 7515/20000: Train Loss = 0.442831, Test Loss = 0.243861, Learning Rate = 5.751790e-05\n",
      "Epoch 7516/20000: Train Loss = 0.442716, Test Loss = 0.244393, Learning Rate = 5.749605e-05\n",
      "Epoch 7517/20000: Train Loss = 0.442778, Test Loss = 0.246872, Learning Rate = 5.747420e-05\n",
      "Epoch 7518/20000: Train Loss = 0.442817, Test Loss = 0.248760, Learning Rate = 5.745236e-05\n",
      "Epoch 7519/20000: Train Loss = 0.443457, Test Loss = 0.250692, Learning Rate = 5.743053e-05\n",
      "Epoch 7520/20000: Train Loss = 0.442921, Test Loss = 0.249423, Learning Rate = 5.740871e-05\n",
      "Epoch 7521/20000: Train Loss = 0.442986, Test Loss = 0.253826, Learning Rate = 5.738689e-05\n",
      "Epoch 7522/20000: Train Loss = 0.442675, Test Loss = 0.251931, Learning Rate = 5.736509e-05\n",
      "Epoch 7523/20000: Train Loss = 0.442788, Test Loss = 0.246571, Learning Rate = 5.734329e-05\n",
      "Epoch 7524/20000: Train Loss = 0.442695, Test Loss = 0.249665, Learning Rate = 5.732150e-05\n",
      "Epoch 7525/20000: Train Loss = 0.442916, Test Loss = 0.250281, Learning Rate = 5.729972e-05\n",
      "Epoch 7526/20000: Train Loss = 0.442913, Test Loss = 0.250209, Learning Rate = 5.727795e-05\n",
      "Epoch 7527/20000: Train Loss = 0.442806, Test Loss = 0.249359, Learning Rate = 5.725619e-05\n",
      "Epoch 7528/20000: Train Loss = 0.442558, Test Loss = 0.248613, Learning Rate = 5.723443e-05\n",
      "Epoch 7529/20000: Train Loss = 0.442748, Test Loss = 0.250460, Learning Rate = 5.721268e-05\n",
      "Epoch 7530/20000: Train Loss = 0.442721, Test Loss = 0.254380, Learning Rate = 5.719094e-05\n",
      "Epoch 7531/20000: Train Loss = 0.442742, Test Loss = 0.249155, Learning Rate = 5.716921e-05\n",
      "Epoch 7532/20000: Train Loss = 0.442713, Test Loss = 0.251021, Learning Rate = 5.714749e-05\n",
      "Epoch 7533/20000: Train Loss = 0.442585, Test Loss = 0.248375, Learning Rate = 5.712577e-05\n",
      "Epoch 7534/20000: Train Loss = 0.442671, Test Loss = 0.248934, Learning Rate = 5.710407e-05\n",
      "Epoch 7535/20000: Train Loss = 0.442631, Test Loss = 0.246505, Learning Rate = 5.708237e-05\n",
      "Epoch 7536/20000: Train Loss = 0.442626, Test Loss = 0.245645, Learning Rate = 5.706068e-05\n",
      "Epoch 7537/20000: Train Loss = 0.442887, Test Loss = 0.249546, Learning Rate = 5.703900e-05\n",
      "Epoch 7538/20000: Train Loss = 0.442909, Test Loss = 0.246966, Learning Rate = 5.701733e-05\n",
      "Epoch 7539/20000: Train Loss = 0.442675, Test Loss = 0.250669, Learning Rate = 5.699566e-05\n",
      "Epoch 7540/20000: Train Loss = 0.442788, Test Loss = 0.249012, Learning Rate = 5.697400e-05\n",
      "Epoch 7541/20000: Train Loss = 0.442791, Test Loss = 0.246946, Learning Rate = 5.695236e-05\n",
      "Epoch 7542/20000: Train Loss = 0.442684, Test Loss = 0.250443, Learning Rate = 5.693072e-05\n",
      "Epoch 7543/20000: Train Loss = 0.442740, Test Loss = 0.250681, Learning Rate = 5.690908e-05\n",
      "Epoch 7544/20000: Train Loss = 0.442605, Test Loss = 0.249330, Learning Rate = 5.688746e-05\n",
      "Epoch 7545/20000: Train Loss = 0.442588, Test Loss = 0.248662, Learning Rate = 5.686584e-05\n",
      "Epoch 7546/20000: Train Loss = 0.442901, Test Loss = 0.245092, Learning Rate = 5.684424e-05\n",
      "Epoch 7547/20000: Train Loss = 0.442964, Test Loss = 0.248724, Learning Rate = 5.682264e-05\n",
      "Epoch 7548/20000: Train Loss = 0.442587, Test Loss = 0.246528, Learning Rate = 5.680105e-05\n",
      "Epoch 7549/20000: Train Loss = 0.442718, Test Loss = 0.247522, Learning Rate = 5.677946e-05\n",
      "Epoch 7550/20000: Train Loss = 0.442635, Test Loss = 0.248692, Learning Rate = 5.675789e-05\n",
      "Epoch 7551/20000: Train Loss = 0.442755, Test Loss = 0.246261, Learning Rate = 5.673632e-05\n",
      "Epoch 7552/20000: Train Loss = 0.442702, Test Loss = 0.247151, Learning Rate = 5.671476e-05\n",
      "Epoch 7553/20000: Train Loss = 0.442590, Test Loss = 0.248951, Learning Rate = 5.669321e-05\n",
      "Epoch 7554/20000: Train Loss = 0.442872, Test Loss = 0.247208, Learning Rate = 5.667167e-05\n",
      "Epoch 7555/20000: Train Loss = 0.442639, Test Loss = 0.246100, Learning Rate = 5.665014e-05\n",
      "Epoch 7556/20000: Train Loss = 0.442837, Test Loss = 0.246056, Learning Rate = 5.662861e-05\n",
      "Epoch 7557/20000: Train Loss = 0.442885, Test Loss = 0.245152, Learning Rate = 5.660709e-05\n",
      "Epoch 7558/20000: Train Loss = 0.442574, Test Loss = 0.248198, Learning Rate = 5.658559e-05\n",
      "Epoch 7559/20000: Train Loss = 0.442646, Test Loss = 0.253471, Learning Rate = 5.656408e-05\n",
      "Epoch 7560/20000: Train Loss = 0.442648, Test Loss = 0.252892, Learning Rate = 5.654259e-05\n",
      "Epoch 7561/20000: Train Loss = 0.442882, Test Loss = 0.251369, Learning Rate = 5.652111e-05\n",
      "Epoch 7562/20000: Train Loss = 0.442647, Test Loss = 0.251929, Learning Rate = 5.649963e-05\n",
      "Epoch 7563/20000: Train Loss = 0.442590, Test Loss = 0.251114, Learning Rate = 5.647816e-05\n",
      "Epoch 7564/20000: Train Loss = 0.442767, Test Loss = 0.250439, Learning Rate = 5.645670e-05\n",
      "Epoch 7565/20000: Train Loss = 0.442894, Test Loss = 0.249804, Learning Rate = 5.643525e-05\n",
      "Epoch 7566/20000: Train Loss = 0.442595, Test Loss = 0.250025, Learning Rate = 5.641381e-05\n",
      "Epoch 7567/20000: Train Loss = 0.443164, Test Loss = 0.253290, Learning Rate = 5.639237e-05\n",
      "Epoch 7568/20000: Train Loss = 0.443182, Test Loss = 0.248745, Learning Rate = 5.637094e-05\n",
      "Epoch 7569/20000: Train Loss = 0.442505, Test Loss = 0.251260, Learning Rate = 5.634952e-05\n",
      "Epoch 7570/20000: Train Loss = 0.442670, Test Loss = 0.251570, Learning Rate = 5.632811e-05\n",
      "Epoch 7571/20000: Train Loss = 0.442946, Test Loss = 0.249537, Learning Rate = 5.630671e-05\n",
      "Epoch 7572/20000: Train Loss = 0.442692, Test Loss = 0.247341, Learning Rate = 5.628531e-05\n",
      "Epoch 7573/20000: Train Loss = 0.442619, Test Loss = 0.246728, Learning Rate = 5.626393e-05\n",
      "Epoch 7574/20000: Train Loss = 0.442579, Test Loss = 0.248650, Learning Rate = 5.624255e-05\n",
      "Epoch 7575/20000: Train Loss = 0.442606, Test Loss = 0.246371, Learning Rate = 5.622118e-05\n",
      "Epoch 7576/20000: Train Loss = 0.442815, Test Loss = 0.245307, Learning Rate = 5.619982e-05\n",
      "Epoch 7577/20000: Train Loss = 0.442746, Test Loss = 0.246556, Learning Rate = 5.617846e-05\n",
      "Epoch 7578/20000: Train Loss = 0.442780, Test Loss = 0.247307, Learning Rate = 5.615711e-05\n",
      "Epoch 7579/20000: Train Loss = 0.442597, Test Loss = 0.247093, Learning Rate = 5.613578e-05\n",
      "Epoch 7580/20000: Train Loss = 0.442580, Test Loss = 0.245506, Learning Rate = 5.611445e-05\n",
      "Epoch 7581/20000: Train Loss = 0.443074, Test Loss = 0.246930, Learning Rate = 5.609312e-05\n",
      "Epoch 7582/20000: Train Loss = 0.442678, Test Loss = 0.244929, Learning Rate = 5.607181e-05\n",
      "Epoch 7583/20000: Train Loss = 0.443120, Test Loss = 0.241973, Learning Rate = 5.605050e-05\n",
      "Epoch 7584/20000: Train Loss = 0.442403, Test Loss = 0.245347, Learning Rate = 5.602921e-05\n",
      "Epoch 7585/20000: Train Loss = 0.442762, Test Loss = 0.245621, Learning Rate = 5.600792e-05\n",
      "Epoch 7586/20000: Train Loss = 0.442621, Test Loss = 0.245852, Learning Rate = 5.598664e-05\n",
      "Epoch 7587/20000: Train Loss = 0.442596, Test Loss = 0.245261, Learning Rate = 5.596536e-05\n",
      "Epoch 7588/20000: Train Loss = 0.442647, Test Loss = 0.245190, Learning Rate = 5.594410e-05\n",
      "Epoch 7589/20000: Train Loss = 0.442731, Test Loss = 0.243542, Learning Rate = 5.592284e-05\n",
      "Epoch 7590/20000: Train Loss = 0.442661, Test Loss = 0.246639, Learning Rate = 5.590159e-05\n",
      "Epoch 7591/20000: Train Loss = 0.442626, Test Loss = 0.247033, Learning Rate = 5.588035e-05\n",
      "Epoch 7592/20000: Train Loss = 0.442520, Test Loss = 0.247291, Learning Rate = 5.585912e-05\n",
      "Epoch 7593/20000: Train Loss = 0.442853, Test Loss = 0.247797, Learning Rate = 5.583789e-05\n",
      "Epoch 7594/20000: Train Loss = 0.442689, Test Loss = 0.246359, Learning Rate = 5.581668e-05\n",
      "Epoch 7595/20000: Train Loss = 0.442630, Test Loss = 0.248569, Learning Rate = 5.579547e-05\n",
      "Epoch 7596/20000: Train Loss = 0.442693, Test Loss = 0.248113, Learning Rate = 5.577427e-05\n",
      "Epoch 7597/20000: Train Loss = 0.442906, Test Loss = 0.247324, Learning Rate = 5.575307e-05\n",
      "Epoch 7598/20000: Train Loss = 0.442664, Test Loss = 0.249696, Learning Rate = 5.573189e-05\n",
      "Epoch 7599/20000: Train Loss = 0.442580, Test Loss = 0.248202, Learning Rate = 5.571071e-05\n",
      "Epoch 7600/20000: Train Loss = 0.442811, Test Loss = 0.248330, Learning Rate = 5.568954e-05\n",
      "Epoch 7601/20000: Train Loss = 0.442824, Test Loss = 0.246597, Learning Rate = 5.566838e-05\n",
      "Epoch 7602/20000: Train Loss = 0.442658, Test Loss = 0.246278, Learning Rate = 5.564723e-05\n",
      "Epoch 7603/20000: Train Loss = 0.442832, Test Loss = 0.247881, Learning Rate = 5.562609e-05\n",
      "Epoch 7604/20000: Train Loss = 0.442590, Test Loss = 0.249733, Learning Rate = 5.560495e-05\n",
      "Epoch 7605/20000: Train Loss = 0.442786, Test Loss = 0.250312, Learning Rate = 5.558382e-05\n",
      "Epoch 7606/20000: Train Loss = 0.442666, Test Loss = 0.248675, Learning Rate = 5.556270e-05\n",
      "Epoch 7607/20000: Train Loss = 0.443096, Test Loss = 0.245700, Learning Rate = 5.554159e-05\n",
      "Epoch 7608/20000: Train Loss = 0.442731, Test Loss = 0.245477, Learning Rate = 5.552048e-05\n",
      "Epoch 7609/20000: Train Loss = 0.442688, Test Loss = 0.246583, Learning Rate = 5.549939e-05\n",
      "Epoch 7610/20000: Train Loss = 0.442685, Test Loss = 0.246604, Learning Rate = 5.547830e-05\n",
      "Epoch 7611/20000: Train Loss = 0.442738, Test Loss = 0.245415, Learning Rate = 5.545722e-05\n",
      "Epoch 7612/20000: Train Loss = 0.442794, Test Loss = 0.245272, Learning Rate = 5.543615e-05\n",
      "Epoch 7613/20000: Train Loss = 0.442510, Test Loss = 0.248262, Learning Rate = 5.541508e-05\n",
      "Epoch 7614/20000: Train Loss = 0.442485, Test Loss = 0.248933, Learning Rate = 5.539403e-05\n",
      "Epoch 7615/20000: Train Loss = 0.442845, Test Loss = 0.248386, Learning Rate = 5.537298e-05\n",
      "Epoch 7616/20000: Train Loss = 0.442738, Test Loss = 0.247774, Learning Rate = 5.535194e-05\n",
      "Epoch 7617/20000: Train Loss = 0.442671, Test Loss = 0.249473, Learning Rate = 5.533091e-05\n",
      "Epoch 7618/20000: Train Loss = 0.442612, Test Loss = 0.249347, Learning Rate = 5.530988e-05\n",
      "Epoch 7619/20000: Train Loss = 0.443056, Test Loss = 0.250085, Learning Rate = 5.528887e-05\n",
      "Epoch 7620/20000: Train Loss = 0.442871, Test Loss = 0.246457, Learning Rate = 5.526786e-05\n",
      "Epoch 7621/20000: Train Loss = 0.442709, Test Loss = 0.247467, Learning Rate = 5.524686e-05\n",
      "Epoch 7622/20000: Train Loss = 0.442616, Test Loss = 0.245920, Learning Rate = 5.522586e-05\n",
      "Epoch 7623/20000: Train Loss = 0.442756, Test Loss = 0.244723, Learning Rate = 5.520488e-05\n",
      "Epoch 7624/20000: Train Loss = 0.442712, Test Loss = 0.247119, Learning Rate = 5.518390e-05\n",
      "Epoch 7625/20000: Train Loss = 0.442606, Test Loss = 0.245821, Learning Rate = 5.516294e-05\n",
      "Epoch 7626/20000: Train Loss = 0.442624, Test Loss = 0.244620, Learning Rate = 5.514197e-05\n",
      "Epoch 7627/20000: Train Loss = 0.442948, Test Loss = 0.242530, Learning Rate = 5.512102e-05\n",
      "Epoch 7628/20000: Train Loss = 0.442594, Test Loss = 0.246634, Learning Rate = 5.510008e-05\n",
      "Epoch 7629/20000: Train Loss = 0.442496, Test Loss = 0.251146, Learning Rate = 5.507914e-05\n",
      "Epoch 7630/20000: Train Loss = 0.442653, Test Loss = 0.251658, Learning Rate = 5.505821e-05\n",
      "Epoch 7631/20000: Train Loss = 0.442725, Test Loss = 0.248530, Learning Rate = 5.503729e-05\n",
      "Epoch 7632/20000: Train Loss = 0.442813, Test Loss = 0.248178, Learning Rate = 5.501638e-05\n",
      "Epoch 7633/20000: Train Loss = 0.442789, Test Loss = 0.250195, Learning Rate = 5.499547e-05\n",
      "Epoch 7634/20000: Train Loss = 0.442492, Test Loss = 0.247459, Learning Rate = 5.497458e-05\n",
      "Epoch 7635/20000: Train Loss = 0.442562, Test Loss = 0.247332, Learning Rate = 5.495369e-05\n",
      "Epoch 7636/20000: Train Loss = 0.442713, Test Loss = 0.244860, Learning Rate = 5.493281e-05\n",
      "Epoch 7637/20000: Train Loss = 0.442853, Test Loss = 0.243425, Learning Rate = 5.491193e-05\n",
      "Epoch 7638/20000: Train Loss = 0.443001, Test Loss = 0.248928, Learning Rate = 5.489107e-05\n",
      "Epoch 7639/20000: Train Loss = 0.442613, Test Loss = 0.247632, Learning Rate = 5.487021e-05\n",
      "Epoch 7640/20000: Train Loss = 0.442547, Test Loss = 0.248718, Learning Rate = 5.484936e-05\n",
      "Epoch 7641/20000: Train Loss = 0.442736, Test Loss = 0.248058, Learning Rate = 5.482852e-05\n",
      "Epoch 7642/20000: Train Loss = 0.442976, Test Loss = 0.248698, Learning Rate = 5.480769e-05\n",
      "Epoch 7643/20000: Train Loss = 0.442601, Test Loss = 0.244967, Learning Rate = 5.478686e-05\n",
      "Epoch 7644/20000: Train Loss = 0.442498, Test Loss = 0.247515, Learning Rate = 5.476605e-05\n",
      "Epoch 7645/20000: Train Loss = 0.442585, Test Loss = 0.249493, Learning Rate = 5.474524e-05\n",
      "Epoch 7646/20000: Train Loss = 0.442643, Test Loss = 0.249237, Learning Rate = 5.472443e-05\n",
      "Epoch 7647/20000: Train Loss = 0.442663, Test Loss = 0.249908, Learning Rate = 5.470364e-05\n",
      "Epoch 7648/20000: Train Loss = 0.442605, Test Loss = 0.247096, Learning Rate = 5.468286e-05\n",
      "Epoch 7649/20000: Train Loss = 0.442627, Test Loss = 0.249044, Learning Rate = 5.466208e-05\n",
      "Epoch 7650/20000: Train Loss = 0.442677, Test Loss = 0.247748, Learning Rate = 5.464131e-05\n",
      "Epoch 7651/20000: Train Loss = 0.442778, Test Loss = 0.246483, Learning Rate = 5.462054e-05\n",
      "Epoch 7652/20000: Train Loss = 0.442771, Test Loss = 0.248918, Learning Rate = 5.459979e-05\n",
      "Epoch 7653/20000: Train Loss = 0.442583, Test Loss = 0.251246, Learning Rate = 5.457904e-05\n",
      "Epoch 7654/20000: Train Loss = 0.443241, Test Loss = 0.249285, Learning Rate = 5.455831e-05\n",
      "Epoch 7655/20000: Train Loss = 0.442448, Test Loss = 0.245834, Learning Rate = 5.453757e-05\n",
      "Epoch 7656/20000: Train Loss = 0.442735, Test Loss = 0.247571, Learning Rate = 5.451685e-05\n",
      "Epoch 7657/20000: Train Loss = 0.442740, Test Loss = 0.249662, Learning Rate = 5.449614e-05\n",
      "Epoch 7658/20000: Train Loss = 0.442739, Test Loss = 0.247355, Learning Rate = 5.447543e-05\n",
      "Epoch 7659/20000: Train Loss = 0.442619, Test Loss = 0.247100, Learning Rate = 5.445473e-05\n",
      "Epoch 7660/20000: Train Loss = 0.442852, Test Loss = 0.247916, Learning Rate = 5.443404e-05\n",
      "Epoch 7661/20000: Train Loss = 0.442559, Test Loss = 0.251862, Learning Rate = 5.441336e-05\n",
      "Epoch 7662/20000: Train Loss = 0.442701, Test Loss = 0.250780, Learning Rate = 5.439268e-05\n",
      "Epoch 7663/20000: Train Loss = 0.442558, Test Loss = 0.250604, Learning Rate = 5.437201e-05\n",
      "Epoch 7664/20000: Train Loss = 0.442677, Test Loss = 0.250796, Learning Rate = 5.435135e-05\n",
      "Epoch 7665/20000: Train Loss = 0.442619, Test Loss = 0.246828, Learning Rate = 5.433070e-05\n",
      "Epoch 7666/20000: Train Loss = 0.442546, Test Loss = 0.249726, Learning Rate = 5.431006e-05\n",
      "Epoch 7667/20000: Train Loss = 0.442662, Test Loss = 0.252522, Learning Rate = 5.428942e-05\n",
      "Epoch 7668/20000: Train Loss = 0.442723, Test Loss = 0.251724, Learning Rate = 5.426879e-05\n",
      "Epoch 7669/20000: Train Loss = 0.443011, Test Loss = 0.246817, Learning Rate = 5.424817e-05\n",
      "Epoch 7670/20000: Train Loss = 0.442662, Test Loss = 0.249039, Learning Rate = 5.422756e-05\n",
      "Epoch 7671/20000: Train Loss = 0.442794, Test Loss = 0.243965, Learning Rate = 5.420695e-05\n",
      "Epoch 7672/20000: Train Loss = 0.442726, Test Loss = 0.245753, Learning Rate = 5.418636e-05\n",
      "Epoch 7673/20000: Train Loss = 0.442984, Test Loss = 0.248279, Learning Rate = 5.416577e-05\n",
      "Epoch 7674/20000: Train Loss = 0.442558, Test Loss = 0.242937, Learning Rate = 5.414519e-05\n",
      "Epoch 7675/20000: Train Loss = 0.442732, Test Loss = 0.247719, Learning Rate = 5.412461e-05\n",
      "Epoch 7676/20000: Train Loss = 0.442940, Test Loss = 0.250309, Learning Rate = 5.410405e-05\n",
      "Epoch 7677/20000: Train Loss = 0.442555, Test Loss = 0.245020, Learning Rate = 5.408349e-05\n",
      "Epoch 7678/20000: Train Loss = 0.442790, Test Loss = 0.246871, Learning Rate = 5.406294e-05\n",
      "Epoch 7679/20000: Train Loss = 0.442929, Test Loss = 0.244726, Learning Rate = 5.404239e-05\n",
      "Epoch 7680/20000: Train Loss = 0.442531, Test Loss = 0.246764, Learning Rate = 5.402186e-05\n",
      "Epoch 7681/20000: Train Loss = 0.442779, Test Loss = 0.249043, Learning Rate = 5.400133e-05\n",
      "Epoch 7682/20000: Train Loss = 0.442797, Test Loss = 0.247715, Learning Rate = 5.398081e-05\n",
      "Epoch 7683/20000: Train Loss = 0.442700, Test Loss = 0.248359, Learning Rate = 5.396030e-05\n",
      "Epoch 7684/20000: Train Loss = 0.442974, Test Loss = 0.247569, Learning Rate = 5.393980e-05\n",
      "Epoch 7685/20000: Train Loss = 0.442738, Test Loss = 0.250354, Learning Rate = 5.391930e-05\n",
      "Epoch 7686/20000: Train Loss = 0.442827, Test Loss = 0.249459, Learning Rate = 5.389882e-05\n",
      "Epoch 7687/20000: Train Loss = 0.442541, Test Loss = 0.248783, Learning Rate = 5.387834e-05\n",
      "Epoch 7688/20000: Train Loss = 0.442581, Test Loss = 0.247575, Learning Rate = 5.385786e-05\n",
      "Epoch 7689/20000: Train Loss = 0.442593, Test Loss = 0.248254, Learning Rate = 5.383740e-05\n",
      "Epoch 7690/20000: Train Loss = 0.442599, Test Loss = 0.249824, Learning Rate = 5.381694e-05\n",
      "Epoch 7691/20000: Train Loss = 0.442886, Test Loss = 0.247262, Learning Rate = 5.379649e-05\n",
      "Epoch 7692/20000: Train Loss = 0.442630, Test Loss = 0.248892, Learning Rate = 5.377605e-05\n",
      "Epoch 7693/20000: Train Loss = 0.443013, Test Loss = 0.248985, Learning Rate = 5.375562e-05\n",
      "Epoch 7694/20000: Train Loss = 0.442793, Test Loss = 0.251801, Learning Rate = 5.373519e-05\n",
      "Epoch 7695/20000: Train Loss = 0.442524, Test Loss = 0.247365, Learning Rate = 5.371477e-05\n",
      "Epoch 7696/20000: Train Loss = 0.442826, Test Loss = 0.249883, Learning Rate = 5.369436e-05\n",
      "Epoch 7697/20000: Train Loss = 0.442543, Test Loss = 0.246365, Learning Rate = 5.367396e-05\n",
      "Epoch 7698/20000: Train Loss = 0.442760, Test Loss = 0.249803, Learning Rate = 5.365357e-05\n",
      "Epoch 7699/20000: Train Loss = 0.443159, Test Loss = 0.247249, Learning Rate = 5.363318e-05\n",
      "Epoch 7700/20000: Train Loss = 0.442574, Test Loss = 0.247323, Learning Rate = 5.361280e-05\n",
      "Epoch 7701/20000: Train Loss = 0.442810, Test Loss = 0.251256, Learning Rate = 5.359243e-05\n",
      "Epoch 7702/20000: Train Loss = 0.442760, Test Loss = 0.252193, Learning Rate = 5.357207e-05\n",
      "Epoch 7703/20000: Train Loss = 0.442690, Test Loss = 0.251847, Learning Rate = 5.355171e-05\n",
      "Epoch 7704/20000: Train Loss = 0.442887, Test Loss = 0.247340, Learning Rate = 5.353136e-05\n",
      "Epoch 7705/20000: Train Loss = 0.442695, Test Loss = 0.248625, Learning Rate = 5.351102e-05\n",
      "Epoch 7706/20000: Train Loss = 0.442641, Test Loss = 0.249222, Learning Rate = 5.349069e-05\n",
      "Epoch 7707/20000: Train Loss = 0.442571, Test Loss = 0.249119, Learning Rate = 5.347036e-05\n",
      "Epoch 7708/20000: Train Loss = 0.442586, Test Loss = 0.246419, Learning Rate = 5.345005e-05\n",
      "Epoch 7709/20000: Train Loss = 0.442673, Test Loss = 0.247862, Learning Rate = 5.342974e-05\n",
      "Epoch 7710/20000: Train Loss = 0.442600, Test Loss = 0.245549, Learning Rate = 5.340944e-05\n",
      "Epoch 7711/20000: Train Loss = 0.442621, Test Loss = 0.247174, Learning Rate = 5.338914e-05\n",
      "Epoch 7712/20000: Train Loss = 0.442593, Test Loss = 0.249386, Learning Rate = 5.336885e-05\n",
      "Epoch 7713/20000: Train Loss = 0.442676, Test Loss = 0.248673, Learning Rate = 5.334858e-05\n",
      "Epoch 7714/20000: Train Loss = 0.442899, Test Loss = 0.250700, Learning Rate = 5.332831e-05\n",
      "Epoch 7715/20000: Train Loss = 0.442530, Test Loss = 0.247982, Learning Rate = 5.330804e-05\n",
      "Epoch 7716/20000: Train Loss = 0.442840, Test Loss = 0.244465, Learning Rate = 5.328779e-05\n",
      "Epoch 7717/20000: Train Loss = 0.442770, Test Loss = 0.248585, Learning Rate = 5.326754e-05\n",
      "Epoch 7718/20000: Train Loss = 0.442685, Test Loss = 0.247877, Learning Rate = 5.324730e-05\n",
      "Epoch 7719/20000: Train Loss = 0.442719, Test Loss = 0.245808, Learning Rate = 5.322707e-05\n",
      "Epoch 7720/20000: Train Loss = 0.442606, Test Loss = 0.245977, Learning Rate = 5.320684e-05\n",
      "Epoch 7721/20000: Train Loss = 0.442718, Test Loss = 0.250419, Learning Rate = 5.318662e-05\n",
      "Epoch 7722/20000: Train Loss = 0.442576, Test Loss = 0.249106, Learning Rate = 5.316641e-05\n",
      "Epoch 7723/20000: Train Loss = 0.442946, Test Loss = 0.248409, Learning Rate = 5.314621e-05\n",
      "Epoch 7724/20000: Train Loss = 0.442758, Test Loss = 0.248613, Learning Rate = 5.312602e-05\n",
      "Epoch 7725/20000: Train Loss = 0.442713, Test Loss = 0.245971, Learning Rate = 5.310583e-05\n",
      "Epoch 7726/20000: Train Loss = 0.442742, Test Loss = 0.246721, Learning Rate = 5.308565e-05\n",
      "Epoch 7727/20000: Train Loss = 0.442663, Test Loss = 0.247993, Learning Rate = 5.306548e-05\n",
      "Epoch 7728/20000: Train Loss = 0.442862, Test Loss = 0.247276, Learning Rate = 5.304532e-05\n",
      "Epoch 7729/20000: Train Loss = 0.442691, Test Loss = 0.246028, Learning Rate = 5.302516e-05\n",
      "Epoch 7730/20000: Train Loss = 0.442864, Test Loss = 0.243939, Learning Rate = 5.300501e-05\n",
      "Epoch 7731/20000: Train Loss = 0.442775, Test Loss = 0.249945, Learning Rate = 5.298487e-05\n",
      "Epoch 7732/20000: Train Loss = 0.442646, Test Loss = 0.247233, Learning Rate = 5.296474e-05\n",
      "Epoch 7733/20000: Train Loss = 0.442589, Test Loss = 0.247390, Learning Rate = 5.294462e-05\n",
      "Epoch 7734/20000: Train Loss = 0.442613, Test Loss = 0.250630, Learning Rate = 5.292450e-05\n",
      "Epoch 7735/20000: Train Loss = 0.442564, Test Loss = 0.248650, Learning Rate = 5.290439e-05\n",
      "Epoch 7736/20000: Train Loss = 0.442568, Test Loss = 0.247390, Learning Rate = 5.288429e-05\n",
      "Epoch 7737/20000: Train Loss = 0.442670, Test Loss = 0.247449, Learning Rate = 5.286419e-05\n",
      "Epoch 7738/20000: Train Loss = 0.442758, Test Loss = 0.246215, Learning Rate = 5.284410e-05\n",
      "Epoch 7739/20000: Train Loss = 0.442799, Test Loss = 0.246347, Learning Rate = 5.282403e-05\n",
      "Epoch 7740/20000: Train Loss = 0.442741, Test Loss = 0.246290, Learning Rate = 5.280395e-05\n",
      "Epoch 7741/20000: Train Loss = 0.443336, Test Loss = 0.249202, Learning Rate = 5.278389e-05\n",
      "Epoch 7742/20000: Train Loss = 0.442570, Test Loss = 0.247679, Learning Rate = 5.276383e-05\n",
      "Epoch 7743/20000: Train Loss = 0.442856, Test Loss = 0.248391, Learning Rate = 5.274378e-05\n",
      "Epoch 7744/20000: Train Loss = 0.442920, Test Loss = 0.244016, Learning Rate = 5.272374e-05\n",
      "Epoch 7745/20000: Train Loss = 0.442790, Test Loss = 0.246445, Learning Rate = 5.270371e-05\n",
      "Epoch 7746/20000: Train Loss = 0.442714, Test Loss = 0.245868, Learning Rate = 5.268368e-05\n",
      "Epoch 7747/20000: Train Loss = 0.442765, Test Loss = 0.245842, Learning Rate = 5.266367e-05\n",
      "Epoch 7748/20000: Train Loss = 0.442670, Test Loss = 0.248481, Learning Rate = 5.264365e-05\n",
      "Epoch 7749/20000: Train Loss = 0.442544, Test Loss = 0.247904, Learning Rate = 5.262365e-05\n",
      "Epoch 7750/20000: Train Loss = 0.442900, Test Loss = 0.249021, Learning Rate = 5.260366e-05\n",
      "Epoch 7751/20000: Train Loss = 0.442635, Test Loss = 0.246629, Learning Rate = 5.258367e-05\n",
      "Epoch 7752/20000: Train Loss = 0.442761, Test Loss = 0.246899, Learning Rate = 5.256369e-05\n",
      "Epoch 7753/20000: Train Loss = 0.442734, Test Loss = 0.244524, Learning Rate = 5.254371e-05\n",
      "Epoch 7754/20000: Train Loss = 0.442889, Test Loss = 0.243877, Learning Rate = 5.252375e-05\n",
      "Epoch 7755/20000: Train Loss = 0.442486, Test Loss = 0.246617, Learning Rate = 5.250379e-05\n",
      "Epoch 7756/20000: Train Loss = 0.442821, Test Loss = 0.248402, Learning Rate = 5.248384e-05\n",
      "Epoch 7757/20000: Train Loss = 0.442843, Test Loss = 0.250030, Learning Rate = 5.246390e-05\n",
      "Epoch 7758/20000: Train Loss = 0.442663, Test Loss = 0.246048, Learning Rate = 5.244396e-05\n",
      "Epoch 7759/20000: Train Loss = 0.442713, Test Loss = 0.247346, Learning Rate = 5.242404e-05\n",
      "Epoch 7760/20000: Train Loss = 0.442633, Test Loss = 0.248998, Learning Rate = 5.240412e-05\n",
      "Epoch 7761/20000: Train Loss = 0.442618, Test Loss = 0.250474, Learning Rate = 5.238421e-05\n",
      "Epoch 7762/20000: Train Loss = 0.442888, Test Loss = 0.249897, Learning Rate = 5.236430e-05\n",
      "Epoch 7763/20000: Train Loss = 0.442578, Test Loss = 0.248074, Learning Rate = 5.234440e-05\n",
      "Epoch 7764/20000: Train Loss = 0.442645, Test Loss = 0.247710, Learning Rate = 5.232451e-05\n",
      "Epoch 7765/20000: Train Loss = 0.442686, Test Loss = 0.248696, Learning Rate = 5.230463e-05\n",
      "Epoch 7766/20000: Train Loss = 0.442744, Test Loss = 0.247777, Learning Rate = 5.228476e-05\n",
      "Epoch 7767/20000: Train Loss = 0.443015, Test Loss = 0.246593, Learning Rate = 5.226489e-05\n",
      "Epoch 7768/20000: Train Loss = 0.442597, Test Loss = 0.248032, Learning Rate = 5.224503e-05\n",
      "Epoch 7769/20000: Train Loss = 0.442840, Test Loss = 0.250302, Learning Rate = 5.222518e-05\n",
      "Epoch 7770/20000: Train Loss = 0.442698, Test Loss = 0.250227, Learning Rate = 5.220534e-05\n",
      "Epoch 7771/20000: Train Loss = 0.443087, Test Loss = 0.244805, Learning Rate = 5.218550e-05\n",
      "Epoch 7772/20000: Train Loss = 0.442589, Test Loss = 0.249706, Learning Rate = 5.216567e-05\n",
      "Epoch 7773/20000: Train Loss = 0.442966, Test Loss = 0.253424, Learning Rate = 5.214585e-05\n",
      "Epoch 7774/20000: Train Loss = 0.442620, Test Loss = 0.245976, Learning Rate = 5.212604e-05\n",
      "Epoch 7775/20000: Train Loss = 0.442795, Test Loss = 0.244204, Learning Rate = 5.210623e-05\n",
      "Epoch 7776/20000: Train Loss = 0.442761, Test Loss = 0.248402, Learning Rate = 5.208643e-05\n",
      "Epoch 7777/20000: Train Loss = 0.442681, Test Loss = 0.248893, Learning Rate = 5.206664e-05\n",
      "Epoch 7778/20000: Train Loss = 0.442643, Test Loss = 0.249435, Learning Rate = 5.204685e-05\n",
      "Epoch 7779/20000: Train Loss = 0.443019, Test Loss = 0.245313, Learning Rate = 5.202708e-05\n",
      "Epoch 7780/20000: Train Loss = 0.442716, Test Loss = 0.248361, Learning Rate = 5.200731e-05\n",
      "Epoch 7781/20000: Train Loss = 0.442550, Test Loss = 0.246811, Learning Rate = 5.198755e-05\n",
      "Epoch 7782/20000: Train Loss = 0.442937, Test Loss = 0.245063, Learning Rate = 5.196779e-05\n",
      "Epoch 7783/20000: Train Loss = 0.442572, Test Loss = 0.246767, Learning Rate = 5.194805e-05\n",
      "Epoch 7784/20000: Train Loss = 0.442549, Test Loss = 0.247624, Learning Rate = 5.192831e-05\n",
      "Epoch 7785/20000: Train Loss = 0.442654, Test Loss = 0.245215, Learning Rate = 5.190858e-05\n",
      "Epoch 7786/20000: Train Loss = 0.442651, Test Loss = 0.244503, Learning Rate = 5.188885e-05\n",
      "Epoch 7787/20000: Train Loss = 0.442662, Test Loss = 0.246242, Learning Rate = 5.186914e-05\n",
      "Epoch 7788/20000: Train Loss = 0.442643, Test Loss = 0.247841, Learning Rate = 5.184943e-05\n",
      "Epoch 7789/20000: Train Loss = 0.442659, Test Loss = 0.247847, Learning Rate = 5.182973e-05\n",
      "Epoch 7790/20000: Train Loss = 0.442728, Test Loss = 0.245765, Learning Rate = 5.181003e-05\n",
      "Epoch 7791/20000: Train Loss = 0.442803, Test Loss = 0.247897, Learning Rate = 5.179035e-05\n",
      "Epoch 7792/20000: Train Loss = 0.442602, Test Loss = 0.246073, Learning Rate = 5.177067e-05\n",
      "Epoch 7793/20000: Train Loss = 0.442598, Test Loss = 0.243826, Learning Rate = 5.175100e-05\n",
      "Epoch 7794/20000: Train Loss = 0.442752, Test Loss = 0.244490, Learning Rate = 5.173133e-05\n",
      "Epoch 7795/20000: Train Loss = 0.442621, Test Loss = 0.242356, Learning Rate = 5.171168e-05\n",
      "Epoch 7796/20000: Train Loss = 0.442773, Test Loss = 0.244343, Learning Rate = 5.169203e-05\n",
      "Epoch 7797/20000: Train Loss = 0.442760, Test Loss = 0.245609, Learning Rate = 5.167239e-05\n",
      "Epoch 7798/20000: Train Loss = 0.442706, Test Loss = 0.246887, Learning Rate = 5.165275e-05\n",
      "Epoch 7799/20000: Train Loss = 0.442706, Test Loss = 0.246300, Learning Rate = 5.163312e-05\n",
      "Epoch 7800/20000: Train Loss = 0.442767, Test Loss = 0.245081, Learning Rate = 5.161351e-05\n",
      "Epoch 7801/20000: Train Loss = 0.442609, Test Loss = 0.248546, Learning Rate = 5.159389e-05\n",
      "Epoch 7802/20000: Train Loss = 0.442470, Test Loss = 0.245838, Learning Rate = 5.157429e-05\n",
      "Epoch 7803/20000: Train Loss = 0.442707, Test Loss = 0.246392, Learning Rate = 5.155469e-05\n",
      "Epoch 7804/20000: Train Loss = 0.442503, Test Loss = 0.246837, Learning Rate = 5.153510e-05\n",
      "Epoch 7805/20000: Train Loss = 0.442789, Test Loss = 0.249274, Learning Rate = 5.151552e-05\n",
      "Epoch 7806/20000: Train Loss = 0.442545, Test Loss = 0.248292, Learning Rate = 5.149595e-05\n",
      "Epoch 7807/20000: Train Loss = 0.442842, Test Loss = 0.249189, Learning Rate = 5.147638e-05\n",
      "Epoch 7808/20000: Train Loss = 0.442500, Test Loss = 0.246572, Learning Rate = 5.145682e-05\n",
      "Epoch 7809/20000: Train Loss = 0.442784, Test Loss = 0.247960, Learning Rate = 5.143727e-05\n",
      "Epoch 7810/20000: Train Loss = 0.442755, Test Loss = 0.245747, Learning Rate = 5.141772e-05\n",
      "Epoch 7811/20000: Train Loss = 0.443130, Test Loss = 0.243705, Learning Rate = 5.139819e-05\n",
      "Epoch 7812/20000: Train Loss = 0.442623, Test Loss = 0.244388, Learning Rate = 5.137866e-05\n",
      "Epoch 7813/20000: Train Loss = 0.442579, Test Loss = 0.245466, Learning Rate = 5.135913e-05\n",
      "Epoch 7814/20000: Train Loss = 0.442703, Test Loss = 0.247175, Learning Rate = 5.133962e-05\n",
      "Epoch 7815/20000: Train Loss = 0.442647, Test Loss = 0.247647, Learning Rate = 5.132011e-05\n",
      "Epoch 7816/20000: Train Loss = 0.442795, Test Loss = 0.247884, Learning Rate = 5.130061e-05\n",
      "Epoch 7817/20000: Train Loss = 0.442593, Test Loss = 0.247104, Learning Rate = 5.128112e-05\n",
      "Epoch 7818/20000: Train Loss = 0.442540, Test Loss = 0.245775, Learning Rate = 5.126163e-05\n",
      "Epoch 7819/20000: Train Loss = 0.442654, Test Loss = 0.247384, Learning Rate = 5.124215e-05\n",
      "Epoch 7820/20000: Train Loss = 0.442731, Test Loss = 0.250260, Learning Rate = 5.122268e-05\n",
      "Epoch 7821/20000: Train Loss = 0.442700, Test Loss = 0.243881, Learning Rate = 5.120322e-05\n",
      "Epoch 7822/20000: Train Loss = 0.442920, Test Loss = 0.245316, Learning Rate = 5.118376e-05\n",
      "Epoch 7823/20000: Train Loss = 0.442702, Test Loss = 0.245731, Learning Rate = 5.116432e-05\n",
      "Epoch 7824/20000: Train Loss = 0.442497, Test Loss = 0.246491, Learning Rate = 5.114487e-05\n",
      "Epoch 7825/20000: Train Loss = 0.442643, Test Loss = 0.246045, Learning Rate = 5.112544e-05\n",
      "Epoch 7826/20000: Train Loss = 0.442818, Test Loss = 0.246539, Learning Rate = 5.110601e-05\n",
      "Epoch 7827/20000: Train Loss = 0.442749, Test Loss = 0.251307, Learning Rate = 5.108660e-05\n",
      "Epoch 7828/20000: Train Loss = 0.442703, Test Loss = 0.246970, Learning Rate = 5.106718e-05\n",
      "Epoch 7829/20000: Train Loss = 0.442635, Test Loss = 0.249291, Learning Rate = 5.104778e-05\n",
      "Epoch 7830/20000: Train Loss = 0.442669, Test Loss = 0.246718, Learning Rate = 5.102838e-05\n",
      "Epoch 7831/20000: Train Loss = 0.442684, Test Loss = 0.249122, Learning Rate = 5.100899e-05\n",
      "Epoch 7832/20000: Train Loss = 0.442624, Test Loss = 0.250103, Learning Rate = 5.098961e-05\n",
      "Epoch 7833/20000: Train Loss = 0.442718, Test Loss = 0.248842, Learning Rate = 5.097024e-05\n",
      "Epoch 7834/20000: Train Loss = 0.442507, Test Loss = 0.245663, Learning Rate = 5.095087e-05\n",
      "Epoch 7835/20000: Train Loss = 0.442798, Test Loss = 0.247690, Learning Rate = 5.093151e-05\n",
      "Epoch 7836/20000: Train Loss = 0.442493, Test Loss = 0.245415, Learning Rate = 5.091216e-05\n",
      "Epoch 7837/20000: Train Loss = 0.442680, Test Loss = 0.245855, Learning Rate = 5.089281e-05\n",
      "Epoch 7838/20000: Train Loss = 0.442547, Test Loss = 0.244961, Learning Rate = 5.087347e-05\n",
      "Epoch 7839/20000: Train Loss = 0.442903, Test Loss = 0.246755, Learning Rate = 5.085414e-05\n",
      "Epoch 7840/20000: Train Loss = 0.442648, Test Loss = 0.246565, Learning Rate = 5.083482e-05\n",
      "Epoch 7841/20000: Train Loss = 0.442795, Test Loss = 0.246981, Learning Rate = 5.081550e-05\n",
      "Epoch 7842/20000: Train Loss = 0.442663, Test Loss = 0.246910, Learning Rate = 5.079620e-05\n",
      "Epoch 7843/20000: Train Loss = 0.442783, Test Loss = 0.247390, Learning Rate = 5.077689e-05\n",
      "Epoch 7844/20000: Train Loss = 0.442594, Test Loss = 0.246576, Learning Rate = 5.075760e-05\n",
      "Epoch 7845/20000: Train Loss = 0.442597, Test Loss = 0.247767, Learning Rate = 5.073831e-05\n",
      "Epoch 7846/20000: Train Loss = 0.442778, Test Loss = 0.250583, Learning Rate = 5.071904e-05\n",
      "Epoch 7847/20000: Train Loss = 0.442585, Test Loss = 0.249943, Learning Rate = 5.069976e-05\n",
      "Epoch 7848/20000: Train Loss = 0.442682, Test Loss = 0.250571, Learning Rate = 5.068050e-05\n",
      "Epoch 7849/20000: Train Loss = 0.442685, Test Loss = 0.247600, Learning Rate = 5.066124e-05\n",
      "Epoch 7850/20000: Train Loss = 0.442674, Test Loss = 0.249800, Learning Rate = 5.064199e-05\n",
      "Epoch 7851/20000: Train Loss = 0.442893, Test Loss = 0.246483, Learning Rate = 5.062275e-05\n",
      "Epoch 7852/20000: Train Loss = 0.442697, Test Loss = 0.249846, Learning Rate = 5.060351e-05\n",
      "Epoch 7853/20000: Train Loss = 0.442583, Test Loss = 0.247006, Learning Rate = 5.058429e-05\n",
      "Epoch 7854/20000: Train Loss = 0.442584, Test Loss = 0.249699, Learning Rate = 5.056507e-05\n",
      "Epoch 7855/20000: Train Loss = 0.442825, Test Loss = 0.247610, Learning Rate = 5.054585e-05\n",
      "Epoch 7856/20000: Train Loss = 0.442674, Test Loss = 0.250688, Learning Rate = 5.052665e-05\n",
      "Epoch 7857/20000: Train Loss = 0.442724, Test Loss = 0.248676, Learning Rate = 5.050745e-05\n",
      "Epoch 7858/20000: Train Loss = 0.442597, Test Loss = 0.247845, Learning Rate = 5.048826e-05\n",
      "Epoch 7859/20000: Train Loss = 0.442673, Test Loss = 0.247958, Learning Rate = 5.046907e-05\n",
      "Epoch 7860/20000: Train Loss = 0.442774, Test Loss = 0.249088, Learning Rate = 5.044989e-05\n",
      "Epoch 7861/20000: Train Loss = 0.442768, Test Loss = 0.248089, Learning Rate = 5.043073e-05\n",
      "Epoch 7862/20000: Train Loss = 0.443085, Test Loss = 0.248762, Learning Rate = 5.041156e-05\n",
      "Epoch 7863/20000: Train Loss = 0.442932, Test Loss = 0.248127, Learning Rate = 5.039241e-05\n",
      "Epoch 7864/20000: Train Loss = 0.442746, Test Loss = 0.248683, Learning Rate = 5.037326e-05\n",
      "Epoch 7865/20000: Train Loss = 0.442658, Test Loss = 0.245111, Learning Rate = 5.035412e-05\n",
      "Epoch 7866/20000: Train Loss = 0.442757, Test Loss = 0.246389, Learning Rate = 5.033499e-05\n",
      "Epoch 7867/20000: Train Loss = 0.442693, Test Loss = 0.246393, Learning Rate = 5.031586e-05\n",
      "Epoch 7868/20000: Train Loss = 0.442893, Test Loss = 0.245536, Learning Rate = 5.029674e-05\n",
      "Epoch 7869/20000: Train Loss = 0.442527, Test Loss = 0.249558, Learning Rate = 5.027763e-05\n",
      "Epoch 7870/20000: Train Loss = 0.442798, Test Loss = 0.248309, Learning Rate = 5.025853e-05\n",
      "Epoch 7871/20000: Train Loss = 0.442728, Test Loss = 0.247716, Learning Rate = 5.023943e-05\n",
      "Epoch 7872/20000: Train Loss = 0.442696, Test Loss = 0.251016, Learning Rate = 5.022034e-05\n",
      "Epoch 7873/20000: Train Loss = 0.442863, Test Loss = 0.251288, Learning Rate = 5.020126e-05\n",
      "Epoch 7874/20000: Train Loss = 0.442583, Test Loss = 0.248707, Learning Rate = 5.018218e-05\n",
      "Epoch 7875/20000: Train Loss = 0.442616, Test Loss = 0.248643, Learning Rate = 5.016311e-05\n",
      "Epoch 7876/20000: Train Loss = 0.442714, Test Loss = 0.247985, Learning Rate = 5.014405e-05\n",
      "Epoch 7877/20000: Train Loss = 0.442584, Test Loss = 0.246444, Learning Rate = 5.012500e-05\n",
      "Epoch 7878/20000: Train Loss = 0.442788, Test Loss = 0.246492, Learning Rate = 5.010595e-05\n",
      "Epoch 7879/20000: Train Loss = 0.442823, Test Loss = 0.245195, Learning Rate = 5.008692e-05\n",
      "Epoch 7880/20000: Train Loss = 0.442970, Test Loss = 0.245717, Learning Rate = 5.006788e-05\n",
      "Epoch 7881/20000: Train Loss = 0.442783, Test Loss = 0.243267, Learning Rate = 5.004886e-05\n",
      "Epoch 7882/20000: Train Loss = 0.442805, Test Loss = 0.248134, Learning Rate = 5.002984e-05\n",
      "Epoch 7883/20000: Train Loss = 0.442561, Test Loss = 0.248343, Learning Rate = 5.001083e-05\n",
      "Epoch 7884/20000: Train Loss = 0.442737, Test Loss = 0.251006, Learning Rate = 4.999183e-05\n",
      "Epoch 7885/20000: Train Loss = 0.442455, Test Loss = 0.248351, Learning Rate = 4.997283e-05\n",
      "Epoch 7886/20000: Train Loss = 0.442512, Test Loss = 0.247320, Learning Rate = 4.995385e-05\n",
      "Epoch 7887/20000: Train Loss = 0.442616, Test Loss = 0.248076, Learning Rate = 4.993486e-05\n",
      "Epoch 7888/20000: Train Loss = 0.442752, Test Loss = 0.248438, Learning Rate = 4.991589e-05\n",
      "Epoch 7889/20000: Train Loss = 0.442546, Test Loss = 0.246059, Learning Rate = 4.989692e-05\n",
      "Epoch 7890/20000: Train Loss = 0.442537, Test Loss = 0.249955, Learning Rate = 4.987796e-05\n",
      "Epoch 7891/20000: Train Loss = 0.442635, Test Loss = 0.249167, Learning Rate = 4.985901e-05\n",
      "Epoch 7892/20000: Train Loss = 0.442625, Test Loss = 0.247898, Learning Rate = 4.984007e-05\n",
      "Epoch 7893/20000: Train Loss = 0.442600, Test Loss = 0.247900, Learning Rate = 4.982113e-05\n",
      "Epoch 7894/20000: Train Loss = 0.442715, Test Loss = 0.249942, Learning Rate = 4.980220e-05\n",
      "Epoch 7895/20000: Train Loss = 0.443021, Test Loss = 0.250954, Learning Rate = 4.978327e-05\n",
      "Epoch 7896/20000: Train Loss = 0.442584, Test Loss = 0.252405, Learning Rate = 4.976436e-05\n",
      "Epoch 7897/20000: Train Loss = 0.442602, Test Loss = 0.250545, Learning Rate = 4.974545e-05\n",
      "Epoch 7898/20000: Train Loss = 0.442766, Test Loss = 0.247375, Learning Rate = 4.972655e-05\n",
      "Epoch 7899/20000: Train Loss = 0.442682, Test Loss = 0.249402, Learning Rate = 4.970765e-05\n",
      "Epoch 7900/20000: Train Loss = 0.442825, Test Loss = 0.247757, Learning Rate = 4.968877e-05\n",
      "Epoch 7901/20000: Train Loss = 0.442706, Test Loss = 0.248414, Learning Rate = 4.966988e-05\n",
      "Epoch 7902/20000: Train Loss = 0.442770, Test Loss = 0.247552, Learning Rate = 4.965101e-05\n",
      "Epoch 7903/20000: Train Loss = 0.442503, Test Loss = 0.248439, Learning Rate = 4.963215e-05\n",
      "Epoch 7904/20000: Train Loss = 0.442851, Test Loss = 0.250507, Learning Rate = 4.961329e-05\n",
      "Epoch 7905/20000: Train Loss = 0.442760, Test Loss = 0.247248, Learning Rate = 4.959443e-05\n",
      "Epoch 7906/20000: Train Loss = 0.442514, Test Loss = 0.248224, Learning Rate = 4.957559e-05\n",
      "Epoch 7907/20000: Train Loss = 0.442626, Test Loss = 0.247730, Learning Rate = 4.955675e-05\n",
      "Epoch 7908/20000: Train Loss = 0.443069, Test Loss = 0.245307, Learning Rate = 4.953792e-05\n",
      "Epoch 7909/20000: Train Loss = 0.442585, Test Loss = 0.250037, Learning Rate = 4.951910e-05\n",
      "Epoch 7910/20000: Train Loss = 0.442663, Test Loss = 0.250360, Learning Rate = 4.950028e-05\n",
      "Epoch 7911/20000: Train Loss = 0.442714, Test Loss = 0.248768, Learning Rate = 4.948147e-05\n",
      "Epoch 7912/20000: Train Loss = 0.442699, Test Loss = 0.248181, Learning Rate = 4.946267e-05\n",
      "Epoch 7913/20000: Train Loss = 0.442982, Test Loss = 0.247963, Learning Rate = 4.944388e-05\n",
      "Epoch 7914/20000: Train Loss = 0.442561, Test Loss = 0.249727, Learning Rate = 4.942509e-05\n",
      "Epoch 7915/20000: Train Loss = 0.442887, Test Loss = 0.249131, Learning Rate = 4.940631e-05\n",
      "Epoch 7916/20000: Train Loss = 0.442629, Test Loss = 0.250740, Learning Rate = 4.938754e-05\n",
      "Epoch 7917/20000: Train Loss = 0.442673, Test Loss = 0.251138, Learning Rate = 4.936877e-05\n",
      "Epoch 7918/20000: Train Loss = 0.442721, Test Loss = 0.250357, Learning Rate = 4.935001e-05\n",
      "Epoch 7919/20000: Train Loss = 0.442583, Test Loss = 0.250045, Learning Rate = 4.933126e-05\n",
      "Epoch 7920/20000: Train Loss = 0.442522, Test Loss = 0.249061, Learning Rate = 4.931252e-05\n",
      "Epoch 7921/20000: Train Loss = 0.442697, Test Loss = 0.247480, Learning Rate = 4.929378e-05\n",
      "Epoch 7922/20000: Train Loss = 0.442736, Test Loss = 0.248247, Learning Rate = 4.927505e-05\n",
      "Epoch 7923/20000: Train Loss = 0.442583, Test Loss = 0.247200, Learning Rate = 4.925633e-05\n",
      "Epoch 7924/20000: Train Loss = 0.442579, Test Loss = 0.247131, Learning Rate = 4.923761e-05\n",
      "Epoch 7925/20000: Train Loss = 0.442874, Test Loss = 0.250346, Learning Rate = 4.921890e-05\n",
      "Epoch 7926/20000: Train Loss = 0.442558, Test Loss = 0.250361, Learning Rate = 4.920020e-05\n",
      "Epoch 7927/20000: Train Loss = 0.442713, Test Loss = 0.250200, Learning Rate = 4.918150e-05\n",
      "Epoch 7928/20000: Train Loss = 0.442718, Test Loss = 0.253443, Learning Rate = 4.916282e-05\n",
      "Epoch 7929/20000: Train Loss = 0.442644, Test Loss = 0.248743, Learning Rate = 4.914414e-05\n",
      "Epoch 7930/20000: Train Loss = 0.442782, Test Loss = 0.246093, Learning Rate = 4.912546e-05\n",
      "Epoch 7931/20000: Train Loss = 0.442554, Test Loss = 0.247687, Learning Rate = 4.910680e-05\n",
      "Epoch 7932/20000: Train Loss = 0.442648, Test Loss = 0.247779, Learning Rate = 4.908814e-05\n",
      "Epoch 7933/20000: Train Loss = 0.443093, Test Loss = 0.249471, Learning Rate = 4.906949e-05\n",
      "Epoch 7934/20000: Train Loss = 0.442654, Test Loss = 0.245352, Learning Rate = 4.905084e-05\n",
      "Epoch 7935/20000: Train Loss = 0.442906, Test Loss = 0.242500, Learning Rate = 4.903220e-05\n",
      "Epoch 7936/20000: Train Loss = 0.442653, Test Loss = 0.246172, Learning Rate = 4.901357e-05\n",
      "Epoch 7937/20000: Train Loss = 0.442803, Test Loss = 0.245753, Learning Rate = 4.899495e-05\n",
      "Epoch 7938/20000: Train Loss = 0.442630, Test Loss = 0.247360, Learning Rate = 4.897633e-05\n",
      "Epoch 7939/20000: Train Loss = 0.442576, Test Loss = 0.246546, Learning Rate = 4.895772e-05\n",
      "Epoch 7940/20000: Train Loss = 0.442709, Test Loss = 0.244416, Learning Rate = 4.893912e-05\n",
      "Epoch 7941/20000: Train Loss = 0.442845, Test Loss = 0.251172, Learning Rate = 4.892052e-05\n",
      "Epoch 7942/20000: Train Loss = 0.442510, Test Loss = 0.250399, Learning Rate = 4.890193e-05\n",
      "Epoch 7943/20000: Train Loss = 0.442757, Test Loss = 0.251095, Learning Rate = 4.888335e-05\n",
      "Epoch 7944/20000: Train Loss = 0.442523, Test Loss = 0.249694, Learning Rate = 4.886478e-05\n",
      "Epoch 7945/20000: Train Loss = 0.442593, Test Loss = 0.247245, Learning Rate = 4.884621e-05\n",
      "Epoch 7946/20000: Train Loss = 0.442507, Test Loss = 0.248027, Learning Rate = 4.882765e-05\n",
      "Epoch 7947/20000: Train Loss = 0.442604, Test Loss = 0.250826, Learning Rate = 4.880910e-05\n",
      "Epoch 7948/20000: Train Loss = 0.442673, Test Loss = 0.249438, Learning Rate = 4.879055e-05\n",
      "Epoch 7949/20000: Train Loss = 0.442621, Test Loss = 0.247917, Learning Rate = 4.877201e-05\n",
      "Epoch 7950/20000: Train Loss = 0.443083, Test Loss = 0.251227, Learning Rate = 4.875348e-05\n",
      "Epoch 7951/20000: Train Loss = 0.442910, Test Loss = 0.245674, Learning Rate = 4.873496e-05\n",
      "Epoch 7952/20000: Train Loss = 0.442872, Test Loss = 0.246215, Learning Rate = 4.871644e-05\n",
      "Epoch 7953/20000: Train Loss = 0.442647, Test Loss = 0.247538, Learning Rate = 4.869793e-05\n",
      "Epoch 7954/20000: Train Loss = 0.442903, Test Loss = 0.251687, Learning Rate = 4.867942e-05\n",
      "Epoch 7955/20000: Train Loss = 0.442786, Test Loss = 0.249010, Learning Rate = 4.866093e-05\n",
      "Epoch 7956/20000: Train Loss = 0.442708, Test Loss = 0.249628, Learning Rate = 4.864244e-05\n",
      "Epoch 7957/20000: Train Loss = 0.442709, Test Loss = 0.247883, Learning Rate = 4.862395e-05\n",
      "Epoch 7958/20000: Train Loss = 0.442849, Test Loss = 0.246868, Learning Rate = 4.860548e-05\n",
      "Epoch 7959/20000: Train Loss = 0.442839, Test Loss = 0.248776, Learning Rate = 4.858701e-05\n",
      "Epoch 7960/20000: Train Loss = 0.442580, Test Loss = 0.245576, Learning Rate = 4.856855e-05\n",
      "Epoch 7961/20000: Train Loss = 0.442700, Test Loss = 0.247880, Learning Rate = 4.855009e-05\n",
      "Epoch 7962/20000: Train Loss = 0.442663, Test Loss = 0.247250, Learning Rate = 4.853164e-05\n",
      "Epoch 7963/20000: Train Loss = 0.442658, Test Loss = 0.246853, Learning Rate = 4.851320e-05\n",
      "Epoch 7964/20000: Train Loss = 0.442720, Test Loss = 0.243134, Learning Rate = 4.849477e-05\n",
      "Epoch 7965/20000: Train Loss = 0.442580, Test Loss = 0.244896, Learning Rate = 4.847634e-05\n",
      "Epoch 7966/20000: Train Loss = 0.442730, Test Loss = 0.246202, Learning Rate = 4.845792e-05\n",
      "Epoch 7967/20000: Train Loss = 0.442874, Test Loss = 0.244628, Learning Rate = 4.843951e-05\n",
      "Epoch 7968/20000: Train Loss = 0.442751, Test Loss = 0.245226, Learning Rate = 4.842111e-05\n",
      "Epoch 7969/20000: Train Loss = 0.442833, Test Loss = 0.246798, Learning Rate = 4.840271e-05\n",
      "Epoch 7970/20000: Train Loss = 0.442636, Test Loss = 0.245830, Learning Rate = 4.838432e-05\n",
      "Epoch 7971/20000: Train Loss = 0.442689, Test Loss = 0.244865, Learning Rate = 4.836593e-05\n",
      "Epoch 7972/20000: Train Loss = 0.442621, Test Loss = 0.246265, Learning Rate = 4.834755e-05\n",
      "Epoch 7973/20000: Train Loss = 0.442640, Test Loss = 0.245299, Learning Rate = 4.832918e-05\n",
      "Epoch 7974/20000: Train Loss = 0.442588, Test Loss = 0.245735, Learning Rate = 4.831082e-05\n",
      "Epoch 7975/20000: Train Loss = 0.442739, Test Loss = 0.250019, Learning Rate = 4.829246e-05\n",
      "Epoch 7976/20000: Train Loss = 0.442677, Test Loss = 0.246292, Learning Rate = 4.827411e-05\n",
      "Epoch 7977/20000: Train Loss = 0.442610, Test Loss = 0.248323, Learning Rate = 4.825577e-05\n",
      "Epoch 7978/20000: Train Loss = 0.442600, Test Loss = 0.247535, Learning Rate = 4.823743e-05\n",
      "Epoch 7979/20000: Train Loss = 0.442778, Test Loss = 0.249671, Learning Rate = 4.821910e-05\n",
      "Epoch 7980/20000: Train Loss = 0.442585, Test Loss = 0.247435, Learning Rate = 4.820078e-05\n",
      "Epoch 7981/20000: Train Loss = 0.442478, Test Loss = 0.245288, Learning Rate = 4.818247e-05\n",
      "Epoch 7982/20000: Train Loss = 0.442879, Test Loss = 0.247485, Learning Rate = 4.816416e-05\n",
      "Epoch 7983/20000: Train Loss = 0.442826, Test Loss = 0.247394, Learning Rate = 4.814586e-05\n",
      "Epoch 7984/20000: Train Loss = 0.442681, Test Loss = 0.249763, Learning Rate = 4.812756e-05\n",
      "Epoch 7985/20000: Train Loss = 0.442585, Test Loss = 0.244741, Learning Rate = 4.810928e-05\n",
      "Epoch 7986/20000: Train Loss = 0.442870, Test Loss = 0.247083, Learning Rate = 4.809100e-05\n",
      "Epoch 7987/20000: Train Loss = 0.442724, Test Loss = 0.247480, Learning Rate = 4.807272e-05\n",
      "Epoch 7988/20000: Train Loss = 0.442923, Test Loss = 0.248650, Learning Rate = 4.805446e-05\n",
      "Epoch 7989/20000: Train Loss = 0.442442, Test Loss = 0.248123, Learning Rate = 4.803620e-05\n",
      "Epoch 7990/20000: Train Loss = 0.442612, Test Loss = 0.247094, Learning Rate = 4.801794e-05\n",
      "Epoch 7991/20000: Train Loss = 0.442696, Test Loss = 0.245510, Learning Rate = 4.799970e-05\n",
      "Epoch 7992/20000: Train Loss = 0.442595, Test Loss = 0.244716, Learning Rate = 4.798146e-05\n",
      "Epoch 7993/20000: Train Loss = 0.442783, Test Loss = 0.245673, Learning Rate = 4.796323e-05\n",
      "Epoch 7994/20000: Train Loss = 0.442814, Test Loss = 0.247980, Learning Rate = 4.794500e-05\n",
      "Epoch 7995/20000: Train Loss = 0.442553, Test Loss = 0.245392, Learning Rate = 4.792679e-05\n",
      "Epoch 7996/20000: Train Loss = 0.442653, Test Loss = 0.245006, Learning Rate = 4.790858e-05\n",
      "Epoch 7997/20000: Train Loss = 0.442650, Test Loss = 0.246403, Learning Rate = 4.789037e-05\n",
      "Epoch 7998/20000: Train Loss = 0.442581, Test Loss = 0.246248, Learning Rate = 4.787217e-05\n",
      "Epoch 7999/20000: Train Loss = 0.442620, Test Loss = 0.244490, Learning Rate = 4.785398e-05\n",
      "Epoch 8000/20000: Train Loss = 0.442570, Test Loss = 0.245063, Learning Rate = 4.783580e-05\n",
      "Epoch 8001/20000: Train Loss = 0.443127, Test Loss = 0.248406, Learning Rate = 4.781762e-05\n",
      "Epoch 8002/20000: Train Loss = 0.442603, Test Loss = 0.248362, Learning Rate = 4.779946e-05\n",
      "Epoch 8003/20000: Train Loss = 0.442523, Test Loss = 0.248011, Learning Rate = 4.778129e-05\n",
      "Epoch 8004/20000: Train Loss = 0.442609, Test Loss = 0.247315, Learning Rate = 4.776314e-05\n",
      "Epoch 8005/20000: Train Loss = 0.442825, Test Loss = 0.246429, Learning Rate = 4.774499e-05\n",
      "Epoch 8006/20000: Train Loss = 0.442677, Test Loss = 0.249335, Learning Rate = 4.772685e-05\n",
      "Epoch 8007/20000: Train Loss = 0.442639, Test Loss = 0.248269, Learning Rate = 4.770871e-05\n",
      "Epoch 8008/20000: Train Loss = 0.442645, Test Loss = 0.249710, Learning Rate = 4.769058e-05\n",
      "Epoch 8009/20000: Train Loss = 0.442739, Test Loss = 0.250497, Learning Rate = 4.767246e-05\n",
      "Epoch 8010/20000: Train Loss = 0.442567, Test Loss = 0.248146, Learning Rate = 4.765435e-05\n",
      "Epoch 8011/20000: Train Loss = 0.442703, Test Loss = 0.248928, Learning Rate = 4.763624e-05\n",
      "Epoch 8012/20000: Train Loss = 0.442850, Test Loss = 0.247976, Learning Rate = 4.761814e-05\n",
      "Epoch 8013/20000: Train Loss = 0.443151, Test Loss = 0.243128, Learning Rate = 4.760005e-05\n",
      "Epoch 8014/20000: Train Loss = 0.442816, Test Loss = 0.246720, Learning Rate = 4.758196e-05\n",
      "Epoch 8015/20000: Train Loss = 0.442807, Test Loss = 0.246783, Learning Rate = 4.756388e-05\n",
      "Epoch 8016/20000: Train Loss = 0.442657, Test Loss = 0.246306, Learning Rate = 4.754581e-05\n",
      "Epoch 8017/20000: Train Loss = 0.442684, Test Loss = 0.247540, Learning Rate = 4.752774e-05\n",
      "Epoch 8018/20000: Train Loss = 0.442581, Test Loss = 0.249190, Learning Rate = 4.750968e-05\n",
      "Epoch 8019/20000: Train Loss = 0.442737, Test Loss = 0.251067, Learning Rate = 4.749163e-05\n",
      "Epoch 8020/20000: Train Loss = 0.442802, Test Loss = 0.247626, Learning Rate = 4.747358e-05\n",
      "Epoch 8021/20000: Train Loss = 0.442567, Test Loss = 0.248659, Learning Rate = 4.745555e-05\n",
      "Epoch 8022/20000: Train Loss = 0.442651, Test Loss = 0.245013, Learning Rate = 4.743751e-05\n",
      "Epoch 8023/20000: Train Loss = 0.442504, Test Loss = 0.247849, Learning Rate = 4.741949e-05\n",
      "Epoch 8024/20000: Train Loss = 0.442948, Test Loss = 0.246722, Learning Rate = 4.740147e-05\n",
      "Epoch 8025/20000: Train Loss = 0.442524, Test Loss = 0.249710, Learning Rate = 4.738346e-05\n",
      "Epoch 8026/20000: Train Loss = 0.442989, Test Loss = 0.248963, Learning Rate = 4.736546e-05\n",
      "Epoch 8027/20000: Train Loss = 0.442611, Test Loss = 0.246691, Learning Rate = 4.734746e-05\n",
      "Epoch 8028/20000: Train Loss = 0.442570, Test Loss = 0.246693, Learning Rate = 4.732947e-05\n",
      "Epoch 8029/20000: Train Loss = 0.442698, Test Loss = 0.248904, Learning Rate = 4.731148e-05\n",
      "Epoch 8030/20000: Train Loss = 0.442620, Test Loss = 0.248141, Learning Rate = 4.729351e-05\n",
      "Epoch 8031/20000: Train Loss = 0.442513, Test Loss = 0.248328, Learning Rate = 4.727554e-05\n",
      "Epoch 8032/20000: Train Loss = 0.442578, Test Loss = 0.248754, Learning Rate = 4.725757e-05\n",
      "Epoch 8033/20000: Train Loss = 0.442624, Test Loss = 0.247258, Learning Rate = 4.723962e-05\n",
      "Epoch 8034/20000: Train Loss = 0.442588, Test Loss = 0.245472, Learning Rate = 4.722167e-05\n",
      "Epoch 8035/20000: Train Loss = 0.442642, Test Loss = 0.243636, Learning Rate = 4.720372e-05\n",
      "Epoch 8036/20000: Train Loss = 0.442764, Test Loss = 0.246297, Learning Rate = 4.718579e-05\n",
      "Epoch 8037/20000: Train Loss = 0.442621, Test Loss = 0.247072, Learning Rate = 4.716786e-05\n",
      "Epoch 8038/20000: Train Loss = 0.442769, Test Loss = 0.246980, Learning Rate = 4.714993e-05\n",
      "Epoch 8039/20000: Train Loss = 0.442589, Test Loss = 0.248003, Learning Rate = 4.713202e-05\n",
      "Epoch 8040/20000: Train Loss = 0.442667, Test Loss = 0.250388, Learning Rate = 4.711411e-05\n",
      "Epoch 8041/20000: Train Loss = 0.443048, Test Loss = 0.246744, Learning Rate = 4.709621e-05\n",
      "Epoch 8042/20000: Train Loss = 0.442635, Test Loss = 0.249004, Learning Rate = 4.707831e-05\n",
      "Epoch 8043/20000: Train Loss = 0.443068, Test Loss = 0.250134, Learning Rate = 4.706042e-05\n",
      "Epoch 8044/20000: Train Loss = 0.442470, Test Loss = 0.248029, Learning Rate = 4.704254e-05\n",
      "Epoch 8045/20000: Train Loss = 0.442763, Test Loss = 0.247664, Learning Rate = 4.702467e-05\n",
      "Epoch 8046/20000: Train Loss = 0.442618, Test Loss = 0.250022, Learning Rate = 4.700680e-05\n",
      "Epoch 8047/20000: Train Loss = 0.442659, Test Loss = 0.249549, Learning Rate = 4.698894e-05\n",
      "Epoch 8048/20000: Train Loss = 0.442702, Test Loss = 0.249176, Learning Rate = 4.697108e-05\n",
      "Epoch 8049/20000: Train Loss = 0.442661, Test Loss = 0.248760, Learning Rate = 4.695324e-05\n",
      "Epoch 8050/20000: Train Loss = 0.442603, Test Loss = 0.247903, Learning Rate = 4.693540e-05\n",
      "Epoch 8051/20000: Train Loss = 0.442722, Test Loss = 0.252015, Learning Rate = 4.691756e-05\n",
      "Epoch 8052/20000: Train Loss = 0.442591, Test Loss = 0.249924, Learning Rate = 4.689973e-05\n",
      "Epoch 8053/20000: Train Loss = 0.442516, Test Loss = 0.245341, Learning Rate = 4.688191e-05\n",
      "Epoch 8054/20000: Train Loss = 0.442586, Test Loss = 0.246561, Learning Rate = 4.686410e-05\n",
      "Epoch 8055/20000: Train Loss = 0.442832, Test Loss = 0.248564, Learning Rate = 4.684629e-05\n",
      "Epoch 8056/20000: Train Loss = 0.442679, Test Loss = 0.249109, Learning Rate = 4.682849e-05\n",
      "Epoch 8057/20000: Train Loss = 0.442936, Test Loss = 0.245711, Learning Rate = 4.681070e-05\n",
      "Epoch 8058/20000: Train Loss = 0.442718, Test Loss = 0.249098, Learning Rate = 4.679291e-05\n",
      "Epoch 8059/20000: Train Loss = 0.442647, Test Loss = 0.246772, Learning Rate = 4.677513e-05\n",
      "Epoch 8060/20000: Train Loss = 0.443045, Test Loss = 0.243455, Learning Rate = 4.675736e-05\n",
      "Epoch 8061/20000: Train Loss = 0.442635, Test Loss = 0.246542, Learning Rate = 4.673959e-05\n",
      "Epoch 8062/20000: Train Loss = 0.442572, Test Loss = 0.246453, Learning Rate = 4.672183e-05\n",
      "Epoch 8063/20000: Train Loss = 0.442717, Test Loss = 0.244083, Learning Rate = 4.670408e-05\n",
      "Epoch 8064/20000: Train Loss = 0.442606, Test Loss = 0.245745, Learning Rate = 4.668633e-05\n",
      "Epoch 8065/20000: Train Loss = 0.443089, Test Loss = 0.246013, Learning Rate = 4.666859e-05\n",
      "Epoch 8066/20000: Train Loss = 0.442570, Test Loss = 0.247868, Learning Rate = 4.665086e-05\n",
      "Epoch 8067/20000: Train Loss = 0.442569, Test Loss = 0.248325, Learning Rate = 4.663313e-05\n",
      "Epoch 8068/20000: Train Loss = 0.442827, Test Loss = 0.249802, Learning Rate = 4.661541e-05\n",
      "Epoch 8069/20000: Train Loss = 0.442637, Test Loss = 0.245814, Learning Rate = 4.659770e-05\n",
      "Epoch 8070/20000: Train Loss = 0.442670, Test Loss = 0.247105, Learning Rate = 4.658000e-05\n",
      "Epoch 8071/20000: Train Loss = 0.442637, Test Loss = 0.248241, Learning Rate = 4.656230e-05\n",
      "Epoch 8072/20000: Train Loss = 0.442690, Test Loss = 0.245738, Learning Rate = 4.654460e-05\n",
      "Epoch 8073/20000: Train Loss = 0.442579, Test Loss = 0.247731, Learning Rate = 4.652692e-05\n",
      "Epoch 8074/20000: Train Loss = 0.442545, Test Loss = 0.246941, Learning Rate = 4.650924e-05\n",
      "Epoch 8075/20000: Train Loss = 0.442524, Test Loss = 0.246763, Learning Rate = 4.649157e-05\n",
      "Epoch 8076/20000: Train Loss = 0.442513, Test Loss = 0.245445, Learning Rate = 4.647390e-05\n",
      "Epoch 8077/20000: Train Loss = 0.442913, Test Loss = 0.247334, Learning Rate = 4.645624e-05\n",
      "Epoch 8078/20000: Train Loss = 0.442669, Test Loss = 0.248534, Learning Rate = 4.643859e-05\n",
      "Epoch 8079/20000: Train Loss = 0.442652, Test Loss = 0.248312, Learning Rate = 4.642095e-05\n",
      "Epoch 8080/20000: Train Loss = 0.442621, Test Loss = 0.249197, Learning Rate = 4.640331e-05\n",
      "Epoch 8081/20000: Train Loss = 0.442494, Test Loss = 0.245824, Learning Rate = 4.638568e-05\n",
      "Epoch 8082/20000: Train Loss = 0.442677, Test Loss = 0.244668, Learning Rate = 4.636805e-05\n",
      "Epoch 8083/20000: Train Loss = 0.442603, Test Loss = 0.247755, Learning Rate = 4.635043e-05\n",
      "Epoch 8084/20000: Train Loss = 0.442598, Test Loss = 0.248987, Learning Rate = 4.633282e-05\n",
      "Epoch 8085/20000: Train Loss = 0.442685, Test Loss = 0.245520, Learning Rate = 4.631521e-05\n",
      "Epoch 8086/20000: Train Loss = 0.442617, Test Loss = 0.250461, Learning Rate = 4.629762e-05\n",
      "Epoch 8087/20000: Train Loss = 0.442666, Test Loss = 0.247990, Learning Rate = 4.628002e-05\n",
      "Epoch 8088/20000: Train Loss = 0.442635, Test Loss = 0.252617, Learning Rate = 4.626244e-05\n",
      "Epoch 8089/20000: Train Loss = 0.442684, Test Loss = 0.250682, Learning Rate = 4.624486e-05\n",
      "Epoch 8090/20000: Train Loss = 0.442552, Test Loss = 0.250010, Learning Rate = 4.622729e-05\n",
      "Epoch 8091/20000: Train Loss = 0.442539, Test Loss = 0.250655, Learning Rate = 4.620972e-05\n",
      "Epoch 8092/20000: Train Loss = 0.442818, Test Loss = 0.246545, Learning Rate = 4.619216e-05\n",
      "Epoch 8093/20000: Train Loss = 0.442662, Test Loss = 0.250046, Learning Rate = 4.617461e-05\n",
      "Epoch 8094/20000: Train Loss = 0.442720, Test Loss = 0.247044, Learning Rate = 4.615707e-05\n",
      "Epoch 8095/20000: Train Loss = 0.442489, Test Loss = 0.245624, Learning Rate = 4.613953e-05\n",
      "Epoch 8096/20000: Train Loss = 0.442544, Test Loss = 0.246873, Learning Rate = 4.612200e-05\n",
      "Epoch 8097/20000: Train Loss = 0.442620, Test Loss = 0.244691, Learning Rate = 4.610447e-05\n",
      "Epoch 8098/20000: Train Loss = 0.442800, Test Loss = 0.245740, Learning Rate = 4.608695e-05\n",
      "Epoch 8099/20000: Train Loss = 0.442693, Test Loss = 0.244644, Learning Rate = 4.606944e-05\n",
      "Epoch 8100/20000: Train Loss = 0.442636, Test Loss = 0.246339, Learning Rate = 4.605194e-05\n",
      "Epoch 8101/20000: Train Loss = 0.442665, Test Loss = 0.246705, Learning Rate = 4.603444e-05\n",
      "Epoch 8102/20000: Train Loss = 0.442893, Test Loss = 0.247341, Learning Rate = 4.601695e-05\n",
      "Epoch 8103/20000: Train Loss = 0.442558, Test Loss = 0.250088, Learning Rate = 4.599946e-05\n",
      "Epoch 8104/20000: Train Loss = 0.442628, Test Loss = 0.250325, Learning Rate = 4.598198e-05\n",
      "Epoch 8105/20000: Train Loss = 0.442525, Test Loss = 0.249365, Learning Rate = 4.596451e-05\n",
      "Epoch 8106/20000: Train Loss = 0.442780, Test Loss = 0.251475, Learning Rate = 4.594705e-05\n",
      "Epoch 8107/20000: Train Loss = 0.442450, Test Loss = 0.248033, Learning Rate = 4.592959e-05\n",
      "Epoch 8108/20000: Train Loss = 0.442630, Test Loss = 0.243326, Learning Rate = 4.591214e-05\n",
      "Epoch 8109/20000: Train Loss = 0.442593, Test Loss = 0.244332, Learning Rate = 4.589469e-05\n",
      "Epoch 8110/20000: Train Loss = 0.442496, Test Loss = 0.248215, Learning Rate = 4.587725e-05\n",
      "Epoch 8111/20000: Train Loss = 0.442858, Test Loss = 0.250894, Learning Rate = 4.585982e-05\n",
      "Epoch 8112/20000: Train Loss = 0.442659, Test Loss = 0.247267, Learning Rate = 4.584239e-05\n",
      "Epoch 8113/20000: Train Loss = 0.442645, Test Loss = 0.247888, Learning Rate = 4.582497e-05\n",
      "Epoch 8114/20000: Train Loss = 0.442603, Test Loss = 0.246621, Learning Rate = 4.580756e-05\n",
      "Epoch 8115/20000: Train Loss = 0.442605, Test Loss = 0.246761, Learning Rate = 4.579016e-05\n",
      "Epoch 8116/20000: Train Loss = 0.442490, Test Loss = 0.248815, Learning Rate = 4.577276e-05\n",
      "Epoch 8117/20000: Train Loss = 0.442512, Test Loss = 0.247253, Learning Rate = 4.575537e-05\n",
      "Epoch 8118/20000: Train Loss = 0.442579, Test Loss = 0.244738, Learning Rate = 4.573798e-05\n",
      "Epoch 8119/20000: Train Loss = 0.442722, Test Loss = 0.248116, Learning Rate = 4.572060e-05\n",
      "Epoch 8120/20000: Train Loss = 0.442511, Test Loss = 0.248348, Learning Rate = 4.570323e-05\n",
      "Epoch 8121/20000: Train Loss = 0.442624, Test Loss = 0.246076, Learning Rate = 4.568586e-05\n",
      "Epoch 8122/20000: Train Loss = 0.442766, Test Loss = 0.248758, Learning Rate = 4.566850e-05\n",
      "Epoch 8123/20000: Train Loss = 0.442611, Test Loss = 0.247648, Learning Rate = 4.565115e-05\n",
      "Epoch 8124/20000: Train Loss = 0.442546, Test Loss = 0.245773, Learning Rate = 4.563380e-05\n",
      "Epoch 8125/20000: Train Loss = 0.442756, Test Loss = 0.245023, Learning Rate = 4.561646e-05\n",
      "Epoch 8126/20000: Train Loss = 0.442639, Test Loss = 0.246291, Learning Rate = 4.559913e-05\n",
      "Epoch 8127/20000: Train Loss = 0.442537, Test Loss = 0.244360, Learning Rate = 4.558180e-05\n",
      "Epoch 8128/20000: Train Loss = 0.442622, Test Loss = 0.244591, Learning Rate = 4.556448e-05\n",
      "Epoch 8129/20000: Train Loss = 0.442655, Test Loss = 0.244793, Learning Rate = 4.554717e-05\n",
      "Epoch 8130/20000: Train Loss = 0.442686, Test Loss = 0.249378, Learning Rate = 4.552986e-05\n",
      "Epoch 8131/20000: Train Loss = 0.442934, Test Loss = 0.246542, Learning Rate = 4.551256e-05\n",
      "Epoch 8132/20000: Train Loss = 0.442656, Test Loss = 0.247261, Learning Rate = 4.549527e-05\n",
      "Epoch 8133/20000: Train Loss = 0.442538, Test Loss = 0.247762, Learning Rate = 4.547798e-05\n",
      "Epoch 8134/20000: Train Loss = 0.442650, Test Loss = 0.248764, Learning Rate = 4.546070e-05\n",
      "Epoch 8135/20000: Train Loss = 0.442618, Test Loss = 0.245873, Learning Rate = 4.544343e-05\n",
      "Epoch 8136/20000: Train Loss = 0.442895, Test Loss = 0.244290, Learning Rate = 4.542616e-05\n",
      "Epoch 8137/20000: Train Loss = 0.442592, Test Loss = 0.247356, Learning Rate = 4.540890e-05\n",
      "Epoch 8138/20000: Train Loss = 0.442662, Test Loss = 0.247052, Learning Rate = 4.539165e-05\n",
      "Epoch 8139/20000: Train Loss = 0.442950, Test Loss = 0.251583, Learning Rate = 4.537440e-05\n",
      "Epoch 8140/20000: Train Loss = 0.442608, Test Loss = 0.247934, Learning Rate = 4.535716e-05\n",
      "Epoch 8141/20000: Train Loss = 0.442590, Test Loss = 0.245987, Learning Rate = 4.533992e-05\n",
      "Epoch 8142/20000: Train Loss = 0.442601, Test Loss = 0.247391, Learning Rate = 4.532270e-05\n",
      "Epoch 8143/20000: Train Loss = 0.442740, Test Loss = 0.249257, Learning Rate = 4.530548e-05\n",
      "Epoch 8144/20000: Train Loss = 0.442606, Test Loss = 0.246756, Learning Rate = 4.528826e-05\n",
      "Epoch 8145/20000: Train Loss = 0.442612, Test Loss = 0.245004, Learning Rate = 4.527105e-05\n",
      "Epoch 8146/20000: Train Loss = 0.442585, Test Loss = 0.247484, Learning Rate = 4.525385e-05\n",
      "Epoch 8147/20000: Train Loss = 0.442651, Test Loss = 0.247481, Learning Rate = 4.523665e-05\n",
      "Epoch 8148/20000: Train Loss = 0.442955, Test Loss = 0.246313, Learning Rate = 4.521947e-05\n",
      "Epoch 8149/20000: Train Loss = 0.442375, Test Loss = 0.245502, Learning Rate = 4.520228e-05\n",
      "Epoch 8150/20000: Train Loss = 0.442745, Test Loss = 0.246744, Learning Rate = 4.518511e-05\n",
      "Epoch 8151/20000: Train Loss = 0.442770, Test Loss = 0.246413, Learning Rate = 4.516794e-05\n",
      "Epoch 8152/20000: Train Loss = 0.442726, Test Loss = 0.246388, Learning Rate = 4.515078e-05\n",
      "Epoch 8153/20000: Train Loss = 0.442571, Test Loss = 0.246778, Learning Rate = 4.513362e-05\n",
      "Epoch 8154/20000: Train Loss = 0.442546, Test Loss = 0.247229, Learning Rate = 4.511647e-05\n",
      "Epoch 8155/20000: Train Loss = 0.442645, Test Loss = 0.245267, Learning Rate = 4.509933e-05\n",
      "Epoch 8156/20000: Train Loss = 0.442559, Test Loss = 0.247947, Learning Rate = 4.508219e-05\n",
      "Epoch 8157/20000: Train Loss = 0.442698, Test Loss = 0.246201, Learning Rate = 4.506506e-05\n",
      "Epoch 8158/20000: Train Loss = 0.443091, Test Loss = 0.249832, Learning Rate = 4.504794e-05\n",
      "Epoch 8159/20000: Train Loss = 0.442670, Test Loss = 0.247526, Learning Rate = 4.503082e-05\n",
      "Epoch 8160/20000: Train Loss = 0.442667, Test Loss = 0.247761, Learning Rate = 4.501371e-05\n",
      "Epoch 8161/20000: Train Loss = 0.442465, Test Loss = 0.245812, Learning Rate = 4.499661e-05\n",
      "Epoch 8162/20000: Train Loss = 0.442545, Test Loss = 0.244615, Learning Rate = 4.497951e-05\n",
      "Epoch 8163/20000: Train Loss = 0.442733, Test Loss = 0.243302, Learning Rate = 4.496242e-05\n",
      "Epoch 8164/20000: Train Loss = 0.442852, Test Loss = 0.245800, Learning Rate = 4.494533e-05\n",
      "Epoch 8165/20000: Train Loss = 0.442922, Test Loss = 0.246196, Learning Rate = 4.492826e-05\n",
      "Epoch 8166/20000: Train Loss = 0.442348, Test Loss = 0.246785, Learning Rate = 4.491118e-05\n",
      "Epoch 8167/20000: Train Loss = 0.442421, Test Loss = 0.244741, Learning Rate = 4.489412e-05\n",
      "Epoch 8168/20000: Train Loss = 0.442689, Test Loss = 0.248019, Learning Rate = 4.487706e-05\n",
      "Epoch 8169/20000: Train Loss = 0.442416, Test Loss = 0.245693, Learning Rate = 4.486001e-05\n",
      "Epoch 8170/20000: Train Loss = 0.443036, Test Loss = 0.247566, Learning Rate = 4.484296e-05\n",
      "Epoch 8171/20000: Train Loss = 0.442548, Test Loss = 0.245357, Learning Rate = 4.482592e-05\n",
      "Epoch 8172/20000: Train Loss = 0.442728, Test Loss = 0.244630, Learning Rate = 4.480889e-05\n",
      "Epoch 8173/20000: Train Loss = 0.442656, Test Loss = 0.244298, Learning Rate = 4.479186e-05\n",
      "Epoch 8174/20000: Train Loss = 0.442519, Test Loss = 0.243679, Learning Rate = 4.477485e-05\n",
      "Epoch 8175/20000: Train Loss = 0.442519, Test Loss = 0.246510, Learning Rate = 4.475783e-05\n",
      "Epoch 8176/20000: Train Loss = 0.442559, Test Loss = 0.248705, Learning Rate = 4.474083e-05\n",
      "Epoch 8177/20000: Train Loss = 0.442424, Test Loss = 0.246352, Learning Rate = 4.472382e-05\n",
      "Epoch 8178/20000: Train Loss = 0.442405, Test Loss = 0.246182, Learning Rate = 4.470683e-05\n",
      "Epoch 8179/20000: Train Loss = 0.442886, Test Loss = 0.246971, Learning Rate = 4.468984e-05\n",
      "Epoch 8180/20000: Train Loss = 0.442457, Test Loss = 0.244300, Learning Rate = 4.467286e-05\n",
      "Epoch 8181/20000: Train Loss = 0.442549, Test Loss = 0.246110, Learning Rate = 4.465589e-05\n",
      "Epoch 8182/20000: Train Loss = 0.442428, Test Loss = 0.245280, Learning Rate = 4.463892e-05\n",
      "Epoch 8183/20000: Train Loss = 0.442659, Test Loss = 0.240899, Learning Rate = 4.462196e-05\n",
      "Epoch 8184/20000: Train Loss = 0.442641, Test Loss = 0.247739, Learning Rate = 4.460500e-05\n",
      "Epoch 8185/20000: Train Loss = 0.442586, Test Loss = 0.248961, Learning Rate = 4.458805e-05\n",
      "Epoch 8186/20000: Train Loss = 0.442679, Test Loss = 0.249085, Learning Rate = 4.457111e-05\n",
      "Epoch 8187/20000: Train Loss = 0.443176, Test Loss = 0.248687, Learning Rate = 4.455418e-05\n",
      "Epoch 8188/20000: Train Loss = 0.442961, Test Loss = 0.254761, Learning Rate = 4.453725e-05\n",
      "Epoch 8189/20000: Train Loss = 0.442621, Test Loss = 0.248615, Learning Rate = 4.452032e-05\n",
      "Epoch 8190/20000: Train Loss = 0.442790, Test Loss = 0.245856, Learning Rate = 4.450341e-05\n",
      "Epoch 8191/20000: Train Loss = 0.442150, Test Loss = 0.245145, Learning Rate = 4.448650e-05\n",
      "Epoch 8192/20000: Train Loss = 0.442376, Test Loss = 0.245727, Learning Rate = 4.446959e-05\n",
      "Epoch 8193/20000: Train Loss = 0.442060, Test Loss = 0.247341, Learning Rate = 4.445270e-05\n",
      "Epoch 8194/20000: Train Loss = 0.442527, Test Loss = 0.245606, Learning Rate = 4.443581e-05\n",
      "Epoch 8195/20000: Train Loss = 0.442334, Test Loss = 0.246453, Learning Rate = 4.441892e-05\n",
      "Epoch 8196/20000: Train Loss = 0.442192, Test Loss = 0.247167, Learning Rate = 4.440204e-05\n",
      "Epoch 8197/20000: Train Loss = 0.442342, Test Loss = 0.243010, Learning Rate = 4.438517e-05\n",
      "Epoch 8198/20000: Train Loss = 0.442750, Test Loss = 0.242498, Learning Rate = 4.436831e-05\n",
      "Epoch 8199/20000: Train Loss = 0.442556, Test Loss = 0.244325, Learning Rate = 4.435145e-05\n",
      "Epoch 8200/20000: Train Loss = 0.442734, Test Loss = 0.245697, Learning Rate = 4.433460e-05\n",
      "Epoch 8201/20000: Train Loss = 0.442487, Test Loss = 0.243176, Learning Rate = 4.431775e-05\n",
      "Epoch 8202/20000: Train Loss = 0.442689, Test Loss = 0.241565, Learning Rate = 4.430091e-05\n",
      "Epoch 8203/20000: Train Loss = 0.442357, Test Loss = 0.240835, Learning Rate = 4.428408e-05\n",
      "Epoch 8204/20000: Train Loss = 0.442695, Test Loss = 0.247518, Learning Rate = 4.426725e-05\n",
      "Epoch 8205/20000: Train Loss = 0.442394, Test Loss = 0.245715, Learning Rate = 4.425043e-05\n",
      "Epoch 8206/20000: Train Loss = 0.442654, Test Loss = 0.249787, Learning Rate = 4.423362e-05\n",
      "Epoch 8207/20000: Train Loss = 0.442636, Test Loss = 0.243812, Learning Rate = 4.421681e-05\n",
      "Epoch 8208/20000: Train Loss = 0.442203, Test Loss = 0.245957, Learning Rate = 4.420001e-05\n",
      "Epoch 8209/20000: Train Loss = 0.442567, Test Loss = 0.248753, Learning Rate = 4.418321e-05\n",
      "Epoch 8210/20000: Train Loss = 0.442085, Test Loss = 0.238403, Learning Rate = 4.416642e-05\n",
      "Epoch 8211/20000: Train Loss = 0.441950, Test Loss = 0.240814, Learning Rate = 4.414964e-05\n",
      "Epoch 8212/20000: Train Loss = 0.442386, Test Loss = 0.243086, Learning Rate = 4.413287e-05\n",
      "Epoch 8213/20000: Train Loss = 0.442226, Test Loss = 0.247529, Learning Rate = 4.411610e-05\n",
      "Epoch 8214/20000: Train Loss = 0.442048, Test Loss = 0.241230, Learning Rate = 4.409933e-05\n",
      "Epoch 8215/20000: Train Loss = 0.442173, Test Loss = 0.240246, Learning Rate = 4.408258e-05\n",
      "Epoch 8216/20000: Train Loss = 0.442068, Test Loss = 0.238943, Learning Rate = 4.406583e-05\n",
      "Epoch 8217/20000: Train Loss = 0.442534, Test Loss = 0.242265, Learning Rate = 4.404908e-05\n",
      "Epoch 8218/20000: Train Loss = 0.442171, Test Loss = 0.236093, Learning Rate = 4.403235e-05\n",
      "Epoch 8219/20000: Train Loss = 0.441693, Test Loss = 0.247785, Learning Rate = 4.401562e-05\n",
      "Epoch 8220/20000: Train Loss = 0.442104, Test Loss = 0.248742, Learning Rate = 4.399889e-05\n",
      "Epoch 8221/20000: Train Loss = 0.442600, Test Loss = 0.253917, Learning Rate = 4.398217e-05\n",
      "Epoch 8222/20000: Train Loss = 0.442053, Test Loss = 0.244563, Learning Rate = 4.396546e-05\n",
      "Epoch 8223/20000: Train Loss = 0.441696, Test Loss = 0.245496, Learning Rate = 4.394875e-05\n",
      "Epoch 8224/20000: Train Loss = 0.442046, Test Loss = 0.232538, Learning Rate = 4.393205e-05\n",
      "Epoch 8225/20000: Train Loss = 0.441689, Test Loss = 0.252973, Learning Rate = 4.391536e-05\n",
      "Epoch 8226/20000: Train Loss = 0.441767, Test Loss = 0.244578, Learning Rate = 4.389868e-05\n",
      "Epoch 8227/20000: Train Loss = 0.441997, Test Loss = 0.249699, Learning Rate = 4.388199e-05\n",
      "Epoch 8228/20000: Train Loss = 0.441966, Test Loss = 0.246986, Learning Rate = 4.386532e-05\n",
      "Epoch 8229/20000: Train Loss = 0.441926, Test Loss = 0.240979, Learning Rate = 4.384865e-05\n",
      "Epoch 8230/20000: Train Loss = 0.442050, Test Loss = 0.250953, Learning Rate = 4.383199e-05\n",
      "Epoch 8231/20000: Train Loss = 0.442361, Test Loss = 0.239185, Learning Rate = 4.381534e-05\n",
      "Epoch 8232/20000: Train Loss = 0.442093, Test Loss = 0.246446, Learning Rate = 4.379869e-05\n",
      "Epoch 8233/20000: Train Loss = 0.441577, Test Loss = 0.243666, Learning Rate = 4.378205e-05\n",
      "Epoch 8234/20000: Train Loss = 0.441982, Test Loss = 0.247536, Learning Rate = 4.376541e-05\n",
      "Epoch 8235/20000: Train Loss = 0.442159, Test Loss = 0.246543, Learning Rate = 4.374878e-05\n",
      "Epoch 8236/20000: Train Loss = 0.442443, Test Loss = 0.240960, Learning Rate = 4.373216e-05\n",
      "Epoch 8237/20000: Train Loss = 0.442163, Test Loss = 0.247680, Learning Rate = 4.371554e-05\n",
      "Epoch 8238/20000: Train Loss = 0.441682, Test Loss = 0.243258, Learning Rate = 4.369893e-05\n",
      "Epoch 8239/20000: Train Loss = 0.441594, Test Loss = 0.242633, Learning Rate = 4.368232e-05\n",
      "Epoch 8240/20000: Train Loss = 0.441874, Test Loss = 0.242127, Learning Rate = 4.366573e-05\n",
      "Epoch 8241/20000: Train Loss = 0.441378, Test Loss = 0.244130, Learning Rate = 4.364914e-05\n",
      "Epoch 8242/20000: Train Loss = 0.441500, Test Loss = 0.235987, Learning Rate = 4.363255e-05\n",
      "Epoch 8243/20000: Train Loss = 0.441755, Test Loss = 0.247575, Learning Rate = 4.361597e-05\n",
      "Epoch 8244/20000: Train Loss = 0.441479, Test Loss = 0.246629, Learning Rate = 4.359940e-05\n",
      "Epoch 8245/20000: Train Loss = 0.441424, Test Loss = 0.244609, Learning Rate = 4.358283e-05\n",
      "Epoch 8246/20000: Train Loss = 0.441443, Test Loss = 0.245384, Learning Rate = 4.356627e-05\n",
      "Epoch 8247/20000: Train Loss = 0.441310, Test Loss = 0.242934, Learning Rate = 4.354972e-05\n",
      "Epoch 8248/20000: Train Loss = 0.442081, Test Loss = 0.249372, Learning Rate = 4.353317e-05\n",
      "Epoch 8249/20000: Train Loss = 0.441922, Test Loss = 0.251232, Learning Rate = 4.351663e-05\n",
      "Epoch 8250/20000: Train Loss = 0.442343, Test Loss = 0.265408, Learning Rate = 4.350009e-05\n",
      "Epoch 8251/20000: Train Loss = 0.441535, Test Loss = 0.240895, Learning Rate = 4.348356e-05\n",
      "Epoch 8252/20000: Train Loss = 0.441893, Test Loss = 0.248091, Learning Rate = 4.346704e-05\n",
      "Epoch 8253/20000: Train Loss = 0.442011, Test Loss = 0.249882, Learning Rate = 4.345052e-05\n",
      "Epoch 8254/20000: Train Loss = 0.441728, Test Loss = 0.238525, Learning Rate = 4.343401e-05\n",
      "Epoch 8255/20000: Train Loss = 0.441752, Test Loss = 0.248472, Learning Rate = 4.341751e-05\n",
      "Epoch 8256/20000: Train Loss = 0.441295, Test Loss = 0.248216, Learning Rate = 4.340101e-05\n",
      "Epoch 8257/20000: Train Loss = 0.445064, Test Loss = 0.264211, Learning Rate = 4.338452e-05\n",
      "Epoch 8258/20000: Train Loss = 0.442014, Test Loss = 0.243538, Learning Rate = 4.336804e-05\n",
      "Epoch 8259/20000: Train Loss = 0.441852, Test Loss = 0.253234, Learning Rate = 4.335156e-05\n",
      "Epoch 8260/20000: Train Loss = 0.441850, Test Loss = 0.252463, Learning Rate = 4.333509e-05\n",
      "Epoch 8261/20000: Train Loss = 0.441267, Test Loss = 0.245652, Learning Rate = 4.331862e-05\n",
      "Epoch 8262/20000: Train Loss = 0.441161, Test Loss = 0.244877, Learning Rate = 4.330216e-05\n",
      "Epoch 8263/20000: Train Loss = 0.443491, Test Loss = 0.244955, Learning Rate = 4.328571e-05\n",
      "Epoch 8264/20000: Train Loss = 0.441928, Test Loss = 0.240104, Learning Rate = 4.326926e-05\n",
      "Epoch 8265/20000: Train Loss = 0.441540, Test Loss = 0.243412, Learning Rate = 4.325282e-05\n",
      "Epoch 8266/20000: Train Loss = 0.441230, Test Loss = 0.239912, Learning Rate = 4.323638e-05\n",
      "Epoch 8267/20000: Train Loss = 0.441376, Test Loss = 0.243281, Learning Rate = 4.321995e-05\n",
      "Epoch 8268/20000: Train Loss = 0.441931, Test Loss = 0.249501, Learning Rate = 4.320353e-05\n",
      "Epoch 8269/20000: Train Loss = 0.441815, Test Loss = 0.246236, Learning Rate = 4.318712e-05\n",
      "Epoch 8270/20000: Train Loss = 0.441260, Test Loss = 0.245231, Learning Rate = 4.317071e-05\n",
      "Epoch 8271/20000: Train Loss = 0.440915, Test Loss = 0.246230, Learning Rate = 4.315430e-05\n",
      "Epoch 8272/20000: Train Loss = 0.441049, Test Loss = 0.250773, Learning Rate = 4.313790e-05\n",
      "Epoch 8273/20000: Train Loss = 0.441202, Test Loss = 0.250988, Learning Rate = 4.312151e-05\n",
      "Epoch 8274/20000: Train Loss = 0.441515, Test Loss = 0.253503, Learning Rate = 4.310513e-05\n",
      "Epoch 8275/20000: Train Loss = 0.441161, Test Loss = 0.250588, Learning Rate = 4.308875e-05\n",
      "Epoch 8276/20000: Train Loss = 0.441451, Test Loss = 0.253555, Learning Rate = 4.307238e-05\n",
      "Epoch 8277/20000: Train Loss = 0.441513, Test Loss = 0.253043, Learning Rate = 4.305601e-05\n",
      "Epoch 8278/20000: Train Loss = 0.441824, Test Loss = 0.248565, Learning Rate = 4.303965e-05\n",
      "Epoch 8279/20000: Train Loss = 0.442214, Test Loss = 0.249445, Learning Rate = 4.302330e-05\n",
      "Epoch 8280/20000: Train Loss = 0.441591, Test Loss = 0.250115, Learning Rate = 4.300695e-05\n",
      "Epoch 8281/20000: Train Loss = 0.441066, Test Loss = 0.244660, Learning Rate = 4.299061e-05\n",
      "Epoch 8282/20000: Train Loss = 0.441092, Test Loss = 0.242637, Learning Rate = 4.297427e-05\n",
      "Epoch 8283/20000: Train Loss = 0.440780, Test Loss = 0.250482, Learning Rate = 4.295794e-05\n",
      "Epoch 8284/20000: Train Loss = 0.440883, Test Loss = 0.246101, Learning Rate = 4.294162e-05\n",
      "Epoch 8285/20000: Train Loss = 0.440826, Test Loss = 0.250540, Learning Rate = 4.292530e-05\n",
      "Epoch 8286/20000: Train Loss = 0.442095, Test Loss = 0.237513, Learning Rate = 4.290899e-05\n",
      "Epoch 8287/20000: Train Loss = 0.442187, Test Loss = 0.233333, Learning Rate = 4.289269e-05\n",
      "Epoch 8288/20000: Train Loss = 0.442235, Test Loss = 0.258330, Learning Rate = 4.287639e-05\n",
      "Epoch 8289/20000: Train Loss = 0.441259, Test Loss = 0.238728, Learning Rate = 4.286010e-05\n",
      "Epoch 8290/20000: Train Loss = 0.440878, Test Loss = 0.251755, Learning Rate = 4.284381e-05\n",
      "Epoch 8291/20000: Train Loss = 0.441412, Test Loss = 0.247666, Learning Rate = 4.282753e-05\n",
      "Epoch 8292/20000: Train Loss = 0.441621, Test Loss = 0.244880, Learning Rate = 4.281126e-05\n",
      "Epoch 8293/20000: Train Loss = 0.440985, Test Loss = 0.255018, Learning Rate = 4.279499e-05\n",
      "Epoch 8294/20000: Train Loss = 0.441411, Test Loss = 0.257779, Learning Rate = 4.277873e-05\n",
      "Epoch 8295/20000: Train Loss = 0.441097, Test Loss = 0.254491, Learning Rate = 4.276248e-05\n",
      "Epoch 8296/20000: Train Loss = 0.441423, Test Loss = 0.251881, Learning Rate = 4.274623e-05\n",
      "Epoch 8297/20000: Train Loss = 0.440719, Test Loss = 0.250190, Learning Rate = 4.272999e-05\n",
      "Epoch 8298/20000: Train Loss = 0.441131, Test Loss = 0.254082, Learning Rate = 4.271375e-05\n",
      "Epoch 8299/20000: Train Loss = 0.441736, Test Loss = 0.243092, Learning Rate = 4.269752e-05\n",
      "Epoch 8300/20000: Train Loss = 0.440953, Test Loss = 0.243815, Learning Rate = 4.268130e-05\n",
      "Epoch 8301/20000: Train Loss = 0.440790, Test Loss = 0.251417, Learning Rate = 4.266508e-05\n",
      "Epoch 8302/20000: Train Loss = 0.440757, Test Loss = 0.230619, Learning Rate = 4.264887e-05\n",
      "Epoch 8303/20000: Train Loss = 0.441745, Test Loss = 0.244455, Learning Rate = 4.263266e-05\n",
      "Epoch 8304/20000: Train Loss = 0.441081, Test Loss = 0.254453, Learning Rate = 4.261646e-05\n",
      "Epoch 8305/20000: Train Loss = 0.440558, Test Loss = 0.239983, Learning Rate = 4.260027e-05\n",
      "Epoch 8306/20000: Train Loss = 0.440833, Test Loss = 0.240968, Learning Rate = 4.258408e-05\n",
      "Epoch 8307/20000: Train Loss = 0.441232, Test Loss = 0.239260, Learning Rate = 4.256790e-05\n",
      "Epoch 8308/20000: Train Loss = 0.441159, Test Loss = 0.270471, Learning Rate = 4.255173e-05\n",
      "Epoch 8309/20000: Train Loss = 0.441526, Test Loss = 0.249098, Learning Rate = 4.253556e-05\n",
      "Epoch 8310/20000: Train Loss = 0.440927, Test Loss = 0.248113, Learning Rate = 4.251940e-05\n",
      "Epoch 8311/20000: Train Loss = 0.441099, Test Loss = 0.261762, Learning Rate = 4.250324e-05\n",
      "Epoch 8312/20000: Train Loss = 0.440660, Test Loss = 0.238082, Learning Rate = 4.248709e-05\n",
      "Epoch 8313/20000: Train Loss = 0.440813, Test Loss = 0.252776, Learning Rate = 4.247095e-05\n",
      "Epoch 8314/20000: Train Loss = 0.440688, Test Loss = 0.239419, Learning Rate = 4.245481e-05\n",
      "Epoch 8315/20000: Train Loss = 0.440791, Test Loss = 0.246775, Learning Rate = 4.243868e-05\n",
      "Epoch 8316/20000: Train Loss = 0.441019, Test Loss = 0.252103, Learning Rate = 4.242255e-05\n",
      "Epoch 8317/20000: Train Loss = 0.440449, Test Loss = 0.243732, Learning Rate = 4.240643e-05\n",
      "Epoch 8318/20000: Train Loss = 0.441010, Test Loss = 0.258200, Learning Rate = 4.239032e-05\n",
      "Epoch 8319/20000: Train Loss = 0.441268, Test Loss = 0.245361, Learning Rate = 4.237421e-05\n",
      "Epoch 8320/20000: Train Loss = 0.440860, Test Loss = 0.240353, Learning Rate = 4.235811e-05\n",
      "Epoch 8321/20000: Train Loss = 0.440733, Test Loss = 0.246920, Learning Rate = 4.234202e-05\n",
      "Epoch 8322/20000: Train Loss = 0.440935, Test Loss = 0.250207, Learning Rate = 4.232593e-05\n",
      "Epoch 8323/20000: Train Loss = 0.440513, Test Loss = 0.253885, Learning Rate = 4.230984e-05\n",
      "Epoch 8324/20000: Train Loss = 0.441055, Test Loss = 0.254036, Learning Rate = 4.229377e-05\n",
      "Epoch 8325/20000: Train Loss = 0.440513, Test Loss = 0.237862, Learning Rate = 4.227770e-05\n",
      "Epoch 8326/20000: Train Loss = 0.441589, Test Loss = 0.242455, Learning Rate = 4.226163e-05\n",
      "Epoch 8327/20000: Train Loss = 0.441718, Test Loss = 0.251780, Learning Rate = 4.224557e-05\n",
      "Epoch 8328/20000: Train Loss = 0.440459, Test Loss = 0.241462, Learning Rate = 4.222952e-05\n",
      "Epoch 8329/20000: Train Loss = 0.441150, Test Loss = 0.242290, Learning Rate = 4.221348e-05\n",
      "Epoch 8330/20000: Train Loss = 0.441829, Test Loss = 0.250998, Learning Rate = 4.219744e-05\n",
      "Epoch 8331/20000: Train Loss = 0.440508, Test Loss = 0.252277, Learning Rate = 4.218140e-05\n",
      "Epoch 8332/20000: Train Loss = 0.440279, Test Loss = 0.244149, Learning Rate = 4.216537e-05\n",
      "Epoch 8333/20000: Train Loss = 0.440309, Test Loss = 0.243771, Learning Rate = 4.214935e-05\n",
      "Epoch 8334/20000: Train Loss = 0.440516, Test Loss = 0.245565, Learning Rate = 4.213334e-05\n",
      "Epoch 8335/20000: Train Loss = 0.441251, Test Loss = 0.256187, Learning Rate = 4.211733e-05\n",
      "Epoch 8336/20000: Train Loss = 0.442323, Test Loss = 0.254951, Learning Rate = 4.210132e-05\n",
      "Epoch 8337/20000: Train Loss = 0.440984, Test Loss = 0.246285, Learning Rate = 4.208533e-05\n",
      "Epoch 8338/20000: Train Loss = 0.441037, Test Loss = 0.251477, Learning Rate = 4.206933e-05\n",
      "Epoch 8339/20000: Train Loss = 0.440280, Test Loss = 0.248384, Learning Rate = 4.205335e-05\n",
      "Epoch 8340/20000: Train Loss = 0.440602, Test Loss = 0.241711, Learning Rate = 4.203737e-05\n",
      "Epoch 8341/20000: Train Loss = 0.440451, Test Loss = 0.247119, Learning Rate = 4.202140e-05\n",
      "Epoch 8342/20000: Train Loss = 0.440528, Test Loss = 0.256437, Learning Rate = 4.200543e-05\n",
      "Epoch 8343/20000: Train Loss = 0.440836, Test Loss = 0.255198, Learning Rate = 4.198947e-05\n",
      "Epoch 8344/20000: Train Loss = 0.440764, Test Loss = 0.245277, Learning Rate = 4.197351e-05\n",
      "Epoch 8345/20000: Train Loss = 0.440225, Test Loss = 0.266232, Learning Rate = 4.195757e-05\n",
      "Epoch 8346/20000: Train Loss = 0.441068, Test Loss = 0.241350, Learning Rate = 4.194162e-05\n",
      "Epoch 8347/20000: Train Loss = 0.440535, Test Loss = 0.254039, Learning Rate = 4.192569e-05\n",
      "Epoch 8348/20000: Train Loss = 0.440744, Test Loss = 0.249721, Learning Rate = 4.190976e-05\n",
      "Epoch 8349/20000: Train Loss = 0.441127, Test Loss = 0.261553, Learning Rate = 4.189383e-05\n",
      "Epoch 8350/20000: Train Loss = 0.440810, Test Loss = 0.252035, Learning Rate = 4.187791e-05\n",
      "Epoch 8351/20000: Train Loss = 0.440491, Test Loss = 0.242116, Learning Rate = 4.186200e-05\n",
      "Epoch 8352/20000: Train Loss = 0.441461, Test Loss = 0.249839, Learning Rate = 4.184609e-05\n",
      "Epoch 8353/20000: Train Loss = 0.440696, Test Loss = 0.244250, Learning Rate = 4.183019e-05\n",
      "Epoch 8354/20000: Train Loss = 0.440298, Test Loss = 0.247587, Learning Rate = 4.181430e-05\n",
      "Epoch 8355/20000: Train Loss = 0.440217, Test Loss = 0.252597, Learning Rate = 4.179841e-05\n",
      "Epoch 8356/20000: Train Loss = 0.440885, Test Loss = 0.251672, Learning Rate = 4.178253e-05\n",
      "Epoch 8357/20000: Train Loss = 0.440274, Test Loss = 0.240144, Learning Rate = 4.176665e-05\n",
      "Epoch 8358/20000: Train Loss = 0.442205, Test Loss = 0.242697, Learning Rate = 4.175078e-05\n",
      "Epoch 8359/20000: Train Loss = 0.441239, Test Loss = 0.245593, Learning Rate = 4.173492e-05\n",
      "Epoch 8360/20000: Train Loss = 0.440489, Test Loss = 0.252560, Learning Rate = 4.171906e-05\n",
      "Epoch 8361/20000: Train Loss = 0.440297, Test Loss = 0.238042, Learning Rate = 4.170321e-05\n",
      "Epoch 8362/20000: Train Loss = 0.441085, Test Loss = 0.245033, Learning Rate = 4.168736e-05\n",
      "Epoch 8363/20000: Train Loss = 0.440922, Test Loss = 0.243472, Learning Rate = 4.167152e-05\n",
      "Epoch 8364/20000: Train Loss = 0.440318, Test Loss = 0.249872, Learning Rate = 4.165569e-05\n",
      "Epoch 8365/20000: Train Loss = 0.440477, Test Loss = 0.250744, Learning Rate = 4.163986e-05\n",
      "Epoch 8366/20000: Train Loss = 0.440204, Test Loss = 0.250026, Learning Rate = 4.162404e-05\n",
      "Epoch 8367/20000: Train Loss = 0.440849, Test Loss = 0.250869, Learning Rate = 4.160822e-05\n",
      "Epoch 8368/20000: Train Loss = 0.440336, Test Loss = 0.246989, Learning Rate = 4.159241e-05\n",
      "Epoch 8369/20000: Train Loss = 0.440314, Test Loss = 0.254744, Learning Rate = 4.157661e-05\n",
      "Epoch 8370/20000: Train Loss = 0.441630, Test Loss = 0.244328, Learning Rate = 4.156081e-05\n",
      "Epoch 8371/20000: Train Loss = 0.441067, Test Loss = 0.244449, Learning Rate = 4.154502e-05\n",
      "Epoch 8372/20000: Train Loss = 0.441045, Test Loss = 0.243065, Learning Rate = 4.152923e-05\n",
      "Epoch 8373/20000: Train Loss = 0.440805, Test Loss = 0.250832, Learning Rate = 4.151345e-05\n",
      "Epoch 8374/20000: Train Loss = 0.441040, Test Loss = 0.240671, Learning Rate = 4.149768e-05\n",
      "Epoch 8375/20000: Train Loss = 0.441306, Test Loss = 0.234436, Learning Rate = 4.148191e-05\n",
      "Epoch 8376/20000: Train Loss = 0.441402, Test Loss = 0.245686, Learning Rate = 4.146615e-05\n",
      "Epoch 8377/20000: Train Loss = 0.440382, Test Loss = 0.249925, Learning Rate = 4.145039e-05\n",
      "Epoch 8378/20000: Train Loss = 0.441329, Test Loss = 0.247614, Learning Rate = 4.143464e-05\n",
      "Epoch 8379/20000: Train Loss = 0.441350, Test Loss = 0.245992, Learning Rate = 4.141890e-05\n",
      "Epoch 8380/20000: Train Loss = 0.440517, Test Loss = 0.248482, Learning Rate = 4.140316e-05\n",
      "Epoch 8381/20000: Train Loss = 0.440408, Test Loss = 0.250457, Learning Rate = 4.138743e-05\n",
      "Epoch 8382/20000: Train Loss = 0.440863, Test Loss = 0.247116, Learning Rate = 4.137170e-05\n",
      "Epoch 8383/20000: Train Loss = 0.440683, Test Loss = 0.246648, Learning Rate = 4.135598e-05\n",
      "Epoch 8384/20000: Train Loss = 0.441070, Test Loss = 0.252973, Learning Rate = 4.134027e-05\n",
      "Epoch 8385/20000: Train Loss = 0.440232, Test Loss = 0.242657, Learning Rate = 4.132456e-05\n",
      "Epoch 8386/20000: Train Loss = 0.440309, Test Loss = 0.250923, Learning Rate = 4.130886e-05\n",
      "Epoch 8387/20000: Train Loss = 0.440614, Test Loss = 0.248852, Learning Rate = 4.129316e-05\n",
      "Epoch 8388/20000: Train Loss = 0.440263, Test Loss = 0.245376, Learning Rate = 4.127747e-05\n",
      "Epoch 8389/20000: Train Loss = 0.440527, Test Loss = 0.244804, Learning Rate = 4.126179e-05\n",
      "Epoch 8390/20000: Train Loss = 0.440393, Test Loss = 0.248013, Learning Rate = 4.124611e-05\n",
      "Epoch 8391/20000: Train Loss = 0.440332, Test Loss = 0.246347, Learning Rate = 4.123043e-05\n",
      "Epoch 8392/20000: Train Loss = 0.440083, Test Loss = 0.258312, Learning Rate = 4.121477e-05\n",
      "Epoch 8393/20000: Train Loss = 0.441151, Test Loss = 0.247959, Learning Rate = 4.119911e-05\n",
      "Epoch 8394/20000: Train Loss = 0.441184, Test Loss = 0.245565, Learning Rate = 4.118345e-05\n",
      "Epoch 8395/20000: Train Loss = 0.441387, Test Loss = 0.261116, Learning Rate = 4.116780e-05\n",
      "Epoch 8396/20000: Train Loss = 0.441009, Test Loss = 0.236767, Learning Rate = 4.115216e-05\n",
      "Epoch 8397/20000: Train Loss = 0.441000, Test Loss = 0.254145, Learning Rate = 4.113653e-05\n",
      "Epoch 8398/20000: Train Loss = 0.441009, Test Loss = 0.248102, Learning Rate = 4.112089e-05\n",
      "Epoch 8399/20000: Train Loss = 0.441684, Test Loss = 0.252720, Learning Rate = 4.110527e-05\n",
      "Epoch 8400/20000: Train Loss = 0.440882, Test Loss = 0.245334, Learning Rate = 4.108965e-05\n",
      "Epoch 8401/20000: Train Loss = 0.440263, Test Loss = 0.254027, Learning Rate = 4.107404e-05\n",
      "Epoch 8402/20000: Train Loss = 0.440311, Test Loss = 0.234392, Learning Rate = 4.105843e-05\n",
      "Epoch 8403/20000: Train Loss = 0.440831, Test Loss = 0.251862, Learning Rate = 4.104283e-05\n",
      "Epoch 8404/20000: Train Loss = 0.440188, Test Loss = 0.243959, Learning Rate = 4.102723e-05\n",
      "Epoch 8405/20000: Train Loss = 0.440426, Test Loss = 0.250103, Learning Rate = 4.101165e-05\n",
      "Epoch 8406/20000: Train Loss = 0.440517, Test Loss = 0.258121, Learning Rate = 4.099606e-05\n",
      "Epoch 8407/20000: Train Loss = 0.440424, Test Loss = 0.254389, Learning Rate = 4.098048e-05\n",
      "Epoch 8408/20000: Train Loss = 0.440540, Test Loss = 0.247711, Learning Rate = 4.096491e-05\n",
      "Epoch 8409/20000: Train Loss = 0.441443, Test Loss = 0.248662, Learning Rate = 4.094935e-05\n",
      "Epoch 8410/20000: Train Loss = 0.440880, Test Loss = 0.256813, Learning Rate = 4.093379e-05\n",
      "Epoch 8411/20000: Train Loss = 0.440364, Test Loss = 0.247661, Learning Rate = 4.091823e-05\n",
      "Epoch 8412/20000: Train Loss = 0.440382, Test Loss = 0.255949, Learning Rate = 4.090269e-05\n",
      "Epoch 8413/20000: Train Loss = 0.440422, Test Loss = 0.246735, Learning Rate = 4.088714e-05\n",
      "Epoch 8414/20000: Train Loss = 0.440609, Test Loss = 0.241061, Learning Rate = 4.087161e-05\n",
      "Epoch 8415/20000: Train Loss = 0.440995, Test Loss = 0.256192, Learning Rate = 4.085608e-05\n",
      "Epoch 8416/20000: Train Loss = 0.440662, Test Loss = 0.244453, Learning Rate = 4.084055e-05\n",
      "Epoch 8417/20000: Train Loss = 0.440080, Test Loss = 0.254492, Learning Rate = 4.082504e-05\n",
      "Epoch 8418/20000: Train Loss = 0.440634, Test Loss = 0.247711, Learning Rate = 4.080952e-05\n",
      "Epoch 8419/20000: Train Loss = 0.441011, Test Loss = 0.242751, Learning Rate = 4.079402e-05\n",
      "Epoch 8420/20000: Train Loss = 0.440928, Test Loss = 0.247778, Learning Rate = 4.077852e-05\n",
      "Epoch 8421/20000: Train Loss = 0.440911, Test Loss = 0.251312, Learning Rate = 4.076302e-05\n",
      "Epoch 8422/20000: Train Loss = 0.440485, Test Loss = 0.249227, Learning Rate = 4.074753e-05\n",
      "Epoch 8423/20000: Train Loss = 0.440012, Test Loss = 0.245924, Learning Rate = 4.073205e-05\n",
      "Epoch 8424/20000: Train Loss = 0.440312, Test Loss = 0.259099, Learning Rate = 4.071657e-05\n",
      "Epoch 8425/20000: Train Loss = 0.440241, Test Loss = 0.245885, Learning Rate = 4.070110e-05\n",
      "Epoch 8426/20000: Train Loss = 0.440199, Test Loss = 0.245474, Learning Rate = 4.068564e-05\n",
      "Epoch 8427/20000: Train Loss = 0.440554, Test Loss = 0.239984, Learning Rate = 4.067018e-05\n",
      "Epoch 8428/20000: Train Loss = 0.440776, Test Loss = 0.250704, Learning Rate = 4.065472e-05\n",
      "Epoch 8429/20000: Train Loss = 0.440662, Test Loss = 0.245137, Learning Rate = 4.063928e-05\n",
      "Epoch 8430/20000: Train Loss = 0.440503, Test Loss = 0.260551, Learning Rate = 4.062383e-05\n",
      "Epoch 8431/20000: Train Loss = 0.440549, Test Loss = 0.243776, Learning Rate = 4.060840e-05\n",
      "Epoch 8432/20000: Train Loss = 0.440062, Test Loss = 0.251964, Learning Rate = 4.059297e-05\n",
      "Epoch 8433/20000: Train Loss = 0.440205, Test Loss = 0.248464, Learning Rate = 4.057754e-05\n",
      "Epoch 8434/20000: Train Loss = 0.440045, Test Loss = 0.252212, Learning Rate = 4.056213e-05\n",
      "Epoch 8435/20000: Train Loss = 0.440637, Test Loss = 0.243550, Learning Rate = 4.054671e-05\n",
      "Epoch 8436/20000: Train Loss = 0.440002, Test Loss = 0.250631, Learning Rate = 4.053131e-05\n",
      "Epoch 8437/20000: Train Loss = 0.439877, Test Loss = 0.248101, Learning Rate = 4.051591e-05\n",
      "Epoch 8438/20000: Train Loss = 0.440186, Test Loss = 0.260308, Learning Rate = 4.050051e-05\n",
      "Epoch 8439/20000: Train Loss = 0.441072, Test Loss = 0.257502, Learning Rate = 4.048512e-05\n",
      "Epoch 8440/20000: Train Loss = 0.440259, Test Loss = 0.254735, Learning Rate = 4.046974e-05\n",
      "Epoch 8441/20000: Train Loss = 0.440335, Test Loss = 0.252928, Learning Rate = 4.045436e-05\n",
      "Epoch 8442/20000: Train Loss = 0.440241, Test Loss = 0.251107, Learning Rate = 4.043899e-05\n",
      "Epoch 8443/20000: Train Loss = 0.440269, Test Loss = 0.250901, Learning Rate = 4.042362e-05\n",
      "Epoch 8444/20000: Train Loss = 0.439949, Test Loss = 0.255276, Learning Rate = 4.040826e-05\n",
      "Epoch 8445/20000: Train Loss = 0.440363, Test Loss = 0.254819, Learning Rate = 4.039291e-05\n",
      "Epoch 8446/20000: Train Loss = 0.440197, Test Loss = 0.244526, Learning Rate = 4.037756e-05\n",
      "Epoch 8447/20000: Train Loss = 0.440343, Test Loss = 0.240596, Learning Rate = 4.036222e-05\n",
      "Epoch 8448/20000: Train Loss = 0.440122, Test Loss = 0.251445, Learning Rate = 4.034688e-05\n",
      "Epoch 8449/20000: Train Loss = 0.440908, Test Loss = 0.243529, Learning Rate = 4.033155e-05\n",
      "Epoch 8450/20000: Train Loss = 0.439844, Test Loss = 0.254102, Learning Rate = 4.031623e-05\n",
      "Epoch 8451/20000: Train Loss = 0.441141, Test Loss = 0.249632, Learning Rate = 4.030091e-05\n",
      "Epoch 8452/20000: Train Loss = 0.440101, Test Loss = 0.251647, Learning Rate = 4.028559e-05\n",
      "Epoch 8453/20000: Train Loss = 0.440368, Test Loss = 0.258880, Learning Rate = 4.027029e-05\n",
      "Epoch 8454/20000: Train Loss = 0.440169, Test Loss = 0.252128, Learning Rate = 4.025499e-05\n",
      "Epoch 8455/20000: Train Loss = 0.440845, Test Loss = 0.259819, Learning Rate = 4.023969e-05\n",
      "Epoch 8456/20000: Train Loss = 0.440589, Test Loss = 0.253543, Learning Rate = 4.022440e-05\n",
      "Epoch 8457/20000: Train Loss = 0.439952, Test Loss = 0.244999, Learning Rate = 4.020912e-05\n",
      "Epoch 8458/20000: Train Loss = 0.440318, Test Loss = 0.246419, Learning Rate = 4.019384e-05\n",
      "Epoch 8459/20000: Train Loss = 0.440531, Test Loss = 0.253303, Learning Rate = 4.017856e-05\n",
      "Epoch 8460/20000: Train Loss = 0.440262, Test Loss = 0.257315, Learning Rate = 4.016330e-05\n",
      "Epoch 8461/20000: Train Loss = 0.440255, Test Loss = 0.238384, Learning Rate = 4.014804e-05\n",
      "Epoch 8462/20000: Train Loss = 0.441253, Test Loss = 0.251479, Learning Rate = 4.013278e-05\n",
      "Epoch 8463/20000: Train Loss = 0.440919, Test Loss = 0.252982, Learning Rate = 4.011753e-05\n",
      "Epoch 8464/20000: Train Loss = 0.440552, Test Loss = 0.253037, Learning Rate = 4.010229e-05\n",
      "Epoch 8465/20000: Train Loss = 0.440308, Test Loss = 0.246023, Learning Rate = 4.008705e-05\n",
      "Epoch 8466/20000: Train Loss = 0.440428, Test Loss = 0.260188, Learning Rate = 4.007182e-05\n",
      "Epoch 8467/20000: Train Loss = 0.440292, Test Loss = 0.250922, Learning Rate = 4.005659e-05\n",
      "Epoch 8468/20000: Train Loss = 0.440361, Test Loss = 0.254819, Learning Rate = 4.004137e-05\n",
      "Epoch 8469/20000: Train Loss = 0.440090, Test Loss = 0.261009, Learning Rate = 4.002616e-05\n",
      "Epoch 8470/20000: Train Loss = 0.440519, Test Loss = 0.253319, Learning Rate = 4.001095e-05\n",
      "Epoch 8471/20000: Train Loss = 0.440015, Test Loss = 0.249428, Learning Rate = 3.999575e-05\n",
      "Epoch 8472/20000: Train Loss = 0.441582, Test Loss = 0.245808, Learning Rate = 3.998055e-05\n",
      "Epoch 8473/20000: Train Loss = 0.439914, Test Loss = 0.255305, Learning Rate = 3.996536e-05\n",
      "Epoch 8474/20000: Train Loss = 0.440006, Test Loss = 0.251228, Learning Rate = 3.995017e-05\n",
      "Epoch 8475/20000: Train Loss = 0.439859, Test Loss = 0.252631, Learning Rate = 3.993499e-05\n",
      "Epoch 8476/20000: Train Loss = 0.439961, Test Loss = 0.239011, Learning Rate = 3.991982e-05\n",
      "Epoch 8477/20000: Train Loss = 0.440979, Test Loss = 0.255169, Learning Rate = 3.990465e-05\n",
      "Epoch 8478/20000: Train Loss = 0.439856, Test Loss = 0.249256, Learning Rate = 3.988949e-05\n",
      "Epoch 8479/20000: Train Loss = 0.440895, Test Loss = 0.256607, Learning Rate = 3.987433e-05\n",
      "Epoch 8480/20000: Train Loss = 0.441156, Test Loss = 0.249740, Learning Rate = 3.985918e-05\n",
      "Epoch 8481/20000: Train Loss = 0.440261, Test Loss = 0.253875, Learning Rate = 3.984403e-05\n",
      "Epoch 8482/20000: Train Loss = 0.440127, Test Loss = 0.251393, Learning Rate = 3.982889e-05\n",
      "Epoch 8483/20000: Train Loss = 0.440159, Test Loss = 0.259908, Learning Rate = 3.981376e-05\n",
      "Epoch 8484/20000: Train Loss = 0.440239, Test Loss = 0.240607, Learning Rate = 3.979863e-05\n",
      "Epoch 8485/20000: Train Loss = 0.440007, Test Loss = 0.254032, Learning Rate = 3.978351e-05\n",
      "Epoch 8486/20000: Train Loss = 0.440713, Test Loss = 0.247223, Learning Rate = 3.976839e-05\n",
      "Epoch 8487/20000: Train Loss = 0.440062, Test Loss = 0.252348, Learning Rate = 3.975328e-05\n",
      "Epoch 8488/20000: Train Loss = 0.439938, Test Loss = 0.243455, Learning Rate = 3.973818e-05\n",
      "Epoch 8489/20000: Train Loss = 0.440164, Test Loss = 0.247138, Learning Rate = 3.972308e-05\n",
      "Epoch 8490/20000: Train Loss = 0.441049, Test Loss = 0.250510, Learning Rate = 3.970798e-05\n",
      "Epoch 8491/20000: Train Loss = 0.440724, Test Loss = 0.253213, Learning Rate = 3.969289e-05\n",
      "Epoch 8492/20000: Train Loss = 0.440014, Test Loss = 0.255044, Learning Rate = 3.967781e-05\n",
      "Epoch 8493/20000: Train Loss = 0.439730, Test Loss = 0.248130, Learning Rate = 3.966274e-05\n",
      "Epoch 8494/20000: Train Loss = 0.440405, Test Loss = 0.247793, Learning Rate = 3.964766e-05\n",
      "Epoch 8495/20000: Train Loss = 0.439959, Test Loss = 0.246247, Learning Rate = 3.963260e-05\n",
      "Epoch 8496/20000: Train Loss = 0.440579, Test Loss = 0.242436, Learning Rate = 3.961754e-05\n",
      "Epoch 8497/20000: Train Loss = 0.440372, Test Loss = 0.249139, Learning Rate = 3.960249e-05\n",
      "Epoch 8498/20000: Train Loss = 0.440694, Test Loss = 0.252279, Learning Rate = 3.958744e-05\n",
      "Epoch 8499/20000: Train Loss = 0.441275, Test Loss = 0.247790, Learning Rate = 3.957240e-05\n",
      "Epoch 8500/20000: Train Loss = 0.439756, Test Loss = 0.256656, Learning Rate = 3.955736e-05\n",
      "Epoch 8501/20000: Train Loss = 0.440295, Test Loss = 0.248673, Learning Rate = 3.954233e-05\n",
      "Epoch 8502/20000: Train Loss = 0.440043, Test Loss = 0.252975, Learning Rate = 3.952730e-05\n",
      "Epoch 8503/20000: Train Loss = 0.440153, Test Loss = 0.256007, Learning Rate = 3.951229e-05\n",
      "Epoch 8504/20000: Train Loss = 0.440014, Test Loss = 0.247044, Learning Rate = 3.949727e-05\n",
      "Epoch 8505/20000: Train Loss = 0.440828, Test Loss = 0.252776, Learning Rate = 3.948226e-05\n",
      "Epoch 8506/20000: Train Loss = 0.440234, Test Loss = 0.256030, Learning Rate = 3.946726e-05\n",
      "Epoch 8507/20000: Train Loss = 0.440625, Test Loss = 0.262438, Learning Rate = 3.945227e-05\n",
      "Epoch 8508/20000: Train Loss = 0.440996, Test Loss = 0.258533, Learning Rate = 3.943727e-05\n",
      "Epoch 8509/20000: Train Loss = 0.441721, Test Loss = 0.246041, Learning Rate = 3.942229e-05\n",
      "Epoch 8510/20000: Train Loss = 0.440479, Test Loss = 0.242083, Learning Rate = 3.940731e-05\n",
      "Epoch 8511/20000: Train Loss = 0.440901, Test Loss = 0.249331, Learning Rate = 3.939234e-05\n",
      "Epoch 8512/20000: Train Loss = 0.440503, Test Loss = 0.241820, Learning Rate = 3.937737e-05\n",
      "Epoch 8513/20000: Train Loss = 0.440189, Test Loss = 0.251599, Learning Rate = 3.936241e-05\n",
      "Epoch 8514/20000: Train Loss = 0.440648, Test Loss = 0.241109, Learning Rate = 3.934745e-05\n",
      "Epoch 8515/20000: Train Loss = 0.440475, Test Loss = 0.243943, Learning Rate = 3.933250e-05\n",
      "Epoch 8516/20000: Train Loss = 0.440379, Test Loss = 0.244243, Learning Rate = 3.931755e-05\n",
      "Epoch 8517/20000: Train Loss = 0.440491, Test Loss = 0.250496, Learning Rate = 3.930261e-05\n",
      "Epoch 8518/20000: Train Loss = 0.440510, Test Loss = 0.245467, Learning Rate = 3.928768e-05\n",
      "Epoch 8519/20000: Train Loss = 0.440988, Test Loss = 0.241528, Learning Rate = 3.927275e-05\n",
      "Epoch 8520/20000: Train Loss = 0.440641, Test Loss = 0.253472, Learning Rate = 3.925783e-05\n",
      "Epoch 8521/20000: Train Loss = 0.440233, Test Loss = 0.247316, Learning Rate = 3.924291e-05\n",
      "Epoch 8522/20000: Train Loss = 0.440101, Test Loss = 0.248748, Learning Rate = 3.922800e-05\n",
      "Epoch 8523/20000: Train Loss = 0.440350, Test Loss = 0.255734, Learning Rate = 3.921309e-05\n",
      "Epoch 8524/20000: Train Loss = 0.440035, Test Loss = 0.243102, Learning Rate = 3.919819e-05\n",
      "Epoch 8525/20000: Train Loss = 0.441477, Test Loss = 0.251737, Learning Rate = 3.918330e-05\n",
      "Epoch 8526/20000: Train Loss = 0.440205, Test Loss = 0.249655, Learning Rate = 3.916841e-05\n",
      "Epoch 8527/20000: Train Loss = 0.440861, Test Loss = 0.251315, Learning Rate = 3.915353e-05\n",
      "Epoch 8528/20000: Train Loss = 0.440491, Test Loss = 0.243510, Learning Rate = 3.913865e-05\n",
      "Epoch 8529/20000: Train Loss = 0.440141, Test Loss = 0.252590, Learning Rate = 3.912378e-05\n",
      "Epoch 8530/20000: Train Loss = 0.440655, Test Loss = 0.251392, Learning Rate = 3.910891e-05\n",
      "Epoch 8531/20000: Train Loss = 0.440848, Test Loss = 0.258478, Learning Rate = 3.909405e-05\n",
      "Epoch 8532/20000: Train Loss = 0.441912, Test Loss = 0.251970, Learning Rate = 3.907920e-05\n",
      "Epoch 8533/20000: Train Loss = 0.440191, Test Loss = 0.249202, Learning Rate = 3.906435e-05\n",
      "Epoch 8534/20000: Train Loss = 0.440123, Test Loss = 0.243937, Learning Rate = 3.904951e-05\n",
      "Epoch 8535/20000: Train Loss = 0.440016, Test Loss = 0.246376, Learning Rate = 3.903467e-05\n",
      "Epoch 8536/20000: Train Loss = 0.440235, Test Loss = 0.242074, Learning Rate = 3.901984e-05\n",
      "Epoch 8537/20000: Train Loss = 0.440321, Test Loss = 0.256175, Learning Rate = 3.900501e-05\n",
      "Epoch 8538/20000: Train Loss = 0.439935, Test Loss = 0.251204, Learning Rate = 3.899019e-05\n",
      "Epoch 8539/20000: Train Loss = 0.440279, Test Loss = 0.257156, Learning Rate = 3.897537e-05\n",
      "Epoch 8540/20000: Train Loss = 0.440297, Test Loss = 0.256999, Learning Rate = 3.896056e-05\n",
      "Epoch 8541/20000: Train Loss = 0.440296, Test Loss = 0.255900, Learning Rate = 3.894576e-05\n",
      "Epoch 8542/20000: Train Loss = 0.440160, Test Loss = 0.249330, Learning Rate = 3.893096e-05\n",
      "Epoch 8543/20000: Train Loss = 0.440663, Test Loss = 0.255396, Learning Rate = 3.891617e-05\n",
      "Epoch 8544/20000: Train Loss = 0.441492, Test Loss = 0.251836, Learning Rate = 3.890138e-05\n",
      "Epoch 8545/20000: Train Loss = 0.440059, Test Loss = 0.248479, Learning Rate = 3.888660e-05\n",
      "Epoch 8546/20000: Train Loss = 0.440257, Test Loss = 0.244800, Learning Rate = 3.887183e-05\n",
      "Epoch 8547/20000: Train Loss = 0.440460, Test Loss = 0.248571, Learning Rate = 3.885706e-05\n",
      "Epoch 8548/20000: Train Loss = 0.439974, Test Loss = 0.244763, Learning Rate = 3.884229e-05\n",
      "Epoch 8549/20000: Train Loss = 0.440222, Test Loss = 0.250042, Learning Rate = 3.882753e-05\n",
      "Epoch 8550/20000: Train Loss = 0.440327, Test Loss = 0.255005, Learning Rate = 3.881278e-05\n",
      "Epoch 8551/20000: Train Loss = 0.440082, Test Loss = 0.251972, Learning Rate = 3.879803e-05\n",
      "Epoch 8552/20000: Train Loss = 0.440067, Test Loss = 0.247732, Learning Rate = 3.878329e-05\n",
      "Epoch 8553/20000: Train Loss = 0.439956, Test Loss = 0.250454, Learning Rate = 3.876855e-05\n",
      "Epoch 8554/20000: Train Loss = 0.440553, Test Loss = 0.250757, Learning Rate = 3.875382e-05\n",
      "Epoch 8555/20000: Train Loss = 0.440173, Test Loss = 0.251612, Learning Rate = 3.873910e-05\n",
      "Epoch 8556/20000: Train Loss = 0.439976, Test Loss = 0.247350, Learning Rate = 3.872438e-05\n",
      "Epoch 8557/20000: Train Loss = 0.440598, Test Loss = 0.253008, Learning Rate = 3.870966e-05\n",
      "Epoch 8558/20000: Train Loss = 0.440880, Test Loss = 0.254204, Learning Rate = 3.869495e-05\n",
      "Epoch 8559/20000: Train Loss = 0.440036, Test Loss = 0.253718, Learning Rate = 3.868025e-05\n",
      "Epoch 8560/20000: Train Loss = 0.440995, Test Loss = 0.258127, Learning Rate = 3.866555e-05\n",
      "Epoch 8561/20000: Train Loss = 0.439930, Test Loss = 0.249253, Learning Rate = 3.865086e-05\n",
      "Epoch 8562/20000: Train Loss = 0.440167, Test Loss = 0.238973, Learning Rate = 3.863617e-05\n",
      "Epoch 8563/20000: Train Loss = 0.440897, Test Loss = 0.250652, Learning Rate = 3.862149e-05\n",
      "Epoch 8564/20000: Train Loss = 0.440125, Test Loss = 0.248058, Learning Rate = 3.860682e-05\n",
      "Epoch 8565/20000: Train Loss = 0.440300, Test Loss = 0.252940, Learning Rate = 3.859215e-05\n",
      "Epoch 8566/20000: Train Loss = 0.439965, Test Loss = 0.254440, Learning Rate = 3.857748e-05\n",
      "Epoch 8567/20000: Train Loss = 0.440033, Test Loss = 0.241464, Learning Rate = 3.856283e-05\n",
      "Epoch 8568/20000: Train Loss = 0.439902, Test Loss = 0.249945, Learning Rate = 3.854817e-05\n",
      "Epoch 8569/20000: Train Loss = 0.440050, Test Loss = 0.245298, Learning Rate = 3.853353e-05\n",
      "Epoch 8570/20000: Train Loss = 0.440652, Test Loss = 0.251241, Learning Rate = 3.851888e-05\n",
      "Epoch 8571/20000: Train Loss = 0.439947, Test Loss = 0.253755, Learning Rate = 3.850425e-05\n",
      "Epoch 8572/20000: Train Loss = 0.440484, Test Loss = 0.258551, Learning Rate = 3.848962e-05\n",
      "Epoch 8573/20000: Train Loss = 0.440058, Test Loss = 0.252484, Learning Rate = 3.847499e-05\n",
      "Epoch 8574/20000: Train Loss = 0.440553, Test Loss = 0.250072, Learning Rate = 3.846037e-05\n",
      "Epoch 8575/20000: Train Loss = 0.440468, Test Loss = 0.241198, Learning Rate = 3.844576e-05\n",
      "Epoch 8576/20000: Train Loss = 0.440472, Test Loss = 0.252845, Learning Rate = 3.843115e-05\n",
      "Epoch 8577/20000: Train Loss = 0.440117, Test Loss = 0.253222, Learning Rate = 3.841655e-05\n",
      "Epoch 8578/20000: Train Loss = 0.440095, Test Loss = 0.259070, Learning Rate = 3.840195e-05\n",
      "Epoch 8579/20000: Train Loss = 0.440454, Test Loss = 0.250591, Learning Rate = 3.838736e-05\n",
      "Epoch 8580/20000: Train Loss = 0.440651, Test Loss = 0.259672, Learning Rate = 3.837277e-05\n",
      "Epoch 8581/20000: Train Loss = 0.440521, Test Loss = 0.247556, Learning Rate = 3.835819e-05\n",
      "Epoch 8582/20000: Train Loss = 0.441469, Test Loss = 0.257185, Learning Rate = 3.834362e-05\n",
      "Epoch 8583/20000: Train Loss = 0.439821, Test Loss = 0.242948, Learning Rate = 3.832905e-05\n",
      "Epoch 8584/20000: Train Loss = 0.440425, Test Loss = 0.252789, Learning Rate = 3.831448e-05\n",
      "Epoch 8585/20000: Train Loss = 0.440209, Test Loss = 0.250535, Learning Rate = 3.829993e-05\n",
      "Epoch 8586/20000: Train Loss = 0.440291, Test Loss = 0.256808, Learning Rate = 3.828537e-05\n",
      "Epoch 8587/20000: Train Loss = 0.439942, Test Loss = 0.250333, Learning Rate = 3.827082e-05\n",
      "Epoch 8588/20000: Train Loss = 0.440300, Test Loss = 0.241292, Learning Rate = 3.825628e-05\n",
      "Epoch 8589/20000: Train Loss = 0.440194, Test Loss = 0.247599, Learning Rate = 3.824175e-05\n",
      "Epoch 8590/20000: Train Loss = 0.440273, Test Loss = 0.247606, Learning Rate = 3.822722e-05\n",
      "Epoch 8591/20000: Train Loss = 0.440514, Test Loss = 0.237447, Learning Rate = 3.821269e-05\n",
      "Epoch 8592/20000: Train Loss = 0.440567, Test Loss = 0.245834, Learning Rate = 3.819817e-05\n",
      "Epoch 8593/20000: Train Loss = 0.440040, Test Loss = 0.247119, Learning Rate = 3.818366e-05\n",
      "Epoch 8594/20000: Train Loss = 0.440024, Test Loss = 0.246316, Learning Rate = 3.816915e-05\n",
      "Epoch 8595/20000: Train Loss = 0.440424, Test Loss = 0.251654, Learning Rate = 3.815464e-05\n",
      "Epoch 8596/20000: Train Loss = 0.440513, Test Loss = 0.251819, Learning Rate = 3.814015e-05\n",
      "Epoch 8597/20000: Train Loss = 0.440586, Test Loss = 0.243320, Learning Rate = 3.812565e-05\n",
      "Epoch 8598/20000: Train Loss = 0.440187, Test Loss = 0.258449, Learning Rate = 3.811117e-05\n",
      "Epoch 8599/20000: Train Loss = 0.440430, Test Loss = 0.244019, Learning Rate = 3.809669e-05\n",
      "Epoch 8600/20000: Train Loss = 0.440380, Test Loss = 0.249463, Learning Rate = 3.808221e-05\n",
      "Epoch 8601/20000: Train Loss = 0.440061, Test Loss = 0.248084, Learning Rate = 3.806774e-05\n",
      "Epoch 8602/20000: Train Loss = 0.440246, Test Loss = 0.256570, Learning Rate = 3.805328e-05\n",
      "Epoch 8603/20000: Train Loss = 0.440865, Test Loss = 0.249630, Learning Rate = 3.803882e-05\n",
      "Epoch 8604/20000: Train Loss = 0.440826, Test Loss = 0.243481, Learning Rate = 3.802436e-05\n",
      "Epoch 8605/20000: Train Loss = 0.440320, Test Loss = 0.246370, Learning Rate = 3.800991e-05\n",
      "Epoch 8606/20000: Train Loss = 0.441279, Test Loss = 0.248862, Learning Rate = 3.799547e-05\n",
      "Epoch 8607/20000: Train Loss = 0.440313, Test Loss = 0.255348, Learning Rate = 3.798103e-05\n",
      "Epoch 8608/20000: Train Loss = 0.440200, Test Loss = 0.241075, Learning Rate = 3.796660e-05\n",
      "Epoch 8609/20000: Train Loss = 0.440674, Test Loss = 0.248667, Learning Rate = 3.795218e-05\n",
      "Epoch 8610/20000: Train Loss = 0.440337, Test Loss = 0.244186, Learning Rate = 3.793776e-05\n",
      "Epoch 8611/20000: Train Loss = 0.440106, Test Loss = 0.256498, Learning Rate = 3.792334e-05\n",
      "Epoch 8612/20000: Train Loss = 0.439997, Test Loss = 0.246825, Learning Rate = 3.790893e-05\n",
      "Epoch 8613/20000: Train Loss = 0.441613, Test Loss = 0.255548, Learning Rate = 3.789453e-05\n",
      "Epoch 8614/20000: Train Loss = 0.440131, Test Loss = 0.249246, Learning Rate = 3.788013e-05\n",
      "Epoch 8615/20000: Train Loss = 0.440835, Test Loss = 0.257823, Learning Rate = 3.786573e-05\n",
      "Epoch 8616/20000: Train Loss = 0.439956, Test Loss = 0.248664, Learning Rate = 3.785135e-05\n",
      "Epoch 8617/20000: Train Loss = 0.440447, Test Loss = 0.260802, Learning Rate = 3.783696e-05\n",
      "Epoch 8618/20000: Train Loss = 0.440159, Test Loss = 0.245757, Learning Rate = 3.782259e-05\n",
      "Epoch 8619/20000: Train Loss = 0.440534, Test Loss = 0.253320, Learning Rate = 3.780822e-05\n",
      "Epoch 8620/20000: Train Loss = 0.441079, Test Loss = 0.253704, Learning Rate = 3.779385e-05\n",
      "Epoch 8621/20000: Train Loss = 0.440543, Test Loss = 0.259860, Learning Rate = 3.777949e-05\n",
      "Epoch 8622/20000: Train Loss = 0.440746, Test Loss = 0.251931, Learning Rate = 3.776513e-05\n",
      "Epoch 8623/20000: Train Loss = 0.439853, Test Loss = 0.245063, Learning Rate = 3.775078e-05\n",
      "Epoch 8624/20000: Train Loss = 0.440022, Test Loss = 0.246160, Learning Rate = 3.773644e-05\n",
      "Epoch 8625/20000: Train Loss = 0.440008, Test Loss = 0.242944, Learning Rate = 3.772210e-05\n",
      "Epoch 8626/20000: Train Loss = 0.440139, Test Loss = 0.242902, Learning Rate = 3.770777e-05\n",
      "Epoch 8627/20000: Train Loss = 0.440170, Test Loss = 0.246143, Learning Rate = 3.769344e-05\n",
      "Epoch 8628/20000: Train Loss = 0.439662, Test Loss = 0.248699, Learning Rate = 3.767912e-05\n",
      "Epoch 8629/20000: Train Loss = 0.440306, Test Loss = 0.257294, Learning Rate = 3.766480e-05\n",
      "Epoch 8630/20000: Train Loss = 0.440265, Test Loss = 0.252373, Learning Rate = 3.765049e-05\n",
      "Epoch 8631/20000: Train Loss = 0.440109, Test Loss = 0.244200, Learning Rate = 3.763618e-05\n",
      "Epoch 8632/20000: Train Loss = 0.439988, Test Loss = 0.251530, Learning Rate = 3.762188e-05\n",
      "Epoch 8633/20000: Train Loss = 0.440017, Test Loss = 0.248565, Learning Rate = 3.760759e-05\n",
      "Epoch 8634/20000: Train Loss = 0.440315, Test Loss = 0.247050, Learning Rate = 3.759330e-05\n",
      "Epoch 8635/20000: Train Loss = 0.441742, Test Loss = 0.251448, Learning Rate = 3.757901e-05\n",
      "Epoch 8636/20000: Train Loss = 0.440212, Test Loss = 0.254975, Learning Rate = 3.756473e-05\n",
      "Epoch 8637/20000: Train Loss = 0.440107, Test Loss = 0.249812, Learning Rate = 3.755046e-05\n",
      "Epoch 8638/20000: Train Loss = 0.440133, Test Loss = 0.246997, Learning Rate = 3.753619e-05\n",
      "Epoch 8639/20000: Train Loss = 0.441153, Test Loss = 0.255056, Learning Rate = 3.752193e-05\n",
      "Epoch 8640/20000: Train Loss = 0.440153, Test Loss = 0.260440, Learning Rate = 3.750767e-05\n",
      "Epoch 8641/20000: Train Loss = 0.440987, Test Loss = 0.245344, Learning Rate = 3.749342e-05\n",
      "Epoch 8642/20000: Train Loss = 0.440242, Test Loss = 0.254495, Learning Rate = 3.747917e-05\n",
      "Epoch 8643/20000: Train Loss = 0.440385, Test Loss = 0.253264, Learning Rate = 3.746493e-05\n",
      "Epoch 8644/20000: Train Loss = 0.440789, Test Loss = 0.247989, Learning Rate = 3.745070e-05\n",
      "Epoch 8645/20000: Train Loss = 0.440153, Test Loss = 0.248131, Learning Rate = 3.743647e-05\n",
      "Epoch 8646/20000: Train Loss = 0.439937, Test Loss = 0.250470, Learning Rate = 3.742224e-05\n",
      "Epoch 8647/20000: Train Loss = 0.440300, Test Loss = 0.247745, Learning Rate = 3.740802e-05\n",
      "Epoch 8648/20000: Train Loss = 0.439975, Test Loss = 0.248136, Learning Rate = 3.739381e-05\n",
      "Epoch 8649/20000: Train Loss = 0.439937, Test Loss = 0.246190, Learning Rate = 3.737960e-05\n",
      "Epoch 8650/20000: Train Loss = 0.440010, Test Loss = 0.252142, Learning Rate = 3.736540e-05\n",
      "Epoch 8651/20000: Train Loss = 0.440879, Test Loss = 0.259328, Learning Rate = 3.735120e-05\n",
      "Epoch 8652/20000: Train Loss = 0.439918, Test Loss = 0.253440, Learning Rate = 3.733700e-05\n",
      "Epoch 8653/20000: Train Loss = 0.440170, Test Loss = 0.246181, Learning Rate = 3.732282e-05\n",
      "Epoch 8654/20000: Train Loss = 0.440172, Test Loss = 0.245441, Learning Rate = 3.730864e-05\n",
      "Epoch 8655/20000: Train Loss = 0.439942, Test Loss = 0.246455, Learning Rate = 3.729446e-05\n",
      "Epoch 8656/20000: Train Loss = 0.440102, Test Loss = 0.254707, Learning Rate = 3.728029e-05\n",
      "Epoch 8657/20000: Train Loss = 0.439738, Test Loss = 0.247195, Learning Rate = 3.726612e-05\n",
      "Epoch 8658/20000: Train Loss = 0.441090, Test Loss = 0.253396, Learning Rate = 3.725196e-05\n",
      "Epoch 8659/20000: Train Loss = 0.440128, Test Loss = 0.246577, Learning Rate = 3.723781e-05\n",
      "Epoch 8660/20000: Train Loss = 0.439976, Test Loss = 0.258172, Learning Rate = 3.722366e-05\n",
      "Epoch 8661/20000: Train Loss = 0.440345, Test Loss = 0.251253, Learning Rate = 3.720952e-05\n",
      "Epoch 8662/20000: Train Loss = 0.440240, Test Loss = 0.248785, Learning Rate = 3.719538e-05\n",
      "Epoch 8663/20000: Train Loss = 0.440269, Test Loss = 0.251786, Learning Rate = 3.718124e-05\n",
      "Epoch 8664/20000: Train Loss = 0.439932, Test Loss = 0.242415, Learning Rate = 3.716712e-05\n",
      "Epoch 8665/20000: Train Loss = 0.439993, Test Loss = 0.243650, Learning Rate = 3.715299e-05\n",
      "Epoch 8666/20000: Train Loss = 0.442298, Test Loss = 0.231793, Learning Rate = 3.713888e-05\n",
      "Epoch 8667/20000: Train Loss = 0.440755, Test Loss = 0.251038, Learning Rate = 3.712476e-05\n",
      "Epoch 8668/20000: Train Loss = 0.439958, Test Loss = 0.250847, Learning Rate = 3.711066e-05\n",
      "Epoch 8669/20000: Train Loss = 0.441276, Test Loss = 0.242770, Learning Rate = 3.709656e-05\n",
      "Epoch 8670/20000: Train Loss = 0.440595, Test Loss = 0.258753, Learning Rate = 3.708246e-05\n",
      "Epoch 8671/20000: Train Loss = 0.440253, Test Loss = 0.252650, Learning Rate = 3.706837e-05\n",
      "Epoch 8672/20000: Train Loss = 0.440413, Test Loss = 0.245568, Learning Rate = 3.705429e-05\n",
      "Epoch 8673/20000: Train Loss = 0.440313, Test Loss = 0.242919, Learning Rate = 3.704021e-05\n",
      "Epoch 8674/20000: Train Loss = 0.440833, Test Loss = 0.251386, Learning Rate = 3.702613e-05\n",
      "Epoch 8675/20000: Train Loss = 0.440225, Test Loss = 0.255832, Learning Rate = 3.701206e-05\n",
      "Epoch 8676/20000: Train Loss = 0.440633, Test Loss = 0.254272, Learning Rate = 3.699800e-05\n",
      "Epoch 8677/20000: Train Loss = 0.440320, Test Loss = 0.245152, Learning Rate = 3.698394e-05\n",
      "Epoch 8678/20000: Train Loss = 0.440283, Test Loss = 0.253289, Learning Rate = 3.696989e-05\n",
      "Epoch 8679/20000: Train Loss = 0.439770, Test Loss = 0.242873, Learning Rate = 3.695584e-05\n",
      "Epoch 8680/20000: Train Loss = 0.440381, Test Loss = 0.252704, Learning Rate = 3.694180e-05\n",
      "Epoch 8681/20000: Train Loss = 0.439930, Test Loss = 0.247934, Learning Rate = 3.692776e-05\n",
      "Epoch 8682/20000: Train Loss = 0.439826, Test Loss = 0.245983, Learning Rate = 3.691373e-05\n",
      "Epoch 8683/20000: Train Loss = 0.440325, Test Loss = 0.246599, Learning Rate = 3.689970e-05\n",
      "Epoch 8684/20000: Train Loss = 0.440450, Test Loss = 0.258082, Learning Rate = 3.688568e-05\n",
      "Epoch 8685/20000: Train Loss = 0.440471, Test Loss = 0.247730, Learning Rate = 3.687167e-05\n",
      "Epoch 8686/20000: Train Loss = 0.440172, Test Loss = 0.249428, Learning Rate = 3.685766e-05\n",
      "Epoch 8687/20000: Train Loss = 0.440009, Test Loss = 0.255531, Learning Rate = 3.684365e-05\n",
      "Epoch 8688/20000: Train Loss = 0.440397, Test Loss = 0.255717, Learning Rate = 3.682965e-05\n",
      "Epoch 8689/20000: Train Loss = 0.440450, Test Loss = 0.256240, Learning Rate = 3.681566e-05\n",
      "Epoch 8690/20000: Train Loss = 0.439819, Test Loss = 0.253769, Learning Rate = 3.680167e-05\n",
      "Epoch 8691/20000: Train Loss = 0.439887, Test Loss = 0.256850, Learning Rate = 3.678769e-05\n",
      "Epoch 8692/20000: Train Loss = 0.439799, Test Loss = 0.255083, Learning Rate = 3.677371e-05\n",
      "Epoch 8693/20000: Train Loss = 0.440000, Test Loss = 0.249079, Learning Rate = 3.675973e-05\n",
      "Epoch 8694/20000: Train Loss = 0.440721, Test Loss = 0.249772, Learning Rate = 3.674577e-05\n",
      "Epoch 8695/20000: Train Loss = 0.440647, Test Loss = 0.253259, Learning Rate = 3.673180e-05\n",
      "Epoch 8696/20000: Train Loss = 0.440430, Test Loss = 0.252735, Learning Rate = 3.671785e-05\n",
      "Epoch 8697/20000: Train Loss = 0.441049, Test Loss = 0.241495, Learning Rate = 3.670390e-05\n",
      "Epoch 8698/20000: Train Loss = 0.440443, Test Loss = 0.250085, Learning Rate = 3.668995e-05\n",
      "Epoch 8699/20000: Train Loss = 0.440187, Test Loss = 0.233929, Learning Rate = 3.667601e-05\n",
      "Epoch 8700/20000: Train Loss = 0.441605, Test Loss = 0.250822, Learning Rate = 3.666207e-05\n",
      "Epoch 8701/20000: Train Loss = 0.439958, Test Loss = 0.248406, Learning Rate = 3.664814e-05\n",
      "Epoch 8702/20000: Train Loss = 0.440096, Test Loss = 0.245253, Learning Rate = 3.663422e-05\n",
      "Epoch 8703/20000: Train Loss = 0.440134, Test Loss = 0.251753, Learning Rate = 3.662030e-05\n",
      "Epoch 8704/20000: Train Loss = 0.439838, Test Loss = 0.244119, Learning Rate = 3.660638e-05\n",
      "Epoch 8705/20000: Train Loss = 0.440346, Test Loss = 0.250626, Learning Rate = 3.659247e-05\n",
      "Epoch 8706/20000: Train Loss = 0.440314, Test Loss = 0.243893, Learning Rate = 3.657857e-05\n",
      "Epoch 8707/20000: Train Loss = 0.440020, Test Loss = 0.254864, Learning Rate = 3.656467e-05\n",
      "Epoch 8708/20000: Train Loss = 0.440552, Test Loss = 0.248707, Learning Rate = 3.655078e-05\n",
      "Epoch 8709/20000: Train Loss = 0.441060, Test Loss = 0.246643, Learning Rate = 3.653689e-05\n",
      "Epoch 8710/20000: Train Loss = 0.440307, Test Loss = 0.246373, Learning Rate = 3.652300e-05\n",
      "Epoch 8711/20000: Train Loss = 0.440615, Test Loss = 0.247890, Learning Rate = 3.650913e-05\n",
      "Epoch 8712/20000: Train Loss = 0.441748, Test Loss = 0.245460, Learning Rate = 3.649525e-05\n",
      "Epoch 8713/20000: Train Loss = 0.440108, Test Loss = 0.250455, Learning Rate = 3.648139e-05\n",
      "Epoch 8714/20000: Train Loss = 0.440201, Test Loss = 0.250596, Learning Rate = 3.646752e-05\n",
      "Epoch 8715/20000: Train Loss = 0.439923, Test Loss = 0.245323, Learning Rate = 3.645367e-05\n",
      "Epoch 8716/20000: Train Loss = 0.439921, Test Loss = 0.250417, Learning Rate = 3.643982e-05\n",
      "Epoch 8717/20000: Train Loss = 0.440189, Test Loss = 0.245549, Learning Rate = 3.642597e-05\n",
      "Epoch 8718/20000: Train Loss = 0.439708, Test Loss = 0.247461, Learning Rate = 3.641213e-05\n",
      "Epoch 8719/20000: Train Loss = 0.440256, Test Loss = 0.241153, Learning Rate = 3.639829e-05\n",
      "Epoch 8720/20000: Train Loss = 0.440401, Test Loss = 0.251410, Learning Rate = 3.638446e-05\n",
      "Epoch 8721/20000: Train Loss = 0.440110, Test Loss = 0.249872, Learning Rate = 3.637064e-05\n",
      "Epoch 8722/20000: Train Loss = 0.439758, Test Loss = 0.254997, Learning Rate = 3.635682e-05\n",
      "Epoch 8723/20000: Train Loss = 0.440297, Test Loss = 0.254637, Learning Rate = 3.634300e-05\n",
      "Epoch 8724/20000: Train Loss = 0.440049, Test Loss = 0.248492, Learning Rate = 3.632919e-05\n",
      "Epoch 8725/20000: Train Loss = 0.439975, Test Loss = 0.251474, Learning Rate = 3.631539e-05\n",
      "Epoch 8726/20000: Train Loss = 0.440423, Test Loss = 0.254746, Learning Rate = 3.630159e-05\n",
      "Epoch 8727/20000: Train Loss = 0.440606, Test Loss = 0.250229, Learning Rate = 3.628780e-05\n",
      "Epoch 8728/20000: Train Loss = 0.439953, Test Loss = 0.245469, Learning Rate = 3.627401e-05\n",
      "Epoch 8729/20000: Train Loss = 0.440034, Test Loss = 0.253304, Learning Rate = 3.626023e-05\n",
      "Epoch 8730/20000: Train Loss = 0.439792, Test Loss = 0.246851, Learning Rate = 3.624645e-05\n",
      "Epoch 8731/20000: Train Loss = 0.440348, Test Loss = 0.249083, Learning Rate = 3.623268e-05\n",
      "Epoch 8732/20000: Train Loss = 0.440137, Test Loss = 0.246194, Learning Rate = 3.621891e-05\n",
      "Epoch 8733/20000: Train Loss = 0.440654, Test Loss = 0.252902, Learning Rate = 3.620515e-05\n",
      "Epoch 8734/20000: Train Loss = 0.440981, Test Loss = 0.255000, Learning Rate = 3.619139e-05\n",
      "Epoch 8735/20000: Train Loss = 0.440594, Test Loss = 0.245621, Learning Rate = 3.617764e-05\n",
      "Epoch 8736/20000: Train Loss = 0.441431, Test Loss = 0.249442, Learning Rate = 3.616389e-05\n",
      "Epoch 8737/20000: Train Loss = 0.440251, Test Loss = 0.247959, Learning Rate = 3.615015e-05\n",
      "Epoch 8738/20000: Train Loss = 0.440081, Test Loss = 0.246463, Learning Rate = 3.613641e-05\n",
      "Epoch 8739/20000: Train Loss = 0.440146, Test Loss = 0.247006, Learning Rate = 3.612268e-05\n",
      "Epoch 8740/20000: Train Loss = 0.439876, Test Loss = 0.247087, Learning Rate = 3.610896e-05\n",
      "Epoch 8741/20000: Train Loss = 0.440073, Test Loss = 0.253370, Learning Rate = 3.609524e-05\n",
      "Epoch 8742/20000: Train Loss = 0.439959, Test Loss = 0.247008, Learning Rate = 3.608152e-05\n",
      "Epoch 8743/20000: Train Loss = 0.439908, Test Loss = 0.244625, Learning Rate = 3.606781e-05\n",
      "Epoch 8744/20000: Train Loss = 0.439929, Test Loss = 0.248886, Learning Rate = 3.605411e-05\n",
      "Epoch 8745/20000: Train Loss = 0.440235, Test Loss = 0.250089, Learning Rate = 3.604041e-05\n",
      "Epoch 8746/20000: Train Loss = 0.440019, Test Loss = 0.249273, Learning Rate = 3.602671e-05\n",
      "Epoch 8747/20000: Train Loss = 0.440602, Test Loss = 0.246706, Learning Rate = 3.601302e-05\n",
      "Epoch 8748/20000: Train Loss = 0.440871, Test Loss = 0.250257, Learning Rate = 3.599934e-05\n",
      "Epoch 8749/20000: Train Loss = 0.440044, Test Loss = 0.254111, Learning Rate = 3.598566e-05\n",
      "Epoch 8750/20000: Train Loss = 0.439914, Test Loss = 0.246842, Learning Rate = 3.597199e-05\n",
      "Epoch 8751/20000: Train Loss = 0.440024, Test Loss = 0.247996, Learning Rate = 3.595832e-05\n",
      "Epoch 8752/20000: Train Loss = 0.440154, Test Loss = 0.250557, Learning Rate = 3.594466e-05\n",
      "Epoch 8753/20000: Train Loss = 0.440469, Test Loss = 0.251695, Learning Rate = 3.593100e-05\n",
      "Epoch 8754/20000: Train Loss = 0.440352, Test Loss = 0.248193, Learning Rate = 3.591734e-05\n",
      "Epoch 8755/20000: Train Loss = 0.440180, Test Loss = 0.248322, Learning Rate = 3.590370e-05\n",
      "Epoch 8756/20000: Train Loss = 0.440301, Test Loss = 0.255111, Learning Rate = 3.589005e-05\n",
      "Epoch 8757/20000: Train Loss = 0.439994, Test Loss = 0.250171, Learning Rate = 3.587642e-05\n",
      "Epoch 8758/20000: Train Loss = 0.440355, Test Loss = 0.255601, Learning Rate = 3.586279e-05\n",
      "Epoch 8759/20000: Train Loss = 0.440265, Test Loss = 0.253436, Learning Rate = 3.584916e-05\n",
      "Epoch 8760/20000: Train Loss = 0.439815, Test Loss = 0.253548, Learning Rate = 3.583554e-05\n",
      "Epoch 8761/20000: Train Loss = 0.440066, Test Loss = 0.244254, Learning Rate = 3.582192e-05\n",
      "Epoch 8762/20000: Train Loss = 0.441473, Test Loss = 0.258111, Learning Rate = 3.580831e-05\n",
      "Epoch 8763/20000: Train Loss = 0.439665, Test Loss = 0.249201, Learning Rate = 3.579470e-05\n",
      "Epoch 8764/20000: Train Loss = 0.439858, Test Loss = 0.248506, Learning Rate = 3.578110e-05\n",
      "Epoch 8765/20000: Train Loss = 0.440858, Test Loss = 0.253877, Learning Rate = 3.576751e-05\n",
      "Epoch 8766/20000: Train Loss = 0.440050, Test Loss = 0.245765, Learning Rate = 3.575392e-05\n",
      "Epoch 8767/20000: Train Loss = 0.439971, Test Loss = 0.242444, Learning Rate = 3.574033e-05\n",
      "Epoch 8768/20000: Train Loss = 0.440516, Test Loss = 0.244626, Learning Rate = 3.572675e-05\n",
      "Epoch 8769/20000: Train Loss = 0.440340, Test Loss = 0.243241, Learning Rate = 3.571317e-05\n",
      "Epoch 8770/20000: Train Loss = 0.440109, Test Loss = 0.242398, Learning Rate = 3.569960e-05\n",
      "Epoch 8771/20000: Train Loss = 0.440196, Test Loss = 0.246264, Learning Rate = 3.568604e-05\n",
      "Epoch 8772/20000: Train Loss = 0.439784, Test Loss = 0.249359, Learning Rate = 3.567248e-05\n",
      "Epoch 8773/20000: Train Loss = 0.440675, Test Loss = 0.254803, Learning Rate = 3.565892e-05\n",
      "Epoch 8774/20000: Train Loss = 0.440342, Test Loss = 0.248418, Learning Rate = 3.564538e-05\n",
      "Epoch 8775/20000: Train Loss = 0.440106, Test Loss = 0.258079, Learning Rate = 3.563183e-05\n",
      "Epoch 8776/20000: Train Loss = 0.440295, Test Loss = 0.242984, Learning Rate = 3.561829e-05\n",
      "Epoch 8777/20000: Train Loss = 0.440160, Test Loss = 0.244397, Learning Rate = 3.560476e-05\n",
      "Epoch 8778/20000: Train Loss = 0.440357, Test Loss = 0.251240, Learning Rate = 3.559123e-05\n",
      "Epoch 8779/20000: Train Loss = 0.440265, Test Loss = 0.243149, Learning Rate = 3.557771e-05\n",
      "Epoch 8780/20000: Train Loss = 0.440078, Test Loss = 0.257602, Learning Rate = 3.556419e-05\n",
      "Epoch 8781/20000: Train Loss = 0.440100, Test Loss = 0.244598, Learning Rate = 3.555067e-05\n",
      "Epoch 8782/20000: Train Loss = 0.440169, Test Loss = 0.246956, Learning Rate = 3.553717e-05\n",
      "Epoch 8783/20000: Train Loss = 0.440275, Test Loss = 0.251769, Learning Rate = 3.552366e-05\n",
      "Epoch 8784/20000: Train Loss = 0.439947, Test Loss = 0.251148, Learning Rate = 3.551016e-05\n",
      "Epoch 8785/20000: Train Loss = 0.440109, Test Loss = 0.248354, Learning Rate = 3.549667e-05\n",
      "Epoch 8786/20000: Train Loss = 0.440137, Test Loss = 0.254077, Learning Rate = 3.548318e-05\n",
      "Epoch 8787/20000: Train Loss = 0.440959, Test Loss = 0.252652, Learning Rate = 3.546970e-05\n",
      "Epoch 8788/20000: Train Loss = 0.440116, Test Loss = 0.251191, Learning Rate = 3.545622e-05\n",
      "Epoch 8789/20000: Train Loss = 0.440265, Test Loss = 0.250935, Learning Rate = 3.544275e-05\n",
      "Epoch 8790/20000: Train Loss = 0.439890, Test Loss = 0.243517, Learning Rate = 3.542928e-05\n",
      "Epoch 8791/20000: Train Loss = 0.440135, Test Loss = 0.258426, Learning Rate = 3.541582e-05\n",
      "Epoch 8792/20000: Train Loss = 0.440969, Test Loss = 0.243045, Learning Rate = 3.540236e-05\n",
      "Epoch 8793/20000: Train Loss = 0.440963, Test Loss = 0.241991, Learning Rate = 3.538891e-05\n",
      "Epoch 8794/20000: Train Loss = 0.440073, Test Loss = 0.248480, Learning Rate = 3.537547e-05\n",
      "Epoch 8795/20000: Train Loss = 0.440153, Test Loss = 0.250625, Learning Rate = 3.536202e-05\n",
      "Epoch 8796/20000: Train Loss = 0.439901, Test Loss = 0.250020, Learning Rate = 3.534859e-05\n",
      "Epoch 8797/20000: Train Loss = 0.440273, Test Loss = 0.246228, Learning Rate = 3.533516e-05\n",
      "Epoch 8798/20000: Train Loss = 0.439938, Test Loss = 0.254585, Learning Rate = 3.532173e-05\n",
      "Epoch 8799/20000: Train Loss = 0.440836, Test Loss = 0.251891, Learning Rate = 3.530831e-05\n",
      "Epoch 8800/20000: Train Loss = 0.440272, Test Loss = 0.248904, Learning Rate = 3.529489e-05\n",
      "Epoch 8801/20000: Train Loss = 0.440295, Test Loss = 0.248339, Learning Rate = 3.528148e-05\n",
      "Epoch 8802/20000: Train Loss = 0.440266, Test Loss = 0.249522, Learning Rate = 3.526807e-05\n",
      "Epoch 8803/20000: Train Loss = 0.440075, Test Loss = 0.253263, Learning Rate = 3.525467e-05\n",
      "Epoch 8804/20000: Train Loss = 0.440425, Test Loss = 0.258968, Learning Rate = 3.524128e-05\n",
      "Epoch 8805/20000: Train Loss = 0.439968, Test Loss = 0.244526, Learning Rate = 3.522789e-05\n",
      "Epoch 8806/20000: Train Loss = 0.440308, Test Loss = 0.246754, Learning Rate = 3.521450e-05\n",
      "Epoch 8807/20000: Train Loss = 0.440073, Test Loss = 0.249350, Learning Rate = 3.520112e-05\n",
      "Epoch 8808/20000: Train Loss = 0.439984, Test Loss = 0.249134, Learning Rate = 3.518775e-05\n",
      "Epoch 8809/20000: Train Loss = 0.440535, Test Loss = 0.250956, Learning Rate = 3.517438e-05\n",
      "Epoch 8810/20000: Train Loss = 0.440475, Test Loss = 0.252962, Learning Rate = 3.516101e-05\n",
      "Epoch 8811/20000: Train Loss = 0.440305, Test Loss = 0.251229, Learning Rate = 3.514765e-05\n",
      "Epoch 8812/20000: Train Loss = 0.440290, Test Loss = 0.253877, Learning Rate = 3.513429e-05\n",
      "Epoch 8813/20000: Train Loss = 0.439699, Test Loss = 0.248957, Learning Rate = 3.512094e-05\n",
      "Epoch 8814/20000: Train Loss = 0.440038, Test Loss = 0.257748, Learning Rate = 3.510760e-05\n",
      "Epoch 8815/20000: Train Loss = 0.440222, Test Loss = 0.248814, Learning Rate = 3.509426e-05\n",
      "Epoch 8816/20000: Train Loss = 0.440036, Test Loss = 0.248757, Learning Rate = 3.508092e-05\n",
      "Epoch 8817/20000: Train Loss = 0.439828, Test Loss = 0.251698, Learning Rate = 3.506759e-05\n",
      "Epoch 8818/20000: Train Loss = 0.440109, Test Loss = 0.255196, Learning Rate = 3.505427e-05\n",
      "Epoch 8819/20000: Train Loss = 0.440262, Test Loss = 0.250127, Learning Rate = 3.504095e-05\n",
      "Epoch 8820/20000: Train Loss = 0.440605, Test Loss = 0.253939, Learning Rate = 3.502764e-05\n",
      "Epoch 8821/20000: Train Loss = 0.440074, Test Loss = 0.251848, Learning Rate = 3.501433e-05\n",
      "Epoch 8822/20000: Train Loss = 0.440202, Test Loss = 0.248246, Learning Rate = 3.500102e-05\n",
      "Epoch 8823/20000: Train Loss = 0.439756, Test Loss = 0.248251, Learning Rate = 3.498772e-05\n",
      "Epoch 8824/20000: Train Loss = 0.440355, Test Loss = 0.241292, Learning Rate = 3.497443e-05\n",
      "Epoch 8825/20000: Train Loss = 0.439913, Test Loss = 0.252435, Learning Rate = 3.496114e-05\n",
      "Epoch 8826/20000: Train Loss = 0.440014, Test Loss = 0.248675, Learning Rate = 3.494785e-05\n",
      "Epoch 8827/20000: Train Loss = 0.440377, Test Loss = 0.256626, Learning Rate = 3.493457e-05\n",
      "Epoch 8828/20000: Train Loss = 0.440581, Test Loss = 0.251042, Learning Rate = 3.492130e-05\n",
      "Epoch 8829/20000: Train Loss = 0.440189, Test Loss = 0.247717, Learning Rate = 3.490803e-05\n",
      "Epoch 8830/20000: Train Loss = 0.440794, Test Loss = 0.258163, Learning Rate = 3.489477e-05\n",
      "Epoch 8831/20000: Train Loss = 0.439945, Test Loss = 0.249606, Learning Rate = 3.488151e-05\n",
      "Epoch 8832/20000: Train Loss = 0.440372, Test Loss = 0.255641, Learning Rate = 3.486825e-05\n",
      "Epoch 8833/20000: Train Loss = 0.439860, Test Loss = 0.254912, Learning Rate = 3.485501e-05\n",
      "Epoch 8834/20000: Train Loss = 0.440012, Test Loss = 0.251414, Learning Rate = 3.484176e-05\n",
      "Epoch 8835/20000: Train Loss = 0.439923, Test Loss = 0.249891, Learning Rate = 3.482852e-05\n",
      "Epoch 8836/20000: Train Loss = 0.440530, Test Loss = 0.256314, Learning Rate = 3.481529e-05\n",
      "Epoch 8837/20000: Train Loss = 0.440250, Test Loss = 0.239780, Learning Rate = 3.480206e-05\n",
      "Epoch 8838/20000: Train Loss = 0.441085, Test Loss = 0.255471, Learning Rate = 3.478884e-05\n",
      "Epoch 8839/20000: Train Loss = 0.440670, Test Loss = 0.255442, Learning Rate = 3.477562e-05\n",
      "Epoch 8840/20000: Train Loss = 0.439898, Test Loss = 0.245206, Learning Rate = 3.476240e-05\n",
      "Epoch 8841/20000: Train Loss = 0.439896, Test Loss = 0.247589, Learning Rate = 3.474919e-05\n",
      "Epoch 8842/20000: Train Loss = 0.440003, Test Loss = 0.248098, Learning Rate = 3.473599e-05\n",
      "Epoch 8843/20000: Train Loss = 0.440055, Test Loss = 0.243808, Learning Rate = 3.472279e-05\n",
      "Epoch 8844/20000: Train Loss = 0.440169, Test Loss = 0.255460, Learning Rate = 3.470960e-05\n",
      "Epoch 8845/20000: Train Loss = 0.440140, Test Loss = 0.245625, Learning Rate = 3.469641e-05\n",
      "Epoch 8846/20000: Train Loss = 0.440218, Test Loss = 0.246490, Learning Rate = 3.468323e-05\n",
      "Epoch 8847/20000: Train Loss = 0.440040, Test Loss = 0.248136, Learning Rate = 3.467005e-05\n",
      "Epoch 8848/20000: Train Loss = 0.440749, Test Loss = 0.241292, Learning Rate = 3.465687e-05\n",
      "Epoch 8849/20000: Train Loss = 0.439462, Test Loss = 0.253991, Learning Rate = 3.464370e-05\n",
      "Epoch 8850/20000: Train Loss = 0.439797, Test Loss = 0.244607, Learning Rate = 3.463054e-05\n",
      "Epoch 8851/20000: Train Loss = 0.440362, Test Loss = 0.247589, Learning Rate = 3.461738e-05\n",
      "Epoch 8852/20000: Train Loss = 0.439922, Test Loss = 0.252713, Learning Rate = 3.460423e-05\n",
      "Epoch 8853/20000: Train Loss = 0.440007, Test Loss = 0.247528, Learning Rate = 3.459108e-05\n",
      "Epoch 8854/20000: Train Loss = 0.440928, Test Loss = 0.251315, Learning Rate = 3.457794e-05\n",
      "Epoch 8855/20000: Train Loss = 0.440472, Test Loss = 0.252718, Learning Rate = 3.456480e-05\n",
      "Epoch 8856/20000: Train Loss = 0.440249, Test Loss = 0.250365, Learning Rate = 3.455166e-05\n",
      "Epoch 8857/20000: Train Loss = 0.439817, Test Loss = 0.249557, Learning Rate = 3.453854e-05\n",
      "Epoch 8858/20000: Train Loss = 0.440334, Test Loss = 0.253230, Learning Rate = 3.452541e-05\n",
      "Epoch 8859/20000: Train Loss = 0.440048, Test Loss = 0.248835, Learning Rate = 3.451229e-05\n",
      "Epoch 8860/20000: Train Loss = 0.440343, Test Loss = 0.252728, Learning Rate = 3.449918e-05\n",
      "Epoch 8861/20000: Train Loss = 0.440405, Test Loss = 0.251599, Learning Rate = 3.448607e-05\n",
      "Epoch 8862/20000: Train Loss = 0.440047, Test Loss = 0.251350, Learning Rate = 3.447297e-05\n",
      "Epoch 8863/20000: Train Loss = 0.440316, Test Loss = 0.252953, Learning Rate = 3.445987e-05\n",
      "Epoch 8864/20000: Train Loss = 0.439753, Test Loss = 0.247467, Learning Rate = 3.444677e-05\n",
      "Epoch 8865/20000: Train Loss = 0.440104, Test Loss = 0.243122, Learning Rate = 3.443369e-05\n",
      "Epoch 8866/20000: Train Loss = 0.439607, Test Loss = 0.253316, Learning Rate = 3.442060e-05\n",
      "Epoch 8867/20000: Train Loss = 0.440049, Test Loss = 0.246118, Learning Rate = 3.440752e-05\n",
      "Epoch 8868/20000: Train Loss = 0.440165, Test Loss = 0.252747, Learning Rate = 3.439445e-05\n",
      "Epoch 8869/20000: Train Loss = 0.440449, Test Loss = 0.249013, Learning Rate = 3.438138e-05\n",
      "Epoch 8870/20000: Train Loss = 0.440486, Test Loss = 0.249796, Learning Rate = 3.436832e-05\n",
      "Epoch 8871/20000: Train Loss = 0.439894, Test Loss = 0.245877, Learning Rate = 3.435526e-05\n",
      "Epoch 8872/20000: Train Loss = 0.440083, Test Loss = 0.249787, Learning Rate = 3.434220e-05\n",
      "Epoch 8873/20000: Train Loss = 0.440042, Test Loss = 0.248119, Learning Rate = 3.432915e-05\n",
      "Epoch 8874/20000: Train Loss = 0.440181, Test Loss = 0.248402, Learning Rate = 3.431611e-05\n",
      "Epoch 8875/20000: Train Loss = 0.439896, Test Loss = 0.252373, Learning Rate = 3.430307e-05\n",
      "Epoch 8876/20000: Train Loss = 0.439657, Test Loss = 0.245790, Learning Rate = 3.429004e-05\n",
      "Epoch 8877/20000: Train Loss = 0.439926, Test Loss = 0.247745, Learning Rate = 3.427701e-05\n",
      "Epoch 8878/20000: Train Loss = 0.440214, Test Loss = 0.243313, Learning Rate = 3.426398e-05\n",
      "Epoch 8879/20000: Train Loss = 0.440077, Test Loss = 0.246375, Learning Rate = 3.425096e-05\n",
      "Epoch 8880/20000: Train Loss = 0.440209, Test Loss = 0.243419, Learning Rate = 3.423795e-05\n",
      "Epoch 8881/20000: Train Loss = 0.440318, Test Loss = 0.257722, Learning Rate = 3.422494e-05\n",
      "Epoch 8882/20000: Train Loss = 0.439972, Test Loss = 0.250557, Learning Rate = 3.421193e-05\n",
      "Epoch 8883/20000: Train Loss = 0.440016, Test Loss = 0.254085, Learning Rate = 3.419893e-05\n",
      "Epoch 8884/20000: Train Loss = 0.440057, Test Loss = 0.246686, Learning Rate = 3.418594e-05\n",
      "Epoch 8885/20000: Train Loss = 0.439766, Test Loss = 0.255817, Learning Rate = 3.417295e-05\n",
      "Epoch 8886/20000: Train Loss = 0.440413, Test Loss = 0.254516, Learning Rate = 3.415997e-05\n",
      "Epoch 8887/20000: Train Loss = 0.439849, Test Loss = 0.253829, Learning Rate = 3.414699e-05\n",
      "Epoch 8888/20000: Train Loss = 0.440425, Test Loss = 0.257356, Learning Rate = 3.413401e-05\n",
      "Epoch 8889/20000: Train Loss = 0.440686, Test Loss = 0.253416, Learning Rate = 3.412104e-05\n",
      "Epoch 8890/20000: Train Loss = 0.439931, Test Loss = 0.250754, Learning Rate = 3.410808e-05\n",
      "Epoch 8891/20000: Train Loss = 0.440125, Test Loss = 0.247332, Learning Rate = 3.409512e-05\n",
      "Epoch 8892/20000: Train Loss = 0.440129, Test Loss = 0.240877, Learning Rate = 3.408216e-05\n",
      "Epoch 8893/20000: Train Loss = 0.440192, Test Loss = 0.246436, Learning Rate = 3.406921e-05\n",
      "Epoch 8894/20000: Train Loss = 0.440521, Test Loss = 0.246820, Learning Rate = 3.405626e-05\n",
      "Epoch 8895/20000: Train Loss = 0.439906, Test Loss = 0.247543, Learning Rate = 3.404332e-05\n",
      "Epoch 8896/20000: Train Loss = 0.440543, Test Loss = 0.249422, Learning Rate = 3.403039e-05\n",
      "Epoch 8897/20000: Train Loss = 0.440593, Test Loss = 0.245780, Learning Rate = 3.401746e-05\n",
      "Epoch 8898/20000: Train Loss = 0.440337, Test Loss = 0.250407, Learning Rate = 3.400453e-05\n",
      "Epoch 8899/20000: Train Loss = 0.439915, Test Loss = 0.241267, Learning Rate = 3.399161e-05\n",
      "Epoch 8900/20000: Train Loss = 0.440317, Test Loss = 0.250307, Learning Rate = 3.397870e-05\n",
      "Epoch 8901/20000: Train Loss = 0.440337, Test Loss = 0.246552, Learning Rate = 3.396578e-05\n",
      "Epoch 8902/20000: Train Loss = 0.440252, Test Loss = 0.249900, Learning Rate = 3.395288e-05\n",
      "Epoch 8903/20000: Train Loss = 0.439992, Test Loss = 0.253318, Learning Rate = 3.393998e-05\n",
      "Epoch 8904/20000: Train Loss = 0.439891, Test Loss = 0.249711, Learning Rate = 3.392708e-05\n",
      "Epoch 8905/20000: Train Loss = 0.440064, Test Loss = 0.246894, Learning Rate = 3.391419e-05\n",
      "Epoch 8906/20000: Train Loss = 0.440108, Test Loss = 0.246838, Learning Rate = 3.390130e-05\n",
      "Epoch 8907/20000: Train Loss = 0.440377, Test Loss = 0.246227, Learning Rate = 3.388842e-05\n",
      "Epoch 8908/20000: Train Loss = 0.440289, Test Loss = 0.256868, Learning Rate = 3.387555e-05\n",
      "Epoch 8909/20000: Train Loss = 0.440683, Test Loss = 0.247868, Learning Rate = 3.386267e-05\n",
      "Epoch 8910/20000: Train Loss = 0.440171, Test Loss = 0.244614, Learning Rate = 3.384981e-05\n",
      "Epoch 8911/20000: Train Loss = 0.440370, Test Loss = 0.249733, Learning Rate = 3.383694e-05\n",
      "Epoch 8912/20000: Train Loss = 0.440291, Test Loss = 0.246014, Learning Rate = 3.382409e-05\n",
      "Epoch 8913/20000: Train Loss = 0.440027, Test Loss = 0.249688, Learning Rate = 3.381123e-05\n",
      "Epoch 8914/20000: Train Loss = 0.440669, Test Loss = 0.253059, Learning Rate = 3.379839e-05\n",
      "Epoch 8915/20000: Train Loss = 0.440220, Test Loss = 0.251895, Learning Rate = 3.378555e-05\n",
      "Epoch 8916/20000: Train Loss = 0.439994, Test Loss = 0.245620, Learning Rate = 3.377271e-05\n",
      "Epoch 8917/20000: Train Loss = 0.439967, Test Loss = 0.252929, Learning Rate = 3.375987e-05\n",
      "Epoch 8918/20000: Train Loss = 0.439812, Test Loss = 0.255328, Learning Rate = 3.374705e-05\n",
      "Epoch 8919/20000: Train Loss = 0.440140, Test Loss = 0.252760, Learning Rate = 3.373422e-05\n",
      "Epoch 8920/20000: Train Loss = 0.440483, Test Loss = 0.255402, Learning Rate = 3.372141e-05\n",
      "Epoch 8921/20000: Train Loss = 0.439921, Test Loss = 0.251575, Learning Rate = 3.370859e-05\n",
      "Epoch 8922/20000: Train Loss = 0.440322, Test Loss = 0.255084, Learning Rate = 3.369578e-05\n",
      "Epoch 8923/20000: Train Loss = 0.440024, Test Loss = 0.249835, Learning Rate = 3.368298e-05\n",
      "Epoch 8924/20000: Train Loss = 0.440189, Test Loss = 0.244785, Learning Rate = 3.367018e-05\n",
      "Epoch 8925/20000: Train Loss = 0.439857, Test Loss = 0.254289, Learning Rate = 3.365739e-05\n",
      "Epoch 8926/20000: Train Loss = 0.439801, Test Loss = 0.247468, Learning Rate = 3.364460e-05\n",
      "Epoch 8927/20000: Train Loss = 0.439974, Test Loss = 0.245205, Learning Rate = 3.363182e-05\n",
      "Epoch 8928/20000: Train Loss = 0.440126, Test Loss = 0.257967, Learning Rate = 3.361904e-05\n",
      "Epoch 8929/20000: Train Loss = 0.439760, Test Loss = 0.247879, Learning Rate = 3.360626e-05\n",
      "Epoch 8930/20000: Train Loss = 0.439757, Test Loss = 0.249788, Learning Rate = 3.359349e-05\n",
      "Epoch 8931/20000: Train Loss = 0.440068, Test Loss = 0.251112, Learning Rate = 3.358073e-05\n",
      "Epoch 8932/20000: Train Loss = 0.439899, Test Loss = 0.247721, Learning Rate = 3.356797e-05\n",
      "Epoch 8933/20000: Train Loss = 0.439969, Test Loss = 0.251726, Learning Rate = 3.355521e-05\n",
      "Epoch 8934/20000: Train Loss = 0.439841, Test Loss = 0.254226, Learning Rate = 3.354246e-05\n",
      "Epoch 8935/20000: Train Loss = 0.440008, Test Loss = 0.253922, Learning Rate = 3.352972e-05\n",
      "Epoch 8936/20000: Train Loss = 0.440061, Test Loss = 0.256506, Learning Rate = 3.351698e-05\n",
      "Epoch 8937/20000: Train Loss = 0.439789, Test Loss = 0.246582, Learning Rate = 3.350424e-05\n",
      "Epoch 8938/20000: Train Loss = 0.440050, Test Loss = 0.255658, Learning Rate = 3.349151e-05\n",
      "Epoch 8939/20000: Train Loss = 0.439991, Test Loss = 0.251056, Learning Rate = 3.347879e-05\n",
      "Epoch 8940/20000: Train Loss = 0.439769, Test Loss = 0.248056, Learning Rate = 3.346606e-05\n",
      "Epoch 8941/20000: Train Loss = 0.439947, Test Loss = 0.251785, Learning Rate = 3.345335e-05\n",
      "Epoch 8942/20000: Train Loss = 0.440053, Test Loss = 0.252495, Learning Rate = 3.344064e-05\n",
      "Epoch 8943/20000: Train Loss = 0.440120, Test Loss = 0.249439, Learning Rate = 3.342793e-05\n",
      "Epoch 8944/20000: Train Loss = 0.440119, Test Loss = 0.259340, Learning Rate = 3.341523e-05\n",
      "Epoch 8945/20000: Train Loss = 0.440354, Test Loss = 0.249921, Learning Rate = 3.340253e-05\n",
      "Epoch 8946/20000: Train Loss = 0.440164, Test Loss = 0.253420, Learning Rate = 3.338984e-05\n",
      "Epoch 8947/20000: Train Loss = 0.439847, Test Loss = 0.243602, Learning Rate = 3.337715e-05\n",
      "Epoch 8948/20000: Train Loss = 0.439997, Test Loss = 0.247416, Learning Rate = 3.336447e-05\n",
      "Epoch 8949/20000: Train Loss = 0.440045, Test Loss = 0.248119, Learning Rate = 3.335179e-05\n",
      "Epoch 8950/20000: Train Loss = 0.439850, Test Loss = 0.246674, Learning Rate = 3.333912e-05\n",
      "Epoch 8951/20000: Train Loss = 0.440060, Test Loss = 0.244527, Learning Rate = 3.332645e-05\n",
      "Epoch 8952/20000: Train Loss = 0.439883, Test Loss = 0.247892, Learning Rate = 3.331379e-05\n",
      "Epoch 8953/20000: Train Loss = 0.439949, Test Loss = 0.246969, Learning Rate = 3.330113e-05\n",
      "Epoch 8954/20000: Train Loss = 0.439991, Test Loss = 0.251628, Learning Rate = 3.328848e-05\n",
      "Epoch 8955/20000: Train Loss = 0.440075, Test Loss = 0.249147, Learning Rate = 3.327583e-05\n",
      "Epoch 8956/20000: Train Loss = 0.440106, Test Loss = 0.247248, Learning Rate = 3.326318e-05\n",
      "Epoch 8957/20000: Train Loss = 0.439723, Test Loss = 0.244939, Learning Rate = 3.325054e-05\n",
      "Epoch 8958/20000: Train Loss = 0.440811, Test Loss = 0.242904, Learning Rate = 3.323791e-05\n",
      "Epoch 8959/20000: Train Loss = 0.440130, Test Loss = 0.250900, Learning Rate = 3.322528e-05\n",
      "Epoch 8960/20000: Train Loss = 0.440220, Test Loss = 0.252362, Learning Rate = 3.321266e-05\n",
      "Epoch 8961/20000: Train Loss = 0.440524, Test Loss = 0.251102, Learning Rate = 3.320004e-05\n",
      "Epoch 8962/20000: Train Loss = 0.440257, Test Loss = 0.242786, Learning Rate = 3.318742e-05\n",
      "Epoch 8963/20000: Train Loss = 0.440975, Test Loss = 0.251982, Learning Rate = 3.317481e-05\n",
      "Epoch 8964/20000: Train Loss = 0.440053, Test Loss = 0.248520, Learning Rate = 3.316221e-05\n",
      "Epoch 8965/20000: Train Loss = 0.440017, Test Loss = 0.246320, Learning Rate = 3.314960e-05\n",
      "Epoch 8966/20000: Train Loss = 0.439987, Test Loss = 0.250013, Learning Rate = 3.313701e-05\n",
      "Epoch 8967/20000: Train Loss = 0.440956, Test Loss = 0.246596, Learning Rate = 3.312442e-05\n",
      "Epoch 8968/20000: Train Loss = 0.440315, Test Loss = 0.252721, Learning Rate = 3.311183e-05\n",
      "Epoch 8969/20000: Train Loss = 0.440763, Test Loss = 0.251915, Learning Rate = 3.309925e-05\n",
      "Epoch 8970/20000: Train Loss = 0.440011, Test Loss = 0.251517, Learning Rate = 3.308667e-05\n",
      "Epoch 8971/20000: Train Loss = 0.439987, Test Loss = 0.253625, Learning Rate = 3.307410e-05\n",
      "Epoch 8972/20000: Train Loss = 0.439849, Test Loss = 0.250675, Learning Rate = 3.306153e-05\n",
      "Epoch 8973/20000: Train Loss = 0.441764, Test Loss = 0.249479, Learning Rate = 3.304897e-05\n",
      "Epoch 8974/20000: Train Loss = 0.439861, Test Loss = 0.251730, Learning Rate = 3.303641e-05\n",
      "Epoch 8975/20000: Train Loss = 0.439960, Test Loss = 0.256628, Learning Rate = 3.302386e-05\n",
      "Epoch 8976/20000: Train Loss = 0.439716, Test Loss = 0.250005, Learning Rate = 3.301131e-05\n",
      "Epoch 8977/20000: Train Loss = 0.439844, Test Loss = 0.258450, Learning Rate = 3.299877e-05\n",
      "Epoch 8978/20000: Train Loss = 0.440126, Test Loss = 0.254772, Learning Rate = 3.298623e-05\n",
      "Epoch 8979/20000: Train Loss = 0.439998, Test Loss = 0.257088, Learning Rate = 3.297370e-05\n",
      "Epoch 8980/20000: Train Loss = 0.440006, Test Loss = 0.250937, Learning Rate = 3.296117e-05\n",
      "Epoch 8981/20000: Train Loss = 0.440106, Test Loss = 0.257015, Learning Rate = 3.294864e-05\n",
      "Epoch 8982/20000: Train Loss = 0.439986, Test Loss = 0.245528, Learning Rate = 3.293612e-05\n",
      "Epoch 8983/20000: Train Loss = 0.440265, Test Loss = 0.249420, Learning Rate = 3.292361e-05\n",
      "Epoch 8984/20000: Train Loss = 0.439770, Test Loss = 0.251307, Learning Rate = 3.291110e-05\n",
      "Epoch 8985/20000: Train Loss = 0.439988, Test Loss = 0.251036, Learning Rate = 3.289859e-05\n",
      "Epoch 8986/20000: Train Loss = 0.440023, Test Loss = 0.245164, Learning Rate = 3.288609e-05\n",
      "Epoch 8987/20000: Train Loss = 0.440124, Test Loss = 0.249387, Learning Rate = 3.287360e-05\n",
      "Epoch 8988/20000: Train Loss = 0.440025, Test Loss = 0.252390, Learning Rate = 3.286111e-05\n",
      "Epoch 8989/20000: Train Loss = 0.439905, Test Loss = 0.251838, Learning Rate = 3.284862e-05\n",
      "Epoch 8990/20000: Train Loss = 0.439951, Test Loss = 0.249990, Learning Rate = 3.283614e-05\n",
      "Epoch 8991/20000: Train Loss = 0.440549, Test Loss = 0.255184, Learning Rate = 3.282366e-05\n",
      "Epoch 8992/20000: Train Loss = 0.439963, Test Loss = 0.247682, Learning Rate = 3.281119e-05\n",
      "Epoch 8993/20000: Train Loss = 0.440484, Test Loss = 0.245814, Learning Rate = 3.279872e-05\n",
      "Epoch 8994/20000: Train Loss = 0.440110, Test Loss = 0.252600, Learning Rate = 3.278626e-05\n",
      "Epoch 8995/20000: Train Loss = 0.441084, Test Loss = 0.250894, Learning Rate = 3.277380e-05\n",
      "Epoch 8996/20000: Train Loss = 0.440513, Test Loss = 0.251913, Learning Rate = 3.276135e-05\n",
      "Epoch 8997/20000: Train Loss = 0.439736, Test Loss = 0.246314, Learning Rate = 3.274890e-05\n",
      "Epoch 8998/20000: Train Loss = 0.440248, Test Loss = 0.247893, Learning Rate = 3.273646e-05\n",
      "Epoch 8999/20000: Train Loss = 0.440003, Test Loss = 0.248143, Learning Rate = 3.272402e-05\n",
      "Epoch 9000/20000: Train Loss = 0.440362, Test Loss = 0.247255, Learning Rate = 3.271158e-05\n",
      "Epoch 9001/20000: Train Loss = 0.440638, Test Loss = 0.247114, Learning Rate = 3.269915e-05\n",
      "Epoch 9002/20000: Train Loss = 0.439873, Test Loss = 0.249953, Learning Rate = 3.268673e-05\n",
      "Epoch 9003/20000: Train Loss = 0.440287, Test Loss = 0.247890, Learning Rate = 3.267431e-05\n",
      "Epoch 9004/20000: Train Loss = 0.439810, Test Loss = 0.249328, Learning Rate = 3.266189e-05\n",
      "Epoch 9005/20000: Train Loss = 0.439869, Test Loss = 0.250300, Learning Rate = 3.264948e-05\n",
      "Epoch 9006/20000: Train Loss = 0.439838, Test Loss = 0.249599, Learning Rate = 3.263708e-05\n",
      "Epoch 9007/20000: Train Loss = 0.440783, Test Loss = 0.254101, Learning Rate = 3.262467e-05\n",
      "Epoch 9008/20000: Train Loss = 0.439911, Test Loss = 0.244483, Learning Rate = 3.261228e-05\n",
      "Epoch 9009/20000: Train Loss = 0.439909, Test Loss = 0.248239, Learning Rate = 3.259989e-05\n",
      "Epoch 9010/20000: Train Loss = 0.440239, Test Loss = 0.248128, Learning Rate = 3.258750e-05\n",
      "Epoch 9011/20000: Train Loss = 0.440172, Test Loss = 0.250380, Learning Rate = 3.257512e-05\n",
      "Epoch 9012/20000: Train Loss = 0.440430, Test Loss = 0.249037, Learning Rate = 3.256274e-05\n",
      "Epoch 9013/20000: Train Loss = 0.440330, Test Loss = 0.253954, Learning Rate = 3.255037e-05\n",
      "Epoch 9014/20000: Train Loss = 0.440521, Test Loss = 0.248289, Learning Rate = 3.253800e-05\n",
      "Epoch 9015/20000: Train Loss = 0.439666, Test Loss = 0.258498, Learning Rate = 3.252563e-05\n",
      "Epoch 9016/20000: Train Loss = 0.439797, Test Loss = 0.252582, Learning Rate = 3.251328e-05\n",
      "Epoch 9017/20000: Train Loss = 0.440313, Test Loss = 0.260932, Learning Rate = 3.250092e-05\n",
      "Epoch 9018/20000: Train Loss = 0.440140, Test Loss = 0.248270, Learning Rate = 3.248857e-05\n",
      "Epoch 9019/20000: Train Loss = 0.439849, Test Loss = 0.246420, Learning Rate = 3.247623e-05\n",
      "Epoch 9020/20000: Train Loss = 0.439880, Test Loss = 0.248691, Learning Rate = 3.246389e-05\n",
      "Epoch 9021/20000: Train Loss = 0.440002, Test Loss = 0.255360, Learning Rate = 3.245155e-05\n",
      "Epoch 9022/20000: Train Loss = 0.440019, Test Loss = 0.246358, Learning Rate = 3.243922e-05\n",
      "Epoch 9023/20000: Train Loss = 0.440244, Test Loss = 0.241781, Learning Rate = 3.242690e-05\n",
      "Epoch 9024/20000: Train Loss = 0.440697, Test Loss = 0.249754, Learning Rate = 3.241457e-05\n",
      "Epoch 9025/20000: Train Loss = 0.439741, Test Loss = 0.249982, Learning Rate = 3.240226e-05\n",
      "Epoch 9026/20000: Train Loss = 0.440081, Test Loss = 0.247183, Learning Rate = 3.238995e-05\n",
      "Epoch 9027/20000: Train Loss = 0.440490, Test Loss = 0.253367, Learning Rate = 3.237764e-05\n",
      "Epoch 9028/20000: Train Loss = 0.440185, Test Loss = 0.247187, Learning Rate = 3.236534e-05\n",
      "Epoch 9029/20000: Train Loss = 0.440179, Test Loss = 0.252555, Learning Rate = 3.235304e-05\n",
      "Epoch 9030/20000: Train Loss = 0.439997, Test Loss = 0.245216, Learning Rate = 3.234074e-05\n",
      "Epoch 9031/20000: Train Loss = 0.439979, Test Loss = 0.248091, Learning Rate = 3.232846e-05\n",
      "Epoch 9032/20000: Train Loss = 0.439925, Test Loss = 0.250433, Learning Rate = 3.231617e-05\n",
      "Epoch 9033/20000: Train Loss = 0.440298, Test Loss = 0.252337, Learning Rate = 3.230389e-05\n",
      "Epoch 9034/20000: Train Loss = 0.440234, Test Loss = 0.251934, Learning Rate = 3.229162e-05\n",
      "Epoch 9035/20000: Train Loss = 0.440435, Test Loss = 0.251684, Learning Rate = 3.227935e-05\n",
      "Epoch 9036/20000: Train Loss = 0.440387, Test Loss = 0.257933, Learning Rate = 3.226708e-05\n",
      "Epoch 9037/20000: Train Loss = 0.440006, Test Loss = 0.254823, Learning Rate = 3.225482e-05\n",
      "Epoch 9038/20000: Train Loss = 0.439896, Test Loss = 0.258258, Learning Rate = 3.224257e-05\n",
      "Epoch 9039/20000: Train Loss = 0.439724, Test Loss = 0.248177, Learning Rate = 3.223031e-05\n",
      "Epoch 9040/20000: Train Loss = 0.440485, Test Loss = 0.248716, Learning Rate = 3.221807e-05\n",
      "Epoch 9041/20000: Train Loss = 0.440372, Test Loss = 0.255793, Learning Rate = 3.220583e-05\n",
      "Epoch 9042/20000: Train Loss = 0.440002, Test Loss = 0.245393, Learning Rate = 3.219359e-05\n",
      "Epoch 9043/20000: Train Loss = 0.439966, Test Loss = 0.247914, Learning Rate = 3.218136e-05\n",
      "Epoch 9044/20000: Train Loss = 0.439781, Test Loss = 0.247047, Learning Rate = 3.216913e-05\n",
      "Epoch 9045/20000: Train Loss = 0.440350, Test Loss = 0.245723, Learning Rate = 3.215690e-05\n",
      "Epoch 9046/20000: Train Loss = 0.439971, Test Loss = 0.244037, Learning Rate = 3.214469e-05\n",
      "Epoch 9047/20000: Train Loss = 0.439931, Test Loss = 0.251482, Learning Rate = 3.213247e-05\n",
      "Epoch 9048/20000: Train Loss = 0.440078, Test Loss = 0.245379, Learning Rate = 3.212026e-05\n",
      "Epoch 9049/20000: Train Loss = 0.440212, Test Loss = 0.258423, Learning Rate = 3.210806e-05\n",
      "Epoch 9050/20000: Train Loss = 0.439805, Test Loss = 0.249839, Learning Rate = 3.209586e-05\n",
      "Epoch 9051/20000: Train Loss = 0.439989, Test Loss = 0.252883, Learning Rate = 3.208366e-05\n",
      "Epoch 9052/20000: Train Loss = 0.439737, Test Loss = 0.246875, Learning Rate = 3.207147e-05\n",
      "Epoch 9053/20000: Train Loss = 0.439760, Test Loss = 0.252335, Learning Rate = 3.205928e-05\n",
      "Epoch 9054/20000: Train Loss = 0.439681, Test Loss = 0.251311, Learning Rate = 3.204710e-05\n",
      "Epoch 9055/20000: Train Loss = 0.440378, Test Loss = 0.250475, Learning Rate = 3.203493e-05\n",
      "Epoch 9056/20000: Train Loss = 0.439891, Test Loss = 0.259133, Learning Rate = 3.202275e-05\n",
      "Epoch 9057/20000: Train Loss = 0.440189, Test Loss = 0.252873, Learning Rate = 3.201059e-05\n",
      "Epoch 9058/20000: Train Loss = 0.439806, Test Loss = 0.245302, Learning Rate = 3.199842e-05\n",
      "Epoch 9059/20000: Train Loss = 0.440349, Test Loss = 0.250005, Learning Rate = 3.198626e-05\n",
      "Epoch 9060/20000: Train Loss = 0.440273, Test Loss = 0.249372, Learning Rate = 3.197411e-05\n",
      "Epoch 9061/20000: Train Loss = 0.440191, Test Loss = 0.251765, Learning Rate = 3.196196e-05\n",
      "Epoch 9062/20000: Train Loss = 0.440226, Test Loss = 0.248702, Learning Rate = 3.194982e-05\n",
      "Epoch 9063/20000: Train Loss = 0.440030, Test Loss = 0.252208, Learning Rate = 3.193768e-05\n",
      "Epoch 9064/20000: Train Loss = 0.439929, Test Loss = 0.250425, Learning Rate = 3.192554e-05\n",
      "Epoch 9065/20000: Train Loss = 0.440107, Test Loss = 0.246648, Learning Rate = 3.191341e-05\n",
      "Epoch 9066/20000: Train Loss = 0.440668, Test Loss = 0.256990, Learning Rate = 3.190128e-05\n",
      "Epoch 9067/20000: Train Loss = 0.440527, Test Loss = 0.253376, Learning Rate = 3.188916e-05\n",
      "Epoch 9068/20000: Train Loss = 0.439961, Test Loss = 0.255863, Learning Rate = 3.187704e-05\n",
      "Epoch 9069/20000: Train Loss = 0.440140, Test Loss = 0.247461, Learning Rate = 3.186493e-05\n",
      "Epoch 9070/20000: Train Loss = 0.440393, Test Loss = 0.254703, Learning Rate = 3.185282e-05\n",
      "Epoch 9071/20000: Train Loss = 0.440100, Test Loss = 0.255433, Learning Rate = 3.184072e-05\n",
      "Epoch 9072/20000: Train Loss = 0.439782, Test Loss = 0.250515, Learning Rate = 3.182862e-05\n",
      "Epoch 9073/20000: Train Loss = 0.439894, Test Loss = 0.251818, Learning Rate = 3.181653e-05\n",
      "Epoch 9074/20000: Train Loss = 0.440641, Test Loss = 0.245691, Learning Rate = 3.180444e-05\n",
      "Epoch 9075/20000: Train Loss = 0.440137, Test Loss = 0.245914, Learning Rate = 3.179235e-05\n",
      "Epoch 9076/20000: Train Loss = 0.439547, Test Loss = 0.254735, Learning Rate = 3.178027e-05\n",
      "Epoch 9077/20000: Train Loss = 0.439698, Test Loss = 0.246983, Learning Rate = 3.176820e-05\n",
      "Epoch 9078/20000: Train Loss = 0.439704, Test Loss = 0.252030, Learning Rate = 3.175613e-05\n",
      "Epoch 9079/20000: Train Loss = 0.439840, Test Loss = 0.254256, Learning Rate = 3.174406e-05\n",
      "Epoch 9080/20000: Train Loss = 0.440728, Test Loss = 0.249226, Learning Rate = 3.173200e-05\n",
      "Epoch 9081/20000: Train Loss = 0.440187, Test Loss = 0.255286, Learning Rate = 3.171994e-05\n",
      "Epoch 9082/20000: Train Loss = 0.440012, Test Loss = 0.250536, Learning Rate = 3.170789e-05\n",
      "Epoch 9083/20000: Train Loss = 0.440179, Test Loss = 0.252643, Learning Rate = 3.169584e-05\n",
      "Epoch 9084/20000: Train Loss = 0.440471, Test Loss = 0.248286, Learning Rate = 3.168380e-05\n",
      "Epoch 9085/20000: Train Loss = 0.439992, Test Loss = 0.252151, Learning Rate = 3.167176e-05\n",
      "Epoch 9086/20000: Train Loss = 0.439686, Test Loss = 0.248659, Learning Rate = 3.165972e-05\n",
      "Epoch 9087/20000: Train Loss = 0.440272, Test Loss = 0.255908, Learning Rate = 3.164769e-05\n",
      "Epoch 9088/20000: Train Loss = 0.439792, Test Loss = 0.252648, Learning Rate = 3.163567e-05\n",
      "Epoch 9089/20000: Train Loss = 0.440097, Test Loss = 0.250699, Learning Rate = 3.162365e-05\n",
      "Epoch 9090/20000: Train Loss = 0.439807, Test Loss = 0.247948, Learning Rate = 3.161163e-05\n",
      "Epoch 9091/20000: Train Loss = 0.440047, Test Loss = 0.251757, Learning Rate = 3.159962e-05\n",
      "Epoch 9092/20000: Train Loss = 0.439798, Test Loss = 0.250859, Learning Rate = 3.158761e-05\n",
      "Epoch 9093/20000: Train Loss = 0.440043, Test Loss = 0.257967, Learning Rate = 3.157561e-05\n",
      "Epoch 9094/20000: Train Loss = 0.439949, Test Loss = 0.254652, Learning Rate = 3.156361e-05\n",
      "Epoch 9095/20000: Train Loss = 0.439614, Test Loss = 0.249845, Learning Rate = 3.155162e-05\n",
      "Epoch 9096/20000: Train Loss = 0.439885, Test Loss = 0.251202, Learning Rate = 3.153963e-05\n",
      "Epoch 9097/20000: Train Loss = 0.440012, Test Loss = 0.254422, Learning Rate = 3.152765e-05\n",
      "Epoch 9098/20000: Train Loss = 0.440821, Test Loss = 0.253677, Learning Rate = 3.151567e-05\n",
      "Epoch 9099/20000: Train Loss = 0.439810, Test Loss = 0.244799, Learning Rate = 3.150369e-05\n",
      "Epoch 9100/20000: Train Loss = 0.439986, Test Loss = 0.252607, Learning Rate = 3.149172e-05\n",
      "Epoch 9101/20000: Train Loss = 0.440135, Test Loss = 0.252853, Learning Rate = 3.147976e-05\n",
      "Epoch 9102/20000: Train Loss = 0.440384, Test Loss = 0.251220, Learning Rate = 3.146779e-05\n",
      "Epoch 9103/20000: Train Loss = 0.440133, Test Loss = 0.251642, Learning Rate = 3.145584e-05\n",
      "Epoch 9104/20000: Train Loss = 0.440167, Test Loss = 0.250389, Learning Rate = 3.144388e-05\n",
      "Epoch 9105/20000: Train Loss = 0.439626, Test Loss = 0.250671, Learning Rate = 3.143194e-05\n",
      "Epoch 9106/20000: Train Loss = 0.440258, Test Loss = 0.240501, Learning Rate = 3.141999e-05\n",
      "Epoch 9107/20000: Train Loss = 0.440006, Test Loss = 0.250830, Learning Rate = 3.140806e-05\n",
      "Epoch 9108/20000: Train Loss = 0.439764, Test Loss = 0.245340, Learning Rate = 3.139612e-05\n",
      "Epoch 9109/20000: Train Loss = 0.439935, Test Loss = 0.244002, Learning Rate = 3.138419e-05\n",
      "Epoch 9110/20000: Train Loss = 0.440137, Test Loss = 0.238155, Learning Rate = 3.137227e-05\n",
      "Epoch 9111/20000: Train Loss = 0.440049, Test Loss = 0.246270, Learning Rate = 3.136035e-05\n",
      "Epoch 9112/20000: Train Loss = 0.440749, Test Loss = 0.248614, Learning Rate = 3.134843e-05\n",
      "Epoch 9113/20000: Train Loss = 0.440017, Test Loss = 0.251542, Learning Rate = 3.133652e-05\n",
      "Epoch 9114/20000: Train Loss = 0.440008, Test Loss = 0.249466, Learning Rate = 3.132461e-05\n",
      "Epoch 9115/20000: Train Loss = 0.440293, Test Loss = 0.248683, Learning Rate = 3.131271e-05\n",
      "Epoch 9116/20000: Train Loss = 0.440283, Test Loss = 0.253376, Learning Rate = 3.130081e-05\n",
      "Epoch 9117/20000: Train Loss = 0.439961, Test Loss = 0.245974, Learning Rate = 3.128892e-05\n",
      "Epoch 9118/20000: Train Loss = 0.440038, Test Loss = 0.248351, Learning Rate = 3.127703e-05\n",
      "Epoch 9119/20000: Train Loss = 0.440188, Test Loss = 0.248159, Learning Rate = 3.126514e-05\n",
      "Epoch 9120/20000: Train Loss = 0.440000, Test Loss = 0.249843, Learning Rate = 3.125326e-05\n",
      "Epoch 9121/20000: Train Loss = 0.440504, Test Loss = 0.243763, Learning Rate = 3.124139e-05\n",
      "Epoch 9122/20000: Train Loss = 0.440490, Test Loss = 0.247849, Learning Rate = 3.122952e-05\n",
      "Epoch 9123/20000: Train Loss = 0.441100, Test Loss = 0.251408, Learning Rate = 3.121765e-05\n",
      "Epoch 9124/20000: Train Loss = 0.440126, Test Loss = 0.250440, Learning Rate = 3.120579e-05\n",
      "Epoch 9125/20000: Train Loss = 0.440137, Test Loss = 0.253975, Learning Rate = 3.119393e-05\n",
      "Epoch 9126/20000: Train Loss = 0.440172, Test Loss = 0.248770, Learning Rate = 3.118208e-05\n",
      "Epoch 9127/20000: Train Loss = 0.440661, Test Loss = 0.251153, Learning Rate = 3.117023e-05\n",
      "Epoch 9128/20000: Train Loss = 0.439713, Test Loss = 0.253822, Learning Rate = 3.115839e-05\n",
      "Epoch 9129/20000: Train Loss = 0.439900, Test Loss = 0.252155, Learning Rate = 3.114655e-05\n",
      "Epoch 9130/20000: Train Loss = 0.440019, Test Loss = 0.252085, Learning Rate = 3.113471e-05\n",
      "Epoch 9131/20000: Train Loss = 0.440160, Test Loss = 0.251730, Learning Rate = 3.112288e-05\n",
      "Epoch 9132/20000: Train Loss = 0.439917, Test Loss = 0.250263, Learning Rate = 3.111106e-05\n",
      "Epoch 9133/20000: Train Loss = 0.440160, Test Loss = 0.250667, Learning Rate = 3.109923e-05\n",
      "Epoch 9134/20000: Train Loss = 0.440143, Test Loss = 0.248621, Learning Rate = 3.108742e-05\n",
      "Epoch 9135/20000: Train Loss = 0.439897, Test Loss = 0.246231, Learning Rate = 3.107561e-05\n",
      "Epoch 9136/20000: Train Loss = 0.440205, Test Loss = 0.248157, Learning Rate = 3.106380e-05\n",
      "Epoch 9137/20000: Train Loss = 0.440031, Test Loss = 0.246992, Learning Rate = 3.105199e-05\n",
      "Epoch 9138/20000: Train Loss = 0.439854, Test Loss = 0.246678, Learning Rate = 3.104020e-05\n",
      "Epoch 9139/20000: Train Loss = 0.440149, Test Loss = 0.250879, Learning Rate = 3.102840e-05\n",
      "Epoch 9140/20000: Train Loss = 0.439837, Test Loss = 0.248141, Learning Rate = 3.101661e-05\n",
      "Epoch 9141/20000: Train Loss = 0.439979, Test Loss = 0.253546, Learning Rate = 3.100483e-05\n",
      "Epoch 9142/20000: Train Loss = 0.439534, Test Loss = 0.246573, Learning Rate = 3.099304e-05\n",
      "Epoch 9143/20000: Train Loss = 0.440227, Test Loss = 0.252709, Learning Rate = 3.098127e-05\n",
      "Epoch 9144/20000: Train Loss = 0.439831, Test Loss = 0.249627, Learning Rate = 3.096950e-05\n",
      "Epoch 9145/20000: Train Loss = 0.439931, Test Loss = 0.244520, Learning Rate = 3.095773e-05\n",
      "Epoch 9146/20000: Train Loss = 0.440257, Test Loss = 0.246260, Learning Rate = 3.094597e-05\n",
      "Epoch 9147/20000: Train Loss = 0.440135, Test Loss = 0.253810, Learning Rate = 3.093421e-05\n",
      "Epoch 9148/20000: Train Loss = 0.440033, Test Loss = 0.252073, Learning Rate = 3.092245e-05\n",
      "Epoch 9149/20000: Train Loss = 0.440097, Test Loss = 0.248016, Learning Rate = 3.091070e-05\n",
      "Epoch 9150/20000: Train Loss = 0.440299, Test Loss = 0.250623, Learning Rate = 3.089896e-05\n",
      "Epoch 9151/20000: Train Loss = 0.440345, Test Loss = 0.252292, Learning Rate = 3.088722e-05\n",
      "Epoch 9152/20000: Train Loss = 0.440625, Test Loss = 0.250162, Learning Rate = 3.087548e-05\n",
      "Epoch 9153/20000: Train Loss = 0.440260, Test Loss = 0.249002, Learning Rate = 3.086375e-05\n",
      "Epoch 9154/20000: Train Loss = 0.440131, Test Loss = 0.249805, Learning Rate = 3.085202e-05\n",
      "Epoch 9155/20000: Train Loss = 0.440043, Test Loss = 0.248758, Learning Rate = 3.084030e-05\n",
      "Epoch 9156/20000: Train Loss = 0.439922, Test Loss = 0.251846, Learning Rate = 3.082858e-05\n",
      "Epoch 9157/20000: Train Loss = 0.440224, Test Loss = 0.251654, Learning Rate = 3.081687e-05\n",
      "Epoch 9158/20000: Train Loss = 0.440864, Test Loss = 0.252985, Learning Rate = 3.080516e-05\n",
      "Epoch 9159/20000: Train Loss = 0.439821, Test Loss = 0.252658, Learning Rate = 3.079345e-05\n",
      "Epoch 9160/20000: Train Loss = 0.440257, Test Loss = 0.250454, Learning Rate = 3.078175e-05\n",
      "Epoch 9161/20000: Train Loss = 0.440166, Test Loss = 0.250013, Learning Rate = 3.077005e-05\n",
      "Epoch 9162/20000: Train Loss = 0.439761, Test Loss = 0.252176, Learning Rate = 3.075836e-05\n",
      "Epoch 9163/20000: Train Loss = 0.440404, Test Loss = 0.249327, Learning Rate = 3.074668e-05\n",
      "Epoch 9164/20000: Train Loss = 0.440545, Test Loss = 0.244094, Learning Rate = 3.073499e-05\n",
      "Epoch 9165/20000: Train Loss = 0.440096, Test Loss = 0.250924, Learning Rate = 3.072331e-05\n",
      "Epoch 9166/20000: Train Loss = 0.440013, Test Loss = 0.252106, Learning Rate = 3.071164e-05\n",
      "Epoch 9167/20000: Train Loss = 0.440192, Test Loss = 0.243076, Learning Rate = 3.069997e-05\n",
      "Epoch 9168/20000: Train Loss = 0.439939, Test Loss = 0.251039, Learning Rate = 3.068830e-05\n",
      "Epoch 9169/20000: Train Loss = 0.439610, Test Loss = 0.246535, Learning Rate = 3.067664e-05\n",
      "Epoch 9170/20000: Train Loss = 0.439615, Test Loss = 0.249557, Learning Rate = 3.066499e-05\n",
      "Epoch 9171/20000: Train Loss = 0.440003, Test Loss = 0.252910, Learning Rate = 3.065334e-05\n",
      "Epoch 9172/20000: Train Loss = 0.440088, Test Loss = 0.244496, Learning Rate = 3.064169e-05\n",
      "Epoch 9173/20000: Train Loss = 0.439814, Test Loss = 0.248056, Learning Rate = 3.063005e-05\n",
      "Epoch 9174/20000: Train Loss = 0.440069, Test Loss = 0.246601, Learning Rate = 3.061841e-05\n",
      "Epoch 9175/20000: Train Loss = 0.439864, Test Loss = 0.250955, Learning Rate = 3.060677e-05\n",
      "Epoch 9176/20000: Train Loss = 0.439850, Test Loss = 0.248413, Learning Rate = 3.059514e-05\n",
      "Epoch 9177/20000: Train Loss = 0.439727, Test Loss = 0.246511, Learning Rate = 3.058352e-05\n",
      "Epoch 9178/20000: Train Loss = 0.440019, Test Loss = 0.251215, Learning Rate = 3.057190e-05\n",
      "Epoch 9179/20000: Train Loss = 0.440281, Test Loss = 0.249184, Learning Rate = 3.056028e-05\n",
      "Epoch 9180/20000: Train Loss = 0.439993, Test Loss = 0.247542, Learning Rate = 3.054867e-05\n",
      "Epoch 9181/20000: Train Loss = 0.439994, Test Loss = 0.251142, Learning Rate = 3.053706e-05\n",
      "Epoch 9182/20000: Train Loss = 0.440833, Test Loss = 0.253251, Learning Rate = 3.052546e-05\n",
      "Epoch 9183/20000: Train Loss = 0.439991, Test Loss = 0.254624, Learning Rate = 3.051386e-05\n",
      "Epoch 9184/20000: Train Loss = 0.440747, Test Loss = 0.243172, Learning Rate = 3.050226e-05\n",
      "Epoch 9185/20000: Train Loss = 0.440727, Test Loss = 0.255949, Learning Rate = 3.049067e-05\n",
      "Epoch 9186/20000: Train Loss = 0.440105, Test Loss = 0.250239, Learning Rate = 3.047909e-05\n",
      "Epoch 9187/20000: Train Loss = 0.440517, Test Loss = 0.253418, Learning Rate = 3.046751e-05\n",
      "Epoch 9188/20000: Train Loss = 0.440058, Test Loss = 0.250111, Learning Rate = 3.045593e-05\n",
      "Epoch 9189/20000: Train Loss = 0.439878, Test Loss = 0.244713, Learning Rate = 3.044436e-05\n",
      "Epoch 9190/20000: Train Loss = 0.439698, Test Loss = 0.250107, Learning Rate = 3.043279e-05\n",
      "Epoch 9191/20000: Train Loss = 0.439865, Test Loss = 0.246211, Learning Rate = 3.042123e-05\n",
      "Epoch 9192/20000: Train Loss = 0.440103, Test Loss = 0.247774, Learning Rate = 3.040967e-05\n",
      "Epoch 9193/20000: Train Loss = 0.440036, Test Loss = 0.250130, Learning Rate = 3.039811e-05\n",
      "Epoch 9194/20000: Train Loss = 0.440132, Test Loss = 0.243539, Learning Rate = 3.038656e-05\n",
      "Epoch 9195/20000: Train Loss = 0.439727, Test Loss = 0.246053, Learning Rate = 3.037502e-05\n",
      "Epoch 9196/20000: Train Loss = 0.439591, Test Loss = 0.250445, Learning Rate = 3.036347e-05\n",
      "Epoch 9197/20000: Train Loss = 0.440443, Test Loss = 0.242481, Learning Rate = 3.035194e-05\n",
      "Epoch 9198/20000: Train Loss = 0.440123, Test Loss = 0.246742, Learning Rate = 3.034040e-05\n",
      "Epoch 9199/20000: Train Loss = 0.440094, Test Loss = 0.243277, Learning Rate = 3.032888e-05\n",
      "Epoch 9200/20000: Train Loss = 0.440019, Test Loss = 0.247035, Learning Rate = 3.031735e-05\n",
      "Epoch 9201/20000: Train Loss = 0.439983, Test Loss = 0.247901, Learning Rate = 3.030583e-05\n",
      "Epoch 9202/20000: Train Loss = 0.440153, Test Loss = 0.251453, Learning Rate = 3.029432e-05\n",
      "Epoch 9203/20000: Train Loss = 0.439626, Test Loss = 0.250447, Learning Rate = 3.028280e-05\n",
      "Epoch 9204/20000: Train Loss = 0.440070, Test Loss = 0.249395, Learning Rate = 3.027130e-05\n",
      "Epoch 9205/20000: Train Loss = 0.440111, Test Loss = 0.243578, Learning Rate = 3.025980e-05\n",
      "Epoch 9206/20000: Train Loss = 0.439819, Test Loss = 0.246700, Learning Rate = 3.024830e-05\n",
      "Epoch 9207/20000: Train Loss = 0.439852, Test Loss = 0.242317, Learning Rate = 3.023680e-05\n",
      "Epoch 9208/20000: Train Loss = 0.439784, Test Loss = 0.242805, Learning Rate = 3.022532e-05\n",
      "Epoch 9209/20000: Train Loss = 0.439867, Test Loss = 0.243994, Learning Rate = 3.021383e-05\n",
      "Epoch 9210/20000: Train Loss = 0.439988, Test Loss = 0.238393, Learning Rate = 3.020235e-05\n",
      "Epoch 9211/20000: Train Loss = 0.441057, Test Loss = 0.240517, Learning Rate = 3.019087e-05\n",
      "Epoch 9212/20000: Train Loss = 0.439960, Test Loss = 0.240058, Learning Rate = 3.017940e-05\n",
      "Epoch 9213/20000: Train Loss = 0.440327, Test Loss = 0.244941, Learning Rate = 3.016794e-05\n",
      "Epoch 9214/20000: Train Loss = 0.440081, Test Loss = 0.240839, Learning Rate = 3.015647e-05\n",
      "Epoch 9215/20000: Train Loss = 0.439930, Test Loss = 0.247699, Learning Rate = 3.014501e-05\n",
      "Epoch 9216/20000: Train Loss = 0.439873, Test Loss = 0.246979, Learning Rate = 3.013356e-05\n",
      "Epoch 9217/20000: Train Loss = 0.440302, Test Loss = 0.244288, Learning Rate = 3.012211e-05\n",
      "Epoch 9218/20000: Train Loss = 0.439595, Test Loss = 0.246617, Learning Rate = 3.011066e-05\n",
      "Epoch 9219/20000: Train Loss = 0.439993, Test Loss = 0.247134, Learning Rate = 3.009922e-05\n",
      "Epoch 9220/20000: Train Loss = 0.441474, Test Loss = 0.243951, Learning Rate = 3.008779e-05\n",
      "Epoch 9221/20000: Train Loss = 0.440220, Test Loss = 0.249042, Learning Rate = 3.007635e-05\n",
      "Epoch 9222/20000: Train Loss = 0.440201, Test Loss = 0.251575, Learning Rate = 3.006492e-05\n",
      "Epoch 9223/20000: Train Loss = 0.440533, Test Loss = 0.245345, Learning Rate = 3.005350e-05\n",
      "Epoch 9224/20000: Train Loss = 0.439934, Test Loss = 0.248748, Learning Rate = 3.004208e-05\n",
      "Epoch 9225/20000: Train Loss = 0.441383, Test Loss = 0.255189, Learning Rate = 3.003067e-05\n",
      "Epoch 9226/20000: Train Loss = 0.440263, Test Loss = 0.244162, Learning Rate = 3.001926e-05\n",
      "Epoch 9227/20000: Train Loss = 0.439984, Test Loss = 0.247418, Learning Rate = 3.000785e-05\n",
      "Epoch 9228/20000: Train Loss = 0.440302, Test Loss = 0.246930, Learning Rate = 2.999645e-05\n",
      "Epoch 9229/20000: Train Loss = 0.440318, Test Loss = 0.254368, Learning Rate = 2.998505e-05\n",
      "Epoch 9230/20000: Train Loss = 0.439837, Test Loss = 0.249075, Learning Rate = 2.997366e-05\n",
      "Epoch 9231/20000: Train Loss = 0.439745, Test Loss = 0.249232, Learning Rate = 2.996227e-05\n",
      "Epoch 9232/20000: Train Loss = 0.440438, Test Loss = 0.244767, Learning Rate = 2.995088e-05\n",
      "Epoch 9233/20000: Train Loss = 0.439783, Test Loss = 0.251869, Learning Rate = 2.993950e-05\n",
      "Epoch 9234/20000: Train Loss = 0.439968, Test Loss = 0.249634, Learning Rate = 2.992812e-05\n",
      "Epoch 9235/20000: Train Loss = 0.440086, Test Loss = 0.249358, Learning Rate = 2.991675e-05\n",
      "Epoch 9236/20000: Train Loss = 0.439710, Test Loss = 0.246786, Learning Rate = 2.990539e-05\n",
      "Epoch 9237/20000: Train Loss = 0.440298, Test Loss = 0.251959, Learning Rate = 2.989402e-05\n",
      "Epoch 9238/20000: Train Loss = 0.439888, Test Loss = 0.247517, Learning Rate = 2.988266e-05\n",
      "Epoch 9239/20000: Train Loss = 0.439921, Test Loss = 0.244870, Learning Rate = 2.987131e-05\n",
      "Epoch 9240/20000: Train Loss = 0.439792, Test Loss = 0.249996, Learning Rate = 2.985996e-05\n",
      "Epoch 9241/20000: Train Loss = 0.440121, Test Loss = 0.250061, Learning Rate = 2.984861e-05\n",
      "Epoch 9242/20000: Train Loss = 0.440169, Test Loss = 0.248842, Learning Rate = 2.983727e-05\n",
      "Epoch 9243/20000: Train Loss = 0.440179, Test Loss = 0.249089, Learning Rate = 2.982593e-05\n",
      "Epoch 9244/20000: Train Loss = 0.439914, Test Loss = 0.253748, Learning Rate = 2.981460e-05\n",
      "Epoch 9245/20000: Train Loss = 0.440333, Test Loss = 0.248916, Learning Rate = 2.980327e-05\n",
      "Epoch 9246/20000: Train Loss = 0.439977, Test Loss = 0.250061, Learning Rate = 2.979195e-05\n",
      "Epoch 9247/20000: Train Loss = 0.440007, Test Loss = 0.245392, Learning Rate = 2.978063e-05\n",
      "Epoch 9248/20000: Train Loss = 0.440584, Test Loss = 0.242021, Learning Rate = 2.976931e-05\n",
      "Epoch 9249/20000: Train Loss = 0.439846, Test Loss = 0.254380, Learning Rate = 2.975800e-05\n",
      "Epoch 9250/20000: Train Loss = 0.440066, Test Loss = 0.252914, Learning Rate = 2.974669e-05\n",
      "Epoch 9251/20000: Train Loss = 0.439870, Test Loss = 0.256795, Learning Rate = 2.973539e-05\n",
      "Epoch 9252/20000: Train Loss = 0.440156, Test Loss = 0.258156, Learning Rate = 2.972409e-05\n",
      "Epoch 9253/20000: Train Loss = 0.439783, Test Loss = 0.255312, Learning Rate = 2.971280e-05\n",
      "Epoch 9254/20000: Train Loss = 0.439732, Test Loss = 0.249896, Learning Rate = 2.970151e-05\n",
      "Epoch 9255/20000: Train Loss = 0.440020, Test Loss = 0.247343, Learning Rate = 2.969022e-05\n",
      "Epoch 9256/20000: Train Loss = 0.439985, Test Loss = 0.245450, Learning Rate = 2.967894e-05\n",
      "Epoch 9257/20000: Train Loss = 0.439861, Test Loss = 0.246943, Learning Rate = 2.966766e-05\n",
      "Epoch 9258/20000: Train Loss = 0.439879, Test Loss = 0.250380, Learning Rate = 2.965639e-05\n",
      "Epoch 9259/20000: Train Loss = 0.439983, Test Loss = 0.243296, Learning Rate = 2.964512e-05\n",
      "Epoch 9260/20000: Train Loss = 0.439685, Test Loss = 0.247633, Learning Rate = 2.963386e-05\n",
      "Epoch 9261/20000: Train Loss = 0.439786, Test Loss = 0.254820, Learning Rate = 2.962260e-05\n",
      "Epoch 9262/20000: Train Loss = 0.440213, Test Loss = 0.251056, Learning Rate = 2.961134e-05\n",
      "Epoch 9263/20000: Train Loss = 0.440007, Test Loss = 0.253179, Learning Rate = 2.960009e-05\n",
      "Epoch 9264/20000: Train Loss = 0.439666, Test Loss = 0.250071, Learning Rate = 2.958884e-05\n",
      "Epoch 9265/20000: Train Loss = 0.440004, Test Loss = 0.256078, Learning Rate = 2.957760e-05\n",
      "Epoch 9266/20000: Train Loss = 0.440330, Test Loss = 0.252194, Learning Rate = 2.956636e-05\n",
      "Epoch 9267/20000: Train Loss = 0.441272, Test Loss = 0.253467, Learning Rate = 2.955513e-05\n",
      "Epoch 9268/20000: Train Loss = 0.440370, Test Loss = 0.248342, Learning Rate = 2.954390e-05\n",
      "Epoch 9269/20000: Train Loss = 0.439705, Test Loss = 0.253213, Learning Rate = 2.953267e-05\n",
      "Epoch 9270/20000: Train Loss = 0.439736, Test Loss = 0.256034, Learning Rate = 2.952145e-05\n",
      "Epoch 9271/20000: Train Loss = 0.439925, Test Loss = 0.247815, Learning Rate = 2.951023e-05\n",
      "Epoch 9272/20000: Train Loss = 0.439853, Test Loss = 0.244030, Learning Rate = 2.949902e-05\n",
      "Epoch 9273/20000: Train Loss = 0.439890, Test Loss = 0.251682, Learning Rate = 2.948781e-05\n",
      "Epoch 9274/20000: Train Loss = 0.439772, Test Loss = 0.250067, Learning Rate = 2.947660e-05\n",
      "Epoch 9275/20000: Train Loss = 0.439905, Test Loss = 0.248412, Learning Rate = 2.946540e-05\n",
      "Epoch 9276/20000: Train Loss = 0.439721, Test Loss = 0.247595, Learning Rate = 2.945421e-05\n",
      "Epoch 9277/20000: Train Loss = 0.439690, Test Loss = 0.244731, Learning Rate = 2.944302e-05\n",
      "Epoch 9278/20000: Train Loss = 0.439968, Test Loss = 0.247395, Learning Rate = 2.943183e-05\n",
      "Epoch 9279/20000: Train Loss = 0.439829, Test Loss = 0.254398, Learning Rate = 2.942064e-05\n",
      "Epoch 9280/20000: Train Loss = 0.439955, Test Loss = 0.248316, Learning Rate = 2.940947e-05\n",
      "Epoch 9281/20000: Train Loss = 0.439825, Test Loss = 0.245118, Learning Rate = 2.939829e-05\n",
      "Epoch 9282/20000: Train Loss = 0.440047, Test Loss = 0.252177, Learning Rate = 2.938712e-05\n",
      "Epoch 9283/20000: Train Loss = 0.439754, Test Loss = 0.248166, Learning Rate = 2.937595e-05\n",
      "Epoch 9284/20000: Train Loss = 0.439870, Test Loss = 0.246187, Learning Rate = 2.936479e-05\n",
      "Epoch 9285/20000: Train Loss = 0.440106, Test Loss = 0.251971, Learning Rate = 2.935363e-05\n",
      "Epoch 9286/20000: Train Loss = 0.439862, Test Loss = 0.250078, Learning Rate = 2.934248e-05\n",
      "Epoch 9287/20000: Train Loss = 0.439928, Test Loss = 0.252897, Learning Rate = 2.933133e-05\n",
      "Epoch 9288/20000: Train Loss = 0.439807, Test Loss = 0.247863, Learning Rate = 2.932019e-05\n",
      "Epoch 9289/20000: Train Loss = 0.439761, Test Loss = 0.251307, Learning Rate = 2.930905e-05\n",
      "Epoch 9290/20000: Train Loss = 0.440115, Test Loss = 0.254721, Learning Rate = 2.929791e-05\n",
      "Epoch 9291/20000: Train Loss = 0.439859, Test Loss = 0.250834, Learning Rate = 2.928678e-05\n",
      "Epoch 9292/20000: Train Loss = 0.440553, Test Loss = 0.254582, Learning Rate = 2.927565e-05\n",
      "Epoch 9293/20000: Train Loss = 0.440295, Test Loss = 0.251004, Learning Rate = 2.926452e-05\n",
      "Epoch 9294/20000: Train Loss = 0.439586, Test Loss = 0.249592, Learning Rate = 2.925340e-05\n",
      "Epoch 9295/20000: Train Loss = 0.439705, Test Loss = 0.247097, Learning Rate = 2.924229e-05\n",
      "Epoch 9296/20000: Train Loss = 0.440038, Test Loss = 0.248881, Learning Rate = 2.923118e-05\n",
      "Epoch 9297/20000: Train Loss = 0.439778, Test Loss = 0.247951, Learning Rate = 2.922007e-05\n",
      "Epoch 9298/20000: Train Loss = 0.439793, Test Loss = 0.243989, Learning Rate = 2.920897e-05\n",
      "Epoch 9299/20000: Train Loss = 0.440056, Test Loss = 0.253480, Learning Rate = 2.919787e-05\n",
      "Epoch 9300/20000: Train Loss = 0.439955, Test Loss = 0.248330, Learning Rate = 2.918677e-05\n",
      "Epoch 9301/20000: Train Loss = 0.439782, Test Loss = 0.249705, Learning Rate = 2.917568e-05\n",
      "Epoch 9302/20000: Train Loss = 0.439917, Test Loss = 0.251779, Learning Rate = 2.916460e-05\n",
      "Epoch 9303/20000: Train Loss = 0.440232, Test Loss = 0.254618, Learning Rate = 2.915352e-05\n",
      "Epoch 9304/20000: Train Loss = 0.439732, Test Loss = 0.250269, Learning Rate = 2.914244e-05\n",
      "Epoch 9305/20000: Train Loss = 0.439821, Test Loss = 0.248850, Learning Rate = 2.913137e-05\n",
      "Epoch 9306/20000: Train Loss = 0.440225, Test Loss = 0.247623, Learning Rate = 2.912030e-05\n",
      "Epoch 9307/20000: Train Loss = 0.439795, Test Loss = 0.247216, Learning Rate = 2.910923e-05\n",
      "Epoch 9308/20000: Train Loss = 0.439912, Test Loss = 0.242918, Learning Rate = 2.909817e-05\n",
      "Epoch 9309/20000: Train Loss = 0.440144, Test Loss = 0.243790, Learning Rate = 2.908711e-05\n",
      "Epoch 9310/20000: Train Loss = 0.439917, Test Loss = 0.252885, Learning Rate = 2.907606e-05\n",
      "Epoch 9311/20000: Train Loss = 0.439754, Test Loss = 0.256270, Learning Rate = 2.906501e-05\n",
      "Epoch 9312/20000: Train Loss = 0.439796, Test Loss = 0.255261, Learning Rate = 2.905397e-05\n",
      "Epoch 9313/20000: Train Loss = 0.439844, Test Loss = 0.243266, Learning Rate = 2.904293e-05\n",
      "Epoch 9314/20000: Train Loss = 0.439814, Test Loss = 0.252675, Learning Rate = 2.903189e-05\n",
      "Epoch 9315/20000: Train Loss = 0.439738, Test Loss = 0.248324, Learning Rate = 2.902086e-05\n",
      "Epoch 9316/20000: Train Loss = 0.439902, Test Loss = 0.245365, Learning Rate = 2.900984e-05\n",
      "Epoch 9317/20000: Train Loss = 0.439543, Test Loss = 0.251214, Learning Rate = 2.899881e-05\n",
      "Epoch 9318/20000: Train Loss = 0.439900, Test Loss = 0.254804, Learning Rate = 2.898779e-05\n",
      "Epoch 9319/20000: Train Loss = 0.439827, Test Loss = 0.246509, Learning Rate = 2.897678e-05\n",
      "Epoch 9320/20000: Train Loss = 0.440001, Test Loss = 0.246756, Learning Rate = 2.896577e-05\n",
      "Epoch 9321/20000: Train Loss = 0.440149, Test Loss = 0.244135, Learning Rate = 2.895476e-05\n",
      "Epoch 9322/20000: Train Loss = 0.439648, Test Loss = 0.246937, Learning Rate = 2.894376e-05\n",
      "Epoch 9323/20000: Train Loss = 0.439797, Test Loss = 0.247241, Learning Rate = 2.893276e-05\n",
      "Epoch 9324/20000: Train Loss = 0.440595, Test Loss = 0.246706, Learning Rate = 2.892177e-05\n",
      "Epoch 9325/20000: Train Loss = 0.439980, Test Loss = 0.250345, Learning Rate = 2.891078e-05\n",
      "Epoch 9326/20000: Train Loss = 0.439809, Test Loss = 0.253433, Learning Rate = 2.889980e-05\n",
      "Epoch 9327/20000: Train Loss = 0.439859, Test Loss = 0.249825, Learning Rate = 2.888881e-05\n",
      "Epoch 9328/20000: Train Loss = 0.440154, Test Loss = 0.251981, Learning Rate = 2.887784e-05\n",
      "Epoch 9329/20000: Train Loss = 0.439801, Test Loss = 0.251007, Learning Rate = 2.886686e-05\n",
      "Epoch 9330/20000: Train Loss = 0.440259, Test Loss = 0.250027, Learning Rate = 2.885590e-05\n",
      "Epoch 9331/20000: Train Loss = 0.439894, Test Loss = 0.251169, Learning Rate = 2.884493e-05\n",
      "Epoch 9332/20000: Train Loss = 0.440072, Test Loss = 0.250495, Learning Rate = 2.883397e-05\n",
      "Epoch 9333/20000: Train Loss = 0.439737, Test Loss = 0.249374, Learning Rate = 2.882301e-05\n",
      "Epoch 9334/20000: Train Loss = 0.439862, Test Loss = 0.252518, Learning Rate = 2.881206e-05\n",
      "Epoch 9335/20000: Train Loss = 0.439660, Test Loss = 0.250816, Learning Rate = 2.880111e-05\n",
      "Epoch 9336/20000: Train Loss = 0.439924, Test Loss = 0.252599, Learning Rate = 2.879017e-05\n",
      "Epoch 9337/20000: Train Loss = 0.439822, Test Loss = 0.248697, Learning Rate = 2.877923e-05\n",
      "Epoch 9338/20000: Train Loss = 0.439848, Test Loss = 0.249716, Learning Rate = 2.876830e-05\n",
      "Epoch 9339/20000: Train Loss = 0.439684, Test Loss = 0.249536, Learning Rate = 2.875737e-05\n",
      "Epoch 9340/20000: Train Loss = 0.439975, Test Loss = 0.251009, Learning Rate = 2.874644e-05\n",
      "Epoch 9341/20000: Train Loss = 0.439634, Test Loss = 0.249597, Learning Rate = 2.873552e-05\n",
      "Epoch 9342/20000: Train Loss = 0.440006, Test Loss = 0.251007, Learning Rate = 2.872460e-05\n",
      "Epoch 9343/20000: Train Loss = 0.439727, Test Loss = 0.252491, Learning Rate = 2.871368e-05\n",
      "Epoch 9344/20000: Train Loss = 0.440171, Test Loss = 0.250476, Learning Rate = 2.870277e-05\n",
      "Epoch 9345/20000: Train Loss = 0.440086, Test Loss = 0.257503, Learning Rate = 2.869187e-05\n",
      "Epoch 9346/20000: Train Loss = 0.439873, Test Loss = 0.246558, Learning Rate = 2.868096e-05\n",
      "Epoch 9347/20000: Train Loss = 0.439632, Test Loss = 0.250153, Learning Rate = 2.867007e-05\n",
      "Epoch 9348/20000: Train Loss = 0.439943, Test Loss = 0.248581, Learning Rate = 2.865917e-05\n",
      "Epoch 9349/20000: Train Loss = 0.440296, Test Loss = 0.244372, Learning Rate = 2.864828e-05\n",
      "Epoch 9350/20000: Train Loss = 0.439865, Test Loss = 0.251386, Learning Rate = 2.863740e-05\n",
      "Epoch 9351/20000: Train Loss = 0.440507, Test Loss = 0.254789, Learning Rate = 2.862651e-05\n",
      "Epoch 9352/20000: Train Loss = 0.440528, Test Loss = 0.243487, Learning Rate = 2.861564e-05\n",
      "Epoch 9353/20000: Train Loss = 0.440130, Test Loss = 0.248349, Learning Rate = 2.860476e-05\n",
      "Epoch 9354/20000: Train Loss = 0.439643, Test Loss = 0.253892, Learning Rate = 2.859390e-05\n",
      "Epoch 9355/20000: Train Loss = 0.440013, Test Loss = 0.246466, Learning Rate = 2.858303e-05\n",
      "Epoch 9356/20000: Train Loss = 0.440772, Test Loss = 0.253013, Learning Rate = 2.857217e-05\n",
      "Epoch 9357/20000: Train Loss = 0.439708, Test Loss = 0.247383, Learning Rate = 2.856131e-05\n",
      "Epoch 9358/20000: Train Loss = 0.440030, Test Loss = 0.252124, Learning Rate = 2.855046e-05\n",
      "Epoch 9359/20000: Train Loss = 0.439854, Test Loss = 0.251312, Learning Rate = 2.853961e-05\n",
      "Epoch 9360/20000: Train Loss = 0.439960, Test Loss = 0.249851, Learning Rate = 2.852877e-05\n",
      "Epoch 9361/20000: Train Loss = 0.440020, Test Loss = 0.244458, Learning Rate = 2.851793e-05\n",
      "Epoch 9362/20000: Train Loss = 0.440551, Test Loss = 0.247262, Learning Rate = 2.850709e-05\n",
      "Epoch 9363/20000: Train Loss = 0.440548, Test Loss = 0.250675, Learning Rate = 2.849626e-05\n",
      "Epoch 9364/20000: Train Loss = 0.440069, Test Loss = 0.245992, Learning Rate = 2.848543e-05\n",
      "Epoch 9365/20000: Train Loss = 0.439816, Test Loss = 0.247323, Learning Rate = 2.847461e-05\n",
      "Epoch 9366/20000: Train Loss = 0.439726, Test Loss = 0.248127, Learning Rate = 2.846379e-05\n",
      "Epoch 9367/20000: Train Loss = 0.439887, Test Loss = 0.249224, Learning Rate = 2.845297e-05\n",
      "Epoch 9368/20000: Train Loss = 0.439697, Test Loss = 0.248243, Learning Rate = 2.844216e-05\n",
      "Epoch 9369/20000: Train Loss = 0.439748, Test Loss = 0.250686, Learning Rate = 2.843135e-05\n",
      "Epoch 9370/20000: Train Loss = 0.439831, Test Loss = 0.248094, Learning Rate = 2.842055e-05\n",
      "Epoch 9371/20000: Train Loss = 0.439934, Test Loss = 0.249598, Learning Rate = 2.840975e-05\n",
      "Epoch 9372/20000: Train Loss = 0.440741, Test Loss = 0.246104, Learning Rate = 2.839896e-05\n",
      "Epoch 9373/20000: Train Loss = 0.440231, Test Loss = 0.248014, Learning Rate = 2.838817e-05\n",
      "Epoch 9374/20000: Train Loss = 0.439921, Test Loss = 0.244587, Learning Rate = 2.837738e-05\n",
      "Epoch 9375/20000: Train Loss = 0.439658, Test Loss = 0.251957, Learning Rate = 2.836660e-05\n",
      "Epoch 9376/20000: Train Loss = 0.439716, Test Loss = 0.246925, Learning Rate = 2.835582e-05\n",
      "Epoch 9377/20000: Train Loss = 0.440000, Test Loss = 0.246611, Learning Rate = 2.834504e-05\n",
      "Epoch 9378/20000: Train Loss = 0.439625, Test Loss = 0.242772, Learning Rate = 2.833427e-05\n",
      "Epoch 9379/20000: Train Loss = 0.439935, Test Loss = 0.245900, Learning Rate = 2.832351e-05\n",
      "Epoch 9380/20000: Train Loss = 0.440005, Test Loss = 0.248536, Learning Rate = 2.831275e-05\n",
      "Epoch 9381/20000: Train Loss = 0.439810, Test Loss = 0.249386, Learning Rate = 2.830199e-05\n",
      "Epoch 9382/20000: Train Loss = 0.439865, Test Loss = 0.253046, Learning Rate = 2.829123e-05\n",
      "Epoch 9383/20000: Train Loss = 0.439930, Test Loss = 0.246257, Learning Rate = 2.828048e-05\n",
      "Epoch 9384/20000: Train Loss = 0.439971, Test Loss = 0.248147, Learning Rate = 2.826974e-05\n",
      "Epoch 9385/20000: Train Loss = 0.440307, Test Loss = 0.249872, Learning Rate = 2.825900e-05\n",
      "Epoch 9386/20000: Train Loss = 0.439752, Test Loss = 0.250500, Learning Rate = 2.824826e-05\n",
      "Epoch 9387/20000: Train Loss = 0.440164, Test Loss = 0.252893, Learning Rate = 2.823752e-05\n",
      "Epoch 9388/20000: Train Loss = 0.440106, Test Loss = 0.250224, Learning Rate = 2.822680e-05\n",
      "Epoch 9389/20000: Train Loss = 0.439693, Test Loss = 0.251921, Learning Rate = 2.821607e-05\n",
      "Epoch 9390/20000: Train Loss = 0.439805, Test Loss = 0.242293, Learning Rate = 2.820535e-05\n",
      "Epoch 9391/20000: Train Loss = 0.440311, Test Loss = 0.245039, Learning Rate = 2.819463e-05\n",
      "Epoch 9392/20000: Train Loss = 0.440074, Test Loss = 0.249057, Learning Rate = 2.818392e-05\n",
      "Epoch 9393/20000: Train Loss = 0.440566, Test Loss = 0.251824, Learning Rate = 2.817321e-05\n",
      "Epoch 9394/20000: Train Loss = 0.440440, Test Loss = 0.243423, Learning Rate = 2.816250e-05\n",
      "Epoch 9395/20000: Train Loss = 0.439978, Test Loss = 0.251279, Learning Rate = 2.815180e-05\n",
      "Epoch 9396/20000: Train Loss = 0.439874, Test Loss = 0.249670, Learning Rate = 2.814111e-05\n",
      "Epoch 9397/20000: Train Loss = 0.440351, Test Loss = 0.246056, Learning Rate = 2.813041e-05\n",
      "Epoch 9398/20000: Train Loss = 0.440019, Test Loss = 0.252201, Learning Rate = 2.811972e-05\n",
      "Epoch 9399/20000: Train Loss = 0.439733, Test Loss = 0.247833, Learning Rate = 2.810904e-05\n",
      "Epoch 9400/20000: Train Loss = 0.439551, Test Loss = 0.248583, Learning Rate = 2.809836e-05\n",
      "Epoch 9401/20000: Train Loss = 0.439946, Test Loss = 0.254625, Learning Rate = 2.808768e-05\n",
      "Epoch 9402/20000: Train Loss = 0.439798, Test Loss = 0.248251, Learning Rate = 2.807701e-05\n",
      "Epoch 9403/20000: Train Loss = 0.439939, Test Loss = 0.253104, Learning Rate = 2.806634e-05\n",
      "Epoch 9404/20000: Train Loss = 0.439616, Test Loss = 0.250823, Learning Rate = 2.805568e-05\n",
      "Epoch 9405/20000: Train Loss = 0.439799, Test Loss = 0.251164, Learning Rate = 2.804502e-05\n",
      "Epoch 9406/20000: Train Loss = 0.440083, Test Loss = 0.247204, Learning Rate = 2.803436e-05\n",
      "Epoch 9407/20000: Train Loss = 0.439774, Test Loss = 0.252519, Learning Rate = 2.802371e-05\n",
      "Epoch 9408/20000: Train Loss = 0.440016, Test Loss = 0.247028, Learning Rate = 2.801306e-05\n",
      "Epoch 9409/20000: Train Loss = 0.439957, Test Loss = 0.250050, Learning Rate = 2.800242e-05\n",
      "Epoch 9410/20000: Train Loss = 0.439706, Test Loss = 0.246727, Learning Rate = 2.799177e-05\n",
      "Epoch 9411/20000: Train Loss = 0.440799, Test Loss = 0.250744, Learning Rate = 2.798114e-05\n",
      "Epoch 9412/20000: Train Loss = 0.439692, Test Loss = 0.248725, Learning Rate = 2.797051e-05\n",
      "Epoch 9413/20000: Train Loss = 0.440107, Test Loss = 0.252056, Learning Rate = 2.795988e-05\n",
      "Epoch 9414/20000: Train Loss = 0.440365, Test Loss = 0.246531, Learning Rate = 2.794925e-05\n",
      "Epoch 9415/20000: Train Loss = 0.440360, Test Loss = 0.246368, Learning Rate = 2.793863e-05\n",
      "Epoch 9416/20000: Train Loss = 0.440729, Test Loss = 0.245383, Learning Rate = 2.792802e-05\n",
      "Epoch 9417/20000: Train Loss = 0.439691, Test Loss = 0.247999, Learning Rate = 2.791741e-05\n",
      "Epoch 9418/20000: Train Loss = 0.440434, Test Loss = 0.244563, Learning Rate = 2.790680e-05\n",
      "Epoch 9419/20000: Train Loss = 0.439874, Test Loss = 0.254047, Learning Rate = 2.789620e-05\n",
      "Epoch 9420/20000: Train Loss = 0.439725, Test Loss = 0.247878, Learning Rate = 2.788560e-05\n",
      "Epoch 9421/20000: Train Loss = 0.439779, Test Loss = 0.250999, Learning Rate = 2.787500e-05\n",
      "Epoch 9422/20000: Train Loss = 0.440299, Test Loss = 0.252546, Learning Rate = 2.786441e-05\n",
      "Epoch 9423/20000: Train Loss = 0.440188, Test Loss = 0.247065, Learning Rate = 2.785382e-05\n",
      "Epoch 9424/20000: Train Loss = 0.439919, Test Loss = 0.249865, Learning Rate = 2.784324e-05\n",
      "Epoch 9425/20000: Train Loss = 0.440367, Test Loss = 0.253255, Learning Rate = 2.783266e-05\n",
      "Epoch 9426/20000: Train Loss = 0.439848, Test Loss = 0.250040, Learning Rate = 2.782208e-05\n",
      "Epoch 9427/20000: Train Loss = 0.439636, Test Loss = 0.251204, Learning Rate = 2.781151e-05\n",
      "Epoch 9428/20000: Train Loss = 0.439827, Test Loss = 0.248172, Learning Rate = 2.780094e-05\n",
      "Epoch 9429/20000: Train Loss = 0.439758, Test Loss = 0.249789, Learning Rate = 2.779038e-05\n",
      "Epoch 9430/20000: Train Loss = 0.439705, Test Loss = 0.253098, Learning Rate = 2.777982e-05\n",
      "Epoch 9431/20000: Train Loss = 0.439596, Test Loss = 0.247185, Learning Rate = 2.776926e-05\n",
      "Epoch 9432/20000: Train Loss = 0.439929, Test Loss = 0.248807, Learning Rate = 2.775871e-05\n",
      "Epoch 9433/20000: Train Loss = 0.440698, Test Loss = 0.248864, Learning Rate = 2.774816e-05\n",
      "Epoch 9434/20000: Train Loss = 0.439894, Test Loss = 0.248563, Learning Rate = 2.773762e-05\n",
      "Epoch 9435/20000: Train Loss = 0.440008, Test Loss = 0.242958, Learning Rate = 2.772708e-05\n",
      "Epoch 9436/20000: Train Loss = 0.440384, Test Loss = 0.240794, Learning Rate = 2.771655e-05\n",
      "Epoch 9437/20000: Train Loss = 0.439713, Test Loss = 0.250103, Learning Rate = 2.770601e-05\n",
      "Epoch 9438/20000: Train Loss = 0.439874, Test Loss = 0.242278, Learning Rate = 2.769549e-05\n",
      "Epoch 9439/20000: Train Loss = 0.439885, Test Loss = 0.245942, Learning Rate = 2.768496e-05\n",
      "Epoch 9440/20000: Train Loss = 0.439782, Test Loss = 0.248043, Learning Rate = 2.767444e-05\n",
      "Epoch 9441/20000: Train Loss = 0.440061, Test Loss = 0.248713, Learning Rate = 2.766393e-05\n",
      "Epoch 9442/20000: Train Loss = 0.440051, Test Loss = 0.249016, Learning Rate = 2.765342e-05\n",
      "Epoch 9443/20000: Train Loss = 0.439986, Test Loss = 0.251393, Learning Rate = 2.764291e-05\n",
      "Epoch 9444/20000: Train Loss = 0.440056, Test Loss = 0.249395, Learning Rate = 2.763241e-05\n",
      "Epoch 9445/20000: Train Loss = 0.439852, Test Loss = 0.247233, Learning Rate = 2.762191e-05\n",
      "Epoch 9446/20000: Train Loss = 0.440049, Test Loss = 0.247132, Learning Rate = 2.761141e-05\n",
      "Epoch 9447/20000: Train Loss = 0.439593, Test Loss = 0.248626, Learning Rate = 2.760092e-05\n",
      "Epoch 9448/20000: Train Loss = 0.439684, Test Loss = 0.250675, Learning Rate = 2.759043e-05\n",
      "Epoch 9449/20000: Train Loss = 0.439593, Test Loss = 0.244824, Learning Rate = 2.757995e-05\n",
      "Epoch 9450/20000: Train Loss = 0.439807, Test Loss = 0.245527, Learning Rate = 2.756947e-05\n",
      "Epoch 9451/20000: Train Loss = 0.439817, Test Loss = 0.246890, Learning Rate = 2.755899e-05\n",
      "Epoch 9452/20000: Train Loss = 0.439887, Test Loss = 0.245198, Learning Rate = 2.754852e-05\n",
      "Epoch 9453/20000: Train Loss = 0.439847, Test Loss = 0.252409, Learning Rate = 2.753805e-05\n",
      "Epoch 9454/20000: Train Loss = 0.439873, Test Loss = 0.249727, Learning Rate = 2.752759e-05\n",
      "Epoch 9455/20000: Train Loss = 0.439984, Test Loss = 0.246250, Learning Rate = 2.751713e-05\n",
      "Epoch 9456/20000: Train Loss = 0.439887, Test Loss = 0.244860, Learning Rate = 2.750667e-05\n",
      "Epoch 9457/20000: Train Loss = 0.440274, Test Loss = 0.252621, Learning Rate = 2.749622e-05\n",
      "Epoch 9458/20000: Train Loss = 0.439962, Test Loss = 0.247004, Learning Rate = 2.748577e-05\n",
      "Epoch 9459/20000: Train Loss = 0.440071, Test Loss = 0.251799, Learning Rate = 2.747533e-05\n",
      "Epoch 9460/20000: Train Loss = 0.439862, Test Loss = 0.249763, Learning Rate = 2.746489e-05\n",
      "Epoch 9461/20000: Train Loss = 0.439554, Test Loss = 0.249040, Learning Rate = 2.745445e-05\n",
      "Epoch 9462/20000: Train Loss = 0.439816, Test Loss = 0.249088, Learning Rate = 2.744402e-05\n",
      "Epoch 9463/20000: Train Loss = 0.439700, Test Loss = 0.246106, Learning Rate = 2.743359e-05\n",
      "Epoch 9464/20000: Train Loss = 0.439877, Test Loss = 0.245046, Learning Rate = 2.742317e-05\n",
      "Epoch 9465/20000: Train Loss = 0.439723, Test Loss = 0.248059, Learning Rate = 2.741275e-05\n",
      "Epoch 9466/20000: Train Loss = 0.440007, Test Loss = 0.252092, Learning Rate = 2.740233e-05\n",
      "Epoch 9467/20000: Train Loss = 0.440230, Test Loss = 0.245511, Learning Rate = 2.739192e-05\n",
      "Epoch 9468/20000: Train Loss = 0.439733, Test Loss = 0.247491, Learning Rate = 2.738151e-05\n",
      "Epoch 9469/20000: Train Loss = 0.439763, Test Loss = 0.249447, Learning Rate = 2.737111e-05\n",
      "Epoch 9470/20000: Train Loss = 0.440002, Test Loss = 0.253376, Learning Rate = 2.736071e-05\n",
      "Epoch 9471/20000: Train Loss = 0.439868, Test Loss = 0.244509, Learning Rate = 2.735031e-05\n",
      "Epoch 9472/20000: Train Loss = 0.440253, Test Loss = 0.243753, Learning Rate = 2.733992e-05\n",
      "Epoch 9473/20000: Train Loss = 0.440344, Test Loss = 0.250666, Learning Rate = 2.732953e-05\n",
      "Epoch 9474/20000: Train Loss = 0.440599, Test Loss = 0.251827, Learning Rate = 2.731915e-05\n",
      "Epoch 9475/20000: Train Loss = 0.439730, Test Loss = 0.249260, Learning Rate = 2.730877e-05\n",
      "Epoch 9476/20000: Train Loss = 0.439658, Test Loss = 0.249756, Learning Rate = 2.729839e-05\n",
      "Epoch 9477/20000: Train Loss = 0.439636, Test Loss = 0.246577, Learning Rate = 2.728802e-05\n",
      "Epoch 9478/20000: Train Loss = 0.440037, Test Loss = 0.245564, Learning Rate = 2.727765e-05\n",
      "Epoch 9479/20000: Train Loss = 0.439728, Test Loss = 0.247698, Learning Rate = 2.726728e-05\n",
      "Epoch 9480/20000: Train Loss = 0.439841, Test Loss = 0.250895, Learning Rate = 2.725692e-05\n",
      "Epoch 9481/20000: Train Loss = 0.439689, Test Loss = 0.250417, Learning Rate = 2.724657e-05\n",
      "Epoch 9482/20000: Train Loss = 0.440212, Test Loss = 0.256938, Learning Rate = 2.723621e-05\n",
      "Epoch 9483/20000: Train Loss = 0.439891, Test Loss = 0.252373, Learning Rate = 2.722586e-05\n",
      "Epoch 9484/20000: Train Loss = 0.439913, Test Loss = 0.252740, Learning Rate = 2.721552e-05\n",
      "Epoch 9485/20000: Train Loss = 0.439787, Test Loss = 0.250673, Learning Rate = 2.720518e-05\n",
      "Epoch 9486/20000: Train Loss = 0.439726, Test Loss = 0.251947, Learning Rate = 2.719484e-05\n",
      "Epoch 9487/20000: Train Loss = 0.439882, Test Loss = 0.250569, Learning Rate = 2.718451e-05\n",
      "Epoch 9488/20000: Train Loss = 0.439855, Test Loss = 0.248245, Learning Rate = 2.717418e-05\n",
      "Epoch 9489/20000: Train Loss = 0.439900, Test Loss = 0.247590, Learning Rate = 2.716385e-05\n",
      "Epoch 9490/20000: Train Loss = 0.439855, Test Loss = 0.244822, Learning Rate = 2.715353e-05\n",
      "Epoch 9491/20000: Train Loss = 0.439870, Test Loss = 0.247161, Learning Rate = 2.714321e-05\n",
      "Epoch 9492/20000: Train Loss = 0.440057, Test Loss = 0.247919, Learning Rate = 2.713290e-05\n",
      "Epoch 9493/20000: Train Loss = 0.439918, Test Loss = 0.256151, Learning Rate = 2.712259e-05\n",
      "Epoch 9494/20000: Train Loss = 0.439729, Test Loss = 0.249388, Learning Rate = 2.711228e-05\n",
      "Epoch 9495/20000: Train Loss = 0.439911, Test Loss = 0.255470, Learning Rate = 2.710198e-05\n",
      "Epoch 9496/20000: Train Loss = 0.439660, Test Loss = 0.251140, Learning Rate = 2.709168e-05\n",
      "Epoch 9497/20000: Train Loss = 0.439917, Test Loss = 0.252687, Learning Rate = 2.708139e-05\n",
      "Epoch 9498/20000: Train Loss = 0.440093, Test Loss = 0.250725, Learning Rate = 2.707110e-05\n",
      "Epoch 9499/20000: Train Loss = 0.439992, Test Loss = 0.247186, Learning Rate = 2.706081e-05\n",
      "Epoch 9500/20000: Train Loss = 0.439883, Test Loss = 0.252979, Learning Rate = 2.705053e-05\n",
      "Epoch 9501/20000: Train Loss = 0.439841, Test Loss = 0.247947, Learning Rate = 2.704025e-05\n",
      "Epoch 9502/20000: Train Loss = 0.439626, Test Loss = 0.248824, Learning Rate = 2.702998e-05\n",
      "Epoch 9503/20000: Train Loss = 0.439696, Test Loss = 0.246475, Learning Rate = 2.701971e-05\n",
      "Epoch 9504/20000: Train Loss = 0.439796, Test Loss = 0.246056, Learning Rate = 2.700944e-05\n",
      "Epoch 9505/20000: Train Loss = 0.440140, Test Loss = 0.248693, Learning Rate = 2.699918e-05\n",
      "Epoch 9506/20000: Train Loss = 0.439780, Test Loss = 0.253001, Learning Rate = 2.698892e-05\n",
      "Epoch 9507/20000: Train Loss = 0.440206, Test Loss = 0.248672, Learning Rate = 2.697866e-05\n",
      "Epoch 9508/20000: Train Loss = 0.439564, Test Loss = 0.251407, Learning Rate = 2.696841e-05\n",
      "Epoch 9509/20000: Train Loss = 0.439674, Test Loss = 0.247966, Learning Rate = 2.695817e-05\n",
      "Epoch 9510/20000: Train Loss = 0.439853, Test Loss = 0.246789, Learning Rate = 2.694792e-05\n",
      "Epoch 9511/20000: Train Loss = 0.439757, Test Loss = 0.252643, Learning Rate = 2.693768e-05\n",
      "Epoch 9512/20000: Train Loss = 0.439878, Test Loss = 0.248046, Learning Rate = 2.692745e-05\n",
      "Epoch 9513/20000: Train Loss = 0.439958, Test Loss = 0.246822, Learning Rate = 2.691722e-05\n",
      "Epoch 9514/20000: Train Loss = 0.439736, Test Loss = 0.245470, Learning Rate = 2.690699e-05\n",
      "Epoch 9515/20000: Train Loss = 0.439853, Test Loss = 0.245923, Learning Rate = 2.689676e-05\n",
      "Epoch 9516/20000: Train Loss = 0.439610, Test Loss = 0.243909, Learning Rate = 2.688654e-05\n",
      "Epoch 9517/20000: Train Loss = 0.439759, Test Loss = 0.246652, Learning Rate = 2.687633e-05\n",
      "Epoch 9518/20000: Train Loss = 0.439817, Test Loss = 0.247713, Learning Rate = 2.686612e-05\n",
      "Epoch 9519/20000: Train Loss = 0.439617, Test Loss = 0.245821, Learning Rate = 2.685591e-05\n",
      "Epoch 9520/20000: Train Loss = 0.439700, Test Loss = 0.246510, Learning Rate = 2.684570e-05\n",
      "Epoch 9521/20000: Train Loss = 0.439952, Test Loss = 0.251347, Learning Rate = 2.683550e-05\n",
      "Epoch 9522/20000: Train Loss = 0.439853, Test Loss = 0.252387, Learning Rate = 2.682531e-05\n",
      "Epoch 9523/20000: Train Loss = 0.440270, Test Loss = 0.250990, Learning Rate = 2.681511e-05\n",
      "Epoch 9524/20000: Train Loss = 0.439870, Test Loss = 0.252195, Learning Rate = 2.680492e-05\n",
      "Epoch 9525/20000: Train Loss = 0.439784, Test Loss = 0.252088, Learning Rate = 2.679474e-05\n",
      "Epoch 9526/20000: Train Loss = 0.439688, Test Loss = 0.254703, Learning Rate = 2.678456e-05\n",
      "Epoch 9527/20000: Train Loss = 0.439756, Test Loss = 0.249940, Learning Rate = 2.677438e-05\n",
      "Epoch 9528/20000: Train Loss = 0.440244, Test Loss = 0.253599, Learning Rate = 2.676421e-05\n",
      "Epoch 9529/20000: Train Loss = 0.441026, Test Loss = 0.251835, Learning Rate = 2.675404e-05\n",
      "Epoch 9530/20000: Train Loss = 0.441010, Test Loss = 0.260694, Learning Rate = 2.674387e-05\n",
      "Epoch 9531/20000: Train Loss = 0.439767, Test Loss = 0.247348, Learning Rate = 2.673371e-05\n",
      "Epoch 9532/20000: Train Loss = 0.440118, Test Loss = 0.251468, Learning Rate = 2.672355e-05\n",
      "Epoch 9533/20000: Train Loss = 0.439675, Test Loss = 0.249784, Learning Rate = 2.671340e-05\n",
      "Epoch 9534/20000: Train Loss = 0.439760, Test Loss = 0.247550, Learning Rate = 2.670325e-05\n",
      "Epoch 9535/20000: Train Loss = 0.440156, Test Loss = 0.248011, Learning Rate = 2.669310e-05\n",
      "Epoch 9536/20000: Train Loss = 0.439713, Test Loss = 0.253404, Learning Rate = 2.668296e-05\n",
      "Epoch 9537/20000: Train Loss = 0.439701, Test Loss = 0.245664, Learning Rate = 2.667282e-05\n",
      "Epoch 9538/20000: Train Loss = 0.439894, Test Loss = 0.251938, Learning Rate = 2.666268e-05\n",
      "Epoch 9539/20000: Train Loss = 0.439957, Test Loss = 0.243924, Learning Rate = 2.665255e-05\n",
      "Epoch 9540/20000: Train Loss = 0.439730, Test Loss = 0.245508, Learning Rate = 2.664242e-05\n",
      "Epoch 9541/20000: Train Loss = 0.439644, Test Loss = 0.246376, Learning Rate = 2.663230e-05\n",
      "Epoch 9542/20000: Train Loss = 0.440016, Test Loss = 0.246849, Learning Rate = 2.662218e-05\n",
      "Epoch 9543/20000: Train Loss = 0.439937, Test Loss = 0.244074, Learning Rate = 2.661207e-05\n",
      "Epoch 9544/20000: Train Loss = 0.440103, Test Loss = 0.245851, Learning Rate = 2.660195e-05\n",
      "Epoch 9545/20000: Train Loss = 0.439713, Test Loss = 0.248670, Learning Rate = 2.659185e-05\n",
      "Epoch 9546/20000: Train Loss = 0.439779, Test Loss = 0.248080, Learning Rate = 2.658174e-05\n",
      "Epoch 9547/20000: Train Loss = 0.439599, Test Loss = 0.246026, Learning Rate = 2.657164e-05\n",
      "Epoch 9548/20000: Train Loss = 0.439654, Test Loss = 0.251006, Learning Rate = 2.656154e-05\n",
      "Epoch 9549/20000: Train Loss = 0.440304, Test Loss = 0.248293, Learning Rate = 2.655145e-05\n",
      "Epoch 9550/20000: Train Loss = 0.439720, Test Loss = 0.252482, Learning Rate = 2.654136e-05\n",
      "Epoch 9551/20000: Train Loss = 0.440199, Test Loss = 0.251616, Learning Rate = 2.653128e-05\n",
      "Epoch 9552/20000: Train Loss = 0.439893, Test Loss = 0.246195, Learning Rate = 2.652120e-05\n",
      "Epoch 9553/20000: Train Loss = 0.439934, Test Loss = 0.253443, Learning Rate = 2.651112e-05\n",
      "Epoch 9554/20000: Train Loss = 0.439761, Test Loss = 0.249890, Learning Rate = 2.650105e-05\n",
      "Epoch 9555/20000: Train Loss = 0.440204, Test Loss = 0.251712, Learning Rate = 2.649098e-05\n",
      "Epoch 9556/20000: Train Loss = 0.439882, Test Loss = 0.253684, Learning Rate = 2.648091e-05\n",
      "Epoch 9557/20000: Train Loss = 0.439833, Test Loss = 0.248950, Learning Rate = 2.647085e-05\n",
      "Epoch 9558/20000: Train Loss = 0.439815, Test Loss = 0.253948, Learning Rate = 2.646079e-05\n",
      "Epoch 9559/20000: Train Loss = 0.440292, Test Loss = 0.247931, Learning Rate = 2.645074e-05\n",
      "Epoch 9560/20000: Train Loss = 0.440028, Test Loss = 0.249823, Learning Rate = 2.644069e-05\n",
      "Epoch 9561/20000: Train Loss = 0.439641, Test Loss = 0.254082, Learning Rate = 2.643064e-05\n",
      "Epoch 9562/20000: Train Loss = 0.439988, Test Loss = 0.254787, Learning Rate = 2.642060e-05\n",
      "Epoch 9563/20000: Train Loss = 0.439615, Test Loss = 0.251662, Learning Rate = 2.641056e-05\n",
      "Epoch 9564/20000: Train Loss = 0.439781, Test Loss = 0.249991, Learning Rate = 2.640052e-05\n",
      "Epoch 9565/20000: Train Loss = 0.439778, Test Loss = 0.249865, Learning Rate = 2.639049e-05\n",
      "Epoch 9566/20000: Train Loss = 0.439975, Test Loss = 0.249754, Learning Rate = 2.638046e-05\n",
      "Epoch 9567/20000: Train Loss = 0.439956, Test Loss = 0.249294, Learning Rate = 2.637044e-05\n",
      "Epoch 9568/20000: Train Loss = 0.439902, Test Loss = 0.248589, Learning Rate = 2.636042e-05\n",
      "Epoch 9569/20000: Train Loss = 0.440011, Test Loss = 0.246132, Learning Rate = 2.635040e-05\n",
      "Epoch 9570/20000: Train Loss = 0.439791, Test Loss = 0.251619, Learning Rate = 2.634039e-05\n",
      "Epoch 9571/20000: Train Loss = 0.440030, Test Loss = 0.244268, Learning Rate = 2.633038e-05\n",
      "Epoch 9572/20000: Train Loss = 0.439583, Test Loss = 0.247990, Learning Rate = 2.632038e-05\n",
      "Epoch 9573/20000: Train Loss = 0.439948, Test Loss = 0.250930, Learning Rate = 2.631038e-05\n",
      "Epoch 9574/20000: Train Loss = 0.440001, Test Loss = 0.244523, Learning Rate = 2.630038e-05\n",
      "Epoch 9575/20000: Train Loss = 0.439699, Test Loss = 0.248881, Learning Rate = 2.629038e-05\n",
      "Epoch 9576/20000: Train Loss = 0.439842, Test Loss = 0.249699, Learning Rate = 2.628040e-05\n",
      "Epoch 9577/20000: Train Loss = 0.439657, Test Loss = 0.247713, Learning Rate = 2.627041e-05\n",
      "Epoch 9578/20000: Train Loss = 0.439681, Test Loss = 0.247414, Learning Rate = 2.626043e-05\n",
      "Epoch 9579/20000: Train Loss = 0.439571, Test Loss = 0.252674, Learning Rate = 2.625045e-05\n",
      "Epoch 9580/20000: Train Loss = 0.439748, Test Loss = 0.251807, Learning Rate = 2.624047e-05\n",
      "Epoch 9581/20000: Train Loss = 0.439789, Test Loss = 0.250282, Learning Rate = 2.623050e-05\n",
      "Epoch 9582/20000: Train Loss = 0.440167, Test Loss = 0.247646, Learning Rate = 2.622054e-05\n",
      "Epoch 9583/20000: Train Loss = 0.439832, Test Loss = 0.248896, Learning Rate = 2.621057e-05\n",
      "Epoch 9584/20000: Train Loss = 0.439589, Test Loss = 0.254187, Learning Rate = 2.620061e-05\n",
      "Epoch 9585/20000: Train Loss = 0.439652, Test Loss = 0.248233, Learning Rate = 2.619066e-05\n",
      "Epoch 9586/20000: Train Loss = 0.440061, Test Loss = 0.252893, Learning Rate = 2.618071e-05\n",
      "Epoch 9587/20000: Train Loss = 0.440101, Test Loss = 0.244500, Learning Rate = 2.617076e-05\n",
      "Epoch 9588/20000: Train Loss = 0.439736, Test Loss = 0.245551, Learning Rate = 2.616082e-05\n",
      "Epoch 9589/20000: Train Loss = 0.439776, Test Loss = 0.246531, Learning Rate = 2.615087e-05\n",
      "Epoch 9590/20000: Train Loss = 0.439944, Test Loss = 0.246634, Learning Rate = 2.614094e-05\n",
      "Epoch 9591/20000: Train Loss = 0.439603, Test Loss = 0.250263, Learning Rate = 2.613101e-05\n",
      "Epoch 9592/20000: Train Loss = 0.439704, Test Loss = 0.245827, Learning Rate = 2.612108e-05\n",
      "Epoch 9593/20000: Train Loss = 0.439679, Test Loss = 0.247424, Learning Rate = 2.611115e-05\n",
      "Epoch 9594/20000: Train Loss = 0.439859, Test Loss = 0.245315, Learning Rate = 2.610123e-05\n",
      "Epoch 9595/20000: Train Loss = 0.439970, Test Loss = 0.245192, Learning Rate = 2.609131e-05\n",
      "Epoch 9596/20000: Train Loss = 0.439744, Test Loss = 0.245974, Learning Rate = 2.608140e-05\n",
      "Epoch 9597/20000: Train Loss = 0.439818, Test Loss = 0.248205, Learning Rate = 2.607149e-05\n",
      "Epoch 9598/20000: Train Loss = 0.439629, Test Loss = 0.244949, Learning Rate = 2.606158e-05\n",
      "Epoch 9599/20000: Train Loss = 0.439638, Test Loss = 0.248265, Learning Rate = 2.605168e-05\n",
      "Epoch 9600/20000: Train Loss = 0.439994, Test Loss = 0.245463, Learning Rate = 2.604178e-05\n",
      "Epoch 9601/20000: Train Loss = 0.440012, Test Loss = 0.246765, Learning Rate = 2.603188e-05\n",
      "Epoch 9602/20000: Train Loss = 0.439948, Test Loss = 0.253799, Learning Rate = 2.602199e-05\n",
      "Epoch 9603/20000: Train Loss = 0.440571, Test Loss = 0.249599, Learning Rate = 2.601211e-05\n",
      "Epoch 9604/20000: Train Loss = 0.439697, Test Loss = 0.252153, Learning Rate = 2.600222e-05\n",
      "Epoch 9605/20000: Train Loss = 0.439678, Test Loss = 0.244985, Learning Rate = 2.599234e-05\n",
      "Epoch 9606/20000: Train Loss = 0.439986, Test Loss = 0.247215, Learning Rate = 2.598246e-05\n",
      "Epoch 9607/20000: Train Loss = 0.439657, Test Loss = 0.249158, Learning Rate = 2.597259e-05\n",
      "Epoch 9608/20000: Train Loss = 0.439742, Test Loss = 0.246688, Learning Rate = 2.596272e-05\n",
      "Epoch 9609/20000: Train Loss = 0.439805, Test Loss = 0.248313, Learning Rate = 2.595286e-05\n",
      "Epoch 9610/20000: Train Loss = 0.439916, Test Loss = 0.247662, Learning Rate = 2.594300e-05\n",
      "Epoch 9611/20000: Train Loss = 0.439833, Test Loss = 0.247991, Learning Rate = 2.593314e-05\n",
      "Epoch 9612/20000: Train Loss = 0.439707, Test Loss = 0.249015, Learning Rate = 2.592329e-05\n",
      "Epoch 9613/20000: Train Loss = 0.439673, Test Loss = 0.248319, Learning Rate = 2.591343e-05\n",
      "Epoch 9614/20000: Train Loss = 0.439805, Test Loss = 0.251745, Learning Rate = 2.590359e-05\n",
      "Epoch 9615/20000: Train Loss = 0.439709, Test Loss = 0.247835, Learning Rate = 2.589375e-05\n",
      "Epoch 9616/20000: Train Loss = 0.439743, Test Loss = 0.255039, Learning Rate = 2.588391e-05\n",
      "Epoch 9617/20000: Train Loss = 0.439920, Test Loss = 0.245361, Learning Rate = 2.587407e-05\n",
      "Epoch 9618/20000: Train Loss = 0.439977, Test Loss = 0.251049, Learning Rate = 2.586424e-05\n",
      "Epoch 9619/20000: Train Loss = 0.439744, Test Loss = 0.251577, Learning Rate = 2.585441e-05\n",
      "Epoch 9620/20000: Train Loss = 0.439696, Test Loss = 0.254971, Learning Rate = 2.584459e-05\n",
      "Epoch 9621/20000: Train Loss = 0.440083, Test Loss = 0.252088, Learning Rate = 2.583477e-05\n",
      "Epoch 9622/20000: Train Loss = 0.439796, Test Loss = 0.250239, Learning Rate = 2.582495e-05\n",
      "Epoch 9623/20000: Train Loss = 0.439647, Test Loss = 0.250156, Learning Rate = 2.581514e-05\n",
      "Epoch 9624/20000: Train Loss = 0.439957, Test Loss = 0.245005, Learning Rate = 2.580533e-05\n",
      "Epoch 9625/20000: Train Loss = 0.440028, Test Loss = 0.249601, Learning Rate = 2.579552e-05\n",
      "Epoch 9626/20000: Train Loss = 0.439641, Test Loss = 0.247427, Learning Rate = 2.578572e-05\n",
      "Epoch 9627/20000: Train Loss = 0.440540, Test Loss = 0.242580, Learning Rate = 2.577593e-05\n",
      "Epoch 9628/20000: Train Loss = 0.439861, Test Loss = 0.246095, Learning Rate = 2.576613e-05\n",
      "Epoch 9629/20000: Train Loss = 0.439970, Test Loss = 0.247538, Learning Rate = 2.575634e-05\n",
      "Epoch 9630/20000: Train Loss = 0.440063, Test Loss = 0.247225, Learning Rate = 2.574655e-05\n",
      "Epoch 9631/20000: Train Loss = 0.439706, Test Loss = 0.247949, Learning Rate = 2.573677e-05\n",
      "Epoch 9632/20000: Train Loss = 0.439822, Test Loss = 0.251496, Learning Rate = 2.572699e-05\n",
      "Epoch 9633/20000: Train Loss = 0.439861, Test Loss = 0.252769, Learning Rate = 2.571722e-05\n",
      "Epoch 9634/20000: Train Loss = 0.439869, Test Loss = 0.252155, Learning Rate = 2.570744e-05\n",
      "Epoch 9635/20000: Train Loss = 0.440241, Test Loss = 0.250617, Learning Rate = 2.569768e-05\n",
      "Epoch 9636/20000: Train Loss = 0.440031, Test Loss = 0.249332, Learning Rate = 2.568791e-05\n",
      "Epoch 9637/20000: Train Loss = 0.439841, Test Loss = 0.248422, Learning Rate = 2.567815e-05\n",
      "Epoch 9638/20000: Train Loss = 0.439635, Test Loss = 0.250009, Learning Rate = 2.566839e-05\n",
      "Epoch 9639/20000: Train Loss = 0.440019, Test Loss = 0.249908, Learning Rate = 2.565864e-05\n",
      "Epoch 9640/20000: Train Loss = 0.439877, Test Loss = 0.242573, Learning Rate = 2.564889e-05\n",
      "Epoch 9641/20000: Train Loss = 0.439795, Test Loss = 0.254470, Learning Rate = 2.563915e-05\n",
      "Epoch 9642/20000: Train Loss = 0.439714, Test Loss = 0.250211, Learning Rate = 2.562940e-05\n",
      "Epoch 9643/20000: Train Loss = 0.440103, Test Loss = 0.250182, Learning Rate = 2.561966e-05\n",
      "Epoch 9644/20000: Train Loss = 0.439646, Test Loss = 0.251193, Learning Rate = 2.560993e-05\n",
      "Epoch 9645/20000: Train Loss = 0.439778, Test Loss = 0.248827, Learning Rate = 2.560020e-05\n",
      "Epoch 9646/20000: Train Loss = 0.439895, Test Loss = 0.250074, Learning Rate = 2.559047e-05\n",
      "Epoch 9647/20000: Train Loss = 0.439795, Test Loss = 0.250203, Learning Rate = 2.558075e-05\n",
      "Epoch 9648/20000: Train Loss = 0.439971, Test Loss = 0.256367, Learning Rate = 2.557103e-05\n",
      "Epoch 9649/20000: Train Loss = 0.440771, Test Loss = 0.251191, Learning Rate = 2.556131e-05\n",
      "Epoch 9650/20000: Train Loss = 0.440043, Test Loss = 0.251054, Learning Rate = 2.555160e-05\n",
      "Epoch 9651/20000: Train Loss = 0.439906, Test Loss = 0.253047, Learning Rate = 2.554189e-05\n",
      "Epoch 9652/20000: Train Loss = 0.439741, Test Loss = 0.251562, Learning Rate = 2.553218e-05\n",
      "Epoch 9653/20000: Train Loss = 0.439910, Test Loss = 0.250056, Learning Rate = 2.552248e-05\n",
      "Epoch 9654/20000: Train Loss = 0.439837, Test Loss = 0.251957, Learning Rate = 2.551279e-05\n",
      "Epoch 9655/20000: Train Loss = 0.439618, Test Loss = 0.250883, Learning Rate = 2.550309e-05\n",
      "Epoch 9656/20000: Train Loss = 0.440166, Test Loss = 0.248933, Learning Rate = 2.549340e-05\n",
      "Epoch 9657/20000: Train Loss = 0.439656, Test Loss = 0.247058, Learning Rate = 2.548371e-05\n",
      "Epoch 9658/20000: Train Loss = 0.439842, Test Loss = 0.247446, Learning Rate = 2.547403e-05\n",
      "Epoch 9659/20000: Train Loss = 0.440104, Test Loss = 0.249432, Learning Rate = 2.546435e-05\n",
      "Epoch 9660/20000: Train Loss = 0.440019, Test Loss = 0.250028, Learning Rate = 2.545468e-05\n",
      "Epoch 9661/20000: Train Loss = 0.440478, Test Loss = 0.243929, Learning Rate = 2.544500e-05\n",
      "Epoch 9662/20000: Train Loss = 0.440463, Test Loss = 0.246238, Learning Rate = 2.543533e-05\n",
      "Epoch 9663/20000: Train Loss = 0.439721, Test Loss = 0.255033, Learning Rate = 2.542567e-05\n",
      "Epoch 9664/20000: Train Loss = 0.439921, Test Loss = 0.247226, Learning Rate = 2.541601e-05\n",
      "Epoch 9665/20000: Train Loss = 0.439655, Test Loss = 0.250724, Learning Rate = 2.540635e-05\n",
      "Epoch 9666/20000: Train Loss = 0.440156, Test Loss = 0.251637, Learning Rate = 2.539670e-05\n",
      "Epoch 9667/20000: Train Loss = 0.439845, Test Loss = 0.250090, Learning Rate = 2.538705e-05\n",
      "Epoch 9668/20000: Train Loss = 0.439891, Test Loss = 0.252992, Learning Rate = 2.537740e-05\n",
      "Epoch 9669/20000: Train Loss = 0.439923, Test Loss = 0.251934, Learning Rate = 2.536776e-05\n",
      "Epoch 9670/20000: Train Loss = 0.439757, Test Loss = 0.250980, Learning Rate = 2.535812e-05\n",
      "Epoch 9671/20000: Train Loss = 0.440088, Test Loss = 0.248750, Learning Rate = 2.534848e-05\n",
      "Epoch 9672/20000: Train Loss = 0.439977, Test Loss = 0.248405, Learning Rate = 2.533885e-05\n",
      "Epoch 9673/20000: Train Loss = 0.439799, Test Loss = 0.253266, Learning Rate = 2.532922e-05\n",
      "Epoch 9674/20000: Train Loss = 0.440969, Test Loss = 0.243900, Learning Rate = 2.531960e-05\n",
      "Epoch 9675/20000: Train Loss = 0.440358, Test Loss = 0.253238, Learning Rate = 2.530998e-05\n",
      "Epoch 9676/20000: Train Loss = 0.440453, Test Loss = 0.253475, Learning Rate = 2.530036e-05\n",
      "Epoch 9677/20000: Train Loss = 0.439747, Test Loss = 0.254559, Learning Rate = 2.529075e-05\n",
      "Epoch 9678/20000: Train Loss = 0.440071, Test Loss = 0.251117, Learning Rate = 2.528114e-05\n",
      "Epoch 9679/20000: Train Loss = 0.440013, Test Loss = 0.254610, Learning Rate = 2.527153e-05\n",
      "Epoch 9680/20000: Train Loss = 0.439648, Test Loss = 0.245742, Learning Rate = 2.526193e-05\n",
      "Epoch 9681/20000: Train Loss = 0.439760, Test Loss = 0.244272, Learning Rate = 2.525233e-05\n",
      "Epoch 9682/20000: Train Loss = 0.439636, Test Loss = 0.245923, Learning Rate = 2.524274e-05\n",
      "Epoch 9683/20000: Train Loss = 0.439661, Test Loss = 0.246656, Learning Rate = 2.523314e-05\n",
      "Epoch 9684/20000: Train Loss = 0.439802, Test Loss = 0.250884, Learning Rate = 2.522356e-05\n",
      "Epoch 9685/20000: Train Loss = 0.439690, Test Loss = 0.249629, Learning Rate = 2.521397e-05\n",
      "Epoch 9686/20000: Train Loss = 0.439879, Test Loss = 0.254704, Learning Rate = 2.520439e-05\n",
      "Epoch 9687/20000: Train Loss = 0.439797, Test Loss = 0.250401, Learning Rate = 2.519481e-05\n",
      "Epoch 9688/20000: Train Loss = 0.440168, Test Loss = 0.258044, Learning Rate = 2.518524e-05\n",
      "Epoch 9689/20000: Train Loss = 0.440127, Test Loss = 0.250855, Learning Rate = 2.517567e-05\n",
      "Epoch 9690/20000: Train Loss = 0.439719, Test Loss = 0.251371, Learning Rate = 2.516611e-05\n",
      "Epoch 9691/20000: Train Loss = 0.440050, Test Loss = 0.251870, Learning Rate = 2.515654e-05\n",
      "Epoch 9692/20000: Train Loss = 0.440034, Test Loss = 0.257188, Learning Rate = 2.514698e-05\n",
      "Epoch 9693/20000: Train Loss = 0.439908, Test Loss = 0.248250, Learning Rate = 2.513743e-05\n",
      "Epoch 9694/20000: Train Loss = 0.440312, Test Loss = 0.251972, Learning Rate = 2.512788e-05\n",
      "Epoch 9695/20000: Train Loss = 0.440879, Test Loss = 0.253753, Learning Rate = 2.511833e-05\n",
      "Epoch 9696/20000: Train Loss = 0.440038, Test Loss = 0.251013, Learning Rate = 2.510879e-05\n",
      "Epoch 9697/20000: Train Loss = 0.439646, Test Loss = 0.253296, Learning Rate = 2.509925e-05\n",
      "Epoch 9698/20000: Train Loss = 0.439757, Test Loss = 0.246263, Learning Rate = 2.508971e-05\n",
      "Epoch 9699/20000: Train Loss = 0.439770, Test Loss = 0.248057, Learning Rate = 2.508017e-05\n",
      "Epoch 9700/20000: Train Loss = 0.440010, Test Loss = 0.249431, Learning Rate = 2.507064e-05\n",
      "Epoch 9701/20000: Train Loss = 0.439696, Test Loss = 0.250233, Learning Rate = 2.506112e-05\n",
      "Epoch 9702/20000: Train Loss = 0.439678, Test Loss = 0.247986, Learning Rate = 2.505160e-05\n",
      "Epoch 9703/20000: Train Loss = 0.439582, Test Loss = 0.249110, Learning Rate = 2.504208e-05\n",
      "Epoch 9704/20000: Train Loss = 0.439900, Test Loss = 0.245172, Learning Rate = 2.503256e-05\n",
      "Epoch 9705/20000: Train Loss = 0.439960, Test Loss = 0.252949, Learning Rate = 2.502305e-05\n",
      "Epoch 9706/20000: Train Loss = 0.440087, Test Loss = 0.251741, Learning Rate = 2.501354e-05\n",
      "Epoch 9707/20000: Train Loss = 0.439851, Test Loss = 0.251963, Learning Rate = 2.500404e-05\n",
      "Epoch 9708/20000: Train Loss = 0.439765, Test Loss = 0.250799, Learning Rate = 2.499454e-05\n",
      "Epoch 9709/20000: Train Loss = 0.439721, Test Loss = 0.250133, Learning Rate = 2.498504e-05\n",
      "Epoch 9710/20000: Train Loss = 0.439809, Test Loss = 0.254884, Learning Rate = 2.497555e-05\n",
      "Epoch 9711/20000: Train Loss = 0.439797, Test Loss = 0.253584, Learning Rate = 2.496606e-05\n",
      "Epoch 9712/20000: Train Loss = 0.439866, Test Loss = 0.254269, Learning Rate = 2.495657e-05\n",
      "Epoch 9713/20000: Train Loss = 0.440284, Test Loss = 0.251716, Learning Rate = 2.494709e-05\n",
      "Epoch 9714/20000: Train Loss = 0.440337, Test Loss = 0.257714, Learning Rate = 2.493761e-05\n",
      "Epoch 9715/20000: Train Loss = 0.439545, Test Loss = 0.247270, Learning Rate = 2.492813e-05\n",
      "Epoch 9716/20000: Train Loss = 0.440478, Test Loss = 0.252579, Learning Rate = 2.491866e-05\n",
      "Epoch 9717/20000: Train Loss = 0.439682, Test Loss = 0.251716, Learning Rate = 2.490919e-05\n",
      "Epoch 9718/20000: Train Loss = 0.439736, Test Loss = 0.250253, Learning Rate = 2.489973e-05\n",
      "Epoch 9719/20000: Train Loss = 0.439585, Test Loss = 0.253006, Learning Rate = 2.489027e-05\n",
      "Epoch 9720/20000: Train Loss = 0.439753, Test Loss = 0.256327, Learning Rate = 2.488081e-05\n",
      "Epoch 9721/20000: Train Loss = 0.439640, Test Loss = 0.254505, Learning Rate = 2.487135e-05\n",
      "Epoch 9722/20000: Train Loss = 0.439817, Test Loss = 0.251246, Learning Rate = 2.486190e-05\n",
      "Epoch 9723/20000: Train Loss = 0.439745, Test Loss = 0.247598, Learning Rate = 2.485246e-05\n",
      "Epoch 9724/20000: Train Loss = 0.439818, Test Loss = 0.247709, Learning Rate = 2.484301e-05\n",
      "Epoch 9725/20000: Train Loss = 0.439729, Test Loss = 0.248381, Learning Rate = 2.483357e-05\n",
      "Epoch 9726/20000: Train Loss = 0.439785, Test Loss = 0.247924, Learning Rate = 2.482414e-05\n",
      "Epoch 9727/20000: Train Loss = 0.440247, Test Loss = 0.250034, Learning Rate = 2.481470e-05\n",
      "Epoch 9728/20000: Train Loss = 0.439966, Test Loss = 0.245215, Learning Rate = 2.480528e-05\n",
      "Epoch 9729/20000: Train Loss = 0.439870, Test Loss = 0.247243, Learning Rate = 2.479585e-05\n",
      "Epoch 9730/20000: Train Loss = 0.439961, Test Loss = 0.248222, Learning Rate = 2.478643e-05\n",
      "Epoch 9731/20000: Train Loss = 0.439696, Test Loss = 0.250598, Learning Rate = 2.477701e-05\n",
      "Epoch 9732/20000: Train Loss = 0.439844, Test Loss = 0.245911, Learning Rate = 2.476760e-05\n",
      "Epoch 9733/20000: Train Loss = 0.439619, Test Loss = 0.251757, Learning Rate = 2.475819e-05\n",
      "Epoch 9734/20000: Train Loss = 0.439681, Test Loss = 0.251397, Learning Rate = 2.474878e-05\n",
      "Epoch 9735/20000: Train Loss = 0.439858, Test Loss = 0.249792, Learning Rate = 2.473937e-05\n",
      "Epoch 9736/20000: Train Loss = 0.439759, Test Loss = 0.253165, Learning Rate = 2.472997e-05\n",
      "Epoch 9737/20000: Train Loss = 0.439483, Test Loss = 0.252390, Learning Rate = 2.472058e-05\n",
      "Epoch 9738/20000: Train Loss = 0.439843, Test Loss = 0.251177, Learning Rate = 2.471118e-05\n",
      "Epoch 9739/20000: Train Loss = 0.440009, Test Loss = 0.252464, Learning Rate = 2.470179e-05\n",
      "Epoch 9740/20000: Train Loss = 0.440009, Test Loss = 0.251848, Learning Rate = 2.469241e-05\n",
      "Epoch 9741/20000: Train Loss = 0.440011, Test Loss = 0.247482, Learning Rate = 2.468303e-05\n",
      "Epoch 9742/20000: Train Loss = 0.440013, Test Loss = 0.249516, Learning Rate = 2.467365e-05\n",
      "Epoch 9743/20000: Train Loss = 0.439696, Test Loss = 0.253712, Learning Rate = 2.466427e-05\n",
      "Epoch 9744/20000: Train Loss = 0.440279, Test Loss = 0.250151, Learning Rate = 2.465490e-05\n",
      "Epoch 9745/20000: Train Loss = 0.439719, Test Loss = 0.254912, Learning Rate = 2.464553e-05\n",
      "Epoch 9746/20000: Train Loss = 0.439952, Test Loss = 0.249626, Learning Rate = 2.463617e-05\n",
      "Epoch 9747/20000: Train Loss = 0.439782, Test Loss = 0.252708, Learning Rate = 2.462681e-05\n",
      "Epoch 9748/20000: Train Loss = 0.439714, Test Loss = 0.252554, Learning Rate = 2.461745e-05\n",
      "Epoch 9749/20000: Train Loss = 0.439701, Test Loss = 0.253141, Learning Rate = 2.460809e-05\n",
      "Epoch 9750/20000: Train Loss = 0.439770, Test Loss = 0.248765, Learning Rate = 2.459874e-05\n",
      "Epoch 9751/20000: Train Loss = 0.439649, Test Loss = 0.252387, Learning Rate = 2.458940e-05\n",
      "Epoch 9752/20000: Train Loss = 0.439654, Test Loss = 0.248874, Learning Rate = 2.458005e-05\n",
      "Epoch 9753/20000: Train Loss = 0.440068, Test Loss = 0.256333, Learning Rate = 2.457071e-05\n",
      "Epoch 9754/20000: Train Loss = 0.439628, Test Loss = 0.256878, Learning Rate = 2.456138e-05\n",
      "Epoch 9755/20000: Train Loss = 0.439825, Test Loss = 0.254595, Learning Rate = 2.455204e-05\n",
      "Epoch 9756/20000: Train Loss = 0.439699, Test Loss = 0.256419, Learning Rate = 2.454272e-05\n",
      "Epoch 9757/20000: Train Loss = 0.439751, Test Loss = 0.253165, Learning Rate = 2.453339e-05\n",
      "Epoch 9758/20000: Train Loss = 0.439847, Test Loss = 0.250651, Learning Rate = 2.452407e-05\n",
      "Epoch 9759/20000: Train Loss = 0.439648, Test Loss = 0.251645, Learning Rate = 2.451475e-05\n",
      "Epoch 9760/20000: Train Loss = 0.440019, Test Loss = 0.249806, Learning Rate = 2.450543e-05\n",
      "Epoch 9761/20000: Train Loss = 0.439935, Test Loss = 0.247750, Learning Rate = 2.449612e-05\n",
      "Epoch 9762/20000: Train Loss = 0.440044, Test Loss = 0.249723, Learning Rate = 2.448682e-05\n",
      "Epoch 9763/20000: Train Loss = 0.439515, Test Loss = 0.248518, Learning Rate = 2.447751e-05\n",
      "Epoch 9764/20000: Train Loss = 0.439627, Test Loss = 0.253768, Learning Rate = 2.446821e-05\n",
      "Epoch 9765/20000: Train Loss = 0.440529, Test Loss = 0.248319, Learning Rate = 2.445891e-05\n",
      "Epoch 9766/20000: Train Loss = 0.439725, Test Loss = 0.250985, Learning Rate = 2.444962e-05\n",
      "Epoch 9767/20000: Train Loss = 0.439591, Test Loss = 0.247371, Learning Rate = 2.444033e-05\n",
      "Epoch 9768/20000: Train Loss = 0.439729, Test Loss = 0.250803, Learning Rate = 2.443104e-05\n",
      "Epoch 9769/20000: Train Loss = 0.439613, Test Loss = 0.247530, Learning Rate = 2.442176e-05\n",
      "Epoch 9770/20000: Train Loss = 0.439721, Test Loss = 0.248367, Learning Rate = 2.441248e-05\n",
      "Epoch 9771/20000: Train Loss = 0.439567, Test Loss = 0.252298, Learning Rate = 2.440320e-05\n",
      "Epoch 9772/20000: Train Loss = 0.440066, Test Loss = 0.252363, Learning Rate = 2.439393e-05\n",
      "Epoch 9773/20000: Train Loss = 0.439653, Test Loss = 0.249947, Learning Rate = 2.438466e-05\n",
      "Epoch 9774/20000: Train Loss = 0.439678, Test Loss = 0.251845, Learning Rate = 2.437540e-05\n",
      "Epoch 9775/20000: Train Loss = 0.439456, Test Loss = 0.248299, Learning Rate = 2.436613e-05\n",
      "Epoch 9776/20000: Train Loss = 0.439602, Test Loss = 0.249329, Learning Rate = 2.435688e-05\n",
      "Epoch 9777/20000: Train Loss = 0.439806, Test Loss = 0.248076, Learning Rate = 2.434762e-05\n",
      "Epoch 9778/20000: Train Loss = 0.439786, Test Loss = 0.252029, Learning Rate = 2.433837e-05\n",
      "Epoch 9779/20000: Train Loss = 0.439856, Test Loss = 0.250217, Learning Rate = 2.432912e-05\n",
      "Epoch 9780/20000: Train Loss = 0.439790, Test Loss = 0.248962, Learning Rate = 2.431988e-05\n",
      "Epoch 9781/20000: Train Loss = 0.440346, Test Loss = 0.247349, Learning Rate = 2.431064e-05\n",
      "Epoch 9782/20000: Train Loss = 0.439731, Test Loss = 0.250873, Learning Rate = 2.430140e-05\n",
      "Epoch 9783/20000: Train Loss = 0.439592, Test Loss = 0.250971, Learning Rate = 2.429217e-05\n",
      "Epoch 9784/20000: Train Loss = 0.439722, Test Loss = 0.253791, Learning Rate = 2.428294e-05\n",
      "Epoch 9785/20000: Train Loss = 0.439783, Test Loss = 0.256958, Learning Rate = 2.427371e-05\n",
      "Epoch 9786/20000: Train Loss = 0.439771, Test Loss = 0.253314, Learning Rate = 2.426448e-05\n",
      "Epoch 9787/20000: Train Loss = 0.440068, Test Loss = 0.255170, Learning Rate = 2.425526e-05\n",
      "Epoch 9788/20000: Train Loss = 0.439792, Test Loss = 0.250009, Learning Rate = 2.424605e-05\n",
      "Epoch 9789/20000: Train Loss = 0.439606, Test Loss = 0.250333, Learning Rate = 2.423684e-05\n",
      "Epoch 9790/20000: Train Loss = 0.439993, Test Loss = 0.248205, Learning Rate = 2.422763e-05\n",
      "Epoch 9791/20000: Train Loss = 0.439593, Test Loss = 0.250621, Learning Rate = 2.421842e-05\n",
      "Epoch 9792/20000: Train Loss = 0.439798, Test Loss = 0.249586, Learning Rate = 2.420922e-05\n",
      "Epoch 9793/20000: Train Loss = 0.439846, Test Loss = 0.249103, Learning Rate = 2.420002e-05\n",
      "Epoch 9794/20000: Train Loss = 0.439588, Test Loss = 0.249139, Learning Rate = 2.419082e-05\n",
      "Epoch 9795/20000: Train Loss = 0.439689, Test Loss = 0.254546, Learning Rate = 2.418163e-05\n",
      "Epoch 9796/20000: Train Loss = 0.439923, Test Loss = 0.249859, Learning Rate = 2.417244e-05\n",
      "Epoch 9797/20000: Train Loss = 0.440004, Test Loss = 0.254525, Learning Rate = 2.416326e-05\n",
      "Epoch 9798/20000: Train Loss = 0.439788, Test Loss = 0.245420, Learning Rate = 2.415408e-05\n",
      "Epoch 9799/20000: Train Loss = 0.439727, Test Loss = 0.247494, Learning Rate = 2.414490e-05\n",
      "Epoch 9800/20000: Train Loss = 0.440008, Test Loss = 0.251046, Learning Rate = 2.413573e-05\n",
      "Epoch 9801/20000: Train Loss = 0.440200, Test Loss = 0.248045, Learning Rate = 2.412655e-05\n",
      "Epoch 9802/20000: Train Loss = 0.439728, Test Loss = 0.248603, Learning Rate = 2.411739e-05\n",
      "Epoch 9803/20000: Train Loss = 0.439538, Test Loss = 0.251337, Learning Rate = 2.410822e-05\n",
      "Epoch 9804/20000: Train Loss = 0.439785, Test Loss = 0.249028, Learning Rate = 2.409906e-05\n",
      "Epoch 9805/20000: Train Loss = 0.439733, Test Loss = 0.252432, Learning Rate = 2.408991e-05\n",
      "Epoch 9806/20000: Train Loss = 0.439802, Test Loss = 0.257567, Learning Rate = 2.408075e-05\n",
      "Epoch 9807/20000: Train Loss = 0.439850, Test Loss = 0.257047, Learning Rate = 2.407160e-05\n",
      "Epoch 9808/20000: Train Loss = 0.440170, Test Loss = 0.259246, Learning Rate = 2.406246e-05\n",
      "Epoch 9809/20000: Train Loss = 0.439603, Test Loss = 0.253100, Learning Rate = 2.405331e-05\n",
      "Epoch 9810/20000: Train Loss = 0.439622, Test Loss = 0.253206, Learning Rate = 2.404417e-05\n",
      "Epoch 9811/20000: Train Loss = 0.439842, Test Loss = 0.255026, Learning Rate = 2.403504e-05\n",
      "Epoch 9812/20000: Train Loss = 0.440222, Test Loss = 0.252615, Learning Rate = 2.402590e-05\n",
      "Epoch 9813/20000: Train Loss = 0.439636, Test Loss = 0.247778, Learning Rate = 2.401677e-05\n",
      "Epoch 9814/20000: Train Loss = 0.439684, Test Loss = 0.254737, Learning Rate = 2.400765e-05\n",
      "Epoch 9815/20000: Train Loss = 0.440017, Test Loss = 0.254105, Learning Rate = 2.399853e-05\n",
      "Epoch 9816/20000: Train Loss = 0.440267, Test Loss = 0.252258, Learning Rate = 2.398941e-05\n",
      "Epoch 9817/20000: Train Loss = 0.440058, Test Loss = 0.250554, Learning Rate = 2.398029e-05\n",
      "Epoch 9818/20000: Train Loss = 0.440137, Test Loss = 0.253305, Learning Rate = 2.397118e-05\n",
      "Epoch 9819/20000: Train Loss = 0.440477, Test Loss = 0.242531, Learning Rate = 2.396207e-05\n",
      "Epoch 9820/20000: Train Loss = 0.439885, Test Loss = 0.252624, Learning Rate = 2.395297e-05\n",
      "Epoch 9821/20000: Train Loss = 0.440084, Test Loss = 0.250133, Learning Rate = 2.394387e-05\n",
      "Epoch 9822/20000: Train Loss = 0.440149, Test Loss = 0.245254, Learning Rate = 2.393477e-05\n",
      "Epoch 9823/20000: Train Loss = 0.439559, Test Loss = 0.252491, Learning Rate = 2.392567e-05\n",
      "Epoch 9824/20000: Train Loss = 0.439686, Test Loss = 0.252250, Learning Rate = 2.391658e-05\n",
      "Epoch 9825/20000: Train Loss = 0.439655, Test Loss = 0.249889, Learning Rate = 2.390749e-05\n",
      "Epoch 9826/20000: Train Loss = 0.440441, Test Loss = 0.250941, Learning Rate = 2.389841e-05\n",
      "Epoch 9827/20000: Train Loss = 0.439726, Test Loss = 0.250976, Learning Rate = 2.388933e-05\n",
      "Epoch 9828/20000: Train Loss = 0.439703, Test Loss = 0.251415, Learning Rate = 2.388025e-05\n",
      "Epoch 9829/20000: Train Loss = 0.440044, Test Loss = 0.249001, Learning Rate = 2.387118e-05\n",
      "Epoch 9830/20000: Train Loss = 0.439592, Test Loss = 0.250578, Learning Rate = 2.386211e-05\n",
      "Epoch 9831/20000: Train Loss = 0.440086, Test Loss = 0.245110, Learning Rate = 2.385304e-05\n",
      "Epoch 9832/20000: Train Loss = 0.439849, Test Loss = 0.248228, Learning Rate = 2.384398e-05\n",
      "Epoch 9833/20000: Train Loss = 0.439869, Test Loss = 0.250106, Learning Rate = 2.383492e-05\n",
      "Epoch 9834/20000: Train Loss = 0.440013, Test Loss = 0.245015, Learning Rate = 2.382586e-05\n",
      "Epoch 9835/20000: Train Loss = 0.439780, Test Loss = 0.250359, Learning Rate = 2.381681e-05\n",
      "Epoch 9836/20000: Train Loss = 0.439770, Test Loss = 0.250964, Learning Rate = 2.380776e-05\n",
      "Epoch 9837/20000: Train Loss = 0.439783, Test Loss = 0.248857, Learning Rate = 2.379871e-05\n",
      "Epoch 9838/20000: Train Loss = 0.439644, Test Loss = 0.249445, Learning Rate = 2.378967e-05\n",
      "Epoch 9839/20000: Train Loss = 0.439626, Test Loss = 0.250178, Learning Rate = 2.378063e-05\n",
      "Epoch 9840/20000: Train Loss = 0.439707, Test Loss = 0.247982, Learning Rate = 2.377159e-05\n",
      "Epoch 9841/20000: Train Loss = 0.440146, Test Loss = 0.249386, Learning Rate = 2.376256e-05\n",
      "Epoch 9842/20000: Train Loss = 0.439658, Test Loss = 0.247709, Learning Rate = 2.375353e-05\n",
      "Epoch 9843/20000: Train Loss = 0.439598, Test Loss = 0.249076, Learning Rate = 2.374451e-05\n",
      "Epoch 9844/20000: Train Loss = 0.439881, Test Loss = 0.247186, Learning Rate = 2.373548e-05\n",
      "Epoch 9845/20000: Train Loss = 0.439665, Test Loss = 0.250081, Learning Rate = 2.372646e-05\n",
      "Epoch 9846/20000: Train Loss = 0.439776, Test Loss = 0.250313, Learning Rate = 2.371745e-05\n",
      "Epoch 9847/20000: Train Loss = 0.440099, Test Loss = 0.253922, Learning Rate = 2.370844e-05\n",
      "Epoch 9848/20000: Train Loss = 0.439923, Test Loss = 0.246031, Learning Rate = 2.369943e-05\n",
      "Epoch 9849/20000: Train Loss = 0.439647, Test Loss = 0.246438, Learning Rate = 2.369042e-05\n",
      "Epoch 9850/20000: Train Loss = 0.439592, Test Loss = 0.248880, Learning Rate = 2.368142e-05\n",
      "Epoch 9851/20000: Train Loss = 0.440035, Test Loss = 0.246658, Learning Rate = 2.367242e-05\n",
      "Epoch 9852/20000: Train Loss = 0.439514, Test Loss = 0.250643, Learning Rate = 2.366343e-05\n",
      "Epoch 9853/20000: Train Loss = 0.440176, Test Loss = 0.253298, Learning Rate = 2.365444e-05\n",
      "Epoch 9854/20000: Train Loss = 0.440598, Test Loss = 0.257442, Learning Rate = 2.364545e-05\n",
      "Epoch 9855/20000: Train Loss = 0.439366, Test Loss = 0.245831, Learning Rate = 2.363646e-05\n",
      "Epoch 9856/20000: Train Loss = 0.439795, Test Loss = 0.247805, Learning Rate = 2.362748e-05\n",
      "Epoch 9857/20000: Train Loss = 0.439630, Test Loss = 0.242796, Learning Rate = 2.361851e-05\n",
      "Epoch 9858/20000: Train Loss = 0.441000, Test Loss = 0.250563, Learning Rate = 2.360953e-05\n",
      "Epoch 9859/20000: Train Loss = 0.439859, Test Loss = 0.246420, Learning Rate = 2.360056e-05\n",
      "Epoch 9860/20000: Train Loss = 0.440076, Test Loss = 0.246233, Learning Rate = 2.359159e-05\n",
      "Epoch 9861/20000: Train Loss = 0.439701, Test Loss = 0.243943, Learning Rate = 2.358263e-05\n",
      "Epoch 9862/20000: Train Loss = 0.439961, Test Loss = 0.247069, Learning Rate = 2.357367e-05\n",
      "Epoch 9863/20000: Train Loss = 0.440168, Test Loss = 0.242588, Learning Rate = 2.356471e-05\n",
      "Epoch 9864/20000: Train Loss = 0.440495, Test Loss = 0.250382, Learning Rate = 2.355576e-05\n",
      "Epoch 9865/20000: Train Loss = 0.439836, Test Loss = 0.248336, Learning Rate = 2.354681e-05\n",
      "Epoch 9866/20000: Train Loss = 0.439536, Test Loss = 0.248444, Learning Rate = 2.353786e-05\n",
      "Epoch 9867/20000: Train Loss = 0.439796, Test Loss = 0.247304, Learning Rate = 2.352892e-05\n",
      "Epoch 9868/20000: Train Loss = 0.439498, Test Loss = 0.247788, Learning Rate = 2.351997e-05\n",
      "Epoch 9869/20000: Train Loss = 0.439695, Test Loss = 0.249604, Learning Rate = 2.351104e-05\n",
      "Epoch 9870/20000: Train Loss = 0.439601, Test Loss = 0.252409, Learning Rate = 2.350210e-05\n",
      "Epoch 9871/20000: Train Loss = 0.439637, Test Loss = 0.246786, Learning Rate = 2.349317e-05\n",
      "Epoch 9872/20000: Train Loss = 0.440080, Test Loss = 0.251985, Learning Rate = 2.348425e-05\n",
      "Epoch 9873/20000: Train Loss = 0.439901, Test Loss = 0.250825, Learning Rate = 2.347532e-05\n",
      "Epoch 9874/20000: Train Loss = 0.439613, Test Loss = 0.246910, Learning Rate = 2.346640e-05\n",
      "Epoch 9875/20000: Train Loss = 0.439729, Test Loss = 0.249663, Learning Rate = 2.345749e-05\n",
      "Epoch 9876/20000: Train Loss = 0.439710, Test Loss = 0.248456, Learning Rate = 2.344857e-05\n",
      "Epoch 9877/20000: Train Loss = 0.439664, Test Loss = 0.247311, Learning Rate = 2.343966e-05\n",
      "Epoch 9878/20000: Train Loss = 0.439841, Test Loss = 0.244508, Learning Rate = 2.343076e-05\n",
      "Epoch 9879/20000: Train Loss = 0.439671, Test Loss = 0.247576, Learning Rate = 2.342185e-05\n",
      "Epoch 9880/20000: Train Loss = 0.439733, Test Loss = 0.252164, Learning Rate = 2.341296e-05\n",
      "Epoch 9881/20000: Train Loss = 0.440305, Test Loss = 0.245855, Learning Rate = 2.340406e-05\n",
      "Epoch 9882/20000: Train Loss = 0.439636, Test Loss = 0.249823, Learning Rate = 2.339517e-05\n",
      "Epoch 9883/20000: Train Loss = 0.439576, Test Loss = 0.248648, Learning Rate = 2.338628e-05\n",
      "Epoch 9884/20000: Train Loss = 0.439629, Test Loss = 0.251407, Learning Rate = 2.337739e-05\n",
      "Epoch 9885/20000: Train Loss = 0.439788, Test Loss = 0.252375, Learning Rate = 2.336851e-05\n",
      "Epoch 9886/20000: Train Loss = 0.439968, Test Loss = 0.249393, Learning Rate = 2.335963e-05\n",
      "Epoch 9887/20000: Train Loss = 0.439845, Test Loss = 0.248967, Learning Rate = 2.335075e-05\n",
      "Epoch 9888/20000: Train Loss = 0.440094, Test Loss = 0.248793, Learning Rate = 2.334188e-05\n",
      "Epoch 9889/20000: Train Loss = 0.439655, Test Loss = 0.247460, Learning Rate = 2.333301e-05\n",
      "Epoch 9890/20000: Train Loss = 0.439921, Test Loss = 0.247509, Learning Rate = 2.332414e-05\n",
      "Epoch 9891/20000: Train Loss = 0.439914, Test Loss = 0.244080, Learning Rate = 2.331528e-05\n",
      "Epoch 9892/20000: Train Loss = 0.439761, Test Loss = 0.244685, Learning Rate = 2.330642e-05\n",
      "Epoch 9893/20000: Train Loss = 0.439790, Test Loss = 0.247572, Learning Rate = 2.329757e-05\n",
      "Epoch 9894/20000: Train Loss = 0.440139, Test Loss = 0.251252, Learning Rate = 2.328871e-05\n",
      "Epoch 9895/20000: Train Loss = 0.439828, Test Loss = 0.245731, Learning Rate = 2.327987e-05\n",
      "Epoch 9896/20000: Train Loss = 0.440039, Test Loss = 0.250244, Learning Rate = 2.327102e-05\n",
      "Epoch 9897/20000: Train Loss = 0.439638, Test Loss = 0.249358, Learning Rate = 2.326218e-05\n",
      "Epoch 9898/20000: Train Loss = 0.439486, Test Loss = 0.246577, Learning Rate = 2.325334e-05\n",
      "Epoch 9899/20000: Train Loss = 0.439817, Test Loss = 0.247042, Learning Rate = 2.324450e-05\n",
      "Epoch 9900/20000: Train Loss = 0.440001, Test Loss = 0.253700, Learning Rate = 2.323567e-05\n",
      "Epoch 9901/20000: Train Loss = 0.439603, Test Loss = 0.245516, Learning Rate = 2.322684e-05\n",
      "Epoch 9902/20000: Train Loss = 0.439799, Test Loss = 0.250545, Learning Rate = 2.321802e-05\n",
      "Epoch 9903/20000: Train Loss = 0.439593, Test Loss = 0.250504, Learning Rate = 2.320919e-05\n",
      "Epoch 9904/20000: Train Loss = 0.439710, Test Loss = 0.251804, Learning Rate = 2.320037e-05\n",
      "Epoch 9905/20000: Train Loss = 0.439559, Test Loss = 0.248132, Learning Rate = 2.319156e-05\n",
      "Epoch 9906/20000: Train Loss = 0.439656, Test Loss = 0.248670, Learning Rate = 2.318275e-05\n",
      "Epoch 9907/20000: Train Loss = 0.440192, Test Loss = 0.249359, Learning Rate = 2.317394e-05\n",
      "Epoch 9908/20000: Train Loss = 0.440104, Test Loss = 0.245706, Learning Rate = 2.316513e-05\n",
      "Epoch 9909/20000: Train Loss = 0.439871, Test Loss = 0.246955, Learning Rate = 2.315633e-05\n",
      "Epoch 9910/20000: Train Loss = 0.439857, Test Loss = 0.246372, Learning Rate = 2.314753e-05\n",
      "Epoch 9911/20000: Train Loss = 0.439672, Test Loss = 0.248684, Learning Rate = 2.313874e-05\n",
      "Epoch 9912/20000: Train Loss = 0.439831, Test Loss = 0.249031, Learning Rate = 2.312994e-05\n",
      "Epoch 9913/20000: Train Loss = 0.439726, Test Loss = 0.247420, Learning Rate = 2.312116e-05\n",
      "Epoch 9914/20000: Train Loss = 0.439719, Test Loss = 0.254360, Learning Rate = 2.311237e-05\n",
      "Epoch 9915/20000: Train Loss = 0.439644, Test Loss = 0.251565, Learning Rate = 2.310359e-05\n",
      "Epoch 9916/20000: Train Loss = 0.439676, Test Loss = 0.254094, Learning Rate = 2.309481e-05\n",
      "Epoch 9917/20000: Train Loss = 0.439694, Test Loss = 0.251865, Learning Rate = 2.308603e-05\n",
      "Epoch 9918/20000: Train Loss = 0.439876, Test Loss = 0.254805, Learning Rate = 2.307726e-05\n",
      "Epoch 9919/20000: Train Loss = 0.439855, Test Loss = 0.251406, Learning Rate = 2.306849e-05\n",
      "Epoch 9920/20000: Train Loss = 0.439990, Test Loss = 0.254722, Learning Rate = 2.305973e-05\n",
      "Epoch 9921/20000: Train Loss = 0.439939, Test Loss = 0.252356, Learning Rate = 2.305097e-05\n",
      "Epoch 9922/20000: Train Loss = 0.439863, Test Loss = 0.255997, Learning Rate = 2.304221e-05\n",
      "Epoch 9923/20000: Train Loss = 0.439548, Test Loss = 0.248481, Learning Rate = 2.303345e-05\n",
      "Epoch 9924/20000: Train Loss = 0.439664, Test Loss = 0.248783, Learning Rate = 2.302470e-05\n",
      "Epoch 9925/20000: Train Loss = 0.439530, Test Loss = 0.249988, Learning Rate = 2.301595e-05\n",
      "Epoch 9926/20000: Train Loss = 0.439687, Test Loss = 0.253156, Learning Rate = 2.300721e-05\n",
      "Epoch 9927/20000: Train Loss = 0.439655, Test Loss = 0.252166, Learning Rate = 2.299846e-05\n",
      "Epoch 9928/20000: Train Loss = 0.439887, Test Loss = 0.253870, Learning Rate = 2.298972e-05\n",
      "Epoch 9929/20000: Train Loss = 0.439867, Test Loss = 0.248705, Learning Rate = 2.298099e-05\n",
      "Epoch 9930/20000: Train Loss = 0.439707, Test Loss = 0.244837, Learning Rate = 2.297226e-05\n",
      "Epoch 9931/20000: Train Loss = 0.439590, Test Loss = 0.251124, Learning Rate = 2.296353e-05\n",
      "Epoch 9932/20000: Train Loss = 0.439611, Test Loss = 0.251328, Learning Rate = 2.295480e-05\n",
      "Epoch 9933/20000: Train Loss = 0.439744, Test Loss = 0.252585, Learning Rate = 2.294608e-05\n",
      "Epoch 9934/20000: Train Loss = 0.439851, Test Loss = 0.249191, Learning Rate = 2.293736e-05\n",
      "Epoch 9935/20000: Train Loss = 0.439946, Test Loss = 0.256022, Learning Rate = 2.292865e-05\n",
      "Epoch 9936/20000: Train Loss = 0.439904, Test Loss = 0.256606, Learning Rate = 2.291993e-05\n",
      "Epoch 9937/20000: Train Loss = 0.439911, Test Loss = 0.251525, Learning Rate = 2.291122e-05\n",
      "Epoch 9938/20000: Train Loss = 0.440047, Test Loss = 0.252469, Learning Rate = 2.290252e-05\n",
      "Epoch 9939/20000: Train Loss = 0.440495, Test Loss = 0.246976, Learning Rate = 2.289382e-05\n",
      "Epoch 9940/20000: Train Loss = 0.439861, Test Loss = 0.250403, Learning Rate = 2.288512e-05\n",
      "Epoch 9941/20000: Train Loss = 0.439699, Test Loss = 0.251432, Learning Rate = 2.287642e-05\n",
      "Epoch 9942/20000: Train Loss = 0.439673, Test Loss = 0.252665, Learning Rate = 2.286773e-05\n",
      "Epoch 9943/20000: Train Loss = 0.439673, Test Loss = 0.252233, Learning Rate = 2.285904e-05\n",
      "Epoch 9944/20000: Train Loss = 0.439899, Test Loss = 0.247232, Learning Rate = 2.285035e-05\n",
      "Epoch 9945/20000: Train Loss = 0.439969, Test Loss = 0.249543, Learning Rate = 2.284167e-05\n",
      "Epoch 9946/20000: Train Loss = 0.439776, Test Loss = 0.245902, Learning Rate = 2.283299e-05\n",
      "Epoch 9947/20000: Train Loss = 0.439804, Test Loss = 0.251610, Learning Rate = 2.282432e-05\n",
      "Epoch 9948/20000: Train Loss = 0.439672, Test Loss = 0.247651, Learning Rate = 2.281564e-05\n",
      "Epoch 9949/20000: Train Loss = 0.439623, Test Loss = 0.251040, Learning Rate = 2.280697e-05\n",
      "Epoch 9950/20000: Train Loss = 0.439742, Test Loss = 0.248507, Learning Rate = 2.279831e-05\n",
      "Epoch 9951/20000: Train Loss = 0.439743, Test Loss = 0.253625, Learning Rate = 2.278965e-05\n",
      "Epoch 9952/20000: Train Loss = 0.439655, Test Loss = 0.247422, Learning Rate = 2.278099e-05\n",
      "Epoch 9953/20000: Train Loss = 0.439667, Test Loss = 0.249035, Learning Rate = 2.277233e-05\n",
      "Epoch 9954/20000: Train Loss = 0.439759, Test Loss = 0.252904, Learning Rate = 2.276368e-05\n",
      "Epoch 9955/20000: Train Loss = 0.439719, Test Loss = 0.251316, Learning Rate = 2.275503e-05\n",
      "Epoch 9956/20000: Train Loss = 0.439512, Test Loss = 0.253283, Learning Rate = 2.274638e-05\n",
      "Epoch 9957/20000: Train Loss = 0.440185, Test Loss = 0.250874, Learning Rate = 2.273774e-05\n",
      "Epoch 9958/20000: Train Loss = 0.439815, Test Loss = 0.249818, Learning Rate = 2.272910e-05\n",
      "Epoch 9959/20000: Train Loss = 0.439798, Test Loss = 0.250508, Learning Rate = 2.272046e-05\n",
      "Epoch 9960/20000: Train Loss = 0.439427, Test Loss = 0.255996, Learning Rate = 2.271183e-05\n",
      "Epoch 9961/20000: Train Loss = 0.440028, Test Loss = 0.255125, Learning Rate = 2.270320e-05\n",
      "Epoch 9962/20000: Train Loss = 0.439636, Test Loss = 0.254033, Learning Rate = 2.269457e-05\n",
      "Epoch 9963/20000: Train Loss = 0.439827, Test Loss = 0.255770, Learning Rate = 2.268595e-05\n",
      "Epoch 9964/20000: Train Loss = 0.439673, Test Loss = 0.253518, Learning Rate = 2.267733e-05\n",
      "Epoch 9965/20000: Train Loss = 0.439930, Test Loss = 0.257423, Learning Rate = 2.266871e-05\n",
      "Epoch 9966/20000: Train Loss = 0.439586, Test Loss = 0.250126, Learning Rate = 2.266010e-05\n",
      "Epoch 9967/20000: Train Loss = 0.439710, Test Loss = 0.248765, Learning Rate = 2.265149e-05\n",
      "Epoch 9968/20000: Train Loss = 0.439707, Test Loss = 0.251411, Learning Rate = 2.264288e-05\n",
      "Epoch 9969/20000: Train Loss = 0.439788, Test Loss = 0.252801, Learning Rate = 2.263428e-05\n",
      "Epoch 9970/20000: Train Loss = 0.439676, Test Loss = 0.250171, Learning Rate = 2.262568e-05\n",
      "Epoch 9971/20000: Train Loss = 0.439524, Test Loss = 0.248007, Learning Rate = 2.261708e-05\n",
      "Epoch 9972/20000: Train Loss = 0.439599, Test Loss = 0.251257, Learning Rate = 2.260849e-05\n",
      "Epoch 9973/20000: Train Loss = 0.439796, Test Loss = 0.246860, Learning Rate = 2.259990e-05\n",
      "Epoch 9974/20000: Train Loss = 0.440271, Test Loss = 0.253661, Learning Rate = 2.259131e-05\n",
      "Epoch 9975/20000: Train Loss = 0.439645, Test Loss = 0.248087, Learning Rate = 2.258272e-05\n",
      "Epoch 9976/20000: Train Loss = 0.439704, Test Loss = 0.250241, Learning Rate = 2.257414e-05\n",
      "Epoch 9977/20000: Train Loss = 0.440038, Test Loss = 0.253520, Learning Rate = 2.256557e-05\n",
      "Epoch 9978/20000: Train Loss = 0.439857, Test Loss = 0.247107, Learning Rate = 2.255699e-05\n",
      "Epoch 9979/20000: Train Loss = 0.439754, Test Loss = 0.251867, Learning Rate = 2.254842e-05\n",
      "Epoch 9980/20000: Train Loss = 0.439778, Test Loss = 0.246843, Learning Rate = 2.253985e-05\n",
      "Epoch 9981/20000: Train Loss = 0.439833, Test Loss = 0.250548, Learning Rate = 2.253129e-05\n",
      "Epoch 9982/20000: Train Loss = 0.439911, Test Loss = 0.249971, Learning Rate = 2.252273e-05\n",
      "Epoch 9983/20000: Train Loss = 0.439854, Test Loss = 0.243479, Learning Rate = 2.251417e-05\n",
      "Epoch 9984/20000: Train Loss = 0.439729, Test Loss = 0.250186, Learning Rate = 2.250561e-05\n",
      "Epoch 9985/20000: Train Loss = 0.439754, Test Loss = 0.247298, Learning Rate = 2.249706e-05\n",
      "Epoch 9986/20000: Train Loss = 0.439766, Test Loss = 0.247118, Learning Rate = 2.248851e-05\n",
      "Epoch 9987/20000: Train Loss = 0.439509, Test Loss = 0.247400, Learning Rate = 2.247997e-05\n",
      "Epoch 9988/20000: Train Loss = 0.439800, Test Loss = 0.249749, Learning Rate = 2.247143e-05\n",
      "Epoch 9989/20000: Train Loss = 0.439587, Test Loss = 0.245561, Learning Rate = 2.246289e-05\n",
      "Epoch 9990/20000: Train Loss = 0.439717, Test Loss = 0.242567, Learning Rate = 2.245435e-05\n",
      "Epoch 9991/20000: Train Loss = 0.439876, Test Loss = 0.243635, Learning Rate = 2.244582e-05\n",
      "Epoch 9992/20000: Train Loss = 0.439641, Test Loss = 0.248189, Learning Rate = 2.243729e-05\n",
      "Epoch 9993/20000: Train Loss = 0.439656, Test Loss = 0.251155, Learning Rate = 2.242877e-05\n",
      "Epoch 9994/20000: Train Loss = 0.439807, Test Loss = 0.249203, Learning Rate = 2.242025e-05\n",
      "Epoch 9995/20000: Train Loss = 0.439785, Test Loss = 0.255305, Learning Rate = 2.241173e-05\n",
      "Epoch 9996/20000: Train Loss = 0.439849, Test Loss = 0.250167, Learning Rate = 2.240321e-05\n",
      "Epoch 9997/20000: Train Loss = 0.440181, Test Loss = 0.252908, Learning Rate = 2.239470e-05\n",
      "Epoch 9998/20000: Train Loss = 0.439942, Test Loss = 0.247250, Learning Rate = 2.238619e-05\n",
      "Epoch 9999/20000: Train Loss = 0.439734, Test Loss = 0.249968, Learning Rate = 2.237768e-05\n",
      "Epoch 10000/20000: Train Loss = 0.440045, Test Loss = 0.251123, Learning Rate = 2.236918e-05\n",
      "Epoch 10001/20000: Train Loss = 0.439706, Test Loss = 0.253510, Learning Rate = 2.236068e-05\n",
      "Epoch 10002/20000: Train Loss = 0.439946, Test Loss = 0.250601, Learning Rate = 2.235218e-05\n",
      "Epoch 10003/20000: Train Loss = 0.439790, Test Loss = 0.248653, Learning Rate = 2.234369e-05\n",
      "Epoch 10004/20000: Train Loss = 0.440403, Test Loss = 0.251320, Learning Rate = 2.233520e-05\n",
      "Epoch 10005/20000: Train Loss = 0.439885, Test Loss = 0.251010, Learning Rate = 2.232671e-05\n",
      "Epoch 10006/20000: Train Loss = 0.439681, Test Loss = 0.251357, Learning Rate = 2.231823e-05\n",
      "Epoch 10007/20000: Train Loss = 0.439557, Test Loss = 0.250792, Learning Rate = 2.230975e-05\n",
      "Epoch 10008/20000: Train Loss = 0.439817, Test Loss = 0.247209, Learning Rate = 2.230127e-05\n",
      "Epoch 10009/20000: Train Loss = 0.439705, Test Loss = 0.250987, Learning Rate = 2.229280e-05\n",
      "Epoch 10010/20000: Train Loss = 0.439826, Test Loss = 0.244519, Learning Rate = 2.228433e-05\n",
      "Epoch 10011/20000: Train Loss = 0.439781, Test Loss = 0.246490, Learning Rate = 2.227586e-05\n",
      "Epoch 10012/20000: Train Loss = 0.439880, Test Loss = 0.252526, Learning Rate = 2.226740e-05\n",
      "Epoch 10013/20000: Train Loss = 0.439875, Test Loss = 0.255854, Learning Rate = 2.225894e-05\n",
      "Epoch 10014/20000: Train Loss = 0.439744, Test Loss = 0.247440, Learning Rate = 2.225048e-05\n",
      "Epoch 10015/20000: Train Loss = 0.439690, Test Loss = 0.252025, Learning Rate = 2.224202e-05\n",
      "Epoch 10016/20000: Train Loss = 0.439843, Test Loss = 0.249926, Learning Rate = 2.223357e-05\n",
      "Epoch 10017/20000: Train Loss = 0.439711, Test Loss = 0.250220, Learning Rate = 2.222512e-05\n",
      "Epoch 10018/20000: Train Loss = 0.439663, Test Loss = 0.248534, Learning Rate = 2.221668e-05\n",
      "Epoch 10019/20000: Train Loss = 0.439678, Test Loss = 0.248405, Learning Rate = 2.220824e-05\n",
      "Epoch 10020/20000: Train Loss = 0.439852, Test Loss = 0.250243, Learning Rate = 2.219980e-05\n",
      "Epoch 10021/20000: Train Loss = 0.439677, Test Loss = 0.249307, Learning Rate = 2.219136e-05\n",
      "Epoch 10022/20000: Train Loss = 0.439613, Test Loss = 0.248082, Learning Rate = 2.218293e-05\n",
      "Epoch 10023/20000: Train Loss = 0.439698, Test Loss = 0.243850, Learning Rate = 2.217450e-05\n",
      "Epoch 10024/20000: Train Loss = 0.439584, Test Loss = 0.248756, Learning Rate = 2.216608e-05\n",
      "Epoch 10025/20000: Train Loss = 0.439686, Test Loss = 0.250471, Learning Rate = 2.215765e-05\n",
      "Epoch 10026/20000: Train Loss = 0.439921, Test Loss = 0.250783, Learning Rate = 2.214923e-05\n",
      "Epoch 10027/20000: Train Loss = 0.439906, Test Loss = 0.246784, Learning Rate = 2.214082e-05\n",
      "Epoch 10028/20000: Train Loss = 0.439670, Test Loss = 0.251695, Learning Rate = 2.213241e-05\n",
      "Epoch 10029/20000: Train Loss = 0.440004, Test Loss = 0.252553, Learning Rate = 2.212400e-05\n",
      "Epoch 10030/20000: Train Loss = 0.439563, Test Loss = 0.248750, Learning Rate = 2.211559e-05\n",
      "Epoch 10031/20000: Train Loss = 0.439730, Test Loss = 0.249805, Learning Rate = 2.210719e-05\n",
      "Epoch 10032/20000: Train Loss = 0.439588, Test Loss = 0.250061, Learning Rate = 2.209879e-05\n",
      "Epoch 10033/20000: Train Loss = 0.440270, Test Loss = 0.249838, Learning Rate = 2.209039e-05\n",
      "Epoch 10034/20000: Train Loss = 0.439736, Test Loss = 0.253661, Learning Rate = 2.208199e-05\n",
      "Epoch 10035/20000: Train Loss = 0.439979, Test Loss = 0.248591, Learning Rate = 2.207360e-05\n",
      "Epoch 10036/20000: Train Loss = 0.439763, Test Loss = 0.249816, Learning Rate = 2.206522e-05\n",
      "Epoch 10037/20000: Train Loss = 0.439769, Test Loss = 0.245724, Learning Rate = 2.205683e-05\n",
      "Epoch 10038/20000: Train Loss = 0.439886, Test Loss = 0.246377, Learning Rate = 2.204845e-05\n",
      "Epoch 10039/20000: Train Loss = 0.439850, Test Loss = 0.245910, Learning Rate = 2.204007e-05\n",
      "Epoch 10040/20000: Train Loss = 0.439606, Test Loss = 0.247301, Learning Rate = 2.203170e-05\n",
      "Epoch 10041/20000: Train Loss = 0.439653, Test Loss = 0.250913, Learning Rate = 2.202333e-05\n",
      "Epoch 10042/20000: Train Loss = 0.439866, Test Loss = 0.251828, Learning Rate = 2.201496e-05\n",
      "Epoch 10043/20000: Train Loss = 0.439791, Test Loss = 0.249459, Learning Rate = 2.200659e-05\n",
      "Epoch 10044/20000: Train Loss = 0.440247, Test Loss = 0.249197, Learning Rate = 2.199823e-05\n",
      "Epoch 10045/20000: Train Loss = 0.439905, Test Loss = 0.250013, Learning Rate = 2.198987e-05\n",
      "Epoch 10046/20000: Train Loss = 0.439692, Test Loss = 0.249300, Learning Rate = 2.198152e-05\n",
      "Epoch 10047/20000: Train Loss = 0.439785, Test Loss = 0.252536, Learning Rate = 2.197317e-05\n",
      "Epoch 10048/20000: Train Loss = 0.439896, Test Loss = 0.251386, Learning Rate = 2.196482e-05\n",
      "Epoch 10049/20000: Train Loss = 0.439976, Test Loss = 0.252811, Learning Rate = 2.195647e-05\n",
      "Epoch 10050/20000: Train Loss = 0.439654, Test Loss = 0.252713, Learning Rate = 2.194813e-05\n",
      "Epoch 10051/20000: Train Loss = 0.439704, Test Loss = 0.248311, Learning Rate = 2.193979e-05\n",
      "Epoch 10052/20000: Train Loss = 0.439804, Test Loss = 0.248379, Learning Rate = 2.193145e-05\n",
      "Epoch 10053/20000: Train Loss = 0.439978, Test Loss = 0.253413, Learning Rate = 2.192312e-05\n",
      "Epoch 10054/20000: Train Loss = 0.439733, Test Loss = 0.250410, Learning Rate = 2.191479e-05\n",
      "Epoch 10055/20000: Train Loss = 0.439647, Test Loss = 0.251562, Learning Rate = 2.190646e-05\n",
      "Epoch 10056/20000: Train Loss = 0.439664, Test Loss = 0.248934, Learning Rate = 2.189814e-05\n",
      "Epoch 10057/20000: Train Loss = 0.439612, Test Loss = 0.252558, Learning Rate = 2.188982e-05\n",
      "Epoch 10058/20000: Train Loss = 0.440894, Test Loss = 0.244192, Learning Rate = 2.188150e-05\n",
      "Epoch 10059/20000: Train Loss = 0.439725, Test Loss = 0.249903, Learning Rate = 2.187318e-05\n",
      "Epoch 10060/20000: Train Loss = 0.439580, Test Loss = 0.254638, Learning Rate = 2.186487e-05\n",
      "Epoch 10061/20000: Train Loss = 0.439462, Test Loss = 0.249821, Learning Rate = 2.185657e-05\n",
      "Epoch 10062/20000: Train Loss = 0.440085, Test Loss = 0.248360, Learning Rate = 2.184826e-05\n",
      "Epoch 10063/20000: Train Loss = 0.439746, Test Loss = 0.252045, Learning Rate = 2.183996e-05\n",
      "Epoch 10064/20000: Train Loss = 0.439892, Test Loss = 0.248040, Learning Rate = 2.183166e-05\n",
      "Epoch 10065/20000: Train Loss = 0.439747, Test Loss = 0.247116, Learning Rate = 2.182336e-05\n",
      "Epoch 10066/20000: Train Loss = 0.439625, Test Loss = 0.249237, Learning Rate = 2.181507e-05\n",
      "Epoch 10067/20000: Train Loss = 0.439748, Test Loss = 0.248138, Learning Rate = 2.180678e-05\n",
      "Epoch 10068/20000: Train Loss = 0.440110, Test Loss = 0.250845, Learning Rate = 2.179850e-05\n",
      "Epoch 10069/20000: Train Loss = 0.440030, Test Loss = 0.249120, Learning Rate = 2.179021e-05\n",
      "Epoch 10070/20000: Train Loss = 0.439735, Test Loss = 0.249818, Learning Rate = 2.178193e-05\n",
      "Epoch 10071/20000: Train Loss = 0.439490, Test Loss = 0.249148, Learning Rate = 2.177366e-05\n",
      "Epoch 10072/20000: Train Loss = 0.439717, Test Loss = 0.245892, Learning Rate = 2.176538e-05\n",
      "Epoch 10073/20000: Train Loss = 0.439492, Test Loss = 0.249550, Learning Rate = 2.175711e-05\n",
      "Epoch 10074/20000: Train Loss = 0.439655, Test Loss = 0.251205, Learning Rate = 2.174885e-05\n",
      "Epoch 10075/20000: Train Loss = 0.439957, Test Loss = 0.250338, Learning Rate = 2.174058e-05\n",
      "Epoch 10076/20000: Train Loss = 0.439811, Test Loss = 0.252675, Learning Rate = 2.173232e-05\n",
      "Epoch 10077/20000: Train Loss = 0.439883, Test Loss = 0.250470, Learning Rate = 2.172406e-05\n",
      "Epoch 10078/20000: Train Loss = 0.440271, Test Loss = 0.247813, Learning Rate = 2.171581e-05\n",
      "Epoch 10079/20000: Train Loss = 0.439646, Test Loss = 0.250257, Learning Rate = 2.170756e-05\n",
      "Epoch 10080/20000: Train Loss = 0.439628, Test Loss = 0.250223, Learning Rate = 2.169931e-05\n",
      "Epoch 10081/20000: Train Loss = 0.439586, Test Loss = 0.247942, Learning Rate = 2.169107e-05\n",
      "Epoch 10082/20000: Train Loss = 0.439591, Test Loss = 0.250547, Learning Rate = 2.168282e-05\n",
      "Epoch 10083/20000: Train Loss = 0.439805, Test Loss = 0.247958, Learning Rate = 2.167458e-05\n",
      "Epoch 10084/20000: Train Loss = 0.439499, Test Loss = 0.252279, Learning Rate = 2.166635e-05\n",
      "Epoch 10085/20000: Train Loss = 0.440266, Test Loss = 0.254023, Learning Rate = 2.165812e-05\n",
      "Epoch 10086/20000: Train Loss = 0.439971, Test Loss = 0.249263, Learning Rate = 2.164989e-05\n",
      "Epoch 10087/20000: Train Loss = 0.439774, Test Loss = 0.250490, Learning Rate = 2.164166e-05\n",
      "Epoch 10088/20000: Train Loss = 0.439598, Test Loss = 0.248820, Learning Rate = 2.163344e-05\n",
      "Epoch 10089/20000: Train Loss = 0.439770, Test Loss = 0.250439, Learning Rate = 2.162522e-05\n",
      "Epoch 10090/20000: Train Loss = 0.439569, Test Loss = 0.248421, Learning Rate = 2.161700e-05\n",
      "Epoch 10091/20000: Train Loss = 0.439694, Test Loss = 0.248820, Learning Rate = 2.160879e-05\n",
      "Epoch 10092/20000: Train Loss = 0.439574, Test Loss = 0.253201, Learning Rate = 2.160058e-05\n",
      "Epoch 10093/20000: Train Loss = 0.439453, Test Loss = 0.249173, Learning Rate = 2.159237e-05\n",
      "Epoch 10094/20000: Train Loss = 0.439607, Test Loss = 0.246730, Learning Rate = 2.158416e-05\n",
      "Epoch 10095/20000: Train Loss = 0.439686, Test Loss = 0.248246, Learning Rate = 2.157596e-05\n",
      "Epoch 10096/20000: Train Loss = 0.439585, Test Loss = 0.248392, Learning Rate = 2.156776e-05\n",
      "Epoch 10097/20000: Train Loss = 0.439930, Test Loss = 0.248353, Learning Rate = 2.155957e-05\n",
      "Epoch 10098/20000: Train Loss = 0.439388, Test Loss = 0.252225, Learning Rate = 2.155138e-05\n",
      "Epoch 10099/20000: Train Loss = 0.439606, Test Loss = 0.253090, Learning Rate = 2.154319e-05\n",
      "Epoch 10100/20000: Train Loss = 0.439674, Test Loss = 0.252388, Learning Rate = 2.153500e-05\n",
      "Epoch 10101/20000: Train Loss = 0.439812, Test Loss = 0.255820, Learning Rate = 2.152682e-05\n",
      "Epoch 10102/20000: Train Loss = 0.439652, Test Loss = 0.250718, Learning Rate = 2.151864e-05\n",
      "Epoch 10103/20000: Train Loss = 0.439825, Test Loss = 0.252802, Learning Rate = 2.151046e-05\n",
      "Epoch 10104/20000: Train Loss = 0.439672, Test Loss = 0.247303, Learning Rate = 2.150229e-05\n",
      "Epoch 10105/20000: Train Loss = 0.439718, Test Loss = 0.252308, Learning Rate = 2.149412e-05\n",
      "Epoch 10106/20000: Train Loss = 0.439650, Test Loss = 0.252696, Learning Rate = 2.148595e-05\n",
      "Epoch 10107/20000: Train Loss = 0.440003, Test Loss = 0.251057, Learning Rate = 2.147779e-05\n",
      "Epoch 10108/20000: Train Loss = 0.439972, Test Loss = 0.251453, Learning Rate = 2.146963e-05\n",
      "Epoch 10109/20000: Train Loss = 0.439814, Test Loss = 0.249940, Learning Rate = 2.146147e-05\n",
      "Epoch 10110/20000: Train Loss = 0.439665, Test Loss = 0.248349, Learning Rate = 2.145331e-05\n",
      "Epoch 10111/20000: Train Loss = 0.439636, Test Loss = 0.250440, Learning Rate = 2.144516e-05\n",
      "Epoch 10112/20000: Train Loss = 0.439821, Test Loss = 0.252112, Learning Rate = 2.143701e-05\n",
      "Epoch 10113/20000: Train Loss = 0.439577, Test Loss = 0.250221, Learning Rate = 2.142887e-05\n",
      "Epoch 10114/20000: Train Loss = 0.439632, Test Loss = 0.252161, Learning Rate = 2.142073e-05\n",
      "Epoch 10115/20000: Train Loss = 0.439463, Test Loss = 0.247960, Learning Rate = 2.141259e-05\n",
      "Epoch 10116/20000: Train Loss = 0.439701, Test Loss = 0.245396, Learning Rate = 2.140445e-05\n",
      "Epoch 10117/20000: Train Loss = 0.439546, Test Loss = 0.248625, Learning Rate = 2.139632e-05\n",
      "Epoch 10118/20000: Train Loss = 0.439552, Test Loss = 0.249225, Learning Rate = 2.138819e-05\n",
      "Epoch 10119/20000: Train Loss = 0.439809, Test Loss = 0.250380, Learning Rate = 2.138006e-05\n",
      "Epoch 10120/20000: Train Loss = 0.439552, Test Loss = 0.253530, Learning Rate = 2.137194e-05\n",
      "Epoch 10121/20000: Train Loss = 0.439689, Test Loss = 0.245895, Learning Rate = 2.136382e-05\n",
      "Epoch 10122/20000: Train Loss = 0.440467, Test Loss = 0.252961, Learning Rate = 2.135570e-05\n",
      "Epoch 10123/20000: Train Loss = 0.439718, Test Loss = 0.246908, Learning Rate = 2.134758e-05\n",
      "Epoch 10124/20000: Train Loss = 0.439712, Test Loss = 0.248433, Learning Rate = 2.133947e-05\n",
      "Epoch 10125/20000: Train Loss = 0.439684, Test Loss = 0.254576, Learning Rate = 2.133136e-05\n",
      "Epoch 10126/20000: Train Loss = 0.439844, Test Loss = 0.254750, Learning Rate = 2.132326e-05\n",
      "Epoch 10127/20000: Train Loss = 0.439622, Test Loss = 0.251308, Learning Rate = 2.131516e-05\n",
      "Epoch 10128/20000: Train Loss = 0.439691, Test Loss = 0.253491, Learning Rate = 2.130706e-05\n",
      "Epoch 10129/20000: Train Loss = 0.439860, Test Loss = 0.252859, Learning Rate = 2.129896e-05\n",
      "Epoch 10130/20000: Train Loss = 0.439816, Test Loss = 0.251398, Learning Rate = 2.129087e-05\n",
      "Epoch 10131/20000: Train Loss = 0.439605, Test Loss = 0.253339, Learning Rate = 2.128278e-05\n",
      "Epoch 10132/20000: Train Loss = 0.439913, Test Loss = 0.255211, Learning Rate = 2.127469e-05\n",
      "Epoch 10133/20000: Train Loss = 0.439878, Test Loss = 0.257665, Learning Rate = 2.126661e-05\n",
      "Epoch 10134/20000: Train Loss = 0.439902, Test Loss = 0.252555, Learning Rate = 2.125853e-05\n",
      "Epoch 10135/20000: Train Loss = 0.439690, Test Loss = 0.250161, Learning Rate = 2.125045e-05\n",
      "Epoch 10136/20000: Train Loss = 0.439678, Test Loss = 0.256925, Learning Rate = 2.124237e-05\n",
      "Epoch 10137/20000: Train Loss = 0.439995, Test Loss = 0.251808, Learning Rate = 2.123430e-05\n",
      "Epoch 10138/20000: Train Loss = 0.439700, Test Loss = 0.247394, Learning Rate = 2.122623e-05\n",
      "Epoch 10139/20000: Train Loss = 0.439767, Test Loss = 0.253131, Learning Rate = 2.121817e-05\n",
      "Epoch 10140/20000: Train Loss = 0.439619, Test Loss = 0.254681, Learning Rate = 2.121011e-05\n",
      "Epoch 10141/20000: Train Loss = 0.439566, Test Loss = 0.252056, Learning Rate = 2.120205e-05\n",
      "Epoch 10142/20000: Train Loss = 0.439607, Test Loss = 0.253706, Learning Rate = 2.119399e-05\n",
      "Epoch 10143/20000: Train Loss = 0.439558, Test Loss = 0.250994, Learning Rate = 2.118594e-05\n",
      "Epoch 10144/20000: Train Loss = 0.439995, Test Loss = 0.250596, Learning Rate = 2.117789e-05\n",
      "Epoch 10145/20000: Train Loss = 0.440023, Test Loss = 0.245334, Learning Rate = 2.116984e-05\n",
      "Epoch 10146/20000: Train Loss = 0.439774, Test Loss = 0.249101, Learning Rate = 2.116180e-05\n",
      "Epoch 10147/20000: Train Loss = 0.439930, Test Loss = 0.245120, Learning Rate = 2.115376e-05\n",
      "Epoch 10148/20000: Train Loss = 0.439821, Test Loss = 0.245729, Learning Rate = 2.114572e-05\n",
      "Epoch 10149/20000: Train Loss = 0.440218, Test Loss = 0.246851, Learning Rate = 2.113768e-05\n",
      "Epoch 10150/20000: Train Loss = 0.440249, Test Loss = 0.251256, Learning Rate = 2.112965e-05\n",
      "Epoch 10151/20000: Train Loss = 0.439592, Test Loss = 0.250001, Learning Rate = 2.112162e-05\n",
      "Epoch 10152/20000: Train Loss = 0.439717, Test Loss = 0.250136, Learning Rate = 2.111360e-05\n",
      "Epoch 10153/20000: Train Loss = 0.439546, Test Loss = 0.251856, Learning Rate = 2.110557e-05\n",
      "Epoch 10154/20000: Train Loss = 0.439560, Test Loss = 0.250770, Learning Rate = 2.109755e-05\n",
      "Epoch 10155/20000: Train Loss = 0.439698, Test Loss = 0.249078, Learning Rate = 2.108954e-05\n",
      "Epoch 10156/20000: Train Loss = 0.439608, Test Loss = 0.252164, Learning Rate = 2.108152e-05\n",
      "Epoch 10157/20000: Train Loss = 0.439881, Test Loss = 0.251614, Learning Rate = 2.107351e-05\n",
      "Epoch 10158/20000: Train Loss = 0.439931, Test Loss = 0.252559, Learning Rate = 2.106551e-05\n",
      "Epoch 10159/20000: Train Loss = 0.439593, Test Loss = 0.253077, Learning Rate = 2.105750e-05\n",
      "Epoch 10160/20000: Train Loss = 0.439555, Test Loss = 0.253584, Learning Rate = 2.104950e-05\n",
      "Epoch 10161/20000: Train Loss = 0.439854, Test Loss = 0.252765, Learning Rate = 2.104150e-05\n",
      "Epoch 10162/20000: Train Loss = 0.439877, Test Loss = 0.251844, Learning Rate = 2.103351e-05\n",
      "Epoch 10163/20000: Train Loss = 0.439868, Test Loss = 0.250509, Learning Rate = 2.102552e-05\n",
      "Epoch 10164/20000: Train Loss = 0.439698, Test Loss = 0.243679, Learning Rate = 2.101753e-05\n",
      "Epoch 10165/20000: Train Loss = 0.439954, Test Loss = 0.249630, Learning Rate = 2.100954e-05\n",
      "Epoch 10166/20000: Train Loss = 0.439511, Test Loss = 0.247106, Learning Rate = 2.100156e-05\n",
      "Epoch 10167/20000: Train Loss = 0.439982, Test Loss = 0.244580, Learning Rate = 2.099358e-05\n",
      "Epoch 10168/20000: Train Loss = 0.439567, Test Loss = 0.246948, Learning Rate = 2.098560e-05\n",
      "Epoch 10169/20000: Train Loss = 0.439529, Test Loss = 0.249451, Learning Rate = 2.097763e-05\n",
      "Epoch 10170/20000: Train Loss = 0.440050, Test Loss = 0.251679, Learning Rate = 2.096966e-05\n",
      "Epoch 10171/20000: Train Loss = 0.439524, Test Loss = 0.247082, Learning Rate = 2.096169e-05\n",
      "Epoch 10172/20000: Train Loss = 0.440277, Test Loss = 0.246814, Learning Rate = 2.095372e-05\n",
      "Epoch 10173/20000: Train Loss = 0.439925, Test Loss = 0.247690, Learning Rate = 2.094576e-05\n",
      "Epoch 10174/20000: Train Loss = 0.439867, Test Loss = 0.248342, Learning Rate = 2.093780e-05\n",
      "Epoch 10175/20000: Train Loss = 0.440065, Test Loss = 0.250498, Learning Rate = 2.092985e-05\n",
      "Epoch 10176/20000: Train Loss = 0.439606, Test Loss = 0.248323, Learning Rate = 2.092189e-05\n",
      "Epoch 10177/20000: Train Loss = 0.439950, Test Loss = 0.248208, Learning Rate = 2.091394e-05\n",
      "Epoch 10178/20000: Train Loss = 0.440150, Test Loss = 0.247577, Learning Rate = 2.090600e-05\n",
      "Epoch 10179/20000: Train Loss = 0.439628, Test Loss = 0.250116, Learning Rate = 2.089805e-05\n",
      "Epoch 10180/20000: Train Loss = 0.439693, Test Loss = 0.245353, Learning Rate = 2.089011e-05\n",
      "Epoch 10181/20000: Train Loss = 0.439738, Test Loss = 0.250429, Learning Rate = 2.088218e-05\n",
      "Epoch 10182/20000: Train Loss = 0.439779, Test Loss = 0.248404, Learning Rate = 2.087424e-05\n",
      "Epoch 10183/20000: Train Loss = 0.439659, Test Loss = 0.248813, Learning Rate = 2.086631e-05\n",
      "Epoch 10184/20000: Train Loss = 0.439821, Test Loss = 0.248357, Learning Rate = 2.085838e-05\n",
      "Epoch 10185/20000: Train Loss = 0.439444, Test Loss = 0.252427, Learning Rate = 2.085045e-05\n",
      "Epoch 10186/20000: Train Loss = 0.439947, Test Loss = 0.255446, Learning Rate = 2.084253e-05\n",
      "Epoch 10187/20000: Train Loss = 0.439706, Test Loss = 0.253439, Learning Rate = 2.083461e-05\n",
      "Epoch 10188/20000: Train Loss = 0.439642, Test Loss = 0.253095, Learning Rate = 2.082670e-05\n",
      "Epoch 10189/20000: Train Loss = 0.439695, Test Loss = 0.249646, Learning Rate = 2.081878e-05\n",
      "Epoch 10190/20000: Train Loss = 0.439583, Test Loss = 0.249750, Learning Rate = 2.081087e-05\n",
      "Epoch 10191/20000: Train Loss = 0.439484, Test Loss = 0.248824, Learning Rate = 2.080296e-05\n",
      "Epoch 10192/20000: Train Loss = 0.439565, Test Loss = 0.251902, Learning Rate = 2.079506e-05\n",
      "Epoch 10193/20000: Train Loss = 0.439671, Test Loss = 0.250331, Learning Rate = 2.078716e-05\n",
      "Epoch 10194/20000: Train Loss = 0.439694, Test Loss = 0.249476, Learning Rate = 2.077926e-05\n",
      "Epoch 10195/20000: Train Loss = 0.439820, Test Loss = 0.252859, Learning Rate = 2.077136e-05\n",
      "Epoch 10196/20000: Train Loss = 0.440209, Test Loss = 0.247399, Learning Rate = 2.076347e-05\n",
      "Epoch 10197/20000: Train Loss = 0.439563, Test Loss = 0.252539, Learning Rate = 2.075558e-05\n",
      "Epoch 10198/20000: Train Loss = 0.439721, Test Loss = 0.251568, Learning Rate = 2.074770e-05\n",
      "Epoch 10199/20000: Train Loss = 0.439615, Test Loss = 0.252252, Learning Rate = 2.073981e-05\n",
      "Epoch 10200/20000: Train Loss = 0.439872, Test Loss = 0.253853, Learning Rate = 2.073193e-05\n",
      "Epoch 10201/20000: Train Loss = 0.439690, Test Loss = 0.249622, Learning Rate = 2.072405e-05\n",
      "Epoch 10202/20000: Train Loss = 0.439664, Test Loss = 0.249619, Learning Rate = 2.071618e-05\n",
      "Epoch 10203/20000: Train Loss = 0.439580, Test Loss = 0.251371, Learning Rate = 2.070831e-05\n",
      "Epoch 10204/20000: Train Loss = 0.439819, Test Loss = 0.250513, Learning Rate = 2.070044e-05\n",
      "Epoch 10205/20000: Train Loss = 0.439639, Test Loss = 0.247552, Learning Rate = 2.069257e-05\n",
      "Epoch 10206/20000: Train Loss = 0.439569, Test Loss = 0.249489, Learning Rate = 2.068471e-05\n",
      "Epoch 10207/20000: Train Loss = 0.439964, Test Loss = 0.249740, Learning Rate = 2.067685e-05\n",
      "Epoch 10208/20000: Train Loss = 0.440769, Test Loss = 0.256056, Learning Rate = 2.066899e-05\n",
      "Epoch 10209/20000: Train Loss = 0.439655, Test Loss = 0.249309, Learning Rate = 2.066114e-05\n",
      "Epoch 10210/20000: Train Loss = 0.439666, Test Loss = 0.246894, Learning Rate = 2.065329e-05\n",
      "Epoch 10211/20000: Train Loss = 0.439674, Test Loss = 0.247631, Learning Rate = 2.064544e-05\n",
      "Epoch 10212/20000: Train Loss = 0.439592, Test Loss = 0.248491, Learning Rate = 2.063760e-05\n",
      "Epoch 10213/20000: Train Loss = 0.440226, Test Loss = 0.249756, Learning Rate = 2.062976e-05\n",
      "Epoch 10214/20000: Train Loss = 0.439790, Test Loss = 0.250700, Learning Rate = 2.062192e-05\n",
      "Epoch 10215/20000: Train Loss = 0.439607, Test Loss = 0.253499, Learning Rate = 2.061408e-05\n",
      "Epoch 10216/20000: Train Loss = 0.439639, Test Loss = 0.250568, Learning Rate = 2.060625e-05\n",
      "Epoch 10217/20000: Train Loss = 0.439706, Test Loss = 0.251304, Learning Rate = 2.059842e-05\n",
      "Epoch 10218/20000: Train Loss = 0.439816, Test Loss = 0.253835, Learning Rate = 2.059059e-05\n",
      "Epoch 10219/20000: Train Loss = 0.439768, Test Loss = 0.254453, Learning Rate = 2.058277e-05\n",
      "Epoch 10220/20000: Train Loss = 0.439939, Test Loss = 0.251492, Learning Rate = 2.057495e-05\n",
      "Epoch 10221/20000: Train Loss = 0.439606, Test Loss = 0.252463, Learning Rate = 2.056713e-05\n",
      "Epoch 10222/20000: Train Loss = 0.439976, Test Loss = 0.251305, Learning Rate = 2.055931e-05\n",
      "Epoch 10223/20000: Train Loss = 0.439659, Test Loss = 0.250323, Learning Rate = 2.055150e-05\n",
      "Epoch 10224/20000: Train Loss = 0.439622, Test Loss = 0.252292, Learning Rate = 2.054369e-05\n",
      "Epoch 10225/20000: Train Loss = 0.439870, Test Loss = 0.252567, Learning Rate = 2.053589e-05\n",
      "Epoch 10226/20000: Train Loss = 0.439678, Test Loss = 0.250054, Learning Rate = 2.052808e-05\n",
      "Epoch 10227/20000: Train Loss = 0.439692, Test Loss = 0.247700, Learning Rate = 2.052028e-05\n",
      "Epoch 10228/20000: Train Loss = 0.439661, Test Loss = 0.250811, Learning Rate = 2.051249e-05\n",
      "Epoch 10229/20000: Train Loss = 0.439456, Test Loss = 0.249208, Learning Rate = 2.050469e-05\n",
      "Epoch 10230/20000: Train Loss = 0.439633, Test Loss = 0.253035, Learning Rate = 2.049690e-05\n",
      "Epoch 10231/20000: Train Loss = 0.439579, Test Loss = 0.253318, Learning Rate = 2.048911e-05\n",
      "Epoch 10232/20000: Train Loss = 0.439474, Test Loss = 0.251570, Learning Rate = 2.048133e-05\n",
      "Epoch 10233/20000: Train Loss = 0.439763, Test Loss = 0.253933, Learning Rate = 2.047355e-05\n",
      "Epoch 10234/20000: Train Loss = 0.439831, Test Loss = 0.253472, Learning Rate = 2.046577e-05\n",
      "Epoch 10235/20000: Train Loss = 0.439683, Test Loss = 0.250735, Learning Rate = 2.045799e-05\n",
      "Epoch 10236/20000: Train Loss = 0.439813, Test Loss = 0.251597, Learning Rate = 2.045022e-05\n",
      "Epoch 10237/20000: Train Loss = 0.439491, Test Loss = 0.249419, Learning Rate = 2.044245e-05\n",
      "Epoch 10238/20000: Train Loss = 0.439456, Test Loss = 0.248209, Learning Rate = 2.043468e-05\n",
      "Epoch 10239/20000: Train Loss = 0.439792, Test Loss = 0.249018, Learning Rate = 2.042691e-05\n",
      "Epoch 10240/20000: Train Loss = 0.440935, Test Loss = 0.246844, Learning Rate = 2.041915e-05\n",
      "Epoch 10241/20000: Train Loss = 0.439647, Test Loss = 0.248027, Learning Rate = 2.041139e-05\n",
      "Epoch 10242/20000: Train Loss = 0.439591, Test Loss = 0.251763, Learning Rate = 2.040364e-05\n",
      "Epoch 10243/20000: Train Loss = 0.439654, Test Loss = 0.250943, Learning Rate = 2.039588e-05\n",
      "Epoch 10244/20000: Train Loss = 0.439682, Test Loss = 0.249111, Learning Rate = 2.038813e-05\n",
      "Epoch 10245/20000: Train Loss = 0.439505, Test Loss = 0.247727, Learning Rate = 2.038039e-05\n",
      "Epoch 10246/20000: Train Loss = 0.439646, Test Loss = 0.246654, Learning Rate = 2.037264e-05\n",
      "Epoch 10247/20000: Train Loss = 0.439894, Test Loss = 0.251342, Learning Rate = 2.036490e-05\n",
      "Epoch 10248/20000: Train Loss = 0.439681, Test Loss = 0.246802, Learning Rate = 2.035716e-05\n",
      "Epoch 10249/20000: Train Loss = 0.439792, Test Loss = 0.243374, Learning Rate = 2.034943e-05\n",
      "Epoch 10250/20000: Train Loss = 0.439791, Test Loss = 0.252715, Learning Rate = 2.034170e-05\n",
      "Epoch 10251/20000: Train Loss = 0.439559, Test Loss = 0.248533, Learning Rate = 2.033397e-05\n",
      "Epoch 10252/20000: Train Loss = 0.439739, Test Loss = 0.248151, Learning Rate = 2.032624e-05\n",
      "Epoch 10253/20000: Train Loss = 0.440266, Test Loss = 0.245351, Learning Rate = 2.031852e-05\n",
      "Epoch 10254/20000: Train Loss = 0.439892, Test Loss = 0.244395, Learning Rate = 2.031080e-05\n",
      "Epoch 10255/20000: Train Loss = 0.439596, Test Loss = 0.247067, Learning Rate = 2.030308e-05\n",
      "Epoch 10256/20000: Train Loss = 0.439716, Test Loss = 0.247189, Learning Rate = 2.029537e-05\n",
      "Epoch 10257/20000: Train Loss = 0.439562, Test Loss = 0.246773, Learning Rate = 2.028765e-05\n",
      "Epoch 10258/20000: Train Loss = 0.439551, Test Loss = 0.246019, Learning Rate = 2.027994e-05\n",
      "Epoch 10259/20000: Train Loss = 0.440060, Test Loss = 0.243407, Learning Rate = 2.027224e-05\n",
      "Epoch 10260/20000: Train Loss = 0.439869, Test Loss = 0.252438, Learning Rate = 2.026454e-05\n",
      "Epoch 10261/20000: Train Loss = 0.439338, Test Loss = 0.245229, Learning Rate = 2.025684e-05\n",
      "Epoch 10262/20000: Train Loss = 0.439602, Test Loss = 0.247388, Learning Rate = 2.024914e-05\n",
      "Epoch 10263/20000: Train Loss = 0.439577, Test Loss = 0.248543, Learning Rate = 2.024144e-05\n",
      "Epoch 10264/20000: Train Loss = 0.439700, Test Loss = 0.248177, Learning Rate = 2.023375e-05\n",
      "Epoch 10265/20000: Train Loss = 0.439678, Test Loss = 0.245573, Learning Rate = 2.022607e-05\n",
      "Epoch 10266/20000: Train Loss = 0.439535, Test Loss = 0.252258, Learning Rate = 2.021838e-05\n",
      "Epoch 10267/20000: Train Loss = 0.439804, Test Loss = 0.249645, Learning Rate = 2.021070e-05\n",
      "Epoch 10268/20000: Train Loss = 0.439727, Test Loss = 0.253600, Learning Rate = 2.020302e-05\n",
      "Epoch 10269/20000: Train Loss = 0.440147, Test Loss = 0.257911, Learning Rate = 2.019534e-05\n",
      "Epoch 10270/20000: Train Loss = 0.439888, Test Loss = 0.255820, Learning Rate = 2.018767e-05\n",
      "Epoch 10271/20000: Train Loss = 0.439615, Test Loss = 0.253957, Learning Rate = 2.018000e-05\n",
      "Epoch 10272/20000: Train Loss = 0.439724, Test Loss = 0.250950, Learning Rate = 2.017233e-05\n",
      "Epoch 10273/20000: Train Loss = 0.439740, Test Loss = 0.253846, Learning Rate = 2.016466e-05\n",
      "Epoch 10274/20000: Train Loss = 0.439681, Test Loss = 0.250816, Learning Rate = 2.015700e-05\n",
      "Epoch 10275/20000: Train Loss = 0.439797, Test Loss = 0.252061, Learning Rate = 2.014934e-05\n",
      "Epoch 10276/20000: Train Loss = 0.439809, Test Loss = 0.251924, Learning Rate = 2.014169e-05\n",
      "Epoch 10277/20000: Train Loss = 0.439709, Test Loss = 0.250650, Learning Rate = 2.013403e-05\n",
      "Epoch 10278/20000: Train Loss = 0.439832, Test Loss = 0.249090, Learning Rate = 2.012638e-05\n",
      "Epoch 10279/20000: Train Loss = 0.440055, Test Loss = 0.250991, Learning Rate = 2.011874e-05\n",
      "Epoch 10280/20000: Train Loss = 0.439601, Test Loss = 0.247919, Learning Rate = 2.011109e-05\n",
      "Epoch 10281/20000: Train Loss = 0.439731, Test Loss = 0.248647, Learning Rate = 2.010345e-05\n",
      "Epoch 10282/20000: Train Loss = 0.439778, Test Loss = 0.256481, Learning Rate = 2.009581e-05\n",
      "Epoch 10283/20000: Train Loss = 0.439617, Test Loss = 0.250393, Learning Rate = 2.008817e-05\n",
      "Epoch 10284/20000: Train Loss = 0.440249, Test Loss = 0.247105, Learning Rate = 2.008054e-05\n",
      "Epoch 10285/20000: Train Loss = 0.439600, Test Loss = 0.244605, Learning Rate = 2.007291e-05\n",
      "Epoch 10286/20000: Train Loss = 0.439801, Test Loss = 0.248560, Learning Rate = 2.006528e-05\n",
      "Epoch 10287/20000: Train Loss = 0.439694, Test Loss = 0.254495, Learning Rate = 2.005766e-05\n",
      "Epoch 10288/20000: Train Loss = 0.439860, Test Loss = 0.251388, Learning Rate = 2.005004e-05\n",
      "Epoch 10289/20000: Train Loss = 0.439875, Test Loss = 0.251460, Learning Rate = 2.004242e-05\n",
      "Epoch 10290/20000: Train Loss = 0.439621, Test Loss = 0.251245, Learning Rate = 2.003480e-05\n",
      "Epoch 10291/20000: Train Loss = 0.439637, Test Loss = 0.249821, Learning Rate = 2.002719e-05\n",
      "Epoch 10292/20000: Train Loss = 0.440109, Test Loss = 0.250144, Learning Rate = 2.001958e-05\n",
      "Epoch 10293/20000: Train Loss = 0.439637, Test Loss = 0.248885, Learning Rate = 2.001198e-05\n",
      "Epoch 10294/20000: Train Loss = 0.439747, Test Loss = 0.250335, Learning Rate = 2.000437e-05\n",
      "Epoch 10295/20000: Train Loss = 0.439649, Test Loss = 0.250843, Learning Rate = 1.999677e-05\n",
      "Epoch 10296/20000: Train Loss = 0.439488, Test Loss = 0.248467, Learning Rate = 1.998917e-05\n",
      "Epoch 10297/20000: Train Loss = 0.439709, Test Loss = 0.245870, Learning Rate = 1.998158e-05\n",
      "Epoch 10298/20000: Train Loss = 0.439538, Test Loss = 0.247462, Learning Rate = 1.997398e-05\n",
      "Epoch 10299/20000: Train Loss = 0.439627, Test Loss = 0.248889, Learning Rate = 1.996639e-05\n",
      "Epoch 10300/20000: Train Loss = 0.439851, Test Loss = 0.252409, Learning Rate = 1.995881e-05\n",
      "Epoch 10301/20000: Train Loss = 0.439750, Test Loss = 0.247209, Learning Rate = 1.995122e-05\n",
      "Epoch 10302/20000: Train Loss = 0.439961, Test Loss = 0.248494, Learning Rate = 1.994364e-05\n",
      "Epoch 10303/20000: Train Loss = 0.440951, Test Loss = 0.246782, Learning Rate = 1.993607e-05\n",
      "Epoch 10304/20000: Train Loss = 0.439516, Test Loss = 0.253738, Learning Rate = 1.992849e-05\n",
      "Epoch 10305/20000: Train Loss = 0.439830, Test Loss = 0.249862, Learning Rate = 1.992092e-05\n",
      "Epoch 10306/20000: Train Loss = 0.439808, Test Loss = 0.251195, Learning Rate = 1.991335e-05\n",
      "Epoch 10307/20000: Train Loss = 0.439970, Test Loss = 0.248216, Learning Rate = 1.990578e-05\n",
      "Epoch 10308/20000: Train Loss = 0.440163, Test Loss = 0.245774, Learning Rate = 1.989822e-05\n",
      "Epoch 10309/20000: Train Loss = 0.439434, Test Loss = 0.253490, Learning Rate = 1.989066e-05\n",
      "Epoch 10310/20000: Train Loss = 0.439974, Test Loss = 0.252879, Learning Rate = 1.988310e-05\n",
      "Epoch 10311/20000: Train Loss = 0.439581, Test Loss = 0.247902, Learning Rate = 1.987554e-05\n",
      "Epoch 10312/20000: Train Loss = 0.440706, Test Loss = 0.254076, Learning Rate = 1.986799e-05\n",
      "Epoch 10313/20000: Train Loss = 0.439661, Test Loss = 0.254793, Learning Rate = 1.986044e-05\n",
      "Epoch 10314/20000: Train Loss = 0.439698, Test Loss = 0.251814, Learning Rate = 1.985290e-05\n",
      "Epoch 10315/20000: Train Loss = 0.439800, Test Loss = 0.253754, Learning Rate = 1.984535e-05\n",
      "Epoch 10316/20000: Train Loss = 0.439987, Test Loss = 0.250225, Learning Rate = 1.983781e-05\n",
      "Epoch 10317/20000: Train Loss = 0.439592, Test Loss = 0.247119, Learning Rate = 1.983027e-05\n",
      "Epoch 10318/20000: Train Loss = 0.439641, Test Loss = 0.248213, Learning Rate = 1.982274e-05\n",
      "Epoch 10319/20000: Train Loss = 0.439717, Test Loss = 0.251004, Learning Rate = 1.981521e-05\n",
      "Epoch 10320/20000: Train Loss = 0.439617, Test Loss = 0.248602, Learning Rate = 1.980768e-05\n",
      "Epoch 10321/20000: Train Loss = 0.439532, Test Loss = 0.251298, Learning Rate = 1.980015e-05\n",
      "Epoch 10322/20000: Train Loss = 0.439823, Test Loss = 0.250025, Learning Rate = 1.979263e-05\n",
      "Epoch 10323/20000: Train Loss = 0.439615, Test Loss = 0.253083, Learning Rate = 1.978511e-05\n",
      "Epoch 10324/20000: Train Loss = 0.439827, Test Loss = 0.251361, Learning Rate = 1.977759e-05\n",
      "Epoch 10325/20000: Train Loss = 0.439660, Test Loss = 0.250010, Learning Rate = 1.977007e-05\n",
      "Epoch 10326/20000: Train Loss = 0.439853, Test Loss = 0.250953, Learning Rate = 1.976256e-05\n",
      "Epoch 10327/20000: Train Loss = 0.439627, Test Loss = 0.249396, Learning Rate = 1.975505e-05\n",
      "Epoch 10328/20000: Train Loss = 0.439935, Test Loss = 0.248569, Learning Rate = 1.974755e-05\n",
      "Epoch 10329/20000: Train Loss = 0.439549, Test Loss = 0.249256, Learning Rate = 1.974004e-05\n",
      "Epoch 10330/20000: Train Loss = 0.439714, Test Loss = 0.246810, Learning Rate = 1.973254e-05\n",
      "Epoch 10331/20000: Train Loss = 0.439774, Test Loss = 0.250849, Learning Rate = 1.972505e-05\n",
      "Epoch 10332/20000: Train Loss = 0.439705, Test Loss = 0.251763, Learning Rate = 1.971755e-05\n",
      "Epoch 10333/20000: Train Loss = 0.439622, Test Loss = 0.252229, Learning Rate = 1.971006e-05\n",
      "Epoch 10334/20000: Train Loss = 0.439930, Test Loss = 0.255353, Learning Rate = 1.970257e-05\n",
      "Epoch 10335/20000: Train Loss = 0.440292, Test Loss = 0.248809, Learning Rate = 1.969508e-05\n",
      "Epoch 10336/20000: Train Loss = 0.439629, Test Loss = 0.250679, Learning Rate = 1.968760e-05\n",
      "Epoch 10337/20000: Train Loss = 0.440158, Test Loss = 0.251745, Learning Rate = 1.968012e-05\n",
      "Epoch 10338/20000: Train Loss = 0.439604, Test Loss = 0.247673, Learning Rate = 1.967264e-05\n",
      "Epoch 10339/20000: Train Loss = 0.439783, Test Loss = 0.253656, Learning Rate = 1.966517e-05\n",
      "Epoch 10340/20000: Train Loss = 0.439668, Test Loss = 0.247378, Learning Rate = 1.965769e-05\n",
      "Epoch 10341/20000: Train Loss = 0.439437, Test Loss = 0.248837, Learning Rate = 1.965022e-05\n",
      "Epoch 10342/20000: Train Loss = 0.439770, Test Loss = 0.252743, Learning Rate = 1.964276e-05\n",
      "Epoch 10343/20000: Train Loss = 0.439942, Test Loss = 0.249371, Learning Rate = 1.963529e-05\n",
      "Epoch 10344/20000: Train Loss = 0.439828, Test Loss = 0.252766, Learning Rate = 1.962783e-05\n",
      "Epoch 10345/20000: Train Loss = 0.439763, Test Loss = 0.247606, Learning Rate = 1.962037e-05\n",
      "Epoch 10346/20000: Train Loss = 0.439400, Test Loss = 0.249732, Learning Rate = 1.961292e-05\n",
      "Epoch 10347/20000: Train Loss = 0.439634, Test Loss = 0.245706, Learning Rate = 1.960547e-05\n",
      "Epoch 10348/20000: Train Loss = 0.439531, Test Loss = 0.248988, Learning Rate = 1.959802e-05\n",
      "Epoch 10349/20000: Train Loss = 0.439814, Test Loss = 0.250552, Learning Rate = 1.959057e-05\n",
      "Epoch 10350/20000: Train Loss = 0.439667, Test Loss = 0.247551, Learning Rate = 1.958313e-05\n",
      "Epoch 10351/20000: Train Loss = 0.439501, Test Loss = 0.250447, Learning Rate = 1.957569e-05\n",
      "Epoch 10352/20000: Train Loss = 0.440231, Test Loss = 0.254110, Learning Rate = 1.956825e-05\n",
      "Epoch 10353/20000: Train Loss = 0.439795, Test Loss = 0.256941, Learning Rate = 1.956081e-05\n",
      "Epoch 10354/20000: Train Loss = 0.439736, Test Loss = 0.248815, Learning Rate = 1.955338e-05\n",
      "Epoch 10355/20000: Train Loss = 0.439624, Test Loss = 0.257573, Learning Rate = 1.954595e-05\n",
      "Epoch 10356/20000: Train Loss = 0.439666, Test Loss = 0.251397, Learning Rate = 1.953852e-05\n",
      "Epoch 10357/20000: Train Loss = 0.439585, Test Loss = 0.252434, Learning Rate = 1.953110e-05\n",
      "Epoch 10358/20000: Train Loss = 0.439521, Test Loss = 0.251934, Learning Rate = 1.952368e-05\n",
      "Epoch 10359/20000: Train Loss = 0.439491, Test Loss = 0.253466, Learning Rate = 1.951626e-05\n",
      "Epoch 10360/20000: Train Loss = 0.440046, Test Loss = 0.249106, Learning Rate = 1.950884e-05\n",
      "Epoch 10361/20000: Train Loss = 0.439561, Test Loss = 0.253502, Learning Rate = 1.950143e-05\n",
      "Epoch 10362/20000: Train Loss = 0.440100, Test Loss = 0.251061, Learning Rate = 1.949402e-05\n",
      "Epoch 10363/20000: Train Loss = 0.439683, Test Loss = 0.254953, Learning Rate = 1.948661e-05\n",
      "Epoch 10364/20000: Train Loss = 0.439747, Test Loss = 0.248777, Learning Rate = 1.947921e-05\n",
      "Epoch 10365/20000: Train Loss = 0.439847, Test Loss = 0.251641, Learning Rate = 1.947181e-05\n",
      "Epoch 10366/20000: Train Loss = 0.439564, Test Loss = 0.249289, Learning Rate = 1.946441e-05\n",
      "Epoch 10367/20000: Train Loss = 0.439507, Test Loss = 0.246873, Learning Rate = 1.945701e-05\n",
      "Epoch 10368/20000: Train Loss = 0.439576, Test Loss = 0.248717, Learning Rate = 1.944962e-05\n",
      "Epoch 10369/20000: Train Loss = 0.440169, Test Loss = 0.247153, Learning Rate = 1.944223e-05\n",
      "Epoch 10370/20000: Train Loss = 0.439774, Test Loss = 0.250032, Learning Rate = 1.943484e-05\n",
      "Epoch 10371/20000: Train Loss = 0.439644, Test Loss = 0.247085, Learning Rate = 1.942746e-05\n",
      "Epoch 10372/20000: Train Loss = 0.439716, Test Loss = 0.243431, Learning Rate = 1.942007e-05\n",
      "Epoch 10373/20000: Train Loss = 0.439701, Test Loss = 0.246102, Learning Rate = 1.941270e-05\n",
      "Epoch 10374/20000: Train Loss = 0.439769, Test Loss = 0.243928, Learning Rate = 1.940532e-05\n",
      "Epoch 10375/20000: Train Loss = 0.439381, Test Loss = 0.247696, Learning Rate = 1.939795e-05\n",
      "Epoch 10376/20000: Train Loss = 0.440442, Test Loss = 0.243638, Learning Rate = 1.939058e-05\n",
      "Epoch 10377/20000: Train Loss = 0.439798, Test Loss = 0.254390, Learning Rate = 1.938321e-05\n",
      "Epoch 10378/20000: Train Loss = 0.439521, Test Loss = 0.249090, Learning Rate = 1.937584e-05\n",
      "Epoch 10379/20000: Train Loss = 0.439600, Test Loss = 0.251827, Learning Rate = 1.936848e-05\n",
      "Epoch 10380/20000: Train Loss = 0.440268, Test Loss = 0.250951, Learning Rate = 1.936112e-05\n",
      "Epoch 10381/20000: Train Loss = 0.439658, Test Loss = 0.246045, Learning Rate = 1.935376e-05\n",
      "Epoch 10382/20000: Train Loss = 0.439604, Test Loss = 0.253353, Learning Rate = 1.934641e-05\n",
      "Epoch 10383/20000: Train Loss = 0.439652, Test Loss = 0.248676, Learning Rate = 1.933906e-05\n",
      "Epoch 10384/20000: Train Loss = 0.439670, Test Loss = 0.250120, Learning Rate = 1.933171e-05\n",
      "Epoch 10385/20000: Train Loss = 0.439600, Test Loss = 0.249071, Learning Rate = 1.932436e-05\n",
      "Epoch 10386/20000: Train Loss = 0.439465, Test Loss = 0.253220, Learning Rate = 1.931702e-05\n",
      "Epoch 10387/20000: Train Loss = 0.439517, Test Loss = 0.249522, Learning Rate = 1.930968e-05\n",
      "Epoch 10388/20000: Train Loss = 0.439581, Test Loss = 0.252483, Learning Rate = 1.930234e-05\n",
      "Epoch 10389/20000: Train Loss = 0.439539, Test Loss = 0.251859, Learning Rate = 1.929501e-05\n",
      "Epoch 10390/20000: Train Loss = 0.439572, Test Loss = 0.251538, Learning Rate = 1.928768e-05\n",
      "Epoch 10391/20000: Train Loss = 0.439689, Test Loss = 0.251060, Learning Rate = 1.928035e-05\n",
      "Epoch 10392/20000: Train Loss = 0.439442, Test Loss = 0.249431, Learning Rate = 1.927302e-05\n",
      "Epoch 10393/20000: Train Loss = 0.439710, Test Loss = 0.249294, Learning Rate = 1.926570e-05\n",
      "Epoch 10394/20000: Train Loss = 0.439475, Test Loss = 0.252912, Learning Rate = 1.925838e-05\n",
      "Epoch 10395/20000: Train Loss = 0.439714, Test Loss = 0.253617, Learning Rate = 1.925106e-05\n",
      "Epoch 10396/20000: Train Loss = 0.439442, Test Loss = 0.250612, Learning Rate = 1.924375e-05\n",
      "Epoch 10397/20000: Train Loss = 0.439874, Test Loss = 0.254501, Learning Rate = 1.923644e-05\n",
      "Epoch 10398/20000: Train Loss = 0.439602, Test Loss = 0.252564, Learning Rate = 1.922913e-05\n",
      "Epoch 10399/20000: Train Loss = 0.439527, Test Loss = 0.255665, Learning Rate = 1.922182e-05\n",
      "Epoch 10400/20000: Train Loss = 0.439745, Test Loss = 0.253717, Learning Rate = 1.921452e-05\n",
      "Epoch 10401/20000: Train Loss = 0.439609, Test Loss = 0.253546, Learning Rate = 1.920722e-05\n",
      "Epoch 10402/20000: Train Loss = 0.439496, Test Loss = 0.251109, Learning Rate = 1.919992e-05\n",
      "Epoch 10403/20000: Train Loss = 0.439696, Test Loss = 0.252442, Learning Rate = 1.919262e-05\n",
      "Epoch 10404/20000: Train Loss = 0.439600, Test Loss = 0.254340, Learning Rate = 1.918533e-05\n",
      "Epoch 10405/20000: Train Loss = 0.439494, Test Loss = 0.251261, Learning Rate = 1.917804e-05\n",
      "Epoch 10406/20000: Train Loss = 0.439498, Test Loss = 0.249401, Learning Rate = 1.917075e-05\n",
      "Epoch 10407/20000: Train Loss = 0.439638, Test Loss = 0.248870, Learning Rate = 1.916347e-05\n",
      "Epoch 10408/20000: Train Loss = 0.439610, Test Loss = 0.248362, Learning Rate = 1.915619e-05\n",
      "Epoch 10409/20000: Train Loss = 0.439894, Test Loss = 0.247073, Learning Rate = 1.914891e-05\n",
      "Epoch 10410/20000: Train Loss = 0.439790, Test Loss = 0.250770, Learning Rate = 1.914163e-05\n",
      "Epoch 10411/20000: Train Loss = 0.439845, Test Loss = 0.247995, Learning Rate = 1.913436e-05\n",
      "Epoch 10412/20000: Train Loss = 0.439814, Test Loss = 0.246724, Learning Rate = 1.912709e-05\n",
      "Epoch 10413/20000: Train Loss = 0.439699, Test Loss = 0.247193, Learning Rate = 1.911982e-05\n",
      "Epoch 10414/20000: Train Loss = 0.439797, Test Loss = 0.248998, Learning Rate = 1.911255e-05\n",
      "Epoch 10415/20000: Train Loss = 0.439658, Test Loss = 0.252772, Learning Rate = 1.910529e-05\n",
      "Epoch 10416/20000: Train Loss = 0.439554, Test Loss = 0.249721, Learning Rate = 1.909803e-05\n",
      "Epoch 10417/20000: Train Loss = 0.439774, Test Loss = 0.249221, Learning Rate = 1.909078e-05\n",
      "Epoch 10418/20000: Train Loss = 0.439459, Test Loss = 0.250164, Learning Rate = 1.908352e-05\n",
      "Epoch 10419/20000: Train Loss = 0.440025, Test Loss = 0.252464, Learning Rate = 1.907627e-05\n",
      "Epoch 10420/20000: Train Loss = 0.439738, Test Loss = 0.246378, Learning Rate = 1.906902e-05\n",
      "Epoch 10421/20000: Train Loss = 0.439850, Test Loss = 0.250127, Learning Rate = 1.906178e-05\n",
      "Epoch 10422/20000: Train Loss = 0.439564, Test Loss = 0.252847, Learning Rate = 1.905453e-05\n",
      "Epoch 10423/20000: Train Loss = 0.439544, Test Loss = 0.256558, Learning Rate = 1.904729e-05\n",
      "Epoch 10424/20000: Train Loss = 0.439770, Test Loss = 0.253813, Learning Rate = 1.904006e-05\n",
      "Epoch 10425/20000: Train Loss = 0.439928, Test Loss = 0.252725, Learning Rate = 1.903282e-05\n",
      "Epoch 10426/20000: Train Loss = 0.439764, Test Loss = 0.251695, Learning Rate = 1.902559e-05\n",
      "Epoch 10427/20000: Train Loss = 0.439551, Test Loss = 0.253544, Learning Rate = 1.901836e-05\n",
      "Epoch 10428/20000: Train Loss = 0.439521, Test Loss = 0.254510, Learning Rate = 1.901113e-05\n",
      "Epoch 10429/20000: Train Loss = 0.439589, Test Loss = 0.253242, Learning Rate = 1.900391e-05\n",
      "Epoch 10430/20000: Train Loss = 0.439512, Test Loss = 0.249147, Learning Rate = 1.899669e-05\n",
      "Epoch 10431/20000: Train Loss = 0.439826, Test Loss = 0.256871, Learning Rate = 1.898947e-05\n",
      "Epoch 10432/20000: Train Loss = 0.439520, Test Loss = 0.251132, Learning Rate = 1.898226e-05\n",
      "Epoch 10433/20000: Train Loss = 0.439496, Test Loss = 0.252112, Learning Rate = 1.897504e-05\n",
      "Epoch 10434/20000: Train Loss = 0.439563, Test Loss = 0.254103, Learning Rate = 1.896783e-05\n",
      "Epoch 10435/20000: Train Loss = 0.439540, Test Loss = 0.249261, Learning Rate = 1.896063e-05\n",
      "Epoch 10436/20000: Train Loss = 0.439510, Test Loss = 0.249605, Learning Rate = 1.895342e-05\n",
      "Epoch 10437/20000: Train Loss = 0.439563, Test Loss = 0.253099, Learning Rate = 1.894622e-05\n",
      "Epoch 10438/20000: Train Loss = 0.439585, Test Loss = 0.250452, Learning Rate = 1.893902e-05\n",
      "Epoch 10439/20000: Train Loss = 0.439671, Test Loss = 0.250479, Learning Rate = 1.893182e-05\n",
      "Epoch 10440/20000: Train Loss = 0.439699, Test Loss = 0.252431, Learning Rate = 1.892463e-05\n",
      "Epoch 10441/20000: Train Loss = 0.439688, Test Loss = 0.251645, Learning Rate = 1.891744e-05\n",
      "Epoch 10442/20000: Train Loss = 0.439846, Test Loss = 0.252711, Learning Rate = 1.891025e-05\n",
      "Epoch 10443/20000: Train Loss = 0.439557, Test Loss = 0.247694, Learning Rate = 1.890307e-05\n",
      "Epoch 10444/20000: Train Loss = 0.439650, Test Loss = 0.251206, Learning Rate = 1.889588e-05\n",
      "Epoch 10445/20000: Train Loss = 0.439683, Test Loss = 0.248591, Learning Rate = 1.888870e-05\n",
      "Epoch 10446/20000: Train Loss = 0.439449, Test Loss = 0.249715, Learning Rate = 1.888153e-05\n",
      "Epoch 10447/20000: Train Loss = 0.439473, Test Loss = 0.250411, Learning Rate = 1.887435e-05\n",
      "Epoch 10448/20000: Train Loss = 0.439841, Test Loss = 0.248195, Learning Rate = 1.886718e-05\n",
      "Epoch 10449/20000: Train Loss = 0.439542, Test Loss = 0.249118, Learning Rate = 1.886001e-05\n",
      "Epoch 10450/20000: Train Loss = 0.439906, Test Loss = 0.249125, Learning Rate = 1.885284e-05\n",
      "Epoch 10451/20000: Train Loss = 0.439797, Test Loss = 0.251334, Learning Rate = 1.884568e-05\n",
      "Epoch 10452/20000: Train Loss = 0.439570, Test Loss = 0.247986, Learning Rate = 1.883852e-05\n",
      "Epoch 10453/20000: Train Loss = 0.439671, Test Loss = 0.251967, Learning Rate = 1.883136e-05\n",
      "Epoch 10454/20000: Train Loss = 0.439686, Test Loss = 0.249301, Learning Rate = 1.882421e-05\n",
      "Epoch 10455/20000: Train Loss = 0.440462, Test Loss = 0.248003, Learning Rate = 1.881705e-05\n",
      "Epoch 10456/20000: Train Loss = 0.439628, Test Loss = 0.253779, Learning Rate = 1.880990e-05\n",
      "Epoch 10457/20000: Train Loss = 0.439793, Test Loss = 0.251968, Learning Rate = 1.880276e-05\n",
      "Epoch 10458/20000: Train Loss = 0.439540, Test Loss = 0.250125, Learning Rate = 1.879561e-05\n",
      "Epoch 10459/20000: Train Loss = 0.439755, Test Loss = 0.248967, Learning Rate = 1.878847e-05\n",
      "Epoch 10460/20000: Train Loss = 0.439503, Test Loss = 0.250842, Learning Rate = 1.878133e-05\n",
      "Epoch 10461/20000: Train Loss = 0.439583, Test Loss = 0.250296, Learning Rate = 1.877419e-05\n",
      "Epoch 10462/20000: Train Loss = 0.439634, Test Loss = 0.248886, Learning Rate = 1.876706e-05\n",
      "Epoch 10463/20000: Train Loss = 0.439695, Test Loss = 0.252556, Learning Rate = 1.875993e-05\n",
      "Epoch 10464/20000: Train Loss = 0.439812, Test Loss = 0.250242, Learning Rate = 1.875280e-05\n",
      "Epoch 10465/20000: Train Loss = 0.439718, Test Loss = 0.248154, Learning Rate = 1.874568e-05\n",
      "Epoch 10466/20000: Train Loss = 0.439661, Test Loss = 0.248259, Learning Rate = 1.873855e-05\n",
      "Epoch 10467/20000: Train Loss = 0.439991, Test Loss = 0.253764, Learning Rate = 1.873143e-05\n",
      "Epoch 10468/20000: Train Loss = 0.439722, Test Loss = 0.244959, Learning Rate = 1.872432e-05\n",
      "Epoch 10469/20000: Train Loss = 0.439786, Test Loss = 0.244426, Learning Rate = 1.871720e-05\n",
      "Epoch 10470/20000: Train Loss = 0.439531, Test Loss = 0.248507, Learning Rate = 1.871009e-05\n",
      "Epoch 10471/20000: Train Loss = 0.439655, Test Loss = 0.248962, Learning Rate = 1.870298e-05\n",
      "Epoch 10472/20000: Train Loss = 0.439690, Test Loss = 0.248559, Learning Rate = 1.869587e-05\n",
      "Epoch 10473/20000: Train Loss = 0.439512, Test Loss = 0.246931, Learning Rate = 1.868877e-05\n",
      "Epoch 10474/20000: Train Loss = 0.439568, Test Loss = 0.247641, Learning Rate = 1.868167e-05\n",
      "Epoch 10475/20000: Train Loss = 0.439705, Test Loss = 0.244505, Learning Rate = 1.867457e-05\n",
      "Epoch 10476/20000: Train Loss = 0.439694, Test Loss = 0.245788, Learning Rate = 1.866747e-05\n",
      "Epoch 10477/20000: Train Loss = 0.439899, Test Loss = 0.244886, Learning Rate = 1.866038e-05\n",
      "Epoch 10478/20000: Train Loss = 0.439484, Test Loss = 0.249804, Learning Rate = 1.865329e-05\n",
      "Epoch 10479/20000: Train Loss = 0.439688, Test Loss = 0.247543, Learning Rate = 1.864620e-05\n",
      "Epoch 10480/20000: Train Loss = 0.439668, Test Loss = 0.246116, Learning Rate = 1.863912e-05\n",
      "Epoch 10481/20000: Train Loss = 0.439730, Test Loss = 0.245295, Learning Rate = 1.863203e-05\n",
      "Epoch 10482/20000: Train Loss = 0.439806, Test Loss = 0.245056, Learning Rate = 1.862495e-05\n",
      "Epoch 10483/20000: Train Loss = 0.439548, Test Loss = 0.245639, Learning Rate = 1.861788e-05\n",
      "Epoch 10484/20000: Train Loss = 0.439566, Test Loss = 0.244535, Learning Rate = 1.861080e-05\n",
      "Epoch 10485/20000: Train Loss = 0.439787, Test Loss = 0.245992, Learning Rate = 1.860373e-05\n",
      "Epoch 10486/20000: Train Loss = 0.439690, Test Loss = 0.248327, Learning Rate = 1.859666e-05\n",
      "Epoch 10487/20000: Train Loss = 0.439472, Test Loss = 0.248075, Learning Rate = 1.858960e-05\n",
      "Epoch 10488/20000: Train Loss = 0.439803, Test Loss = 0.251147, Learning Rate = 1.858253e-05\n",
      "Epoch 10489/20000: Train Loss = 0.439612, Test Loss = 0.244930, Learning Rate = 1.857547e-05\n",
      "Epoch 10490/20000: Train Loss = 0.439977, Test Loss = 0.248124, Learning Rate = 1.856841e-05\n",
      "Epoch 10491/20000: Train Loss = 0.439723, Test Loss = 0.248530, Learning Rate = 1.856136e-05\n",
      "Epoch 10492/20000: Train Loss = 0.439596, Test Loss = 0.251875, Learning Rate = 1.855431e-05\n",
      "Epoch 10493/20000: Train Loss = 0.439700, Test Loss = 0.251798, Learning Rate = 1.854726e-05\n",
      "Epoch 10494/20000: Train Loss = 0.439601, Test Loss = 0.248000, Learning Rate = 1.854021e-05\n",
      "Epoch 10495/20000: Train Loss = 0.439441, Test Loss = 0.251587, Learning Rate = 1.853316e-05\n",
      "Epoch 10496/20000: Train Loss = 0.439678, Test Loss = 0.252693, Learning Rate = 1.852612e-05\n",
      "Epoch 10497/20000: Train Loss = 0.439627, Test Loss = 0.251619, Learning Rate = 1.851908e-05\n",
      "Epoch 10498/20000: Train Loss = 0.440070, Test Loss = 0.252585, Learning Rate = 1.851205e-05\n",
      "Epoch 10499/20000: Train Loss = 0.439588, Test Loss = 0.253319, Learning Rate = 1.850501e-05\n",
      "Epoch 10500/20000: Train Loss = 0.439740, Test Loss = 0.252904, Learning Rate = 1.849798e-05\n",
      "Epoch 10501/20000: Train Loss = 0.439395, Test Loss = 0.250802, Learning Rate = 1.849095e-05\n",
      "Epoch 10502/20000: Train Loss = 0.440207, Test Loss = 0.249695, Learning Rate = 1.848393e-05\n",
      "Epoch 10503/20000: Train Loss = 0.439712, Test Loss = 0.250381, Learning Rate = 1.847690e-05\n",
      "Epoch 10504/20000: Train Loss = 0.439457, Test Loss = 0.249532, Learning Rate = 1.846988e-05\n",
      "Epoch 10505/20000: Train Loss = 0.439545, Test Loss = 0.251906, Learning Rate = 1.846286e-05\n",
      "Epoch 10506/20000: Train Loss = 0.439530, Test Loss = 0.249813, Learning Rate = 1.845585e-05\n",
      "Epoch 10507/20000: Train Loss = 0.439467, Test Loss = 0.250916, Learning Rate = 1.844883e-05\n",
      "Epoch 10508/20000: Train Loss = 0.439541, Test Loss = 0.251740, Learning Rate = 1.844182e-05\n",
      "Epoch 10509/20000: Train Loss = 0.439508, Test Loss = 0.252346, Learning Rate = 1.843482e-05\n",
      "Epoch 10510/20000: Train Loss = 0.439553, Test Loss = 0.251921, Learning Rate = 1.842781e-05\n",
      "Epoch 10511/20000: Train Loss = 0.439681, Test Loss = 0.248832, Learning Rate = 1.842081e-05\n",
      "Epoch 10512/20000: Train Loss = 0.439611, Test Loss = 0.247381, Learning Rate = 1.841381e-05\n",
      "Epoch 10513/20000: Train Loss = 0.440200, Test Loss = 0.242082, Learning Rate = 1.840681e-05\n",
      "Epoch 10514/20000: Train Loss = 0.439933, Test Loss = 0.249732, Learning Rate = 1.839982e-05\n",
      "Epoch 10515/20000: Train Loss = 0.439500, Test Loss = 0.244225, Learning Rate = 1.839283e-05\n",
      "Epoch 10516/20000: Train Loss = 0.439656, Test Loss = 0.246373, Learning Rate = 1.838584e-05\n",
      "Epoch 10517/20000: Train Loss = 0.439610, Test Loss = 0.250346, Learning Rate = 1.837885e-05\n",
      "Epoch 10518/20000: Train Loss = 0.439646, Test Loss = 0.247019, Learning Rate = 1.837187e-05\n",
      "Epoch 10519/20000: Train Loss = 0.439656, Test Loss = 0.254423, Learning Rate = 1.836489e-05\n",
      "Epoch 10520/20000: Train Loss = 0.439876, Test Loss = 0.249501, Learning Rate = 1.835791e-05\n",
      "Epoch 10521/20000: Train Loss = 0.439768, Test Loss = 0.250855, Learning Rate = 1.835094e-05\n",
      "Epoch 10522/20000: Train Loss = 0.439819, Test Loss = 0.252223, Learning Rate = 1.834396e-05\n",
      "Epoch 10523/20000: Train Loss = 0.439779, Test Loss = 0.247859, Learning Rate = 1.833699e-05\n",
      "Epoch 10524/20000: Train Loss = 0.439672, Test Loss = 0.248664, Learning Rate = 1.833003e-05\n",
      "Epoch 10525/20000: Train Loss = 0.439774, Test Loss = 0.247812, Learning Rate = 1.832306e-05\n",
      "Epoch 10526/20000: Train Loss = 0.439753, Test Loss = 0.247551, Learning Rate = 1.831610e-05\n",
      "Epoch 10527/20000: Train Loss = 0.439618, Test Loss = 0.249229, Learning Rate = 1.830914e-05\n",
      "Epoch 10528/20000: Train Loss = 0.439517, Test Loss = 0.248842, Learning Rate = 1.830218e-05\n",
      "Epoch 10529/20000: Train Loss = 0.439592, Test Loss = 0.246204, Learning Rate = 1.829523e-05\n",
      "Epoch 10530/20000: Train Loss = 0.439697, Test Loss = 0.243398, Learning Rate = 1.828828e-05\n",
      "Epoch 10531/20000: Train Loss = 0.439758, Test Loss = 0.244564, Learning Rate = 1.828133e-05\n",
      "Epoch 10532/20000: Train Loss = 0.439734, Test Loss = 0.242768, Learning Rate = 1.827438e-05\n",
      "Epoch 10533/20000: Train Loss = 0.439647, Test Loss = 0.250280, Learning Rate = 1.826744e-05\n",
      "Epoch 10534/20000: Train Loss = 0.439638, Test Loss = 0.249965, Learning Rate = 1.826050e-05\n",
      "Epoch 10535/20000: Train Loss = 0.439519, Test Loss = 0.248230, Learning Rate = 1.825356e-05\n",
      "Epoch 10536/20000: Train Loss = 0.439580, Test Loss = 0.249474, Learning Rate = 1.824662e-05\n",
      "Epoch 10537/20000: Train Loss = 0.439699, Test Loss = 0.250453, Learning Rate = 1.823969e-05\n",
      "Epoch 10538/20000: Train Loss = 0.439688, Test Loss = 0.248274, Learning Rate = 1.823276e-05\n",
      "Epoch 10539/20000: Train Loss = 0.439733, Test Loss = 0.247212, Learning Rate = 1.822583e-05\n",
      "Epoch 10540/20000: Train Loss = 0.439673, Test Loss = 0.248879, Learning Rate = 1.821890e-05\n",
      "Epoch 10541/20000: Train Loss = 0.439681, Test Loss = 0.251173, Learning Rate = 1.821198e-05\n",
      "Epoch 10542/20000: Train Loss = 0.439615, Test Loss = 0.248848, Learning Rate = 1.820506e-05\n",
      "Epoch 10543/20000: Train Loss = 0.439839, Test Loss = 0.249970, Learning Rate = 1.819814e-05\n",
      "Epoch 10544/20000: Train Loss = 0.439640, Test Loss = 0.249607, Learning Rate = 1.819123e-05\n",
      "Epoch 10545/20000: Train Loss = 0.439488, Test Loss = 0.250756, Learning Rate = 1.818432e-05\n",
      "Epoch 10546/20000: Train Loss = 0.439674, Test Loss = 0.251458, Learning Rate = 1.817741e-05\n",
      "Epoch 10547/20000: Train Loss = 0.439465, Test Loss = 0.250149, Learning Rate = 1.817050e-05\n",
      "Epoch 10548/20000: Train Loss = 0.439956, Test Loss = 0.253393, Learning Rate = 1.816360e-05\n",
      "Epoch 10549/20000: Train Loss = 0.439643, Test Loss = 0.246238, Learning Rate = 1.815669e-05\n",
      "Epoch 10550/20000: Train Loss = 0.439391, Test Loss = 0.247451, Learning Rate = 1.814980e-05\n",
      "Epoch 10551/20000: Train Loss = 0.439485, Test Loss = 0.248584, Learning Rate = 1.814290e-05\n",
      "Epoch 10552/20000: Train Loss = 0.439579, Test Loss = 0.246069, Learning Rate = 1.813600e-05\n",
      "Epoch 10553/20000: Train Loss = 0.439614, Test Loss = 0.245878, Learning Rate = 1.812911e-05\n",
      "Epoch 10554/20000: Train Loss = 0.439842, Test Loss = 0.250049, Learning Rate = 1.812223e-05\n",
      "Epoch 10555/20000: Train Loss = 0.439767, Test Loss = 0.243219, Learning Rate = 1.811534e-05\n",
      "Epoch 10556/20000: Train Loss = 0.439684, Test Loss = 0.253324, Learning Rate = 1.810846e-05\n",
      "Epoch 10557/20000: Train Loss = 0.439766, Test Loss = 0.250651, Learning Rate = 1.810158e-05\n",
      "Epoch 10558/20000: Train Loss = 0.439651, Test Loss = 0.250492, Learning Rate = 1.809470e-05\n",
      "Epoch 10559/20000: Train Loss = 0.439880, Test Loss = 0.250553, Learning Rate = 1.808782e-05\n",
      "Epoch 10560/20000: Train Loss = 0.439966, Test Loss = 0.247823, Learning Rate = 1.808095e-05\n",
      "Epoch 10561/20000: Train Loss = 0.439657, Test Loss = 0.249448, Learning Rate = 1.807408e-05\n",
      "Epoch 10562/20000: Train Loss = 0.439508, Test Loss = 0.246059, Learning Rate = 1.806721e-05\n",
      "Epoch 10563/20000: Train Loss = 0.439786, Test Loss = 0.249762, Learning Rate = 1.806035e-05\n",
      "Epoch 10564/20000: Train Loss = 0.439617, Test Loss = 0.253528, Learning Rate = 1.805348e-05\n",
      "Epoch 10565/20000: Train Loss = 0.439926, Test Loss = 0.250022, Learning Rate = 1.804662e-05\n",
      "Epoch 10566/20000: Train Loss = 0.439777, Test Loss = 0.250087, Learning Rate = 1.803977e-05\n",
      "Epoch 10567/20000: Train Loss = 0.439823, Test Loss = 0.253524, Learning Rate = 1.803291e-05\n",
      "Epoch 10568/20000: Train Loss = 0.439712, Test Loss = 0.252116, Learning Rate = 1.802606e-05\n",
      "Epoch 10569/20000: Train Loss = 0.440724, Test Loss = 0.253035, Learning Rate = 1.801921e-05\n",
      "Epoch 10570/20000: Train Loss = 0.439713, Test Loss = 0.249062, Learning Rate = 1.801236e-05\n",
      "Epoch 10571/20000: Train Loss = 0.439587, Test Loss = 0.256535, Learning Rate = 1.800552e-05\n",
      "Epoch 10572/20000: Train Loss = 0.439622, Test Loss = 0.251610, Learning Rate = 1.799868e-05\n",
      "Epoch 10573/20000: Train Loss = 0.439752, Test Loss = 0.255626, Learning Rate = 1.799184e-05\n",
      "Epoch 10574/20000: Train Loss = 0.439633, Test Loss = 0.250493, Learning Rate = 1.798500e-05\n",
      "Epoch 10575/20000: Train Loss = 0.439654, Test Loss = 0.250525, Learning Rate = 1.797817e-05\n",
      "Epoch 10576/20000: Train Loss = 0.439509, Test Loss = 0.250842, Learning Rate = 1.797134e-05\n",
      "Epoch 10577/20000: Train Loss = 0.439643, Test Loss = 0.252655, Learning Rate = 1.796451e-05\n",
      "Epoch 10578/20000: Train Loss = 0.439560, Test Loss = 0.251456, Learning Rate = 1.795768e-05\n",
      "Epoch 10579/20000: Train Loss = 0.439800, Test Loss = 0.252822, Learning Rate = 1.795086e-05\n",
      "Epoch 10580/20000: Train Loss = 0.439495, Test Loss = 0.253300, Learning Rate = 1.794404e-05\n",
      "Epoch 10581/20000: Train Loss = 0.439546, Test Loss = 0.251458, Learning Rate = 1.793722e-05\n",
      "Epoch 10582/20000: Train Loss = 0.439629, Test Loss = 0.253620, Learning Rate = 1.793040e-05\n",
      "Epoch 10583/20000: Train Loss = 0.439846, Test Loss = 0.250221, Learning Rate = 1.792359e-05\n",
      "Epoch 10584/20000: Train Loss = 0.439969, Test Loss = 0.251998, Learning Rate = 1.791678e-05\n",
      "Epoch 10585/20000: Train Loss = 0.439613, Test Loss = 0.250131, Learning Rate = 1.790997e-05\n",
      "Epoch 10586/20000: Train Loss = 0.440039, Test Loss = 0.252213, Learning Rate = 1.790317e-05\n",
      "Epoch 10587/20000: Train Loss = 0.440320, Test Loss = 0.250006, Learning Rate = 1.789636e-05\n",
      "Epoch 10588/20000: Train Loss = 0.439890, Test Loss = 0.250560, Learning Rate = 1.788956e-05\n",
      "Epoch 10589/20000: Train Loss = 0.439641, Test Loss = 0.250320, Learning Rate = 1.788277e-05\n",
      "Epoch 10590/20000: Train Loss = 0.439468, Test Loss = 0.253008, Learning Rate = 1.787597e-05\n",
      "Epoch 10591/20000: Train Loss = 0.439664, Test Loss = 0.251968, Learning Rate = 1.786918e-05\n",
      "Epoch 10592/20000: Train Loss = 0.439497, Test Loss = 0.252468, Learning Rate = 1.786239e-05\n",
      "Epoch 10593/20000: Train Loss = 0.439514, Test Loss = 0.250950, Learning Rate = 1.785560e-05\n",
      "Epoch 10594/20000: Train Loss = 0.439469, Test Loss = 0.251614, Learning Rate = 1.784882e-05\n",
      "Epoch 10595/20000: Train Loss = 0.439538, Test Loss = 0.248762, Learning Rate = 1.784204e-05\n",
      "Epoch 10596/20000: Train Loss = 0.439589, Test Loss = 0.249087, Learning Rate = 1.783526e-05\n",
      "Epoch 10597/20000: Train Loss = 0.439454, Test Loss = 0.250766, Learning Rate = 1.782848e-05\n",
      "Epoch 10598/20000: Train Loss = 0.439624, Test Loss = 0.251209, Learning Rate = 1.782171e-05\n",
      "Epoch 10599/20000: Train Loss = 0.439551, Test Loss = 0.246721, Learning Rate = 1.781493e-05\n",
      "Epoch 10600/20000: Train Loss = 0.439622, Test Loss = 0.248755, Learning Rate = 1.780816e-05\n",
      "Epoch 10601/20000: Train Loss = 0.439676, Test Loss = 0.245735, Learning Rate = 1.780140e-05\n",
      "Epoch 10602/20000: Train Loss = 0.439682, Test Loss = 0.248223, Learning Rate = 1.779463e-05\n",
      "Epoch 10603/20000: Train Loss = 0.439467, Test Loss = 0.250362, Learning Rate = 1.778787e-05\n",
      "Epoch 10604/20000: Train Loss = 0.439465, Test Loss = 0.253579, Learning Rate = 1.778111e-05\n",
      "Epoch 10605/20000: Train Loss = 0.439524, Test Loss = 0.251910, Learning Rate = 1.777436e-05\n",
      "Epoch 10606/20000: Train Loss = 0.439633, Test Loss = 0.250533, Learning Rate = 1.776760e-05\n",
      "Epoch 10607/20000: Train Loss = 0.439534, Test Loss = 0.253363, Learning Rate = 1.776085e-05\n",
      "Epoch 10608/20000: Train Loss = 0.439581, Test Loss = 0.251862, Learning Rate = 1.775410e-05\n",
      "Epoch 10609/20000: Train Loss = 0.439741, Test Loss = 0.246156, Learning Rate = 1.774736e-05\n",
      "Epoch 10610/20000: Train Loss = 0.439457, Test Loss = 0.250941, Learning Rate = 1.774061e-05\n",
      "Epoch 10611/20000: Train Loss = 0.439552, Test Loss = 0.251338, Learning Rate = 1.773387e-05\n",
      "Epoch 10612/20000: Train Loss = 0.439555, Test Loss = 0.250393, Learning Rate = 1.772713e-05\n",
      "Epoch 10613/20000: Train Loss = 0.439573, Test Loss = 0.248094, Learning Rate = 1.772040e-05\n",
      "Epoch 10614/20000: Train Loss = 0.439438, Test Loss = 0.250433, Learning Rate = 1.771367e-05\n",
      "Epoch 10615/20000: Train Loss = 0.439594, Test Loss = 0.253819, Learning Rate = 1.770693e-05\n",
      "Epoch 10616/20000: Train Loss = 0.439574, Test Loss = 0.249683, Learning Rate = 1.770021e-05\n",
      "Epoch 10617/20000: Train Loss = 0.439428, Test Loss = 0.249271, Learning Rate = 1.769348e-05\n",
      "Epoch 10618/20000: Train Loss = 0.439490, Test Loss = 0.249877, Learning Rate = 1.768676e-05\n",
      "Epoch 10619/20000: Train Loss = 0.439519, Test Loss = 0.249563, Learning Rate = 1.768004e-05\n",
      "Epoch 10620/20000: Train Loss = 0.439585, Test Loss = 0.248480, Learning Rate = 1.767332e-05\n",
      "Epoch 10621/20000: Train Loss = 0.439695, Test Loss = 0.246668, Learning Rate = 1.766660e-05\n",
      "Epoch 10622/20000: Train Loss = 0.440040, Test Loss = 0.248950, Learning Rate = 1.765989e-05\n",
      "Epoch 10623/20000: Train Loss = 0.439678, Test Loss = 0.249241, Learning Rate = 1.765318e-05\n",
      "Epoch 10624/20000: Train Loss = 0.439698, Test Loss = 0.252735, Learning Rate = 1.764647e-05\n",
      "Epoch 10625/20000: Train Loss = 0.439570, Test Loss = 0.250211, Learning Rate = 1.763977e-05\n",
      "Epoch 10626/20000: Train Loss = 0.439846, Test Loss = 0.250579, Learning Rate = 1.763307e-05\n",
      "Epoch 10627/20000: Train Loss = 0.439479, Test Loss = 0.247600, Learning Rate = 1.762637e-05\n",
      "Epoch 10628/20000: Train Loss = 0.439705, Test Loss = 0.248898, Learning Rate = 1.761967e-05\n",
      "Epoch 10629/20000: Train Loss = 0.439539, Test Loss = 0.249071, Learning Rate = 1.761297e-05\n",
      "Epoch 10630/20000: Train Loss = 0.439581, Test Loss = 0.248589, Learning Rate = 1.760628e-05\n",
      "Epoch 10631/20000: Train Loss = 0.439841, Test Loss = 0.252682, Learning Rate = 1.759959e-05\n",
      "Epoch 10632/20000: Train Loss = 0.439635, Test Loss = 0.249495, Learning Rate = 1.759290e-05\n",
      "Epoch 10633/20000: Train Loss = 0.439606, Test Loss = 0.253870, Learning Rate = 1.758622e-05\n",
      "Epoch 10634/20000: Train Loss = 0.439787, Test Loss = 0.248080, Learning Rate = 1.757954e-05\n",
      "Epoch 10635/20000: Train Loss = 0.439692, Test Loss = 0.255065, Learning Rate = 1.757286e-05\n",
      "Epoch 10636/20000: Train Loss = 0.439643, Test Loss = 0.253864, Learning Rate = 1.756618e-05\n",
      "Epoch 10637/20000: Train Loss = 0.439435, Test Loss = 0.248295, Learning Rate = 1.755950e-05\n",
      "Epoch 10638/20000: Train Loss = 0.439475, Test Loss = 0.251578, Learning Rate = 1.755283e-05\n",
      "Epoch 10639/20000: Train Loss = 0.439720, Test Loss = 0.253452, Learning Rate = 1.754616e-05\n",
      "Epoch 10640/20000: Train Loss = 0.439704, Test Loss = 0.254725, Learning Rate = 1.753950e-05\n",
      "Epoch 10641/20000: Train Loss = 0.439851, Test Loss = 0.256155, Learning Rate = 1.753283e-05\n",
      "Epoch 10642/20000: Train Loss = 0.439565, Test Loss = 0.249906, Learning Rate = 1.752617e-05\n",
      "Epoch 10643/20000: Train Loss = 0.439822, Test Loss = 0.252818, Learning Rate = 1.751951e-05\n",
      "Epoch 10644/20000: Train Loss = 0.439482, Test Loss = 0.249411, Learning Rate = 1.751285e-05\n",
      "Epoch 10645/20000: Train Loss = 0.439692, Test Loss = 0.252107, Learning Rate = 1.750620e-05\n",
      "Epoch 10646/20000: Train Loss = 0.439708, Test Loss = 0.248742, Learning Rate = 1.749955e-05\n",
      "Epoch 10647/20000: Train Loss = 0.439835, Test Loss = 0.249138, Learning Rate = 1.749290e-05\n",
      "Epoch 10648/20000: Train Loss = 0.439739, Test Loss = 0.251739, Learning Rate = 1.748625e-05\n",
      "Epoch 10649/20000: Train Loss = 0.439602, Test Loss = 0.252369, Learning Rate = 1.747961e-05\n",
      "Epoch 10650/20000: Train Loss = 0.439793, Test Loss = 0.248239, Learning Rate = 1.747296e-05\n",
      "Epoch 10651/20000: Train Loss = 0.439708, Test Loss = 0.250426, Learning Rate = 1.746632e-05\n",
      "Epoch 10652/20000: Train Loss = 0.439562, Test Loss = 0.251553, Learning Rate = 1.745969e-05\n",
      "Epoch 10653/20000: Train Loss = 0.439554, Test Loss = 0.246354, Learning Rate = 1.745305e-05\n",
      "Epoch 10654/20000: Train Loss = 0.439372, Test Loss = 0.252466, Learning Rate = 1.744642e-05\n",
      "Epoch 10655/20000: Train Loss = 0.439667, Test Loss = 0.250101, Learning Rate = 1.743979e-05\n",
      "Epoch 10656/20000: Train Loss = 0.439691, Test Loss = 0.247971, Learning Rate = 1.743317e-05\n",
      "Epoch 10657/20000: Train Loss = 0.439642, Test Loss = 0.248999, Learning Rate = 1.742654e-05\n",
      "Epoch 10658/20000: Train Loss = 0.439613, Test Loss = 0.250185, Learning Rate = 1.741992e-05\n",
      "Epoch 10659/20000: Train Loss = 0.439662, Test Loss = 0.251087, Learning Rate = 1.741330e-05\n",
      "Epoch 10660/20000: Train Loss = 0.439666, Test Loss = 0.247510, Learning Rate = 1.740668e-05\n",
      "Epoch 10661/20000: Train Loss = 0.439498, Test Loss = 0.247905, Learning Rate = 1.740007e-05\n",
      "Epoch 10662/20000: Train Loss = 0.439804, Test Loss = 0.248376, Learning Rate = 1.739346e-05\n",
      "Epoch 10663/20000: Train Loss = 0.439547, Test Loss = 0.250730, Learning Rate = 1.738685e-05\n",
      "Epoch 10664/20000: Train Loss = 0.439542, Test Loss = 0.250993, Learning Rate = 1.738024e-05\n",
      "Epoch 10665/20000: Train Loss = 0.439579, Test Loss = 0.251808, Learning Rate = 1.737364e-05\n",
      "Epoch 10666/20000: Train Loss = 0.439417, Test Loss = 0.252201, Learning Rate = 1.736704e-05\n",
      "Epoch 10667/20000: Train Loss = 0.439575, Test Loss = 0.253758, Learning Rate = 1.736044e-05\n",
      "Epoch 10668/20000: Train Loss = 0.439460, Test Loss = 0.250607, Learning Rate = 1.735384e-05\n",
      "Epoch 10669/20000: Train Loss = 0.439672, Test Loss = 0.249812, Learning Rate = 1.734725e-05\n",
      "Epoch 10670/20000: Train Loss = 0.439490, Test Loss = 0.253366, Learning Rate = 1.734066e-05\n",
      "Epoch 10671/20000: Train Loss = 0.439826, Test Loss = 0.254618, Learning Rate = 1.733407e-05\n",
      "Epoch 10672/20000: Train Loss = 0.439467, Test Loss = 0.252974, Learning Rate = 1.732748e-05\n",
      "Epoch 10673/20000: Train Loss = 0.439894, Test Loss = 0.254554, Learning Rate = 1.732090e-05\n",
      "Epoch 10674/20000: Train Loss = 0.439530, Test Loss = 0.255643, Learning Rate = 1.731432e-05\n",
      "Epoch 10675/20000: Train Loss = 0.440026, Test Loss = 0.260798, Learning Rate = 1.730774e-05\n",
      "Epoch 10676/20000: Train Loss = 0.439730, Test Loss = 0.255305, Learning Rate = 1.730116e-05\n",
      "Epoch 10677/20000: Train Loss = 0.439587, Test Loss = 0.253423, Learning Rate = 1.729459e-05\n",
      "Epoch 10678/20000: Train Loss = 0.439634, Test Loss = 0.255788, Learning Rate = 1.728802e-05\n",
      "Epoch 10679/20000: Train Loss = 0.439804, Test Loss = 0.257280, Learning Rate = 1.728145e-05\n",
      "Epoch 10680/20000: Train Loss = 0.439754, Test Loss = 0.253453, Learning Rate = 1.727488e-05\n",
      "Epoch 10681/20000: Train Loss = 0.439676, Test Loss = 0.252468, Learning Rate = 1.726832e-05\n",
      "Epoch 10682/20000: Train Loss = 0.439564, Test Loss = 0.253145, Learning Rate = 1.726175e-05\n",
      "Epoch 10683/20000: Train Loss = 0.439578, Test Loss = 0.253337, Learning Rate = 1.725520e-05\n",
      "Epoch 10684/20000: Train Loss = 0.439499, Test Loss = 0.253577, Learning Rate = 1.724864e-05\n",
      "Epoch 10685/20000: Train Loss = 0.439426, Test Loss = 0.252034, Learning Rate = 1.724208e-05\n",
      "Epoch 10686/20000: Train Loss = 0.439676, Test Loss = 0.252142, Learning Rate = 1.723553e-05\n",
      "Epoch 10687/20000: Train Loss = 0.439733, Test Loss = 0.249022, Learning Rate = 1.722898e-05\n",
      "Epoch 10688/20000: Train Loss = 0.439483, Test Loss = 0.251009, Learning Rate = 1.722244e-05\n",
      "Epoch 10689/20000: Train Loss = 0.439430, Test Loss = 0.248889, Learning Rate = 1.721589e-05\n",
      "Epoch 10690/20000: Train Loss = 0.439550, Test Loss = 0.247046, Learning Rate = 1.720935e-05\n",
      "Epoch 10691/20000: Train Loss = 0.439615, Test Loss = 0.251475, Learning Rate = 1.720281e-05\n",
      "Epoch 10692/20000: Train Loss = 0.439440, Test Loss = 0.247809, Learning Rate = 1.719628e-05\n",
      "Epoch 10693/20000: Train Loss = 0.439515, Test Loss = 0.247622, Learning Rate = 1.718974e-05\n",
      "Epoch 10694/20000: Train Loss = 0.439647, Test Loss = 0.248626, Learning Rate = 1.718321e-05\n",
      "Epoch 10695/20000: Train Loss = 0.439514, Test Loss = 0.249128, Learning Rate = 1.717668e-05\n",
      "Epoch 10696/20000: Train Loss = 0.439487, Test Loss = 0.247061, Learning Rate = 1.717015e-05\n",
      "Epoch 10697/20000: Train Loss = 0.439570, Test Loss = 0.245160, Learning Rate = 1.716363e-05\n",
      "Epoch 10698/20000: Train Loss = 0.439684, Test Loss = 0.248804, Learning Rate = 1.715711e-05\n",
      "Epoch 10699/20000: Train Loss = 0.439589, Test Loss = 0.250402, Learning Rate = 1.715059e-05\n",
      "Epoch 10700/20000: Train Loss = 0.439532, Test Loss = 0.250447, Learning Rate = 1.714407e-05\n",
      "Epoch 10701/20000: Train Loss = 0.439655, Test Loss = 0.252298, Learning Rate = 1.713756e-05\n",
      "Epoch 10702/20000: Train Loss = 0.439522, Test Loss = 0.255059, Learning Rate = 1.713105e-05\n",
      "Epoch 10703/20000: Train Loss = 0.439465, Test Loss = 0.251177, Learning Rate = 1.712454e-05\n",
      "Epoch 10704/20000: Train Loss = 0.439529, Test Loss = 0.252486, Learning Rate = 1.711803e-05\n",
      "Epoch 10705/20000: Train Loss = 0.439485, Test Loss = 0.249210, Learning Rate = 1.711153e-05\n",
      "Epoch 10706/20000: Train Loss = 0.439572, Test Loss = 0.248524, Learning Rate = 1.710502e-05\n",
      "Epoch 10707/20000: Train Loss = 0.439517, Test Loss = 0.248907, Learning Rate = 1.709852e-05\n",
      "Epoch 10708/20000: Train Loss = 0.439510, Test Loss = 0.246712, Learning Rate = 1.709203e-05\n",
      "Epoch 10709/20000: Train Loss = 0.439677, Test Loss = 0.245027, Learning Rate = 1.708553e-05\n",
      "Epoch 10710/20000: Train Loss = 0.439723, Test Loss = 0.244557, Learning Rate = 1.707904e-05\n",
      "Epoch 10711/20000: Train Loss = 0.439662, Test Loss = 0.245100, Learning Rate = 1.707255e-05\n",
      "Epoch 10712/20000: Train Loss = 0.439673, Test Loss = 0.249559, Learning Rate = 1.706606e-05\n",
      "Epoch 10713/20000: Train Loss = 0.439610, Test Loss = 0.245378, Learning Rate = 1.705958e-05\n",
      "Epoch 10714/20000: Train Loss = 0.439707, Test Loss = 0.248959, Learning Rate = 1.705310e-05\n",
      "Epoch 10715/20000: Train Loss = 0.439647, Test Loss = 0.245473, Learning Rate = 1.704662e-05\n",
      "Epoch 10716/20000: Train Loss = 0.439700, Test Loss = 0.248109, Learning Rate = 1.704014e-05\n",
      "Epoch 10717/20000: Train Loss = 0.439457, Test Loss = 0.245227, Learning Rate = 1.703367e-05\n",
      "Epoch 10718/20000: Train Loss = 0.439582, Test Loss = 0.247188, Learning Rate = 1.702719e-05\n",
      "Epoch 10719/20000: Train Loss = 0.439636, Test Loss = 0.245758, Learning Rate = 1.702072e-05\n",
      "Epoch 10720/20000: Train Loss = 0.439568, Test Loss = 0.248481, Learning Rate = 1.701426e-05\n",
      "Epoch 10721/20000: Train Loss = 0.439589, Test Loss = 0.249783, Learning Rate = 1.700779e-05\n",
      "Epoch 10722/20000: Train Loss = 0.439796, Test Loss = 0.246555, Learning Rate = 1.700133e-05\n",
      "Epoch 10723/20000: Train Loss = 0.439907, Test Loss = 0.249824, Learning Rate = 1.699487e-05\n",
      "Epoch 10724/20000: Train Loss = 0.439404, Test Loss = 0.252595, Learning Rate = 1.698841e-05\n",
      "Epoch 10725/20000: Train Loss = 0.439543, Test Loss = 0.250599, Learning Rate = 1.698196e-05\n",
      "Epoch 10726/20000: Train Loss = 0.439440, Test Loss = 0.248284, Learning Rate = 1.697550e-05\n",
      "Epoch 10727/20000: Train Loss = 0.439550, Test Loss = 0.248584, Learning Rate = 1.696905e-05\n",
      "Epoch 10728/20000: Train Loss = 0.439802, Test Loss = 0.251299, Learning Rate = 1.696261e-05\n",
      "Epoch 10729/20000: Train Loss = 0.439783, Test Loss = 0.249280, Learning Rate = 1.695616e-05\n",
      "Epoch 10730/20000: Train Loss = 0.440387, Test Loss = 0.251244, Learning Rate = 1.694972e-05\n",
      "Epoch 10731/20000: Train Loss = 0.439949, Test Loss = 0.247125, Learning Rate = 1.694328e-05\n",
      "Epoch 10732/20000: Train Loss = 0.439600, Test Loss = 0.248299, Learning Rate = 1.693684e-05\n",
      "Epoch 10733/20000: Train Loss = 0.439540, Test Loss = 0.250301, Learning Rate = 1.693040e-05\n",
      "Epoch 10734/20000: Train Loss = 0.439726, Test Loss = 0.246761, Learning Rate = 1.692397e-05\n",
      "Epoch 10735/20000: Train Loss = 0.439717, Test Loss = 0.248432, Learning Rate = 1.691754e-05\n",
      "Epoch 10736/20000: Train Loss = 0.439436, Test Loss = 0.246716, Learning Rate = 1.691111e-05\n",
      "Epoch 10737/20000: Train Loss = 0.439513, Test Loss = 0.247515, Learning Rate = 1.690469e-05\n",
      "Epoch 10738/20000: Train Loss = 0.439525, Test Loss = 0.246488, Learning Rate = 1.689826e-05\n",
      "Epoch 10739/20000: Train Loss = 0.439644, Test Loss = 0.252509, Learning Rate = 1.689184e-05\n",
      "Epoch 10740/20000: Train Loss = 0.439589, Test Loss = 0.249419, Learning Rate = 1.688542e-05\n",
      "Epoch 10741/20000: Train Loss = 0.439760, Test Loss = 0.249743, Learning Rate = 1.687901e-05\n",
      "Epoch 10742/20000: Train Loss = 0.439591, Test Loss = 0.247106, Learning Rate = 1.687259e-05\n",
      "Epoch 10743/20000: Train Loss = 0.439525, Test Loss = 0.249215, Learning Rate = 1.686618e-05\n",
      "Epoch 10744/20000: Train Loss = 0.439654, Test Loss = 0.248605, Learning Rate = 1.685977e-05\n",
      "Epoch 10745/20000: Train Loss = 0.439594, Test Loss = 0.248723, Learning Rate = 1.685337e-05\n",
      "Epoch 10746/20000: Train Loss = 0.439480, Test Loss = 0.249695, Learning Rate = 1.684696e-05\n",
      "Epoch 10747/20000: Train Loss = 0.439522, Test Loss = 0.250218, Learning Rate = 1.684056e-05\n",
      "Epoch 10748/20000: Train Loss = 0.439604, Test Loss = 0.248836, Learning Rate = 1.683416e-05\n",
      "Epoch 10749/20000: Train Loss = 0.439523, Test Loss = 0.252557, Learning Rate = 1.682777e-05\n",
      "Epoch 10750/20000: Train Loss = 0.439558, Test Loss = 0.246312, Learning Rate = 1.682137e-05\n",
      "Epoch 10751/20000: Train Loss = 0.439617, Test Loss = 0.249395, Learning Rate = 1.681498e-05\n",
      "Epoch 10752/20000: Train Loss = 0.439429, Test Loss = 0.248437, Learning Rate = 1.680859e-05\n",
      "Epoch 10753/20000: Train Loss = 0.439632, Test Loss = 0.248894, Learning Rate = 1.680220e-05\n",
      "Epoch 10754/20000: Train Loss = 0.439696, Test Loss = 0.252108, Learning Rate = 1.679582e-05\n",
      "Epoch 10755/20000: Train Loss = 0.439460, Test Loss = 0.248567, Learning Rate = 1.678944e-05\n",
      "Epoch 10756/20000: Train Loss = 0.439867, Test Loss = 0.252024, Learning Rate = 1.678306e-05\n",
      "Epoch 10757/20000: Train Loss = 0.439709, Test Loss = 0.252155, Learning Rate = 1.677668e-05\n",
      "Epoch 10758/20000: Train Loss = 0.440025, Test Loss = 0.249282, Learning Rate = 1.677031e-05\n",
      "Epoch 10759/20000: Train Loss = 0.439476, Test Loss = 0.256365, Learning Rate = 1.676393e-05\n",
      "Epoch 10760/20000: Train Loss = 0.439518, Test Loss = 0.254096, Learning Rate = 1.675757e-05\n",
      "Epoch 10761/20000: Train Loss = 0.439503, Test Loss = 0.252124, Learning Rate = 1.675120e-05\n",
      "Epoch 10762/20000: Train Loss = 0.439950, Test Loss = 0.251330, Learning Rate = 1.674483e-05\n",
      "Epoch 10763/20000: Train Loss = 0.439593, Test Loss = 0.251729, Learning Rate = 1.673847e-05\n",
      "Epoch 10764/20000: Train Loss = 0.439472, Test Loss = 0.253198, Learning Rate = 1.673211e-05\n",
      "Epoch 10765/20000: Train Loss = 0.439608, Test Loss = 0.252798, Learning Rate = 1.672575e-05\n",
      "Epoch 10766/20000: Train Loss = 0.439679, Test Loss = 0.254160, Learning Rate = 1.671940e-05\n",
      "Epoch 10767/20000: Train Loss = 0.439752, Test Loss = 0.249937, Learning Rate = 1.671304e-05\n",
      "Epoch 10768/20000: Train Loss = 0.439464, Test Loss = 0.249600, Learning Rate = 1.670669e-05\n",
      "Epoch 10769/20000: Train Loss = 0.439486, Test Loss = 0.251401, Learning Rate = 1.670035e-05\n",
      "Epoch 10770/20000: Train Loss = 0.439897, Test Loss = 0.251976, Learning Rate = 1.669400e-05\n",
      "Epoch 10771/20000: Train Loss = 0.439863, Test Loss = 0.250527, Learning Rate = 1.668766e-05\n",
      "Epoch 10772/20000: Train Loss = 0.439431, Test Loss = 0.254463, Learning Rate = 1.668132e-05\n",
      "Epoch 10773/20000: Train Loss = 0.439420, Test Loss = 0.255258, Learning Rate = 1.667498e-05\n",
      "Epoch 10774/20000: Train Loss = 0.439890, Test Loss = 0.256680, Learning Rate = 1.666864e-05\n",
      "Epoch 10775/20000: Train Loss = 0.439754, Test Loss = 0.251497, Learning Rate = 1.666231e-05\n",
      "Epoch 10776/20000: Train Loss = 0.439440, Test Loss = 0.255423, Learning Rate = 1.665598e-05\n",
      "Epoch 10777/20000: Train Loss = 0.439540, Test Loss = 0.251443, Learning Rate = 1.664965e-05\n",
      "Epoch 10778/20000: Train Loss = 0.439580, Test Loss = 0.246961, Learning Rate = 1.664332e-05\n",
      "Epoch 10779/20000: Train Loss = 0.439680, Test Loss = 0.247743, Learning Rate = 1.663700e-05\n",
      "Epoch 10780/20000: Train Loss = 0.439472, Test Loss = 0.249320, Learning Rate = 1.663068e-05\n",
      "Epoch 10781/20000: Train Loss = 0.439481, Test Loss = 0.249222, Learning Rate = 1.662436e-05\n",
      "Epoch 10782/20000: Train Loss = 0.439520, Test Loss = 0.246729, Learning Rate = 1.661804e-05\n",
      "Epoch 10783/20000: Train Loss = 0.439465, Test Loss = 0.245036, Learning Rate = 1.661172e-05\n",
      "Epoch 10784/20000: Train Loss = 0.439478, Test Loss = 0.244818, Learning Rate = 1.660541e-05\n",
      "Epoch 10785/20000: Train Loss = 0.439534, Test Loss = 0.249216, Learning Rate = 1.659910e-05\n",
      "Epoch 10786/20000: Train Loss = 0.439600, Test Loss = 0.249611, Learning Rate = 1.659280e-05\n",
      "Epoch 10787/20000: Train Loss = 0.439771, Test Loss = 0.246770, Learning Rate = 1.658649e-05\n",
      "Epoch 10788/20000: Train Loss = 0.439581, Test Loss = 0.249013, Learning Rate = 1.658019e-05\n",
      "Epoch 10789/20000: Train Loss = 0.439457, Test Loss = 0.247040, Learning Rate = 1.657389e-05\n",
      "Epoch 10790/20000: Train Loss = 0.439671, Test Loss = 0.247622, Learning Rate = 1.656759e-05\n",
      "Epoch 10791/20000: Train Loss = 0.439636, Test Loss = 0.249361, Learning Rate = 1.656130e-05\n",
      "Epoch 10792/20000: Train Loss = 0.439702, Test Loss = 0.247005, Learning Rate = 1.655500e-05\n",
      "Epoch 10793/20000: Train Loss = 0.439339, Test Loss = 0.251385, Learning Rate = 1.654871e-05\n",
      "Epoch 10794/20000: Train Loss = 0.439575, Test Loss = 0.252298, Learning Rate = 1.654242e-05\n",
      "Epoch 10795/20000: Train Loss = 0.439588, Test Loss = 0.249024, Learning Rate = 1.653614e-05\n",
      "Epoch 10796/20000: Train Loss = 0.439533, Test Loss = 0.250889, Learning Rate = 1.652986e-05\n",
      "Epoch 10797/20000: Train Loss = 0.439402, Test Loss = 0.247534, Learning Rate = 1.652357e-05\n",
      "Epoch 10798/20000: Train Loss = 0.439475, Test Loss = 0.245298, Learning Rate = 1.651730e-05\n",
      "Epoch 10799/20000: Train Loss = 0.439505, Test Loss = 0.245817, Learning Rate = 1.651102e-05\n",
      "Epoch 10800/20000: Train Loss = 0.439435, Test Loss = 0.253089, Learning Rate = 1.650475e-05\n",
      "Epoch 10801/20000: Train Loss = 0.439432, Test Loss = 0.250596, Learning Rate = 1.649847e-05\n",
      "Epoch 10802/20000: Train Loss = 0.439516, Test Loss = 0.251872, Learning Rate = 1.649221e-05\n",
      "Epoch 10803/20000: Train Loss = 0.439894, Test Loss = 0.247653, Learning Rate = 1.648594e-05\n",
      "Epoch 10804/20000: Train Loss = 0.439542, Test Loss = 0.248711, Learning Rate = 1.647968e-05\n",
      "Epoch 10805/20000: Train Loss = 0.440131, Test Loss = 0.248547, Learning Rate = 1.647341e-05\n",
      "Epoch 10806/20000: Train Loss = 0.439917, Test Loss = 0.249788, Learning Rate = 1.646715e-05\n",
      "Epoch 10807/20000: Train Loss = 0.439515, Test Loss = 0.246224, Learning Rate = 1.646090e-05\n",
      "Epoch 10808/20000: Train Loss = 0.439466, Test Loss = 0.249509, Learning Rate = 1.645464e-05\n",
      "Epoch 10809/20000: Train Loss = 0.439565, Test Loss = 0.248956, Learning Rate = 1.644839e-05\n",
      "Epoch 10810/20000: Train Loss = 0.439540, Test Loss = 0.250524, Learning Rate = 1.644214e-05\n",
      "Epoch 10811/20000: Train Loss = 0.439796, Test Loss = 0.247693, Learning Rate = 1.643589e-05\n",
      "Epoch 10812/20000: Train Loss = 0.439637, Test Loss = 0.250875, Learning Rate = 1.642965e-05\n",
      "Epoch 10813/20000: Train Loss = 0.439735, Test Loss = 0.246988, Learning Rate = 1.642340e-05\n",
      "Epoch 10814/20000: Train Loss = 0.439705, Test Loss = 0.251239, Learning Rate = 1.641716e-05\n",
      "Epoch 10815/20000: Train Loss = 0.439558, Test Loss = 0.249661, Learning Rate = 1.641093e-05\n",
      "Epoch 10816/20000: Train Loss = 0.439614, Test Loss = 0.248105, Learning Rate = 1.640469e-05\n",
      "Epoch 10817/20000: Train Loss = 0.439804, Test Loss = 0.252476, Learning Rate = 1.639846e-05\n",
      "Epoch 10818/20000: Train Loss = 0.439579, Test Loss = 0.247421, Learning Rate = 1.639223e-05\n",
      "Epoch 10819/20000: Train Loss = 0.440010, Test Loss = 0.250522, Learning Rate = 1.638600e-05\n",
      "Epoch 10820/20000: Train Loss = 0.439562, Test Loss = 0.250162, Learning Rate = 1.637977e-05\n",
      "Epoch 10821/20000: Train Loss = 0.439792, Test Loss = 0.247054, Learning Rate = 1.637355e-05\n",
      "Epoch 10822/20000: Train Loss = 0.439673, Test Loss = 0.244733, Learning Rate = 1.636733e-05\n",
      "Epoch 10823/20000: Train Loss = 0.439684, Test Loss = 0.246527, Learning Rate = 1.636111e-05\n",
      "Epoch 10824/20000: Train Loss = 0.439633, Test Loss = 0.244228, Learning Rate = 1.635489e-05\n",
      "Epoch 10825/20000: Train Loss = 0.439585, Test Loss = 0.249142, Learning Rate = 1.634868e-05\n",
      "Epoch 10826/20000: Train Loss = 0.439591, Test Loss = 0.250963, Learning Rate = 1.634246e-05\n",
      "Epoch 10827/20000: Train Loss = 0.439529, Test Loss = 0.247823, Learning Rate = 1.633625e-05\n",
      "Epoch 10828/20000: Train Loss = 0.439535, Test Loss = 0.253960, Learning Rate = 1.633005e-05\n",
      "Epoch 10829/20000: Train Loss = 0.439498, Test Loss = 0.251307, Learning Rate = 1.632384e-05\n",
      "Epoch 10830/20000: Train Loss = 0.439481, Test Loss = 0.248983, Learning Rate = 1.631764e-05\n",
      "Epoch 10831/20000: Train Loss = 0.439580, Test Loss = 0.250166, Learning Rate = 1.631144e-05\n",
      "Epoch 10832/20000: Train Loss = 0.439579, Test Loss = 0.249154, Learning Rate = 1.630524e-05\n",
      "Epoch 10833/20000: Train Loss = 0.439526, Test Loss = 0.251180, Learning Rate = 1.629904e-05\n",
      "Epoch 10834/20000: Train Loss = 0.439581, Test Loss = 0.252353, Learning Rate = 1.629285e-05\n",
      "Epoch 10835/20000: Train Loss = 0.439367, Test Loss = 0.249197, Learning Rate = 1.628666e-05\n",
      "Epoch 10836/20000: Train Loss = 0.439374, Test Loss = 0.247537, Learning Rate = 1.628047e-05\n",
      "Epoch 10837/20000: Train Loss = 0.439470, Test Loss = 0.247101, Learning Rate = 1.627429e-05\n",
      "Epoch 10838/20000: Train Loss = 0.439582, Test Loss = 0.250741, Learning Rate = 1.626810e-05\n",
      "Epoch 10839/20000: Train Loss = 0.439476, Test Loss = 0.248456, Learning Rate = 1.626192e-05\n",
      "Epoch 10840/20000: Train Loss = 0.439490, Test Loss = 0.251004, Learning Rate = 1.625574e-05\n",
      "Epoch 10841/20000: Train Loss = 0.439771, Test Loss = 0.247228, Learning Rate = 1.624957e-05\n",
      "Epoch 10842/20000: Train Loss = 0.439612, Test Loss = 0.248658, Learning Rate = 1.624339e-05\n",
      "Epoch 10843/20000: Train Loss = 0.439504, Test Loss = 0.249400, Learning Rate = 1.623722e-05\n",
      "Epoch 10844/20000: Train Loss = 0.439567, Test Loss = 0.248599, Learning Rate = 1.623105e-05\n",
      "Epoch 10845/20000: Train Loss = 0.439891, Test Loss = 0.245177, Learning Rate = 1.622488e-05\n",
      "Epoch 10846/20000: Train Loss = 0.439434, Test Loss = 0.252182, Learning Rate = 1.621872e-05\n",
      "Epoch 10847/20000: Train Loss = 0.439660, Test Loss = 0.252130, Learning Rate = 1.621255e-05\n",
      "Epoch 10848/20000: Train Loss = 0.439546, Test Loss = 0.248272, Learning Rate = 1.620639e-05\n",
      "Epoch 10849/20000: Train Loss = 0.439790, Test Loss = 0.246484, Learning Rate = 1.620024e-05\n",
      "Epoch 10850/20000: Train Loss = 0.439537, Test Loss = 0.249182, Learning Rate = 1.619408e-05\n",
      "Epoch 10851/20000: Train Loss = 0.439726, Test Loss = 0.251071, Learning Rate = 1.618793e-05\n",
      "Epoch 10852/20000: Train Loss = 0.439591, Test Loss = 0.248245, Learning Rate = 1.618178e-05\n",
      "Epoch 10853/20000: Train Loss = 0.439488, Test Loss = 0.248599, Learning Rate = 1.617563e-05\n",
      "Epoch 10854/20000: Train Loss = 0.439581, Test Loss = 0.250946, Learning Rate = 1.616948e-05\n",
      "Epoch 10855/20000: Train Loss = 0.440112, Test Loss = 0.246304, Learning Rate = 1.616334e-05\n",
      "Epoch 10856/20000: Train Loss = 0.439552, Test Loss = 0.252359, Learning Rate = 1.615720e-05\n",
      "Epoch 10857/20000: Train Loss = 0.439645, Test Loss = 0.245112, Learning Rate = 1.615106e-05\n",
      "Epoch 10858/20000: Train Loss = 0.439717, Test Loss = 0.246616, Learning Rate = 1.614492e-05\n",
      "Epoch 10859/20000: Train Loss = 0.439589, Test Loss = 0.245598, Learning Rate = 1.613878e-05\n",
      "Epoch 10860/20000: Train Loss = 0.439569, Test Loss = 0.247325, Learning Rate = 1.613265e-05\n",
      "Epoch 10861/20000: Train Loss = 0.439404, Test Loss = 0.248022, Learning Rate = 1.612652e-05\n",
      "Epoch 10862/20000: Train Loss = 0.439486, Test Loss = 0.248983, Learning Rate = 1.612039e-05\n",
      "Epoch 10863/20000: Train Loss = 0.439774, Test Loss = 0.246499, Learning Rate = 1.611427e-05\n",
      "Epoch 10864/20000: Train Loss = 0.439466, Test Loss = 0.251656, Learning Rate = 1.610815e-05\n",
      "Epoch 10865/20000: Train Loss = 0.439424, Test Loss = 0.248510, Learning Rate = 1.610203e-05\n",
      "Epoch 10866/20000: Train Loss = 0.439529, Test Loss = 0.250258, Learning Rate = 1.609591e-05\n",
      "Epoch 10867/20000: Train Loss = 0.439763, Test Loss = 0.250411, Learning Rate = 1.608979e-05\n",
      "Epoch 10868/20000: Train Loss = 0.439555, Test Loss = 0.248158, Learning Rate = 1.608368e-05\n",
      "Epoch 10869/20000: Train Loss = 0.439841, Test Loss = 0.252746, Learning Rate = 1.607757e-05\n",
      "Epoch 10870/20000: Train Loss = 0.439681, Test Loss = 0.250097, Learning Rate = 1.607146e-05\n",
      "Epoch 10871/20000: Train Loss = 0.439507, Test Loss = 0.251030, Learning Rate = 1.606535e-05\n",
      "Epoch 10872/20000: Train Loss = 0.439426, Test Loss = 0.248083, Learning Rate = 1.605925e-05\n",
      "Epoch 10873/20000: Train Loss = 0.439731, Test Loss = 0.250398, Learning Rate = 1.605314e-05\n",
      "Epoch 10874/20000: Train Loss = 0.439642, Test Loss = 0.246410, Learning Rate = 1.604704e-05\n",
      "Epoch 10875/20000: Train Loss = 0.439654, Test Loss = 0.247093, Learning Rate = 1.604095e-05\n",
      "Epoch 10876/20000: Train Loss = 0.439483, Test Loss = 0.244611, Learning Rate = 1.603485e-05\n",
      "Epoch 10877/20000: Train Loss = 0.439983, Test Loss = 0.248738, Learning Rate = 1.602876e-05\n",
      "Epoch 10878/20000: Train Loss = 0.439698, Test Loss = 0.250759, Learning Rate = 1.602267e-05\n",
      "Epoch 10879/20000: Train Loss = 0.439822, Test Loss = 0.251097, Learning Rate = 1.601658e-05\n",
      "Epoch 10880/20000: Train Loss = 0.439776, Test Loss = 0.246820, Learning Rate = 1.601049e-05\n",
      "Epoch 10881/20000: Train Loss = 0.439738, Test Loss = 0.249886, Learning Rate = 1.600441e-05\n",
      "Epoch 10882/20000: Train Loss = 0.439689, Test Loss = 0.251176, Learning Rate = 1.599833e-05\n",
      "Epoch 10883/20000: Train Loss = 0.439909, Test Loss = 0.251288, Learning Rate = 1.599225e-05\n",
      "Epoch 10884/20000: Train Loss = 0.439445, Test Loss = 0.249264, Learning Rate = 1.598617e-05\n",
      "Epoch 10885/20000: Train Loss = 0.439451, Test Loss = 0.250705, Learning Rate = 1.598010e-05\n",
      "Epoch 10886/20000: Train Loss = 0.439492, Test Loss = 0.253710, Learning Rate = 1.597403e-05\n",
      "Epoch 10887/20000: Train Loss = 0.439476, Test Loss = 0.251340, Learning Rate = 1.596796e-05\n",
      "Epoch 10888/20000: Train Loss = 0.439537, Test Loss = 0.251378, Learning Rate = 1.596189e-05\n",
      "Epoch 10889/20000: Train Loss = 0.439395, Test Loss = 0.251253, Learning Rate = 1.595583e-05\n",
      "Epoch 10890/20000: Train Loss = 0.439702, Test Loss = 0.255765, Learning Rate = 1.594976e-05\n",
      "Epoch 10891/20000: Train Loss = 0.439352, Test Loss = 0.251904, Learning Rate = 1.594370e-05\n",
      "Epoch 10892/20000: Train Loss = 0.439899, Test Loss = 0.255416, Learning Rate = 1.593764e-05\n",
      "Epoch 10893/20000: Train Loss = 0.439512, Test Loss = 0.251491, Learning Rate = 1.593159e-05\n",
      "Epoch 10894/20000: Train Loss = 0.439517, Test Loss = 0.253393, Learning Rate = 1.592553e-05\n",
      "Epoch 10895/20000: Train Loss = 0.439771, Test Loss = 0.249369, Learning Rate = 1.591948e-05\n",
      "Epoch 10896/20000: Train Loss = 0.439522, Test Loss = 0.249399, Learning Rate = 1.591343e-05\n",
      "Epoch 10897/20000: Train Loss = 0.439375, Test Loss = 0.250021, Learning Rate = 1.590739e-05\n",
      "Epoch 10898/20000: Train Loss = 0.439578, Test Loss = 0.250239, Learning Rate = 1.590134e-05\n",
      "Epoch 10899/20000: Train Loss = 0.439440, Test Loss = 0.249561, Learning Rate = 1.589530e-05\n",
      "Epoch 10900/20000: Train Loss = 0.439685, Test Loss = 0.251888, Learning Rate = 1.588926e-05\n",
      "Epoch 10901/20000: Train Loss = 0.439881, Test Loss = 0.251829, Learning Rate = 1.588322e-05\n",
      "Epoch 10902/20000: Train Loss = 0.439598, Test Loss = 0.249761, Learning Rate = 1.587719e-05\n",
      "Epoch 10903/20000: Train Loss = 0.439669, Test Loss = 0.254642, Learning Rate = 1.587116e-05\n",
      "Epoch 10904/20000: Train Loss = 0.439476, Test Loss = 0.251204, Learning Rate = 1.586512e-05\n",
      "Epoch 10905/20000: Train Loss = 0.440066, Test Loss = 0.251462, Learning Rate = 1.585910e-05\n",
      "Epoch 10906/20000: Train Loss = 0.439509, Test Loss = 0.249759, Learning Rate = 1.585307e-05\n",
      "Epoch 10907/20000: Train Loss = 0.439562, Test Loss = 0.250919, Learning Rate = 1.584705e-05\n",
      "Epoch 10908/20000: Train Loss = 0.439973, Test Loss = 0.247859, Learning Rate = 1.584103e-05\n",
      "Epoch 10909/20000: Train Loss = 0.439600, Test Loss = 0.250765, Learning Rate = 1.583501e-05\n",
      "Epoch 10910/20000: Train Loss = 0.439490, Test Loss = 0.250998, Learning Rate = 1.582899e-05\n",
      "Epoch 10911/20000: Train Loss = 0.439575, Test Loss = 0.251296, Learning Rate = 1.582297e-05\n",
      "Epoch 10912/20000: Train Loss = 0.439484, Test Loss = 0.250389, Learning Rate = 1.581696e-05\n",
      "Epoch 10913/20000: Train Loss = 0.439425, Test Loss = 0.249593, Learning Rate = 1.581095e-05\n",
      "Epoch 10914/20000: Train Loss = 0.439958, Test Loss = 0.251525, Learning Rate = 1.580494e-05\n",
      "Epoch 10915/20000: Train Loss = 0.439706, Test Loss = 0.248687, Learning Rate = 1.579894e-05\n",
      "Epoch 10916/20000: Train Loss = 0.439703, Test Loss = 0.251311, Learning Rate = 1.579294e-05\n",
      "Epoch 10917/20000: Train Loss = 0.439446, Test Loss = 0.250436, Learning Rate = 1.578694e-05\n",
      "Epoch 10918/20000: Train Loss = 0.439498, Test Loss = 0.250837, Learning Rate = 1.578094e-05\n",
      "Epoch 10919/20000: Train Loss = 0.439481, Test Loss = 0.252072, Learning Rate = 1.577494e-05\n",
      "Epoch 10920/20000: Train Loss = 0.439606, Test Loss = 0.252638, Learning Rate = 1.576895e-05\n",
      "Epoch 10921/20000: Train Loss = 0.439495, Test Loss = 0.251526, Learning Rate = 1.576295e-05\n",
      "Epoch 10922/20000: Train Loss = 0.439448, Test Loss = 0.248908, Learning Rate = 1.575697e-05\n",
      "Epoch 10923/20000: Train Loss = 0.439447, Test Loss = 0.249207, Learning Rate = 1.575098e-05\n",
      "Epoch 10924/20000: Train Loss = 0.439425, Test Loss = 0.248856, Learning Rate = 1.574499e-05\n",
      "Epoch 10925/20000: Train Loss = 0.439569, Test Loss = 0.250838, Learning Rate = 1.573901e-05\n",
      "Epoch 10926/20000: Train Loss = 0.439347, Test Loss = 0.248051, Learning Rate = 1.573303e-05\n",
      "Epoch 10927/20000: Train Loss = 0.439634, Test Loss = 0.247602, Learning Rate = 1.572705e-05\n",
      "Epoch 10928/20000: Train Loss = 0.439500, Test Loss = 0.249070, Learning Rate = 1.572108e-05\n",
      "Epoch 10929/20000: Train Loss = 0.439548, Test Loss = 0.247915, Learning Rate = 1.571510e-05\n",
      "Epoch 10930/20000: Train Loss = 0.439576, Test Loss = 0.248848, Learning Rate = 1.570913e-05\n",
      "Epoch 10931/20000: Train Loss = 0.439664, Test Loss = 0.248565, Learning Rate = 1.570316e-05\n",
      "Epoch 10932/20000: Train Loss = 0.439613, Test Loss = 0.245193, Learning Rate = 1.569720e-05\n",
      "Epoch 10933/20000: Train Loss = 0.439906, Test Loss = 0.243678, Learning Rate = 1.569123e-05\n",
      "Epoch 10934/20000: Train Loss = 0.439710, Test Loss = 0.250050, Learning Rate = 1.568527e-05\n",
      "Epoch 10935/20000: Train Loss = 0.439638, Test Loss = 0.250646, Learning Rate = 1.567931e-05\n",
      "Epoch 10936/20000: Train Loss = 0.439455, Test Loss = 0.247714, Learning Rate = 1.567335e-05\n",
      "Epoch 10937/20000: Train Loss = 0.439409, Test Loss = 0.249594, Learning Rate = 1.566740e-05\n",
      "Epoch 10938/20000: Train Loss = 0.439457, Test Loss = 0.249362, Learning Rate = 1.566144e-05\n",
      "Epoch 10939/20000: Train Loss = 0.439375, Test Loss = 0.251265, Learning Rate = 1.565549e-05\n",
      "Epoch 10940/20000: Train Loss = 0.439654, Test Loss = 0.249863, Learning Rate = 1.564954e-05\n",
      "Epoch 10941/20000: Train Loss = 0.439668, Test Loss = 0.247840, Learning Rate = 1.564360e-05\n",
      "Epoch 10942/20000: Train Loss = 0.439388, Test Loss = 0.248332, Learning Rate = 1.563765e-05\n",
      "Epoch 10943/20000: Train Loss = 0.440084, Test Loss = 0.247573, Learning Rate = 1.563171e-05\n",
      "Epoch 10944/20000: Train Loss = 0.439560, Test Loss = 0.251150, Learning Rate = 1.562577e-05\n",
      "Epoch 10945/20000: Train Loss = 0.439603, Test Loss = 0.249667, Learning Rate = 1.561983e-05\n",
      "Epoch 10946/20000: Train Loss = 0.439418, Test Loss = 0.249214, Learning Rate = 1.561390e-05\n",
      "Epoch 10947/20000: Train Loss = 0.439451, Test Loss = 0.250012, Learning Rate = 1.560797e-05\n",
      "Epoch 10948/20000: Train Loss = 0.439709, Test Loss = 0.249097, Learning Rate = 1.560203e-05\n",
      "Epoch 10949/20000: Train Loss = 0.439857, Test Loss = 0.246913, Learning Rate = 1.559611e-05\n",
      "Epoch 10950/20000: Train Loss = 0.439749, Test Loss = 0.248471, Learning Rate = 1.559018e-05\n",
      "Epoch 10951/20000: Train Loss = 0.439361, Test Loss = 0.249240, Learning Rate = 1.558426e-05\n",
      "Epoch 10952/20000: Train Loss = 0.439407, Test Loss = 0.251130, Learning Rate = 1.557833e-05\n",
      "Epoch 10953/20000: Train Loss = 0.439615, Test Loss = 0.247477, Learning Rate = 1.557242e-05\n",
      "Epoch 10954/20000: Train Loss = 0.440050, Test Loss = 0.248492, Learning Rate = 1.556650e-05\n",
      "Epoch 10955/20000: Train Loss = 0.439735, Test Loss = 0.247360, Learning Rate = 1.556058e-05\n",
      "Epoch 10956/20000: Train Loss = 0.439300, Test Loss = 0.250814, Learning Rate = 1.555467e-05\n",
      "Epoch 10957/20000: Train Loss = 0.439511, Test Loss = 0.248753, Learning Rate = 1.554876e-05\n",
      "Epoch 10958/20000: Train Loss = 0.439602, Test Loss = 0.251942, Learning Rate = 1.554285e-05\n",
      "Epoch 10959/20000: Train Loss = 0.439467, Test Loss = 0.247350, Learning Rate = 1.553695e-05\n",
      "Epoch 10960/20000: Train Loss = 0.439541, Test Loss = 0.252416, Learning Rate = 1.553104e-05\n",
      "Epoch 10961/20000: Train Loss = 0.439414, Test Loss = 0.249075, Learning Rate = 1.552514e-05\n",
      "Epoch 10962/20000: Train Loss = 0.439476, Test Loss = 0.249409, Learning Rate = 1.551924e-05\n",
      "Epoch 10963/20000: Train Loss = 0.439456, Test Loss = 0.251952, Learning Rate = 1.551335e-05\n",
      "Epoch 10964/20000: Train Loss = 0.439435, Test Loss = 0.251384, Learning Rate = 1.550745e-05\n",
      "Epoch 10965/20000: Train Loss = 0.439804, Test Loss = 0.253528, Learning Rate = 1.550156e-05\n",
      "Epoch 10966/20000: Train Loss = 0.439622, Test Loss = 0.249609, Learning Rate = 1.549567e-05\n",
      "Epoch 10967/20000: Train Loss = 0.439523, Test Loss = 0.246633, Learning Rate = 1.548978e-05\n",
      "Epoch 10968/20000: Train Loss = 0.439574, Test Loss = 0.249425, Learning Rate = 1.548389e-05\n",
      "Epoch 10969/20000: Train Loss = 0.439514, Test Loss = 0.248962, Learning Rate = 1.547801e-05\n",
      "Epoch 10970/20000: Train Loss = 0.439348, Test Loss = 0.250729, Learning Rate = 1.547213e-05\n",
      "Epoch 10971/20000: Train Loss = 0.439682, Test Loss = 0.248553, Learning Rate = 1.546625e-05\n",
      "Epoch 10972/20000: Train Loss = 0.439631, Test Loss = 0.250488, Learning Rate = 1.546037e-05\n",
      "Epoch 10973/20000: Train Loss = 0.439700, Test Loss = 0.247595, Learning Rate = 1.545450e-05\n",
      "Epoch 10974/20000: Train Loss = 0.439383, Test Loss = 0.249984, Learning Rate = 1.544863e-05\n",
      "Epoch 10975/20000: Train Loss = 0.439566, Test Loss = 0.250323, Learning Rate = 1.544276e-05\n",
      "Epoch 10976/20000: Train Loss = 0.439394, Test Loss = 0.253084, Learning Rate = 1.543689e-05\n",
      "Epoch 10977/20000: Train Loss = 0.439687, Test Loss = 0.249738, Learning Rate = 1.543102e-05\n",
      "Epoch 10978/20000: Train Loss = 0.439494, Test Loss = 0.249279, Learning Rate = 1.542516e-05\n",
      "Epoch 10979/20000: Train Loss = 0.439472, Test Loss = 0.253135, Learning Rate = 1.541930e-05\n",
      "Epoch 10980/20000: Train Loss = 0.439502, Test Loss = 0.249838, Learning Rate = 1.541344e-05\n",
      "Epoch 10981/20000: Train Loss = 0.439377, Test Loss = 0.250933, Learning Rate = 1.540758e-05\n",
      "Epoch 10982/20000: Train Loss = 0.439477, Test Loss = 0.249464, Learning Rate = 1.540173e-05\n",
      "Epoch 10983/20000: Train Loss = 0.439811, Test Loss = 0.253844, Learning Rate = 1.539588e-05\n",
      "Epoch 10984/20000: Train Loss = 0.439578, Test Loss = 0.249269, Learning Rate = 1.539003e-05\n",
      "Epoch 10985/20000: Train Loss = 0.439453, Test Loss = 0.249094, Learning Rate = 1.538418e-05\n",
      "Epoch 10986/20000: Train Loss = 0.439682, Test Loss = 0.250093, Learning Rate = 1.537833e-05\n",
      "Epoch 10987/20000: Train Loss = 0.439401, Test Loss = 0.249214, Learning Rate = 1.537249e-05\n",
      "Epoch 10988/20000: Train Loss = 0.439620, Test Loss = 0.247176, Learning Rate = 1.536665e-05\n",
      "Epoch 10989/20000: Train Loss = 0.439970, Test Loss = 0.243461, Learning Rate = 1.536081e-05\n",
      "Epoch 10990/20000: Train Loss = 0.439514, Test Loss = 0.245714, Learning Rate = 1.535497e-05\n",
      "Epoch 10991/20000: Train Loss = 0.439517, Test Loss = 0.242849, Learning Rate = 1.534914e-05\n",
      "Epoch 10992/20000: Train Loss = 0.439510, Test Loss = 0.249037, Learning Rate = 1.534331e-05\n",
      "Epoch 10993/20000: Train Loss = 0.439542, Test Loss = 0.247394, Learning Rate = 1.533748e-05\n",
      "Epoch 10994/20000: Train Loss = 0.439409, Test Loss = 0.249010, Learning Rate = 1.533165e-05\n",
      "Epoch 10995/20000: Train Loss = 0.439480, Test Loss = 0.245992, Learning Rate = 1.532582e-05\n",
      "Epoch 10996/20000: Train Loss = 0.439374, Test Loss = 0.247038, Learning Rate = 1.532000e-05\n",
      "Epoch 10997/20000: Train Loss = 0.439543, Test Loss = 0.248372, Learning Rate = 1.531418e-05\n",
      "Epoch 10998/20000: Train Loss = 0.439426, Test Loss = 0.251241, Learning Rate = 1.530836e-05\n",
      "Epoch 10999/20000: Train Loss = 0.439647, Test Loss = 0.250151, Learning Rate = 1.530254e-05\n",
      "Epoch 11000/20000: Train Loss = 0.439566, Test Loss = 0.250953, Learning Rate = 1.529673e-05\n",
      "Epoch 11001/20000: Train Loss = 0.439574, Test Loss = 0.248449, Learning Rate = 1.529092e-05\n",
      "Epoch 11002/20000: Train Loss = 0.439614, Test Loss = 0.250380, Learning Rate = 1.528511e-05\n",
      "Epoch 11003/20000: Train Loss = 0.439482, Test Loss = 0.249178, Learning Rate = 1.527930e-05\n",
      "Epoch 11004/20000: Train Loss = 0.439600, Test Loss = 0.247509, Learning Rate = 1.527349e-05\n",
      "Epoch 11005/20000: Train Loss = 0.439460, Test Loss = 0.250819, Learning Rate = 1.526769e-05\n",
      "Epoch 11006/20000: Train Loss = 0.439500, Test Loss = 0.249014, Learning Rate = 1.526189e-05\n",
      "Epoch 11007/20000: Train Loss = 0.439422, Test Loss = 0.250705, Learning Rate = 1.525609e-05\n",
      "Epoch 11008/20000: Train Loss = 0.439497, Test Loss = 0.253928, Learning Rate = 1.525029e-05\n",
      "Epoch 11009/20000: Train Loss = 0.439419, Test Loss = 0.250371, Learning Rate = 1.524450e-05\n",
      "Epoch 11010/20000: Train Loss = 0.439622, Test Loss = 0.248940, Learning Rate = 1.523870e-05\n",
      "Epoch 11011/20000: Train Loss = 0.439589, Test Loss = 0.251702, Learning Rate = 1.523291e-05\n",
      "Epoch 11012/20000: Train Loss = 0.439888, Test Loss = 0.248630, Learning Rate = 1.522713e-05\n",
      "Epoch 11013/20000: Train Loss = 0.439445, Test Loss = 0.250862, Learning Rate = 1.522134e-05\n",
      "Epoch 11014/20000: Train Loss = 0.439439, Test Loss = 0.248565, Learning Rate = 1.521556e-05\n",
      "Epoch 11015/20000: Train Loss = 0.439721, Test Loss = 0.249839, Learning Rate = 1.520977e-05\n",
      "Epoch 11016/20000: Train Loss = 0.439424, Test Loss = 0.246487, Learning Rate = 1.520400e-05\n",
      "Epoch 11017/20000: Train Loss = 0.439697, Test Loss = 0.245318, Learning Rate = 1.519822e-05\n",
      "Epoch 11018/20000: Train Loss = 0.439863, Test Loss = 0.249508, Learning Rate = 1.519244e-05\n",
      "Epoch 11019/20000: Train Loss = 0.439547, Test Loss = 0.243475, Learning Rate = 1.518667e-05\n",
      "Epoch 11020/20000: Train Loss = 0.439864, Test Loss = 0.246187, Learning Rate = 1.518090e-05\n",
      "Epoch 11021/20000: Train Loss = 0.439534, Test Loss = 0.246895, Learning Rate = 1.517513e-05\n",
      "Epoch 11022/20000: Train Loss = 0.439497, Test Loss = 0.246010, Learning Rate = 1.516937e-05\n",
      "Epoch 11023/20000: Train Loss = 0.439783, Test Loss = 0.248237, Learning Rate = 1.516360e-05\n",
      "Epoch 11024/20000: Train Loss = 0.439368, Test Loss = 0.249128, Learning Rate = 1.515784e-05\n",
      "Epoch 11025/20000: Train Loss = 0.439534, Test Loss = 0.247357, Learning Rate = 1.515208e-05\n",
      "Epoch 11026/20000: Train Loss = 0.439688, Test Loss = 0.254117, Learning Rate = 1.514632e-05\n",
      "Epoch 11027/20000: Train Loss = 0.440117, Test Loss = 0.247403, Learning Rate = 1.514057e-05\n",
      "Epoch 11028/20000: Train Loss = 0.440014, Test Loss = 0.252159, Learning Rate = 1.513481e-05\n",
      "Epoch 11029/20000: Train Loss = 0.439507, Test Loss = 0.251051, Learning Rate = 1.512906e-05\n",
      "Epoch 11030/20000: Train Loss = 0.439445, Test Loss = 0.252270, Learning Rate = 1.512332e-05\n",
      "Epoch 11031/20000: Train Loss = 0.439427, Test Loss = 0.250351, Learning Rate = 1.511757e-05\n",
      "Epoch 11032/20000: Train Loss = 0.439590, Test Loss = 0.251555, Learning Rate = 1.511182e-05\n",
      "Epoch 11033/20000: Train Loss = 0.439389, Test Loss = 0.249601, Learning Rate = 1.510608e-05\n",
      "Epoch 11034/20000: Train Loss = 0.439587, Test Loss = 0.249899, Learning Rate = 1.510034e-05\n",
      "Epoch 11035/20000: Train Loss = 0.439487, Test Loss = 0.251436, Learning Rate = 1.509460e-05\n",
      "Epoch 11036/20000: Train Loss = 0.439347, Test Loss = 0.253994, Learning Rate = 1.508887e-05\n",
      "Epoch 11037/20000: Train Loss = 0.439368, Test Loss = 0.252466, Learning Rate = 1.508314e-05\n",
      "Epoch 11038/20000: Train Loss = 0.439684, Test Loss = 0.253116, Learning Rate = 1.507740e-05\n",
      "Epoch 11039/20000: Train Loss = 0.439828, Test Loss = 0.256845, Learning Rate = 1.507168e-05\n",
      "Epoch 11040/20000: Train Loss = 0.439447, Test Loss = 0.254895, Learning Rate = 1.506595e-05\n",
      "Epoch 11041/20000: Train Loss = 0.439706, Test Loss = 0.253938, Learning Rate = 1.506022e-05\n",
      "Epoch 11042/20000: Train Loss = 0.439384, Test Loss = 0.251077, Learning Rate = 1.505450e-05\n",
      "Epoch 11043/20000: Train Loss = 0.439428, Test Loss = 0.247838, Learning Rate = 1.504878e-05\n",
      "Epoch 11044/20000: Train Loss = 0.439580, Test Loss = 0.249590, Learning Rate = 1.504306e-05\n",
      "Epoch 11045/20000: Train Loss = 0.439600, Test Loss = 0.252152, Learning Rate = 1.503735e-05\n",
      "Epoch 11046/20000: Train Loss = 0.439584, Test Loss = 0.248048, Learning Rate = 1.503163e-05\n",
      "Epoch 11047/20000: Train Loss = 0.439410, Test Loss = 0.250814, Learning Rate = 1.502592e-05\n",
      "Epoch 11048/20000: Train Loss = 0.439539, Test Loss = 0.252717, Learning Rate = 1.502021e-05\n",
      "Epoch 11049/20000: Train Loss = 0.439787, Test Loss = 0.246865, Learning Rate = 1.501451e-05\n",
      "Epoch 11050/20000: Train Loss = 0.439494, Test Loss = 0.247254, Learning Rate = 1.500880e-05\n",
      "Epoch 11051/20000: Train Loss = 0.439641, Test Loss = 0.252840, Learning Rate = 1.500310e-05\n",
      "Epoch 11052/20000: Train Loss = 0.439659, Test Loss = 0.248511, Learning Rate = 1.499740e-05\n",
      "Epoch 11053/20000: Train Loss = 0.439483, Test Loss = 0.250383, Learning Rate = 1.499170e-05\n",
      "Epoch 11054/20000: Train Loss = 0.439504, Test Loss = 0.249185, Learning Rate = 1.498600e-05\n",
      "Epoch 11055/20000: Train Loss = 0.439540, Test Loss = 0.249383, Learning Rate = 1.498031e-05\n",
      "Epoch 11056/20000: Train Loss = 0.439560, Test Loss = 0.250438, Learning Rate = 1.497462e-05\n",
      "Epoch 11057/20000: Train Loss = 0.439414, Test Loss = 0.248405, Learning Rate = 1.496893e-05\n",
      "Epoch 11058/20000: Train Loss = 0.439370, Test Loss = 0.249808, Learning Rate = 1.496324e-05\n",
      "Epoch 11059/20000: Train Loss = 0.439419, Test Loss = 0.248303, Learning Rate = 1.495755e-05\n",
      "Epoch 11060/20000: Train Loss = 0.439497, Test Loss = 0.249999, Learning Rate = 1.495187e-05\n",
      "Epoch 11061/20000: Train Loss = 0.439577, Test Loss = 0.251766, Learning Rate = 1.494619e-05\n",
      "Epoch 11062/20000: Train Loss = 0.439467, Test Loss = 0.252844, Learning Rate = 1.494051e-05\n",
      "Epoch 11063/20000: Train Loss = 0.439422, Test Loss = 0.250771, Learning Rate = 1.493483e-05\n",
      "Epoch 11064/20000: Train Loss = 0.439491, Test Loss = 0.252091, Learning Rate = 1.492916e-05\n",
      "Epoch 11065/20000: Train Loss = 0.439441, Test Loss = 0.250543, Learning Rate = 1.492348e-05\n",
      "Epoch 11066/20000: Train Loss = 0.439368, Test Loss = 0.252170, Learning Rate = 1.491781e-05\n",
      "Epoch 11067/20000: Train Loss = 0.439773, Test Loss = 0.250450, Learning Rate = 1.491214e-05\n",
      "Epoch 11068/20000: Train Loss = 0.439470, Test Loss = 0.250467, Learning Rate = 1.490648e-05\n",
      "Epoch 11069/20000: Train Loss = 0.439510, Test Loss = 0.251523, Learning Rate = 1.490081e-05\n",
      "Epoch 11070/20000: Train Loss = 0.439677, Test Loss = 0.250138, Learning Rate = 1.489515e-05\n",
      "Epoch 11071/20000: Train Loss = 0.439649, Test Loss = 0.252705, Learning Rate = 1.488949e-05\n",
      "Epoch 11072/20000: Train Loss = 0.439738, Test Loss = 0.249851, Learning Rate = 1.488383e-05\n",
      "Epoch 11073/20000: Train Loss = 0.439684, Test Loss = 0.250935, Learning Rate = 1.487818e-05\n",
      "Epoch 11074/20000: Train Loss = 0.439370, Test Loss = 0.249836, Learning Rate = 1.487253e-05\n",
      "Epoch 11075/20000: Train Loss = 0.439740, Test Loss = 0.253575, Learning Rate = 1.486688e-05\n",
      "Epoch 11076/20000: Train Loss = 0.439415, Test Loss = 0.250222, Learning Rate = 1.486123e-05\n",
      "Epoch 11077/20000: Train Loss = 0.439483, Test Loss = 0.252480, Learning Rate = 1.485558e-05\n",
      "Epoch 11078/20000: Train Loss = 0.439579, Test Loss = 0.252649, Learning Rate = 1.484993e-05\n",
      "Epoch 11079/20000: Train Loss = 0.439759, Test Loss = 0.251846, Learning Rate = 1.484429e-05\n",
      "Epoch 11080/20000: Train Loss = 0.439690, Test Loss = 0.252170, Learning Rate = 1.483865e-05\n",
      "Epoch 11081/20000: Train Loss = 0.439420, Test Loss = 0.250183, Learning Rate = 1.483301e-05\n",
      "Epoch 11082/20000: Train Loss = 0.439688, Test Loss = 0.253705, Learning Rate = 1.482738e-05\n",
      "Epoch 11083/20000: Train Loss = 0.439736, Test Loss = 0.248638, Learning Rate = 1.482174e-05\n",
      "Epoch 11084/20000: Train Loss = 0.440445, Test Loss = 0.249864, Learning Rate = 1.481611e-05\n",
      "Epoch 11085/20000: Train Loss = 0.439901, Test Loss = 0.247978, Learning Rate = 1.481048e-05\n",
      "Epoch 11086/20000: Train Loss = 0.439479, Test Loss = 0.253505, Learning Rate = 1.480485e-05\n",
      "Epoch 11087/20000: Train Loss = 0.439470, Test Loss = 0.249561, Learning Rate = 1.479923e-05\n",
      "Epoch 11088/20000: Train Loss = 0.439351, Test Loss = 0.252220, Learning Rate = 1.479361e-05\n",
      "Epoch 11089/20000: Train Loss = 0.439502, Test Loss = 0.253982, Learning Rate = 1.478798e-05\n",
      "Epoch 11090/20000: Train Loss = 0.439511, Test Loss = 0.251242, Learning Rate = 1.478236e-05\n",
      "Epoch 11091/20000: Train Loss = 0.439609, Test Loss = 0.249259, Learning Rate = 1.477675e-05\n",
      "Epoch 11092/20000: Train Loss = 0.439630, Test Loss = 0.247317, Learning Rate = 1.477113e-05\n",
      "Epoch 11093/20000: Train Loss = 0.439348, Test Loss = 0.250889, Learning Rate = 1.476552e-05\n",
      "Epoch 11094/20000: Train Loss = 0.439651, Test Loss = 0.250812, Learning Rate = 1.475991e-05\n",
      "Epoch 11095/20000: Train Loss = 0.439608, Test Loss = 0.246296, Learning Rate = 1.475430e-05\n",
      "Epoch 11096/20000: Train Loss = 0.439621, Test Loss = 0.249983, Learning Rate = 1.474870e-05\n",
      "Epoch 11097/20000: Train Loss = 0.439688, Test Loss = 0.246685, Learning Rate = 1.474309e-05\n",
      "Epoch 11098/20000: Train Loss = 0.439443, Test Loss = 0.247587, Learning Rate = 1.473749e-05\n",
      "Epoch 11099/20000: Train Loss = 0.439542, Test Loss = 0.251933, Learning Rate = 1.473189e-05\n",
      "Epoch 11100/20000: Train Loss = 0.439515, Test Loss = 0.252205, Learning Rate = 1.472629e-05\n",
      "Epoch 11101/20000: Train Loss = 0.439496, Test Loss = 0.252793, Learning Rate = 1.472070e-05\n",
      "Epoch 11102/20000: Train Loss = 0.439508, Test Loss = 0.250144, Learning Rate = 1.471510e-05\n",
      "Epoch 11103/20000: Train Loss = 0.439586, Test Loss = 0.249193, Learning Rate = 1.470951e-05\n",
      "Epoch 11104/20000: Train Loss = 0.439651, Test Loss = 0.248799, Learning Rate = 1.470392e-05\n",
      "Epoch 11105/20000: Train Loss = 0.439476, Test Loss = 0.251143, Learning Rate = 1.469834e-05\n",
      "Epoch 11106/20000: Train Loss = 0.439527, Test Loss = 0.249826, Learning Rate = 1.469275e-05\n",
      "Epoch 11107/20000: Train Loss = 0.439512, Test Loss = 0.248850, Learning Rate = 1.468717e-05\n",
      "Epoch 11108/20000: Train Loss = 0.439583, Test Loss = 0.249125, Learning Rate = 1.468159e-05\n",
      "Epoch 11109/20000: Train Loss = 0.439610, Test Loss = 0.255418, Learning Rate = 1.467601e-05\n",
      "Epoch 11110/20000: Train Loss = 0.439525, Test Loss = 0.249402, Learning Rate = 1.467043e-05\n",
      "Epoch 11111/20000: Train Loss = 0.439507, Test Loss = 0.248205, Learning Rate = 1.466486e-05\n",
      "Epoch 11112/20000: Train Loss = 0.439428, Test Loss = 0.249330, Learning Rate = 1.465928e-05\n",
      "Epoch 11113/20000: Train Loss = 0.439450, Test Loss = 0.249352, Learning Rate = 1.465371e-05\n",
      "Epoch 11114/20000: Train Loss = 0.439547, Test Loss = 0.246700, Learning Rate = 1.464815e-05\n",
      "Epoch 11115/20000: Train Loss = 0.439505, Test Loss = 0.250028, Learning Rate = 1.464258e-05\n",
      "Epoch 11116/20000: Train Loss = 0.439654, Test Loss = 0.251089, Learning Rate = 1.463702e-05\n",
      "Epoch 11117/20000: Train Loss = 0.439408, Test Loss = 0.249594, Learning Rate = 1.463146e-05\n",
      "Epoch 11118/20000: Train Loss = 0.439613, Test Loss = 0.252584, Learning Rate = 1.462590e-05\n",
      "Epoch 11119/20000: Train Loss = 0.439442, Test Loss = 0.250361, Learning Rate = 1.462034e-05\n",
      "Epoch 11120/20000: Train Loss = 0.439655, Test Loss = 0.248902, Learning Rate = 1.461478e-05\n",
      "Epoch 11121/20000: Train Loss = 0.439402, Test Loss = 0.249034, Learning Rate = 1.460923e-05\n",
      "Epoch 11122/20000: Train Loss = 0.439602, Test Loss = 0.248442, Learning Rate = 1.460368e-05\n",
      "Epoch 11123/20000: Train Loss = 0.439465, Test Loss = 0.246990, Learning Rate = 1.459813e-05\n",
      "Epoch 11124/20000: Train Loss = 0.439490, Test Loss = 0.245891, Learning Rate = 1.459258e-05\n",
      "Epoch 11125/20000: Train Loss = 0.439400, Test Loss = 0.249197, Learning Rate = 1.458704e-05\n",
      "Epoch 11126/20000: Train Loss = 0.439812, Test Loss = 0.246589, Learning Rate = 1.458150e-05\n",
      "Epoch 11127/20000: Train Loss = 0.439449, Test Loss = 0.246984, Learning Rate = 1.457595e-05\n",
      "Epoch 11128/20000: Train Loss = 0.439371, Test Loss = 0.250238, Learning Rate = 1.457042e-05\n",
      "Epoch 11129/20000: Train Loss = 0.439360, Test Loss = 0.249651, Learning Rate = 1.456488e-05\n",
      "Epoch 11130/20000: Train Loss = 0.439497, Test Loss = 0.249631, Learning Rate = 1.455935e-05\n",
      "Epoch 11131/20000: Train Loss = 0.439439, Test Loss = 0.251247, Learning Rate = 1.455381e-05\n",
      "Epoch 11132/20000: Train Loss = 0.439526, Test Loss = 0.252161, Learning Rate = 1.454828e-05\n",
      "Epoch 11133/20000: Train Loss = 0.439441, Test Loss = 0.250971, Learning Rate = 1.454276e-05\n",
      "Epoch 11134/20000: Train Loss = 0.439394, Test Loss = 0.251414, Learning Rate = 1.453723e-05\n",
      "Epoch 11135/20000: Train Loss = 0.439626, Test Loss = 0.251772, Learning Rate = 1.453171e-05\n",
      "Epoch 11136/20000: Train Loss = 0.439484, Test Loss = 0.253383, Learning Rate = 1.452618e-05\n",
      "Epoch 11137/20000: Train Loss = 0.439869, Test Loss = 0.250201, Learning Rate = 1.452066e-05\n",
      "Epoch 11138/20000: Train Loss = 0.439488, Test Loss = 0.252964, Learning Rate = 1.451515e-05\n",
      "Epoch 11139/20000: Train Loss = 0.439452, Test Loss = 0.250041, Learning Rate = 1.450963e-05\n",
      "Epoch 11140/20000: Train Loss = 0.439642, Test Loss = 0.253970, Learning Rate = 1.450412e-05\n",
      "Epoch 11141/20000: Train Loss = 0.439321, Test Loss = 0.251222, Learning Rate = 1.449861e-05\n",
      "Epoch 11142/20000: Train Loss = 0.439555, Test Loss = 0.252563, Learning Rate = 1.449310e-05\n",
      "Epoch 11143/20000: Train Loss = 0.439427, Test Loss = 0.253674, Learning Rate = 1.448759e-05\n",
      "Epoch 11144/20000: Train Loss = 0.439632, Test Loss = 0.248314, Learning Rate = 1.448209e-05\n",
      "Epoch 11145/20000: Train Loss = 0.439555, Test Loss = 0.252373, Learning Rate = 1.447658e-05\n",
      "Epoch 11146/20000: Train Loss = 0.439525, Test Loss = 0.250585, Learning Rate = 1.447108e-05\n",
      "Epoch 11147/20000: Train Loss = 0.439437, Test Loss = 0.250951, Learning Rate = 1.446558e-05\n",
      "Epoch 11148/20000: Train Loss = 0.439401, Test Loss = 0.256459, Learning Rate = 1.446009e-05\n",
      "Epoch 11149/20000: Train Loss = 0.439543, Test Loss = 0.256309, Learning Rate = 1.445459e-05\n",
      "Epoch 11150/20000: Train Loss = 0.439702, Test Loss = 0.258184, Learning Rate = 1.444910e-05\n",
      "Epoch 11151/20000: Train Loss = 0.439948, Test Loss = 0.252409, Learning Rate = 1.444361e-05\n",
      "Epoch 11152/20000: Train Loss = 0.439666, Test Loss = 0.253276, Learning Rate = 1.443812e-05\n",
      "Epoch 11153/20000: Train Loss = 0.439854, Test Loss = 0.256666, Learning Rate = 1.443264e-05\n",
      "Epoch 11154/20000: Train Loss = 0.439366, Test Loss = 0.250992, Learning Rate = 1.442715e-05\n",
      "Epoch 11155/20000: Train Loss = 0.439440, Test Loss = 0.248884, Learning Rate = 1.442167e-05\n",
      "Epoch 11156/20000: Train Loss = 0.439343, Test Loss = 0.250112, Learning Rate = 1.441619e-05\n",
      "Epoch 11157/20000: Train Loss = 0.439664, Test Loss = 0.251579, Learning Rate = 1.441071e-05\n",
      "Epoch 11158/20000: Train Loss = 0.439760, Test Loss = 0.247895, Learning Rate = 1.440524e-05\n",
      "Epoch 11159/20000: Train Loss = 0.439483, Test Loss = 0.248062, Learning Rate = 1.439976e-05\n",
      "Epoch 11160/20000: Train Loss = 0.439658, Test Loss = 0.245996, Learning Rate = 1.439429e-05\n",
      "Epoch 11161/20000: Train Loss = 0.439555, Test Loss = 0.246111, Learning Rate = 1.438882e-05\n",
      "Epoch 11162/20000: Train Loss = 0.439701, Test Loss = 0.253809, Learning Rate = 1.438336e-05\n",
      "Epoch 11163/20000: Train Loss = 0.439547, Test Loss = 0.249103, Learning Rate = 1.437789e-05\n",
      "Epoch 11164/20000: Train Loss = 0.439688, Test Loss = 0.252105, Learning Rate = 1.437243e-05\n",
      "Epoch 11165/20000: Train Loss = 0.439518, Test Loss = 0.252730, Learning Rate = 1.436697e-05\n",
      "Epoch 11166/20000: Train Loss = 0.439486, Test Loss = 0.250376, Learning Rate = 1.436151e-05\n",
      "Epoch 11167/20000: Train Loss = 0.439421, Test Loss = 0.249497, Learning Rate = 1.435605e-05\n",
      "Epoch 11168/20000: Train Loss = 0.439481, Test Loss = 0.249797, Learning Rate = 1.435059e-05\n",
      "Epoch 11169/20000: Train Loss = 0.439503, Test Loss = 0.251508, Learning Rate = 1.434514e-05\n",
      "Epoch 11170/20000: Train Loss = 0.439511, Test Loss = 0.248821, Learning Rate = 1.433969e-05\n",
      "Epoch 11171/20000: Train Loss = 0.439725, Test Loss = 0.253247, Learning Rate = 1.433424e-05\n",
      "Epoch 11172/20000: Train Loss = 0.439388, Test Loss = 0.250379, Learning Rate = 1.432880e-05\n",
      "Epoch 11173/20000: Train Loss = 0.439408, Test Loss = 0.251415, Learning Rate = 1.432335e-05\n",
      "Epoch 11174/20000: Train Loss = 0.439481, Test Loss = 0.248284, Learning Rate = 1.431791e-05\n",
      "Epoch 11175/20000: Train Loss = 0.439386, Test Loss = 0.248647, Learning Rate = 1.431247e-05\n",
      "Epoch 11176/20000: Train Loss = 0.439569, Test Loss = 0.246870, Learning Rate = 1.430703e-05\n",
      "Epoch 11177/20000: Train Loss = 0.439448, Test Loss = 0.248035, Learning Rate = 1.430159e-05\n",
      "Epoch 11178/20000: Train Loss = 0.439649, Test Loss = 0.250051, Learning Rate = 1.429616e-05\n",
      "Epoch 11179/20000: Train Loss = 0.439592, Test Loss = 0.247296, Learning Rate = 1.429073e-05\n",
      "Epoch 11180/20000: Train Loss = 0.439641, Test Loss = 0.250667, Learning Rate = 1.428530e-05\n",
      "Epoch 11181/20000: Train Loss = 0.439464, Test Loss = 0.246894, Learning Rate = 1.427987e-05\n",
      "Epoch 11182/20000: Train Loss = 0.439510, Test Loss = 0.249610, Learning Rate = 1.427444e-05\n",
      "Epoch 11183/20000: Train Loss = 0.439433, Test Loss = 0.245659, Learning Rate = 1.426902e-05\n",
      "Epoch 11184/20000: Train Loss = 0.439416, Test Loss = 0.247165, Learning Rate = 1.426360e-05\n",
      "Epoch 11185/20000: Train Loss = 0.439480, Test Loss = 0.248424, Learning Rate = 1.425818e-05\n",
      "Epoch 11186/20000: Train Loss = 0.439832, Test Loss = 0.252273, Learning Rate = 1.425276e-05\n",
      "Epoch 11187/20000: Train Loss = 0.439572, Test Loss = 0.250598, Learning Rate = 1.424734e-05\n",
      "Epoch 11188/20000: Train Loss = 0.439650, Test Loss = 0.252934, Learning Rate = 1.424193e-05\n",
      "Epoch 11189/20000: Train Loss = 0.439325, Test Loss = 0.249720, Learning Rate = 1.423652e-05\n",
      "Epoch 11190/20000: Train Loss = 0.439441, Test Loss = 0.252510, Learning Rate = 1.423111e-05\n",
      "Epoch 11191/20000: Train Loss = 0.439615, Test Loss = 0.250392, Learning Rate = 1.422570e-05\n",
      "Epoch 11192/20000: Train Loss = 0.439529, Test Loss = 0.253599, Learning Rate = 1.422030e-05\n",
      "Epoch 11193/20000: Train Loss = 0.439486, Test Loss = 0.250738, Learning Rate = 1.421489e-05\n",
      "Epoch 11194/20000: Train Loss = 0.439574, Test Loss = 0.251512, Learning Rate = 1.420949e-05\n",
      "Epoch 11195/20000: Train Loss = 0.439486, Test Loss = 0.250687, Learning Rate = 1.420409e-05\n",
      "Epoch 11196/20000: Train Loss = 0.439562, Test Loss = 0.248796, Learning Rate = 1.419870e-05\n",
      "Epoch 11197/20000: Train Loss = 0.439517, Test Loss = 0.248588, Learning Rate = 1.419330e-05\n",
      "Epoch 11198/20000: Train Loss = 0.439479, Test Loss = 0.246599, Learning Rate = 1.418791e-05\n",
      "Epoch 11199/20000: Train Loss = 0.439471, Test Loss = 0.249954, Learning Rate = 1.418252e-05\n",
      "Epoch 11200/20000: Train Loss = 0.439590, Test Loss = 0.246492, Learning Rate = 1.417713e-05\n",
      "Epoch 11201/20000: Train Loss = 0.439400, Test Loss = 0.248498, Learning Rate = 1.417174e-05\n",
      "Epoch 11202/20000: Train Loss = 0.439484, Test Loss = 0.255085, Learning Rate = 1.416636e-05\n",
      "Epoch 11203/20000: Train Loss = 0.439638, Test Loss = 0.251117, Learning Rate = 1.416097e-05\n",
      "Epoch 11204/20000: Train Loss = 0.439409, Test Loss = 0.251561, Learning Rate = 1.415559e-05\n",
      "Epoch 11205/20000: Train Loss = 0.439390, Test Loss = 0.252582, Learning Rate = 1.415021e-05\n",
      "Epoch 11206/20000: Train Loss = 0.439548, Test Loss = 0.251017, Learning Rate = 1.414484e-05\n",
      "Epoch 11207/20000: Train Loss = 0.439503, Test Loss = 0.254626, Learning Rate = 1.413946e-05\n",
      "Epoch 11208/20000: Train Loss = 0.439438, Test Loss = 0.253077, Learning Rate = 1.413409e-05\n",
      "Epoch 11209/20000: Train Loss = 0.439395, Test Loss = 0.253244, Learning Rate = 1.412872e-05\n",
      "Epoch 11210/20000: Train Loss = 0.439415, Test Loss = 0.251057, Learning Rate = 1.412335e-05\n",
      "Epoch 11211/20000: Train Loss = 0.439479, Test Loss = 0.251669, Learning Rate = 1.411798e-05\n",
      "Epoch 11212/20000: Train Loss = 0.439400, Test Loss = 0.253614, Learning Rate = 1.411262e-05\n",
      "Epoch 11213/20000: Train Loss = 0.439470, Test Loss = 0.250476, Learning Rate = 1.410726e-05\n",
      "Epoch 11214/20000: Train Loss = 0.439538, Test Loss = 0.250397, Learning Rate = 1.410190e-05\n",
      "Epoch 11215/20000: Train Loss = 0.439445, Test Loss = 0.251096, Learning Rate = 1.409654e-05\n",
      "Epoch 11216/20000: Train Loss = 0.439431, Test Loss = 0.250806, Learning Rate = 1.409118e-05\n",
      "Epoch 11217/20000: Train Loss = 0.439587, Test Loss = 0.248476, Learning Rate = 1.408583e-05\n",
      "Epoch 11218/20000: Train Loss = 0.439382, Test Loss = 0.248337, Learning Rate = 1.408048e-05\n",
      "Epoch 11219/20000: Train Loss = 0.439488, Test Loss = 0.247251, Learning Rate = 1.407513e-05\n",
      "Epoch 11220/20000: Train Loss = 0.439453, Test Loss = 0.249951, Learning Rate = 1.406978e-05\n",
      "Epoch 11221/20000: Train Loss = 0.439539, Test Loss = 0.250303, Learning Rate = 1.406443e-05\n",
      "Epoch 11222/20000: Train Loss = 0.439390, Test Loss = 0.251031, Learning Rate = 1.405909e-05\n",
      "Epoch 11223/20000: Train Loss = 0.439413, Test Loss = 0.253795, Learning Rate = 1.405375e-05\n",
      "Epoch 11224/20000: Train Loss = 0.439455, Test Loss = 0.250867, Learning Rate = 1.404840e-05\n",
      "Epoch 11225/20000: Train Loss = 0.439428, Test Loss = 0.249282, Learning Rate = 1.404307e-05\n",
      "Epoch 11226/20000: Train Loss = 0.439430, Test Loss = 0.250916, Learning Rate = 1.403773e-05\n",
      "Epoch 11227/20000: Train Loss = 0.439448, Test Loss = 0.249082, Learning Rate = 1.403240e-05\n",
      "Epoch 11228/20000: Train Loss = 0.439588, Test Loss = 0.247755, Learning Rate = 1.402707e-05\n",
      "Epoch 11229/20000: Train Loss = 0.439706, Test Loss = 0.252870, Learning Rate = 1.402174e-05\n",
      "Epoch 11230/20000: Train Loss = 0.439762, Test Loss = 0.248533, Learning Rate = 1.401641e-05\n",
      "Epoch 11231/20000: Train Loss = 0.439343, Test Loss = 0.250838, Learning Rate = 1.401108e-05\n",
      "Epoch 11232/20000: Train Loss = 0.439583, Test Loss = 0.252400, Learning Rate = 1.400576e-05\n",
      "Epoch 11233/20000: Train Loss = 0.439487, Test Loss = 0.251020, Learning Rate = 1.400044e-05\n",
      "Epoch 11234/20000: Train Loss = 0.439457, Test Loss = 0.250218, Learning Rate = 1.399512e-05\n",
      "Epoch 11235/20000: Train Loss = 0.439473, Test Loss = 0.248019, Learning Rate = 1.398980e-05\n",
      "Epoch 11236/20000: Train Loss = 0.439357, Test Loss = 0.252388, Learning Rate = 1.398448e-05\n",
      "Epoch 11237/20000: Train Loss = 0.439405, Test Loss = 0.250891, Learning Rate = 1.397917e-05\n",
      "Epoch 11238/20000: Train Loss = 0.439437, Test Loss = 0.248919, Learning Rate = 1.397386e-05\n",
      "Epoch 11239/20000: Train Loss = 0.439393, Test Loss = 0.250377, Learning Rate = 1.396855e-05\n",
      "Epoch 11240/20000: Train Loss = 0.439313, Test Loss = 0.251287, Learning Rate = 1.396324e-05\n",
      "Epoch 11241/20000: Train Loss = 0.439360, Test Loss = 0.249564, Learning Rate = 1.395793e-05\n",
      "Epoch 11242/20000: Train Loss = 0.439409, Test Loss = 0.251055, Learning Rate = 1.395263e-05\n",
      "Epoch 11243/20000: Train Loss = 0.439567, Test Loss = 0.253518, Learning Rate = 1.394733e-05\n",
      "Epoch 11244/20000: Train Loss = 0.439559, Test Loss = 0.251601, Learning Rate = 1.394203e-05\n",
      "Epoch 11245/20000: Train Loss = 0.439433, Test Loss = 0.251483, Learning Rate = 1.393673e-05\n",
      "Epoch 11246/20000: Train Loss = 0.439508, Test Loss = 0.250805, Learning Rate = 1.393144e-05\n",
      "Epoch 11247/20000: Train Loss = 0.439517, Test Loss = 0.246551, Learning Rate = 1.392614e-05\n",
      "Epoch 11248/20000: Train Loss = 0.439701, Test Loss = 0.251778, Learning Rate = 1.392085e-05\n",
      "Epoch 11249/20000: Train Loss = 0.439563, Test Loss = 0.247418, Learning Rate = 1.391556e-05\n",
      "Epoch 11250/20000: Train Loss = 0.439420, Test Loss = 0.248785, Learning Rate = 1.391027e-05\n",
      "Epoch 11251/20000: Train Loss = 0.439447, Test Loss = 0.249953, Learning Rate = 1.390499e-05\n",
      "Epoch 11252/20000: Train Loss = 0.439456, Test Loss = 0.251145, Learning Rate = 1.389970e-05\n",
      "Epoch 11253/20000: Train Loss = 0.439464, Test Loss = 0.249525, Learning Rate = 1.389442e-05\n",
      "Epoch 11254/20000: Train Loss = 0.439608, Test Loss = 0.249666, Learning Rate = 1.388914e-05\n",
      "Epoch 11255/20000: Train Loss = 0.439747, Test Loss = 0.246188, Learning Rate = 1.388387e-05\n",
      "Epoch 11256/20000: Train Loss = 0.439899, Test Loss = 0.251281, Learning Rate = 1.387859e-05\n",
      "Epoch 11257/20000: Train Loss = 0.439944, Test Loss = 0.252078, Learning Rate = 1.387332e-05\n",
      "Epoch 11258/20000: Train Loss = 0.439451, Test Loss = 0.250831, Learning Rate = 1.386805e-05\n",
      "Epoch 11259/20000: Train Loss = 0.439395, Test Loss = 0.249958, Learning Rate = 1.386278e-05\n",
      "Epoch 11260/20000: Train Loss = 0.439451, Test Loss = 0.248910, Learning Rate = 1.385751e-05\n",
      "Epoch 11261/20000: Train Loss = 0.439430, Test Loss = 0.252600, Learning Rate = 1.385224e-05\n",
      "Epoch 11262/20000: Train Loss = 0.439462, Test Loss = 0.251021, Learning Rate = 1.384698e-05\n",
      "Epoch 11263/20000: Train Loss = 0.439653, Test Loss = 0.250863, Learning Rate = 1.384172e-05\n",
      "Epoch 11264/20000: Train Loss = 0.439975, Test Loss = 0.247665, Learning Rate = 1.383646e-05\n",
      "Epoch 11265/20000: Train Loss = 0.439558, Test Loss = 0.255628, Learning Rate = 1.383120e-05\n",
      "Epoch 11266/20000: Train Loss = 0.439368, Test Loss = 0.253297, Learning Rate = 1.382595e-05\n",
      "Epoch 11267/20000: Train Loss = 0.439594, Test Loss = 0.252911, Learning Rate = 1.382069e-05\n",
      "Epoch 11268/20000: Train Loss = 0.439326, Test Loss = 0.251222, Learning Rate = 1.381544e-05\n",
      "Epoch 11269/20000: Train Loss = 0.439355, Test Loss = 0.252781, Learning Rate = 1.381019e-05\n",
      "Epoch 11270/20000: Train Loss = 0.439367, Test Loss = 0.251854, Learning Rate = 1.380494e-05\n",
      "Epoch 11271/20000: Train Loss = 0.439685, Test Loss = 0.253492, Learning Rate = 1.379970e-05\n",
      "Epoch 11272/20000: Train Loss = 0.439691, Test Loss = 0.247563, Learning Rate = 1.379445e-05\n",
      "Epoch 11273/20000: Train Loss = 0.439206, Test Loss = 0.249998, Learning Rate = 1.378921e-05\n",
      "Epoch 11274/20000: Train Loss = 0.439904, Test Loss = 0.249261, Learning Rate = 1.378397e-05\n",
      "Epoch 11275/20000: Train Loss = 0.439485, Test Loss = 0.245248, Learning Rate = 1.377874e-05\n",
      "Epoch 11276/20000: Train Loss = 0.439479, Test Loss = 0.243634, Learning Rate = 1.377350e-05\n",
      "Epoch 11277/20000: Train Loss = 0.439544, Test Loss = 0.243039, Learning Rate = 1.376827e-05\n",
      "Epoch 11278/20000: Train Loss = 0.439505, Test Loss = 0.246721, Learning Rate = 1.376304e-05\n",
      "Epoch 11279/20000: Train Loss = 0.439400, Test Loss = 0.248374, Learning Rate = 1.375781e-05\n",
      "Epoch 11280/20000: Train Loss = 0.439595, Test Loss = 0.244160, Learning Rate = 1.375258e-05\n",
      "Epoch 11281/20000: Train Loss = 0.439457, Test Loss = 0.247889, Learning Rate = 1.374735e-05\n",
      "Epoch 11282/20000: Train Loss = 0.439402, Test Loss = 0.249794, Learning Rate = 1.374213e-05\n",
      "Epoch 11283/20000: Train Loss = 0.439592, Test Loss = 0.246893, Learning Rate = 1.373691e-05\n",
      "Epoch 11284/20000: Train Loss = 0.440439, Test Loss = 0.250065, Learning Rate = 1.373169e-05\n",
      "Epoch 11285/20000: Train Loss = 0.439453, Test Loss = 0.249217, Learning Rate = 1.372647e-05\n",
      "Epoch 11286/20000: Train Loss = 0.439582, Test Loss = 0.248321, Learning Rate = 1.372125e-05\n",
      "Epoch 11287/20000: Train Loss = 0.439859, Test Loss = 0.251136, Learning Rate = 1.371604e-05\n",
      "Epoch 11288/20000: Train Loss = 0.439483, Test Loss = 0.248834, Learning Rate = 1.371083e-05\n",
      "Epoch 11289/20000: Train Loss = 0.439572, Test Loss = 0.249913, Learning Rate = 1.370562e-05\n",
      "Epoch 11290/20000: Train Loss = 0.439551, Test Loss = 0.248920, Learning Rate = 1.370041e-05\n",
      "Epoch 11291/20000: Train Loss = 0.439402, Test Loss = 0.250626, Learning Rate = 1.369521e-05\n",
      "Epoch 11292/20000: Train Loss = 0.439391, Test Loss = 0.249703, Learning Rate = 1.369000e-05\n",
      "Epoch 11293/20000: Train Loss = 0.439490, Test Loss = 0.246393, Learning Rate = 1.368480e-05\n",
      "Epoch 11294/20000: Train Loss = 0.439460, Test Loss = 0.248486, Learning Rate = 1.367960e-05\n",
      "Epoch 11295/20000: Train Loss = 0.439544, Test Loss = 0.245529, Learning Rate = 1.367440e-05\n",
      "Epoch 11296/20000: Train Loss = 0.439334, Test Loss = 0.247971, Learning Rate = 1.366921e-05\n",
      "Epoch 11297/20000: Train Loss = 0.439502, Test Loss = 0.248802, Learning Rate = 1.366401e-05\n",
      "Epoch 11298/20000: Train Loss = 0.439460, Test Loss = 0.246532, Learning Rate = 1.365882e-05\n",
      "Epoch 11299/20000: Train Loss = 0.439637, Test Loss = 0.249625, Learning Rate = 1.365363e-05\n",
      "Epoch 11300/20000: Train Loss = 0.439518, Test Loss = 0.249073, Learning Rate = 1.364844e-05\n",
      "Epoch 11301/20000: Train Loss = 0.439430, Test Loss = 0.249358, Learning Rate = 1.364326e-05\n",
      "Epoch 11302/20000: Train Loss = 0.439468, Test Loss = 0.250370, Learning Rate = 1.363807e-05\n",
      "Epoch 11303/20000: Train Loss = 0.439419, Test Loss = 0.251476, Learning Rate = 1.363289e-05\n",
      "Epoch 11304/20000: Train Loss = 0.439389, Test Loss = 0.249161, Learning Rate = 1.362771e-05\n",
      "Epoch 11305/20000: Train Loss = 0.439761, Test Loss = 0.250274, Learning Rate = 1.362253e-05\n",
      "Epoch 11306/20000: Train Loss = 0.439434, Test Loss = 0.249047, Learning Rate = 1.361736e-05\n",
      "Epoch 11307/20000: Train Loss = 0.439395, Test Loss = 0.249114, Learning Rate = 1.361218e-05\n",
      "Epoch 11308/20000: Train Loss = 0.439641, Test Loss = 0.252373, Learning Rate = 1.360701e-05\n",
      "Epoch 11309/20000: Train Loss = 0.439401, Test Loss = 0.251744, Learning Rate = 1.360184e-05\n",
      "Epoch 11310/20000: Train Loss = 0.439824, Test Loss = 0.253627, Learning Rate = 1.359667e-05\n",
      "Epoch 11311/20000: Train Loss = 0.439406, Test Loss = 0.251685, Learning Rate = 1.359150e-05\n",
      "Epoch 11312/20000: Train Loss = 0.439335, Test Loss = 0.251081, Learning Rate = 1.358634e-05\n",
      "Epoch 11313/20000: Train Loss = 0.439444, Test Loss = 0.252015, Learning Rate = 1.358118e-05\n",
      "Epoch 11314/20000: Train Loss = 0.439571, Test Loss = 0.251823, Learning Rate = 1.357602e-05\n",
      "Epoch 11315/20000: Train Loss = 0.439408, Test Loss = 0.250266, Learning Rate = 1.357086e-05\n",
      "Epoch 11316/20000: Train Loss = 0.439632, Test Loss = 0.248138, Learning Rate = 1.356570e-05\n",
      "Epoch 11317/20000: Train Loss = 0.439721, Test Loss = 0.249207, Learning Rate = 1.356055e-05\n",
      "Epoch 11318/20000: Train Loss = 0.439480, Test Loss = 0.252772, Learning Rate = 1.355539e-05\n",
      "Epoch 11319/20000: Train Loss = 0.439341, Test Loss = 0.248229, Learning Rate = 1.355024e-05\n",
      "Epoch 11320/20000: Train Loss = 0.439367, Test Loss = 0.246480, Learning Rate = 1.354510e-05\n",
      "Epoch 11321/20000: Train Loss = 0.439406, Test Loss = 0.247679, Learning Rate = 1.353995e-05\n",
      "Epoch 11322/20000: Train Loss = 0.439397, Test Loss = 0.250306, Learning Rate = 1.353480e-05\n",
      "Epoch 11323/20000: Train Loss = 0.439834, Test Loss = 0.247879, Learning Rate = 1.352966e-05\n",
      "Epoch 11324/20000: Train Loss = 0.439256, Test Loss = 0.249632, Learning Rate = 1.352452e-05\n",
      "Epoch 11325/20000: Train Loss = 0.439752, Test Loss = 0.248844, Learning Rate = 1.351938e-05\n",
      "Epoch 11326/20000: Train Loss = 0.439263, Test Loss = 0.249495, Learning Rate = 1.351424e-05\n",
      "Epoch 11327/20000: Train Loss = 0.439665, Test Loss = 0.252003, Learning Rate = 1.350911e-05\n",
      "Epoch 11328/20000: Train Loss = 0.439288, Test Loss = 0.252427, Learning Rate = 1.350398e-05\n",
      "Epoch 11329/20000: Train Loss = 0.439444, Test Loss = 0.249403, Learning Rate = 1.349884e-05\n",
      "Epoch 11330/20000: Train Loss = 0.439378, Test Loss = 0.245490, Learning Rate = 1.349372e-05\n",
      "Epoch 11331/20000: Train Loss = 0.439366, Test Loss = 0.244851, Learning Rate = 1.348859e-05\n",
      "Epoch 11332/20000: Train Loss = 0.439493, Test Loss = 0.246420, Learning Rate = 1.348346e-05\n",
      "Epoch 11333/20000: Train Loss = 0.439673, Test Loss = 0.246615, Learning Rate = 1.347834e-05\n",
      "Epoch 11334/20000: Train Loss = 0.439400, Test Loss = 0.249492, Learning Rate = 1.347322e-05\n",
      "Epoch 11335/20000: Train Loss = 0.439475, Test Loss = 0.249579, Learning Rate = 1.346810e-05\n",
      "Epoch 11336/20000: Train Loss = 0.439552, Test Loss = 0.250473, Learning Rate = 1.346298e-05\n",
      "Epoch 11337/20000: Train Loss = 0.439575, Test Loss = 0.251324, Learning Rate = 1.345787e-05\n",
      "Epoch 11338/20000: Train Loss = 0.439401, Test Loss = 0.245775, Learning Rate = 1.345275e-05\n",
      "Epoch 11339/20000: Train Loss = 0.439597, Test Loss = 0.247771, Learning Rate = 1.344764e-05\n",
      "Epoch 11340/20000: Train Loss = 0.439554, Test Loss = 0.248776, Learning Rate = 1.344253e-05\n",
      "Epoch 11341/20000: Train Loss = 0.439448, Test Loss = 0.247944, Learning Rate = 1.343742e-05\n",
      "Epoch 11342/20000: Train Loss = 0.439316, Test Loss = 0.247459, Learning Rate = 1.343232e-05\n",
      "Epoch 11343/20000: Train Loss = 0.439316, Test Loss = 0.247387, Learning Rate = 1.342721e-05\n",
      "Epoch 11344/20000: Train Loss = 0.439560, Test Loss = 0.248679, Learning Rate = 1.342211e-05\n",
      "Epoch 11345/20000: Train Loss = 0.439562, Test Loss = 0.250323, Learning Rate = 1.341701e-05\n",
      "Epoch 11346/20000: Train Loss = 0.439713, Test Loss = 0.252343, Learning Rate = 1.341191e-05\n",
      "Epoch 11347/20000: Train Loss = 0.439584, Test Loss = 0.250430, Learning Rate = 1.340682e-05\n",
      "Epoch 11348/20000: Train Loss = 0.439374, Test Loss = 0.246832, Learning Rate = 1.340172e-05\n",
      "Epoch 11349/20000: Train Loss = 0.439372, Test Loss = 0.248288, Learning Rate = 1.339663e-05\n",
      "Epoch 11350/20000: Train Loss = 0.439381, Test Loss = 0.249005, Learning Rate = 1.339154e-05\n",
      "Epoch 11351/20000: Train Loss = 0.439349, Test Loss = 0.251824, Learning Rate = 1.338645e-05\n",
      "Epoch 11352/20000: Train Loss = 0.439582, Test Loss = 0.250953, Learning Rate = 1.338137e-05\n",
      "Epoch 11353/20000: Train Loss = 0.439401, Test Loss = 0.250555, Learning Rate = 1.337628e-05\n",
      "Epoch 11354/20000: Train Loss = 0.439424, Test Loss = 0.251340, Learning Rate = 1.337120e-05\n",
      "Epoch 11355/20000: Train Loss = 0.439328, Test Loss = 0.251181, Learning Rate = 1.336612e-05\n",
      "Epoch 11356/20000: Train Loss = 0.439560, Test Loss = 0.250114, Learning Rate = 1.336104e-05\n",
      "Epoch 11357/20000: Train Loss = 0.439287, Test Loss = 0.250423, Learning Rate = 1.335596e-05\n",
      "Epoch 11358/20000: Train Loss = 0.439292, Test Loss = 0.249591, Learning Rate = 1.335089e-05\n",
      "Epoch 11359/20000: Train Loss = 0.439894, Test Loss = 0.247358, Learning Rate = 1.334581e-05\n",
      "Epoch 11360/20000: Train Loss = 0.439718, Test Loss = 0.244515, Learning Rate = 1.334074e-05\n",
      "Epoch 11361/20000: Train Loss = 0.439351, Test Loss = 0.249054, Learning Rate = 1.333567e-05\n",
      "Epoch 11362/20000: Train Loss = 0.439394, Test Loss = 0.250296, Learning Rate = 1.333061e-05\n",
      "Epoch 11363/20000: Train Loss = 0.439396, Test Loss = 0.247265, Learning Rate = 1.332554e-05\n",
      "Epoch 11364/20000: Train Loss = 0.439497, Test Loss = 0.248480, Learning Rate = 1.332048e-05\n",
      "Epoch 11365/20000: Train Loss = 0.439334, Test Loss = 0.246138, Learning Rate = 1.331542e-05\n",
      "Epoch 11366/20000: Train Loss = 0.439404, Test Loss = 0.247963, Learning Rate = 1.331036e-05\n",
      "Epoch 11367/20000: Train Loss = 0.439604, Test Loss = 0.249601, Learning Rate = 1.330530e-05\n",
      "Epoch 11368/20000: Train Loss = 0.439774, Test Loss = 0.249148, Learning Rate = 1.330024e-05\n",
      "Epoch 11369/20000: Train Loss = 0.439801, Test Loss = 0.248091, Learning Rate = 1.329519e-05\n",
      "Epoch 11370/20000: Train Loss = 0.439371, Test Loss = 0.250573, Learning Rate = 1.329014e-05\n",
      "Epoch 11371/20000: Train Loss = 0.439405, Test Loss = 0.247527, Learning Rate = 1.328509e-05\n",
      "Epoch 11372/20000: Train Loss = 0.439477, Test Loss = 0.248799, Learning Rate = 1.328004e-05\n",
      "Epoch 11373/20000: Train Loss = 0.439406, Test Loss = 0.252073, Learning Rate = 1.327499e-05\n",
      "Epoch 11374/20000: Train Loss = 0.439265, Test Loss = 0.250349, Learning Rate = 1.326995e-05\n",
      "Epoch 11375/20000: Train Loss = 0.439529, Test Loss = 0.249773, Learning Rate = 1.326491e-05\n",
      "Epoch 11376/20000: Train Loss = 0.439527, Test Loss = 0.247674, Learning Rate = 1.325987e-05\n",
      "Epoch 11377/20000: Train Loss = 0.439488, Test Loss = 0.248895, Learning Rate = 1.325483e-05\n",
      "Epoch 11378/20000: Train Loss = 0.439381, Test Loss = 0.254618, Learning Rate = 1.324979e-05\n",
      "Epoch 11379/20000: Train Loss = 0.439457, Test Loss = 0.249176, Learning Rate = 1.324476e-05\n",
      "Epoch 11380/20000: Train Loss = 0.439240, Test Loss = 0.248843, Learning Rate = 1.323973e-05\n",
      "Epoch 11381/20000: Train Loss = 0.439308, Test Loss = 0.249403, Learning Rate = 1.323469e-05\n",
      "Epoch 11382/20000: Train Loss = 0.439676, Test Loss = 0.246288, Learning Rate = 1.322967e-05\n",
      "Epoch 11383/20000: Train Loss = 0.439488, Test Loss = 0.247411, Learning Rate = 1.322464e-05\n",
      "Epoch 11384/20000: Train Loss = 0.439333, Test Loss = 0.249239, Learning Rate = 1.321961e-05\n",
      "Epoch 11385/20000: Train Loss = 0.439493, Test Loss = 0.246324, Learning Rate = 1.321459e-05\n",
      "Epoch 11386/20000: Train Loss = 0.439465, Test Loss = 0.247100, Learning Rate = 1.320957e-05\n",
      "Epoch 11387/20000: Train Loss = 0.439291, Test Loss = 0.248087, Learning Rate = 1.320455e-05\n",
      "Epoch 11388/20000: Train Loss = 0.439769, Test Loss = 0.247098, Learning Rate = 1.319953e-05\n",
      "Epoch 11389/20000: Train Loss = 0.439538, Test Loss = 0.245993, Learning Rate = 1.319452e-05\n",
      "Epoch 11390/20000: Train Loss = 0.439403, Test Loss = 0.246974, Learning Rate = 1.318950e-05\n",
      "Epoch 11391/20000: Train Loss = 0.439405, Test Loss = 0.245022, Learning Rate = 1.318449e-05\n",
      "Epoch 11392/20000: Train Loss = 0.439278, Test Loss = 0.246658, Learning Rate = 1.317948e-05\n",
      "Epoch 11393/20000: Train Loss = 0.439442, Test Loss = 0.246118, Learning Rate = 1.317447e-05\n",
      "Epoch 11394/20000: Train Loss = 0.439218, Test Loss = 0.246167, Learning Rate = 1.316947e-05\n",
      "Epoch 11395/20000: Train Loss = 0.439323, Test Loss = 0.248729, Learning Rate = 1.316446e-05\n",
      "Epoch 11396/20000: Train Loss = 0.439403, Test Loss = 0.249606, Learning Rate = 1.315946e-05\n",
      "Epoch 11397/20000: Train Loss = 0.439511, Test Loss = 0.248147, Learning Rate = 1.315446e-05\n",
      "Epoch 11398/20000: Train Loss = 0.439258, Test Loss = 0.246988, Learning Rate = 1.314946e-05\n",
      "Epoch 11399/20000: Train Loss = 0.439373, Test Loss = 0.247979, Learning Rate = 1.314447e-05\n",
      "Epoch 11400/20000: Train Loss = 0.439393, Test Loss = 0.248428, Learning Rate = 1.313947e-05\n",
      "Epoch 11401/20000: Train Loss = 0.439380, Test Loss = 0.253065, Learning Rate = 1.313448e-05\n",
      "Epoch 11402/20000: Train Loss = 0.439394, Test Loss = 0.249379, Learning Rate = 1.312949e-05\n",
      "Epoch 11403/20000: Train Loss = 0.439538, Test Loss = 0.249552, Learning Rate = 1.312450e-05\n",
      "Epoch 11404/20000: Train Loss = 0.439558, Test Loss = 0.249188, Learning Rate = 1.311951e-05\n",
      "Epoch 11405/20000: Train Loss = 0.439430, Test Loss = 0.245567, Learning Rate = 1.311453e-05\n",
      "Epoch 11406/20000: Train Loss = 0.439710, Test Loss = 0.247480, Learning Rate = 1.310955e-05\n",
      "Epoch 11407/20000: Train Loss = 0.439425, Test Loss = 0.249588, Learning Rate = 1.310456e-05\n",
      "Epoch 11408/20000: Train Loss = 0.439478, Test Loss = 0.247842, Learning Rate = 1.309959e-05\n",
      "Epoch 11409/20000: Train Loss = 0.439748, Test Loss = 0.250895, Learning Rate = 1.309461e-05\n",
      "Epoch 11410/20000: Train Loss = 0.439630, Test Loss = 0.245195, Learning Rate = 1.308963e-05\n",
      "Epoch 11411/20000: Train Loss = 0.439459, Test Loss = 0.250204, Learning Rate = 1.308466e-05\n",
      "Epoch 11412/20000: Train Loss = 0.439241, Test Loss = 0.249956, Learning Rate = 1.307969e-05\n",
      "Epoch 11413/20000: Train Loss = 0.439387, Test Loss = 0.251451, Learning Rate = 1.307472e-05\n",
      "Epoch 11414/20000: Train Loss = 0.439655, Test Loss = 0.247719, Learning Rate = 1.306975e-05\n",
      "Epoch 11415/20000: Train Loss = 0.439435, Test Loss = 0.246671, Learning Rate = 1.306478e-05\n",
      "Epoch 11416/20000: Train Loss = 0.439316, Test Loss = 0.251113, Learning Rate = 1.305982e-05\n",
      "Epoch 11417/20000: Train Loss = 0.439322, Test Loss = 0.249819, Learning Rate = 1.305486e-05\n",
      "Epoch 11418/20000: Train Loss = 0.439438, Test Loss = 0.251691, Learning Rate = 1.304990e-05\n",
      "Epoch 11419/20000: Train Loss = 0.439698, Test Loss = 0.253304, Learning Rate = 1.304494e-05\n",
      "Epoch 11420/20000: Train Loss = 0.439525, Test Loss = 0.250121, Learning Rate = 1.303998e-05\n",
      "Epoch 11421/20000: Train Loss = 0.439392, Test Loss = 0.250305, Learning Rate = 1.303503e-05\n",
      "Epoch 11422/20000: Train Loss = 0.439293, Test Loss = 0.250565, Learning Rate = 1.303007e-05\n",
      "Epoch 11423/20000: Train Loss = 0.439355, Test Loss = 0.252721, Learning Rate = 1.302512e-05\n",
      "Epoch 11424/20000: Train Loss = 0.439464, Test Loss = 0.253557, Learning Rate = 1.302017e-05\n",
      "Epoch 11425/20000: Train Loss = 0.439323, Test Loss = 0.249842, Learning Rate = 1.301522e-05\n",
      "Epoch 11426/20000: Train Loss = 0.439442, Test Loss = 0.250374, Learning Rate = 1.301028e-05\n",
      "Epoch 11427/20000: Train Loss = 0.439430, Test Loss = 0.251349, Learning Rate = 1.300534e-05\n",
      "Epoch 11428/20000: Train Loss = 0.439397, Test Loss = 0.251909, Learning Rate = 1.300039e-05\n",
      "Epoch 11429/20000: Train Loss = 0.439381, Test Loss = 0.249881, Learning Rate = 1.299545e-05\n",
      "Epoch 11430/20000: Train Loss = 0.439559, Test Loss = 0.247654, Learning Rate = 1.299052e-05\n",
      "Epoch 11431/20000: Train Loss = 0.439293, Test Loss = 0.251229, Learning Rate = 1.298558e-05\n",
      "Epoch 11432/20000: Train Loss = 0.439641, Test Loss = 0.247612, Learning Rate = 1.298065e-05\n",
      "Epoch 11433/20000: Train Loss = 0.439257, Test Loss = 0.249994, Learning Rate = 1.297571e-05\n",
      "Epoch 11434/20000: Train Loss = 0.439436, Test Loss = 0.248177, Learning Rate = 1.297078e-05\n",
      "Epoch 11435/20000: Train Loss = 0.439318, Test Loss = 0.246747, Learning Rate = 1.296585e-05\n",
      "Epoch 11436/20000: Train Loss = 0.439291, Test Loss = 0.247946, Learning Rate = 1.296093e-05\n",
      "Epoch 11437/20000: Train Loss = 0.439330, Test Loss = 0.250399, Learning Rate = 1.295600e-05\n",
      "Epoch 11438/20000: Train Loss = 0.439221, Test Loss = 0.246910, Learning Rate = 1.295108e-05\n",
      "Epoch 11439/20000: Train Loss = 0.439454, Test Loss = 0.247530, Learning Rate = 1.294616e-05\n",
      "Epoch 11440/20000: Train Loss = 0.439752, Test Loss = 0.249011, Learning Rate = 1.294124e-05\n",
      "Epoch 11441/20000: Train Loss = 0.439450, Test Loss = 0.246957, Learning Rate = 1.293632e-05\n",
      "Epoch 11442/20000: Train Loss = 0.439286, Test Loss = 0.248359, Learning Rate = 1.293141e-05\n",
      "Epoch 11443/20000: Train Loss = 0.439233, Test Loss = 0.249485, Learning Rate = 1.292649e-05\n",
      "Epoch 11444/20000: Train Loss = 0.439342, Test Loss = 0.250044, Learning Rate = 1.292158e-05\n",
      "Epoch 11445/20000: Train Loss = 0.439353, Test Loss = 0.247304, Learning Rate = 1.291667e-05\n",
      "Epoch 11446/20000: Train Loss = 0.439507, Test Loss = 0.246160, Learning Rate = 1.291176e-05\n",
      "Epoch 11447/20000: Train Loss = 0.439341, Test Loss = 0.247184, Learning Rate = 1.290686e-05\n",
      "Epoch 11448/20000: Train Loss = 0.439404, Test Loss = 0.246630, Learning Rate = 1.290195e-05\n",
      "Epoch 11449/20000: Train Loss = 0.439395, Test Loss = 0.246568, Learning Rate = 1.289705e-05\n",
      "Epoch 11450/20000: Train Loss = 0.439478, Test Loss = 0.249715, Learning Rate = 1.289215e-05\n",
      "Epoch 11451/20000: Train Loss = 0.439405, Test Loss = 0.247851, Learning Rate = 1.288725e-05\n",
      "Epoch 11452/20000: Train Loss = 0.439646, Test Loss = 0.252552, Learning Rate = 1.288236e-05\n",
      "Epoch 11453/20000: Train Loss = 0.439186, Test Loss = 0.250494, Learning Rate = 1.287746e-05\n",
      "Epoch 11454/20000: Train Loss = 0.439599, Test Loss = 0.245570, Learning Rate = 1.287257e-05\n",
      "Epoch 11455/20000: Train Loss = 0.439646, Test Loss = 0.247771, Learning Rate = 1.286768e-05\n",
      "Epoch 11456/20000: Train Loss = 0.439529, Test Loss = 0.249012, Learning Rate = 1.286279e-05\n",
      "Epoch 11457/20000: Train Loss = 0.439365, Test Loss = 0.245833, Learning Rate = 1.285790e-05\n",
      "Epoch 11458/20000: Train Loss = 0.439476, Test Loss = 0.249181, Learning Rate = 1.285301e-05\n",
      "Epoch 11459/20000: Train Loss = 0.439491, Test Loss = 0.245298, Learning Rate = 1.284813e-05\n",
      "Epoch 11460/20000: Train Loss = 0.439582, Test Loss = 0.248988, Learning Rate = 1.284325e-05\n",
      "Epoch 11461/20000: Train Loss = 0.439337, Test Loss = 0.244610, Learning Rate = 1.283837e-05\n",
      "Epoch 11462/20000: Train Loss = 0.439494, Test Loss = 0.243714, Learning Rate = 1.283349e-05\n",
      "Epoch 11463/20000: Train Loss = 0.439244, Test Loss = 0.248760, Learning Rate = 1.282861e-05\n",
      "Epoch 11464/20000: Train Loss = 0.439411, Test Loss = 0.250018, Learning Rate = 1.282374e-05\n",
      "Epoch 11465/20000: Train Loss = 0.439488, Test Loss = 0.249482, Learning Rate = 1.281887e-05\n",
      "Epoch 11466/20000: Train Loss = 0.439240, Test Loss = 0.252106, Learning Rate = 1.281400e-05\n",
      "Epoch 11467/20000: Train Loss = 0.439521, Test Loss = 0.250768, Learning Rate = 1.280913e-05\n",
      "Epoch 11468/20000: Train Loss = 0.439275, Test Loss = 0.249733, Learning Rate = 1.280426e-05\n",
      "Epoch 11469/20000: Train Loss = 0.439548, Test Loss = 0.249951, Learning Rate = 1.279939e-05\n",
      "Epoch 11470/20000: Train Loss = 0.439295, Test Loss = 0.250102, Learning Rate = 1.279453e-05\n",
      "Epoch 11471/20000: Train Loss = 0.439293, Test Loss = 0.252337, Learning Rate = 1.278967e-05\n",
      "Epoch 11472/20000: Train Loss = 0.439146, Test Loss = 0.249592, Learning Rate = 1.278481e-05\n",
      "Epoch 11473/20000: Train Loss = 0.439289, Test Loss = 0.248863, Learning Rate = 1.277995e-05\n",
      "Epoch 11474/20000: Train Loss = 0.439407, Test Loss = 0.249774, Learning Rate = 1.277510e-05\n",
      "Epoch 11475/20000: Train Loss = 0.439517, Test Loss = 0.250142, Learning Rate = 1.277024e-05\n",
      "Epoch 11476/20000: Train Loss = 0.439299, Test Loss = 0.252815, Learning Rate = 1.276539e-05\n",
      "Epoch 11477/20000: Train Loss = 0.439674, Test Loss = 0.251443, Learning Rate = 1.276054e-05\n",
      "Epoch 11478/20000: Train Loss = 0.439656, Test Loss = 0.251379, Learning Rate = 1.275569e-05\n",
      "Epoch 11479/20000: Train Loss = 0.439860, Test Loss = 0.248406, Learning Rate = 1.275084e-05\n",
      "Epoch 11480/20000: Train Loss = 0.439393, Test Loss = 0.252344, Learning Rate = 1.274600e-05\n",
      "Epoch 11481/20000: Train Loss = 0.439426, Test Loss = 0.248394, Learning Rate = 1.274115e-05\n",
      "Epoch 11482/20000: Train Loss = 0.439260, Test Loss = 0.251311, Learning Rate = 1.273631e-05\n",
      "Epoch 11483/20000: Train Loss = 0.439285, Test Loss = 0.250018, Learning Rate = 1.273147e-05\n",
      "Epoch 11484/20000: Train Loss = 0.439258, Test Loss = 0.248812, Learning Rate = 1.272664e-05\n",
      "Epoch 11485/20000: Train Loss = 0.439286, Test Loss = 0.248289, Learning Rate = 1.272180e-05\n",
      "Epoch 11486/20000: Train Loss = 0.439430, Test Loss = 0.250577, Learning Rate = 1.271697e-05\n",
      "Epoch 11487/20000: Train Loss = 0.439343, Test Loss = 0.246461, Learning Rate = 1.271213e-05\n",
      "Epoch 11488/20000: Train Loss = 0.439230, Test Loss = 0.247120, Learning Rate = 1.270730e-05\n",
      "Epoch 11489/20000: Train Loss = 0.439244, Test Loss = 0.248007, Learning Rate = 1.270248e-05\n",
      "Epoch 11490/20000: Train Loss = 0.439299, Test Loss = 0.247036, Learning Rate = 1.269765e-05\n",
      "Epoch 11491/20000: Train Loss = 0.439650, Test Loss = 0.251976, Learning Rate = 1.269282e-05\n",
      "Epoch 11492/20000: Train Loss = 0.439243, Test Loss = 0.247050, Learning Rate = 1.268800e-05\n",
      "Epoch 11493/20000: Train Loss = 0.439285, Test Loss = 0.249332, Learning Rate = 1.268318e-05\n",
      "Epoch 11494/20000: Train Loss = 0.439259, Test Loss = 0.247226, Learning Rate = 1.267836e-05\n",
      "Epoch 11495/20000: Train Loss = 0.439439, Test Loss = 0.248578, Learning Rate = 1.267354e-05\n",
      "Epoch 11496/20000: Train Loss = 0.439300, Test Loss = 0.246454, Learning Rate = 1.266873e-05\n",
      "Epoch 11497/20000: Train Loss = 0.439421, Test Loss = 0.247947, Learning Rate = 1.266391e-05\n",
      "Epoch 11498/20000: Train Loss = 0.439434, Test Loss = 0.248138, Learning Rate = 1.265910e-05\n",
      "Epoch 11499/20000: Train Loss = 0.439378, Test Loss = 0.250383, Learning Rate = 1.265429e-05\n",
      "Epoch 11500/20000: Train Loss = 0.439333, Test Loss = 0.244941, Learning Rate = 1.264948e-05\n",
      "Epoch 11501/20000: Train Loss = 0.439283, Test Loss = 0.248969, Learning Rate = 1.264468e-05\n",
      "Epoch 11502/20000: Train Loss = 0.439248, Test Loss = 0.250316, Learning Rate = 1.263987e-05\n",
      "Epoch 11503/20000: Train Loss = 0.439307, Test Loss = 0.249013, Learning Rate = 1.263507e-05\n",
      "Epoch 11504/20000: Train Loss = 0.439240, Test Loss = 0.252459, Learning Rate = 1.263027e-05\n",
      "Epoch 11505/20000: Train Loss = 0.439256, Test Loss = 0.247653, Learning Rate = 1.262547e-05\n",
      "Epoch 11506/20000: Train Loss = 0.439482, Test Loss = 0.248818, Learning Rate = 1.262067e-05\n",
      "Epoch 11507/20000: Train Loss = 0.439340, Test Loss = 0.249421, Learning Rate = 1.261588e-05\n",
      "Epoch 11508/20000: Train Loss = 0.439355, Test Loss = 0.246783, Learning Rate = 1.261108e-05\n",
      "Epoch 11509/20000: Train Loss = 0.439255, Test Loss = 0.246599, Learning Rate = 1.260629e-05\n",
      "Epoch 11510/20000: Train Loss = 0.439412, Test Loss = 0.248669, Learning Rate = 1.260150e-05\n",
      "Epoch 11511/20000: Train Loss = 0.439448, Test Loss = 0.247648, Learning Rate = 1.259671e-05\n",
      "Epoch 11512/20000: Train Loss = 0.439490, Test Loss = 0.248959, Learning Rate = 1.259193e-05\n",
      "Epoch 11513/20000: Train Loss = 0.439461, Test Loss = 0.252469, Learning Rate = 1.258714e-05\n",
      "Epoch 11514/20000: Train Loss = 0.439360, Test Loss = 0.250072, Learning Rate = 1.258236e-05\n",
      "Epoch 11515/20000: Train Loss = 0.439187, Test Loss = 0.248780, Learning Rate = 1.257758e-05\n",
      "Epoch 11516/20000: Train Loss = 0.439145, Test Loss = 0.248351, Learning Rate = 1.257280e-05\n",
      "Epoch 11517/20000: Train Loss = 0.439281, Test Loss = 0.246232, Learning Rate = 1.256802e-05\n",
      "Epoch 11518/20000: Train Loss = 0.439374, Test Loss = 0.246972, Learning Rate = 1.256325e-05\n",
      "Epoch 11519/20000: Train Loss = 0.439292, Test Loss = 0.247360, Learning Rate = 1.255847e-05\n",
      "Epoch 11520/20000: Train Loss = 0.439385, Test Loss = 0.245302, Learning Rate = 1.255370e-05\n",
      "Epoch 11521/20000: Train Loss = 0.439421, Test Loss = 0.250128, Learning Rate = 1.254893e-05\n",
      "Epoch 11522/20000: Train Loss = 0.439379, Test Loss = 0.251547, Learning Rate = 1.254416e-05\n",
      "Epoch 11523/20000: Train Loss = 0.439490, Test Loss = 0.248891, Learning Rate = 1.253940e-05\n",
      "Epoch 11524/20000: Train Loss = 0.439351, Test Loss = 0.251383, Learning Rate = 1.253463e-05\n",
      "Epoch 11525/20000: Train Loss = 0.439402, Test Loss = 0.253989, Learning Rate = 1.252987e-05\n",
      "Epoch 11526/20000: Train Loss = 0.439214, Test Loss = 0.251998, Learning Rate = 1.252511e-05\n",
      "Epoch 11527/20000: Train Loss = 0.439342, Test Loss = 0.253201, Learning Rate = 1.252035e-05\n",
      "Epoch 11528/20000: Train Loss = 0.439343, Test Loss = 0.251302, Learning Rate = 1.251559e-05\n",
      "Epoch 11529/20000: Train Loss = 0.439267, Test Loss = 0.251476, Learning Rate = 1.251084e-05\n",
      "Epoch 11530/20000: Train Loss = 0.439563, Test Loss = 0.250902, Learning Rate = 1.250608e-05\n",
      "Epoch 11531/20000: Train Loss = 0.439306, Test Loss = 0.251124, Learning Rate = 1.250133e-05\n",
      "Epoch 11532/20000: Train Loss = 0.439427, Test Loss = 0.252288, Learning Rate = 1.249658e-05\n",
      "Epoch 11533/20000: Train Loss = 0.439084, Test Loss = 0.248847, Learning Rate = 1.249183e-05\n",
      "Epoch 11534/20000: Train Loss = 0.439317, Test Loss = 0.251652, Learning Rate = 1.248708e-05\n",
      "Epoch 11535/20000: Train Loss = 0.439335, Test Loss = 0.248282, Learning Rate = 1.248234e-05\n",
      "Epoch 11536/20000: Train Loss = 0.439351, Test Loss = 0.250842, Learning Rate = 1.247760e-05\n",
      "Epoch 11537/20000: Train Loss = 0.439375, Test Loss = 0.249054, Learning Rate = 1.247286e-05\n",
      "Epoch 11538/20000: Train Loss = 0.439097, Test Loss = 0.249314, Learning Rate = 1.246812e-05\n",
      "Epoch 11539/20000: Train Loss = 0.439380, Test Loss = 0.248651, Learning Rate = 1.246338e-05\n",
      "Epoch 11540/20000: Train Loss = 0.439133, Test Loss = 0.251446, Learning Rate = 1.245864e-05\n",
      "Epoch 11541/20000: Train Loss = 0.439314, Test Loss = 0.248949, Learning Rate = 1.245391e-05\n",
      "Epoch 11542/20000: Train Loss = 0.439422, Test Loss = 0.249662, Learning Rate = 1.244918e-05\n",
      "Epoch 11543/20000: Train Loss = 0.439226, Test Loss = 0.251001, Learning Rate = 1.244445e-05\n",
      "Epoch 11544/20000: Train Loss = 0.439260, Test Loss = 0.250866, Learning Rate = 1.243972e-05\n",
      "Epoch 11545/20000: Train Loss = 0.439267, Test Loss = 0.252461, Learning Rate = 1.243499e-05\n",
      "Epoch 11546/20000: Train Loss = 0.439302, Test Loss = 0.249844, Learning Rate = 1.243027e-05\n",
      "Epoch 11547/20000: Train Loss = 0.439166, Test Loss = 0.248844, Learning Rate = 1.242554e-05\n",
      "Epoch 11548/20000: Train Loss = 0.439416, Test Loss = 0.248146, Learning Rate = 1.242082e-05\n",
      "Epoch 11549/20000: Train Loss = 0.439316, Test Loss = 0.250035, Learning Rate = 1.241610e-05\n",
      "Epoch 11550/20000: Train Loss = 0.440243, Test Loss = 0.243834, Learning Rate = 1.241138e-05\n",
      "Epoch 11551/20000: Train Loss = 0.439639, Test Loss = 0.249959, Learning Rate = 1.240667e-05\n",
      "Epoch 11552/20000: Train Loss = 0.439472, Test Loss = 0.254902, Learning Rate = 1.240195e-05\n",
      "Epoch 11553/20000: Train Loss = 0.439475, Test Loss = 0.250105, Learning Rate = 1.239724e-05\n",
      "Epoch 11554/20000: Train Loss = 0.439241, Test Loss = 0.246902, Learning Rate = 1.239253e-05\n",
      "Epoch 11555/20000: Train Loss = 0.439298, Test Loss = 0.249252, Learning Rate = 1.238782e-05\n",
      "Epoch 11556/20000: Train Loss = 0.439207, Test Loss = 0.252586, Learning Rate = 1.238312e-05\n",
      "Epoch 11557/20000: Train Loss = 0.439256, Test Loss = 0.251293, Learning Rate = 1.237841e-05\n",
      "Epoch 11558/20000: Train Loss = 0.439189, Test Loss = 0.249859, Learning Rate = 1.237371e-05\n",
      "Epoch 11559/20000: Train Loss = 0.439224, Test Loss = 0.250298, Learning Rate = 1.236901e-05\n",
      "Epoch 11560/20000: Train Loss = 0.439300, Test Loss = 0.248277, Learning Rate = 1.236431e-05\n",
      "Epoch 11561/20000: Train Loss = 0.439371, Test Loss = 0.249865, Learning Rate = 1.235961e-05\n",
      "Epoch 11562/20000: Train Loss = 0.439362, Test Loss = 0.246594, Learning Rate = 1.235491e-05\n",
      "Epoch 11563/20000: Train Loss = 0.439222, Test Loss = 0.252810, Learning Rate = 1.235022e-05\n",
      "Epoch 11564/20000: Train Loss = 0.439101, Test Loss = 0.250121, Learning Rate = 1.234552e-05\n",
      "Epoch 11565/20000: Train Loss = 0.439153, Test Loss = 0.249033, Learning Rate = 1.234083e-05\n",
      "Epoch 11566/20000: Train Loss = 0.439174, Test Loss = 0.250035, Learning Rate = 1.233614e-05\n",
      "Epoch 11567/20000: Train Loss = 0.439577, Test Loss = 0.251110, Learning Rate = 1.233146e-05\n",
      "Epoch 11568/20000: Train Loss = 0.439195, Test Loss = 0.251408, Learning Rate = 1.232677e-05\n",
      "Epoch 11569/20000: Train Loss = 0.439198, Test Loss = 0.252431, Learning Rate = 1.232209e-05\n",
      "Epoch 11570/20000: Train Loss = 0.439362, Test Loss = 0.250320, Learning Rate = 1.231740e-05\n",
      "Epoch 11571/20000: Train Loss = 0.439293, Test Loss = 0.248792, Learning Rate = 1.231272e-05\n",
      "Epoch 11572/20000: Train Loss = 0.439114, Test Loss = 0.247633, Learning Rate = 1.230805e-05\n",
      "Epoch 11573/20000: Train Loss = 0.439185, Test Loss = 0.248841, Learning Rate = 1.230337e-05\n",
      "Epoch 11574/20000: Train Loss = 0.439123, Test Loss = 0.250680, Learning Rate = 1.229869e-05\n",
      "Epoch 11575/20000: Train Loss = 0.439480, Test Loss = 0.252169, Learning Rate = 1.229402e-05\n",
      "Epoch 11576/20000: Train Loss = 0.439298, Test Loss = 0.248854, Learning Rate = 1.228935e-05\n",
      "Epoch 11577/20000: Train Loss = 0.439238, Test Loss = 0.251615, Learning Rate = 1.228468e-05\n",
      "Epoch 11578/20000: Train Loss = 0.439161, Test Loss = 0.249686, Learning Rate = 1.228001e-05\n",
      "Epoch 11579/20000: Train Loss = 0.439439, Test Loss = 0.252122, Learning Rate = 1.227535e-05\n",
      "Epoch 11580/20000: Train Loss = 0.439394, Test Loss = 0.247766, Learning Rate = 1.227068e-05\n",
      "Epoch 11581/20000: Train Loss = 0.439096, Test Loss = 0.248549, Learning Rate = 1.226602e-05\n",
      "Epoch 11582/20000: Train Loss = 0.439501, Test Loss = 0.246942, Learning Rate = 1.226136e-05\n",
      "Epoch 11583/20000: Train Loss = 0.439205, Test Loss = 0.246590, Learning Rate = 1.225670e-05\n",
      "Epoch 11584/20000: Train Loss = 0.439336, Test Loss = 0.249159, Learning Rate = 1.225204e-05\n",
      "Epoch 11585/20000: Train Loss = 0.439172, Test Loss = 0.248212, Learning Rate = 1.224739e-05\n",
      "Epoch 11586/20000: Train Loss = 0.439277, Test Loss = 0.247255, Learning Rate = 1.224273e-05\n",
      "Epoch 11587/20000: Train Loss = 0.439107, Test Loss = 0.249834, Learning Rate = 1.223808e-05\n",
      "Epoch 11588/20000: Train Loss = 0.439434, Test Loss = 0.249574, Learning Rate = 1.223343e-05\n",
      "Epoch 11589/20000: Train Loss = 0.439418, Test Loss = 0.248560, Learning Rate = 1.222878e-05\n",
      "Epoch 11590/20000: Train Loss = 0.439253, Test Loss = 0.251345, Learning Rate = 1.222414e-05\n",
      "Epoch 11591/20000: Train Loss = 0.439199, Test Loss = 0.252088, Learning Rate = 1.221949e-05\n",
      "Epoch 11592/20000: Train Loss = 0.440012, Test Loss = 0.248255, Learning Rate = 1.221485e-05\n",
      "Epoch 11593/20000: Train Loss = 0.439042, Test Loss = 0.252081, Learning Rate = 1.221021e-05\n",
      "Epoch 11594/20000: Train Loss = 0.439156, Test Loss = 0.249925, Learning Rate = 1.220557e-05\n",
      "Epoch 11595/20000: Train Loss = 0.439105, Test Loss = 0.250820, Learning Rate = 1.220093e-05\n",
      "Epoch 11596/20000: Train Loss = 0.439148, Test Loss = 0.253317, Learning Rate = 1.219629e-05\n",
      "Epoch 11597/20000: Train Loss = 0.439633, Test Loss = 0.250547, Learning Rate = 1.219166e-05\n",
      "Epoch 11598/20000: Train Loss = 0.439308, Test Loss = 0.248000, Learning Rate = 1.218703e-05\n",
      "Epoch 11599/20000: Train Loss = 0.439381, Test Loss = 0.251194, Learning Rate = 1.218240e-05\n",
      "Epoch 11600/20000: Train Loss = 0.439447, Test Loss = 0.246050, Learning Rate = 1.217777e-05\n",
      "Epoch 11601/20000: Train Loss = 0.439422, Test Loss = 0.247056, Learning Rate = 1.217314e-05\n",
      "Epoch 11602/20000: Train Loss = 0.439277, Test Loss = 0.247292, Learning Rate = 1.216851e-05\n",
      "Epoch 11603/20000: Train Loss = 0.439563, Test Loss = 0.242809, Learning Rate = 1.216389e-05\n",
      "Epoch 11604/20000: Train Loss = 0.439068, Test Loss = 0.246681, Learning Rate = 1.215927e-05\n",
      "Epoch 11605/20000: Train Loss = 0.439435, Test Loss = 0.242521, Learning Rate = 1.215465e-05\n",
      "Epoch 11606/20000: Train Loss = 0.439494, Test Loss = 0.249730, Learning Rate = 1.215003e-05\n",
      "Epoch 11607/20000: Train Loss = 0.439039, Test Loss = 0.247159, Learning Rate = 1.214541e-05\n",
      "Epoch 11608/20000: Train Loss = 0.439148, Test Loss = 0.249193, Learning Rate = 1.214080e-05\n",
      "Epoch 11609/20000: Train Loss = 0.439168, Test Loss = 0.248409, Learning Rate = 1.213619e-05\n",
      "Epoch 11610/20000: Train Loss = 0.439088, Test Loss = 0.249312, Learning Rate = 1.213157e-05\n",
      "Epoch 11611/20000: Train Loss = 0.439318, Test Loss = 0.248628, Learning Rate = 1.212696e-05\n",
      "Epoch 11612/20000: Train Loss = 0.439318, Test Loss = 0.250937, Learning Rate = 1.212236e-05\n",
      "Epoch 11613/20000: Train Loss = 0.439494, Test Loss = 0.250056, Learning Rate = 1.211775e-05\n",
      "Epoch 11614/20000: Train Loss = 0.439398, Test Loss = 0.248393, Learning Rate = 1.211315e-05\n",
      "Epoch 11615/20000: Train Loss = 0.439254, Test Loss = 0.249289, Learning Rate = 1.210854e-05\n",
      "Epoch 11616/20000: Train Loss = 0.439374, Test Loss = 0.249216, Learning Rate = 1.210394e-05\n",
      "Epoch 11617/20000: Train Loss = 0.439192, Test Loss = 0.250864, Learning Rate = 1.209934e-05\n",
      "Epoch 11618/20000: Train Loss = 0.439409, Test Loss = 0.248277, Learning Rate = 1.209475e-05\n",
      "Epoch 11619/20000: Train Loss = 0.439331, Test Loss = 0.245254, Learning Rate = 1.209015e-05\n",
      "Epoch 11620/20000: Train Loss = 0.439207, Test Loss = 0.246105, Learning Rate = 1.208556e-05\n",
      "Epoch 11621/20000: Train Loss = 0.439326, Test Loss = 0.245973, Learning Rate = 1.208096e-05\n",
      "Epoch 11622/20000: Train Loss = 0.439413, Test Loss = 0.245563, Learning Rate = 1.207637e-05\n",
      "Epoch 11623/20000: Train Loss = 0.439012, Test Loss = 0.245721, Learning Rate = 1.207178e-05\n",
      "Epoch 11624/20000: Train Loss = 0.439100, Test Loss = 0.247935, Learning Rate = 1.206720e-05\n",
      "Epoch 11625/20000: Train Loss = 0.439103, Test Loss = 0.248384, Learning Rate = 1.206261e-05\n",
      "Epoch 11626/20000: Train Loss = 0.439781, Test Loss = 0.246812, Learning Rate = 1.205803e-05\n",
      "Epoch 11627/20000: Train Loss = 0.439394, Test Loss = 0.247074, Learning Rate = 1.205345e-05\n",
      "Epoch 11628/20000: Train Loss = 0.439217, Test Loss = 0.251689, Learning Rate = 1.204887e-05\n",
      "Epoch 11629/20000: Train Loss = 0.439194, Test Loss = 0.246820, Learning Rate = 1.204429e-05\n",
      "Epoch 11630/20000: Train Loss = 0.439247, Test Loss = 0.247222, Learning Rate = 1.203971e-05\n",
      "Epoch 11631/20000: Train Loss = 0.439686, Test Loss = 0.248299, Learning Rate = 1.203514e-05\n",
      "Epoch 11632/20000: Train Loss = 0.439137, Test Loss = 0.250446, Learning Rate = 1.203056e-05\n",
      "Epoch 11633/20000: Train Loss = 0.439332, Test Loss = 0.250997, Learning Rate = 1.202599e-05\n",
      "Epoch 11634/20000: Train Loss = 0.439276, Test Loss = 0.250079, Learning Rate = 1.202142e-05\n",
      "Epoch 11635/20000: Train Loss = 0.439035, Test Loss = 0.248289, Learning Rate = 1.201686e-05\n",
      "Epoch 11636/20000: Train Loss = 0.439221, Test Loss = 0.251387, Learning Rate = 1.201229e-05\n",
      "Epoch 11637/20000: Train Loss = 0.439305, Test Loss = 0.248657, Learning Rate = 1.200773e-05\n",
      "Epoch 11638/20000: Train Loss = 0.439326, Test Loss = 0.253926, Learning Rate = 1.200316e-05\n",
      "Epoch 11639/20000: Train Loss = 0.439460, Test Loss = 0.250045, Learning Rate = 1.199860e-05\n",
      "Epoch 11640/20000: Train Loss = 0.439103, Test Loss = 0.249542, Learning Rate = 1.199404e-05\n",
      "Epoch 11641/20000: Train Loss = 0.439193, Test Loss = 0.247730, Learning Rate = 1.198949e-05\n",
      "Epoch 11642/20000: Train Loss = 0.439107, Test Loss = 0.250409, Learning Rate = 1.198493e-05\n",
      "Epoch 11643/20000: Train Loss = 0.439079, Test Loss = 0.250159, Learning Rate = 1.198038e-05\n",
      "Epoch 11644/20000: Train Loss = 0.439140, Test Loss = 0.249276, Learning Rate = 1.197582e-05\n",
      "Epoch 11645/20000: Train Loss = 0.439161, Test Loss = 0.248187, Learning Rate = 1.197127e-05\n",
      "Epoch 11646/20000: Train Loss = 0.438984, Test Loss = 0.251409, Learning Rate = 1.196672e-05\n",
      "Epoch 11647/20000: Train Loss = 0.439221, Test Loss = 0.248222, Learning Rate = 1.196218e-05\n",
      "Epoch 11648/20000: Train Loss = 0.439077, Test Loss = 0.253225, Learning Rate = 1.195763e-05\n",
      "Epoch 11649/20000: Train Loss = 0.438979, Test Loss = 0.252331, Learning Rate = 1.195309e-05\n",
      "Epoch 11650/20000: Train Loss = 0.439750, Test Loss = 0.248173, Learning Rate = 1.194855e-05\n",
      "Epoch 11651/20000: Train Loss = 0.439329, Test Loss = 0.254134, Learning Rate = 1.194401e-05\n",
      "Epoch 11652/20000: Train Loss = 0.439215, Test Loss = 0.253327, Learning Rate = 1.193947e-05\n",
      "Epoch 11653/20000: Train Loss = 0.439146, Test Loss = 0.249106, Learning Rate = 1.193493e-05\n",
      "Epoch 11654/20000: Train Loss = 0.439222, Test Loss = 0.250530, Learning Rate = 1.193040e-05\n",
      "Epoch 11655/20000: Train Loss = 0.439180, Test Loss = 0.251025, Learning Rate = 1.192586e-05\n",
      "Epoch 11656/20000: Train Loss = 0.439145, Test Loss = 0.247775, Learning Rate = 1.192133e-05\n",
      "Epoch 11657/20000: Train Loss = 0.439205, Test Loss = 0.253457, Learning Rate = 1.191680e-05\n",
      "Epoch 11658/20000: Train Loss = 0.439065, Test Loss = 0.251983, Learning Rate = 1.191227e-05\n",
      "Epoch 11659/20000: Train Loss = 0.439162, Test Loss = 0.252427, Learning Rate = 1.190775e-05\n",
      "Epoch 11660/20000: Train Loss = 0.439149, Test Loss = 0.250790, Learning Rate = 1.190322e-05\n",
      "Epoch 11661/20000: Train Loss = 0.439068, Test Loss = 0.253633, Learning Rate = 1.189870e-05\n",
      "Epoch 11662/20000: Train Loss = 0.439197, Test Loss = 0.249673, Learning Rate = 1.189418e-05\n",
      "Epoch 11663/20000: Train Loss = 0.439309, Test Loss = 0.246940, Learning Rate = 1.188966e-05\n",
      "Epoch 11664/20000: Train Loss = 0.439002, Test Loss = 0.252157, Learning Rate = 1.188514e-05\n",
      "Epoch 11665/20000: Train Loss = 0.439139, Test Loss = 0.249500, Learning Rate = 1.188063e-05\n",
      "Epoch 11666/20000: Train Loss = 0.439261, Test Loss = 0.244558, Learning Rate = 1.187611e-05\n",
      "Epoch 11667/20000: Train Loss = 0.439104, Test Loss = 0.250657, Learning Rate = 1.187160e-05\n",
      "Epoch 11668/20000: Train Loss = 0.439068, Test Loss = 0.251778, Learning Rate = 1.186709e-05\n",
      "Epoch 11669/20000: Train Loss = 0.439062, Test Loss = 0.251437, Learning Rate = 1.186258e-05\n",
      "Epoch 11670/20000: Train Loss = 0.439260, Test Loss = 0.250722, Learning Rate = 1.185807e-05\n",
      "Epoch 11671/20000: Train Loss = 0.439110, Test Loss = 0.249285, Learning Rate = 1.185357e-05\n",
      "Epoch 11672/20000: Train Loss = 0.439380, Test Loss = 0.254097, Learning Rate = 1.184906e-05\n",
      "Epoch 11673/20000: Train Loss = 0.439109, Test Loss = 0.247188, Learning Rate = 1.184456e-05\n",
      "Epoch 11674/20000: Train Loss = 0.439178, Test Loss = 0.253016, Learning Rate = 1.184006e-05\n",
      "Epoch 11675/20000: Train Loss = 0.439016, Test Loss = 0.251466, Learning Rate = 1.183556e-05\n",
      "Epoch 11676/20000: Train Loss = 0.439187, Test Loss = 0.250806, Learning Rate = 1.183106e-05\n",
      "Epoch 11677/20000: Train Loss = 0.439129, Test Loss = 0.248462, Learning Rate = 1.182657e-05\n",
      "Epoch 11678/20000: Train Loss = 0.439067, Test Loss = 0.252512, Learning Rate = 1.182207e-05\n",
      "Epoch 11679/20000: Train Loss = 0.438999, Test Loss = 0.249694, Learning Rate = 1.181758e-05\n",
      "Epoch 11680/20000: Train Loss = 0.439015, Test Loss = 0.252515, Learning Rate = 1.181309e-05\n",
      "Epoch 11681/20000: Train Loss = 0.438945, Test Loss = 0.248259, Learning Rate = 1.180860e-05\n",
      "Epoch 11682/20000: Train Loss = 0.439131, Test Loss = 0.251283, Learning Rate = 1.180411e-05\n",
      "Epoch 11683/20000: Train Loss = 0.439000, Test Loss = 0.249534, Learning Rate = 1.179963e-05\n",
      "Epoch 11684/20000: Train Loss = 0.438984, Test Loss = 0.250739, Learning Rate = 1.179515e-05\n",
      "Epoch 11685/20000: Train Loss = 0.439028, Test Loss = 0.251147, Learning Rate = 1.179066e-05\n",
      "Epoch 11686/20000: Train Loss = 0.438940, Test Loss = 0.249833, Learning Rate = 1.178618e-05\n",
      "Epoch 11687/20000: Train Loss = 0.438983, Test Loss = 0.251771, Learning Rate = 1.178171e-05\n",
      "Epoch 11688/20000: Train Loss = 0.439051, Test Loss = 0.251156, Learning Rate = 1.177723e-05\n",
      "Epoch 11689/20000: Train Loss = 0.438989, Test Loss = 0.248111, Learning Rate = 1.177275e-05\n",
      "Epoch 11690/20000: Train Loss = 0.439284, Test Loss = 0.246114, Learning Rate = 1.176828e-05\n",
      "Epoch 11691/20000: Train Loss = 0.438985, Test Loss = 0.249437, Learning Rate = 1.176381e-05\n",
      "Epoch 11692/20000: Train Loss = 0.438872, Test Loss = 0.249015, Learning Rate = 1.175934e-05\n",
      "Epoch 11693/20000: Train Loss = 0.439119, Test Loss = 0.252318, Learning Rate = 1.175487e-05\n",
      "Epoch 11694/20000: Train Loss = 0.439055, Test Loss = 0.257205, Learning Rate = 1.175040e-05\n",
      "Epoch 11695/20000: Train Loss = 0.439034, Test Loss = 0.252604, Learning Rate = 1.174594e-05\n",
      "Epoch 11696/20000: Train Loss = 0.439030, Test Loss = 0.252544, Learning Rate = 1.174148e-05\n",
      "Epoch 11697/20000: Train Loss = 0.439203, Test Loss = 0.249619, Learning Rate = 1.173701e-05\n",
      "Epoch 11698/20000: Train Loss = 0.439041, Test Loss = 0.251575, Learning Rate = 1.173256e-05\n",
      "Epoch 11699/20000: Train Loss = 0.439046, Test Loss = 0.248606, Learning Rate = 1.172810e-05\n",
      "Epoch 11700/20000: Train Loss = 0.438905, Test Loss = 0.247181, Learning Rate = 1.172364e-05\n",
      "Epoch 11701/20000: Train Loss = 0.439030, Test Loss = 0.249040, Learning Rate = 1.171919e-05\n",
      "Epoch 11702/20000: Train Loss = 0.439238, Test Loss = 0.248627, Learning Rate = 1.171473e-05\n",
      "Epoch 11703/20000: Train Loss = 0.438996, Test Loss = 0.249982, Learning Rate = 1.171028e-05\n",
      "Epoch 11704/20000: Train Loss = 0.439036, Test Loss = 0.250015, Learning Rate = 1.170583e-05\n",
      "Epoch 11705/20000: Train Loss = 0.439022, Test Loss = 0.255556, Learning Rate = 1.170138e-05\n",
      "Epoch 11706/20000: Train Loss = 0.438982, Test Loss = 0.250507, Learning Rate = 1.169694e-05\n",
      "Epoch 11707/20000: Train Loss = 0.438881, Test Loss = 0.254017, Learning Rate = 1.169249e-05\n",
      "Epoch 11708/20000: Train Loss = 0.438968, Test Loss = 0.254351, Learning Rate = 1.168805e-05\n",
      "Epoch 11709/20000: Train Loss = 0.438949, Test Loss = 0.252111, Learning Rate = 1.168361e-05\n",
      "Epoch 11710/20000: Train Loss = 0.439796, Test Loss = 0.246610, Learning Rate = 1.167917e-05\n",
      "Epoch 11711/20000: Train Loss = 0.439009, Test Loss = 0.249165, Learning Rate = 1.167473e-05\n",
      "Epoch 11712/20000: Train Loss = 0.439085, Test Loss = 0.248856, Learning Rate = 1.167030e-05\n",
      "Epoch 11713/20000: Train Loss = 0.438976, Test Loss = 0.248380, Learning Rate = 1.166586e-05\n",
      "Epoch 11714/20000: Train Loss = 0.438949, Test Loss = 0.248146, Learning Rate = 1.166143e-05\n",
      "Epoch 11715/20000: Train Loss = 0.439036, Test Loss = 0.249229, Learning Rate = 1.165700e-05\n",
      "Epoch 11716/20000: Train Loss = 0.438877, Test Loss = 0.249867, Learning Rate = 1.165257e-05\n",
      "Epoch 11717/20000: Train Loss = 0.439064, Test Loss = 0.250461, Learning Rate = 1.164814e-05\n",
      "Epoch 11718/20000: Train Loss = 0.439120, Test Loss = 0.244558, Learning Rate = 1.164372e-05\n",
      "Epoch 11719/20000: Train Loss = 0.438998, Test Loss = 0.247448, Learning Rate = 1.163929e-05\n",
      "Epoch 11720/20000: Train Loss = 0.438850, Test Loss = 0.243652, Learning Rate = 1.163487e-05\n",
      "Epoch 11721/20000: Train Loss = 0.439268, Test Loss = 0.247810, Learning Rate = 1.163045e-05\n",
      "Epoch 11722/20000: Train Loss = 0.438967, Test Loss = 0.245498, Learning Rate = 1.162603e-05\n",
      "Epoch 11723/20000: Train Loss = 0.438903, Test Loss = 0.247002, Learning Rate = 1.162161e-05\n",
      "Epoch 11724/20000: Train Loss = 0.438911, Test Loss = 0.245626, Learning Rate = 1.161719e-05\n",
      "Epoch 11725/20000: Train Loss = 0.439118, Test Loss = 0.243935, Learning Rate = 1.161278e-05\n",
      "Epoch 11726/20000: Train Loss = 0.439046, Test Loss = 0.244410, Learning Rate = 1.160837e-05\n",
      "Epoch 11727/20000: Train Loss = 0.439596, Test Loss = 0.248598, Learning Rate = 1.160396e-05\n",
      "Epoch 11728/20000: Train Loss = 0.439125, Test Loss = 0.243329, Learning Rate = 1.159955e-05\n",
      "Epoch 11729/20000: Train Loss = 0.439329, Test Loss = 0.246503, Learning Rate = 1.159514e-05\n",
      "Epoch 11730/20000: Train Loss = 0.439215, Test Loss = 0.247300, Learning Rate = 1.159073e-05\n",
      "Epoch 11731/20000: Train Loss = 0.438976, Test Loss = 0.246746, Learning Rate = 1.158633e-05\n",
      "Epoch 11732/20000: Train Loss = 0.439480, Test Loss = 0.247307, Learning Rate = 1.158193e-05\n",
      "Epoch 11733/20000: Train Loss = 0.439180, Test Loss = 0.249992, Learning Rate = 1.157753e-05\n",
      "Epoch 11734/20000: Train Loss = 0.438900, Test Loss = 0.249750, Learning Rate = 1.157313e-05\n",
      "Epoch 11735/20000: Train Loss = 0.439769, Test Loss = 0.250896, Learning Rate = 1.156873e-05\n",
      "Epoch 11736/20000: Train Loss = 0.438873, Test Loss = 0.249869, Learning Rate = 1.156433e-05\n",
      "Epoch 11737/20000: Train Loss = 0.439151, Test Loss = 0.251491, Learning Rate = 1.155994e-05\n",
      "Epoch 11738/20000: Train Loss = 0.439150, Test Loss = 0.244370, Learning Rate = 1.155555e-05\n",
      "Epoch 11739/20000: Train Loss = 0.438903, Test Loss = 0.244245, Learning Rate = 1.155116e-05\n",
      "Epoch 11740/20000: Train Loss = 0.438934, Test Loss = 0.243926, Learning Rate = 1.154677e-05\n",
      "Epoch 11741/20000: Train Loss = 0.439156, Test Loss = 0.247084, Learning Rate = 1.154238e-05\n",
      "Epoch 11742/20000: Train Loss = 0.439144, Test Loss = 0.249971, Learning Rate = 1.153799e-05\n",
      "Epoch 11743/20000: Train Loss = 0.438930, Test Loss = 0.250133, Learning Rate = 1.153361e-05\n",
      "Epoch 11744/20000: Train Loss = 0.438931, Test Loss = 0.248312, Learning Rate = 1.152923e-05\n",
      "Epoch 11745/20000: Train Loss = 0.439156, Test Loss = 0.248576, Learning Rate = 1.152485e-05\n",
      "Epoch 11746/20000: Train Loss = 0.438947, Test Loss = 0.249496, Learning Rate = 1.152047e-05\n",
      "Epoch 11747/20000: Train Loss = 0.439046, Test Loss = 0.247696, Learning Rate = 1.151609e-05\n",
      "Epoch 11748/20000: Train Loss = 0.438932, Test Loss = 0.246503, Learning Rate = 1.151172e-05\n",
      "Epoch 11749/20000: Train Loss = 0.438999, Test Loss = 0.247151, Learning Rate = 1.150734e-05\n",
      "Epoch 11750/20000: Train Loss = 0.438923, Test Loss = 0.247736, Learning Rate = 1.150297e-05\n",
      "Epoch 11751/20000: Train Loss = 0.438820, Test Loss = 0.248479, Learning Rate = 1.149860e-05\n",
      "Epoch 11752/20000: Train Loss = 0.439617, Test Loss = 0.248731, Learning Rate = 1.149423e-05\n",
      "Epoch 11753/20000: Train Loss = 0.439109, Test Loss = 0.254398, Learning Rate = 1.148986e-05\n",
      "Epoch 11754/20000: Train Loss = 0.438763, Test Loss = 0.254664, Learning Rate = 1.148550e-05\n",
      "Epoch 11755/20000: Train Loss = 0.438802, Test Loss = 0.255017, Learning Rate = 1.148113e-05\n",
      "Epoch 11756/20000: Train Loss = 0.439133, Test Loss = 0.251583, Learning Rate = 1.147677e-05\n",
      "Epoch 11757/20000: Train Loss = 0.439205, Test Loss = 0.248193, Learning Rate = 1.147241e-05\n",
      "Epoch 11758/20000: Train Loss = 0.439212, Test Loss = 0.250248, Learning Rate = 1.146805e-05\n",
      "Epoch 11759/20000: Train Loss = 0.439066, Test Loss = 0.249288, Learning Rate = 1.146369e-05\n",
      "Epoch 11760/20000: Train Loss = 0.439118, Test Loss = 0.248511, Learning Rate = 1.145933e-05\n",
      "Epoch 11761/20000: Train Loss = 0.439346, Test Loss = 0.248126, Learning Rate = 1.145498e-05\n",
      "Epoch 11762/20000: Train Loss = 0.439088, Test Loss = 0.245748, Learning Rate = 1.145063e-05\n",
      "Epoch 11763/20000: Train Loss = 0.438718, Test Loss = 0.249348, Learning Rate = 1.144628e-05\n",
      "Epoch 11764/20000: Train Loss = 0.439667, Test Loss = 0.251814, Learning Rate = 1.144193e-05\n",
      "Epoch 11765/20000: Train Loss = 0.438966, Test Loss = 0.245767, Learning Rate = 1.143758e-05\n",
      "Epoch 11766/20000: Train Loss = 0.438806, Test Loss = 0.246983, Learning Rate = 1.143323e-05\n",
      "Epoch 11767/20000: Train Loss = 0.438945, Test Loss = 0.249808, Learning Rate = 1.142889e-05\n",
      "Epoch 11768/20000: Train Loss = 0.439127, Test Loss = 0.249529, Learning Rate = 1.142455e-05\n",
      "Epoch 11769/20000: Train Loss = 0.438865, Test Loss = 0.250420, Learning Rate = 1.142021e-05\n",
      "Epoch 11770/20000: Train Loss = 0.438933, Test Loss = 0.248803, Learning Rate = 1.141587e-05\n",
      "Epoch 11771/20000: Train Loss = 0.439534, Test Loss = 0.245974, Learning Rate = 1.141153e-05\n",
      "Epoch 11772/20000: Train Loss = 0.439071, Test Loss = 0.252875, Learning Rate = 1.140719e-05\n",
      "Epoch 11773/20000: Train Loss = 0.438974, Test Loss = 0.251660, Learning Rate = 1.140286e-05\n",
      "Epoch 11774/20000: Train Loss = 0.438882, Test Loss = 0.249264, Learning Rate = 1.139853e-05\n",
      "Epoch 11775/20000: Train Loss = 0.439021, Test Loss = 0.246457, Learning Rate = 1.139419e-05\n",
      "Epoch 11776/20000: Train Loss = 0.438926, Test Loss = 0.252062, Learning Rate = 1.138987e-05\n",
      "Epoch 11777/20000: Train Loss = 0.439134, Test Loss = 0.245961, Learning Rate = 1.138554e-05\n",
      "Epoch 11778/20000: Train Loss = 0.438757, Test Loss = 0.247710, Learning Rate = 1.138121e-05\n",
      "Epoch 11779/20000: Train Loss = 0.438866, Test Loss = 0.247955, Learning Rate = 1.137689e-05\n",
      "Epoch 11780/20000: Train Loss = 0.439153, Test Loss = 0.248305, Learning Rate = 1.137256e-05\n",
      "Epoch 11781/20000: Train Loss = 0.438889, Test Loss = 0.250390, Learning Rate = 1.136824e-05\n",
      "Epoch 11782/20000: Train Loss = 0.438790, Test Loss = 0.250831, Learning Rate = 1.136392e-05\n",
      "Epoch 11783/20000: Train Loss = 0.438829, Test Loss = 0.250253, Learning Rate = 1.135960e-05\n",
      "Epoch 11784/20000: Train Loss = 0.438872, Test Loss = 0.249485, Learning Rate = 1.135529e-05\n",
      "Epoch 11785/20000: Train Loss = 0.438820, Test Loss = 0.248736, Learning Rate = 1.135097e-05\n",
      "Epoch 11786/20000: Train Loss = 0.438726, Test Loss = 0.248320, Learning Rate = 1.134666e-05\n",
      "Epoch 11787/20000: Train Loss = 0.438920, Test Loss = 0.248628, Learning Rate = 1.134235e-05\n",
      "Epoch 11788/20000: Train Loss = 0.438857, Test Loss = 0.249283, Learning Rate = 1.133804e-05\n",
      "Epoch 11789/20000: Train Loss = 0.439006, Test Loss = 0.252117, Learning Rate = 1.133373e-05\n",
      "Epoch 11790/20000: Train Loss = 0.438881, Test Loss = 0.253080, Learning Rate = 1.132943e-05\n",
      "Epoch 11791/20000: Train Loss = 0.439060, Test Loss = 0.256411, Learning Rate = 1.132512e-05\n",
      "Epoch 11792/20000: Train Loss = 0.438788, Test Loss = 0.253579, Learning Rate = 1.132082e-05\n",
      "Epoch 11793/20000: Train Loss = 0.438913, Test Loss = 0.250330, Learning Rate = 1.131652e-05\n",
      "Epoch 11794/20000: Train Loss = 0.438809, Test Loss = 0.255141, Learning Rate = 1.131222e-05\n",
      "Epoch 11795/20000: Train Loss = 0.439009, Test Loss = 0.249989, Learning Rate = 1.130792e-05\n",
      "Epoch 11796/20000: Train Loss = 0.439122, Test Loss = 0.250963, Learning Rate = 1.130362e-05\n",
      "Epoch 11797/20000: Train Loss = 0.438619, Test Loss = 0.247169, Learning Rate = 1.129933e-05\n",
      "Epoch 11798/20000: Train Loss = 0.439124, Test Loss = 0.245366, Learning Rate = 1.129503e-05\n",
      "Epoch 11799/20000: Train Loss = 0.438844, Test Loss = 0.249913, Learning Rate = 1.129074e-05\n",
      "Epoch 11800/20000: Train Loss = 0.439016, Test Loss = 0.245579, Learning Rate = 1.128645e-05\n",
      "Epoch 11801/20000: Train Loss = 0.438948, Test Loss = 0.250636, Learning Rate = 1.128216e-05\n",
      "Epoch 11802/20000: Train Loss = 0.438758, Test Loss = 0.245787, Learning Rate = 1.127787e-05\n",
      "Epoch 11803/20000: Train Loss = 0.439149, Test Loss = 0.251739, Learning Rate = 1.127359e-05\n",
      "Epoch 11804/20000: Train Loss = 0.438776, Test Loss = 0.244596, Learning Rate = 1.126931e-05\n",
      "Epoch 11805/20000: Train Loss = 0.438965, Test Loss = 0.246440, Learning Rate = 1.126502e-05\n",
      "Epoch 11806/20000: Train Loss = 0.439014, Test Loss = 0.248006, Learning Rate = 1.126074e-05\n",
      "Epoch 11807/20000: Train Loss = 0.439000, Test Loss = 0.248321, Learning Rate = 1.125646e-05\n",
      "Epoch 11808/20000: Train Loss = 0.438950, Test Loss = 0.247224, Learning Rate = 1.125219e-05\n",
      "Epoch 11809/20000: Train Loss = 0.438819, Test Loss = 0.248204, Learning Rate = 1.124791e-05\n",
      "Epoch 11810/20000: Train Loss = 0.438934, Test Loss = 0.248483, Learning Rate = 1.124364e-05\n",
      "Epoch 11811/20000: Train Loss = 0.438954, Test Loss = 0.251413, Learning Rate = 1.123937e-05\n",
      "Epoch 11812/20000: Train Loss = 0.438935, Test Loss = 0.247757, Learning Rate = 1.123509e-05\n",
      "Epoch 11813/20000: Train Loss = 0.439961, Test Loss = 0.248922, Learning Rate = 1.123083e-05\n",
      "Epoch 11814/20000: Train Loss = 0.438996, Test Loss = 0.251887, Learning Rate = 1.122656e-05\n",
      "Epoch 11815/20000: Train Loss = 0.438706, Test Loss = 0.250740, Learning Rate = 1.122229e-05\n",
      "Epoch 11816/20000: Train Loss = 0.438832, Test Loss = 0.249961, Learning Rate = 1.121803e-05\n",
      "Epoch 11817/20000: Train Loss = 0.439470, Test Loss = 0.245938, Learning Rate = 1.121377e-05\n",
      "Epoch 11818/20000: Train Loss = 0.439026, Test Loss = 0.248909, Learning Rate = 1.120950e-05\n",
      "Epoch 11819/20000: Train Loss = 0.438807, Test Loss = 0.248739, Learning Rate = 1.120525e-05\n",
      "Epoch 11820/20000: Train Loss = 0.438854, Test Loss = 0.249530, Learning Rate = 1.120099e-05\n",
      "Epoch 11821/20000: Train Loss = 0.438940, Test Loss = 0.249319, Learning Rate = 1.119673e-05\n",
      "Epoch 11822/20000: Train Loss = 0.438739, Test Loss = 0.250294, Learning Rate = 1.119248e-05\n",
      "Epoch 11823/20000: Train Loss = 0.438780, Test Loss = 0.247451, Learning Rate = 1.118822e-05\n",
      "Epoch 11824/20000: Train Loss = 0.438797, Test Loss = 0.247076, Learning Rate = 1.118397e-05\n",
      "Epoch 11825/20000: Train Loss = 0.438915, Test Loss = 0.247421, Learning Rate = 1.117972e-05\n",
      "Epoch 11826/20000: Train Loss = 0.439117, Test Loss = 0.247088, Learning Rate = 1.117548e-05\n",
      "Epoch 11827/20000: Train Loss = 0.438773, Test Loss = 0.248125, Learning Rate = 1.117123e-05\n",
      "Epoch 11828/20000: Train Loss = 0.439164, Test Loss = 0.246001, Learning Rate = 1.116698e-05\n",
      "Epoch 11829/20000: Train Loss = 0.438889, Test Loss = 0.245365, Learning Rate = 1.116274e-05\n",
      "Epoch 11830/20000: Train Loss = 0.438759, Test Loss = 0.248112, Learning Rate = 1.115850e-05\n",
      "Epoch 11831/20000: Train Loss = 0.438618, Test Loss = 0.248984, Learning Rate = 1.115426e-05\n",
      "Epoch 11832/20000: Train Loss = 0.438948, Test Loss = 0.246827, Learning Rate = 1.115002e-05\n",
      "Epoch 11833/20000: Train Loss = 0.439045, Test Loss = 0.249853, Learning Rate = 1.114578e-05\n",
      "Epoch 11834/20000: Train Loss = 0.438675, Test Loss = 0.246760, Learning Rate = 1.114155e-05\n",
      "Epoch 11835/20000: Train Loss = 0.438883, Test Loss = 0.253445, Learning Rate = 1.113732e-05\n",
      "Epoch 11836/20000: Train Loss = 0.438977, Test Loss = 0.251518, Learning Rate = 1.113308e-05\n",
      "Epoch 11837/20000: Train Loss = 0.438809, Test Loss = 0.251896, Learning Rate = 1.112885e-05\n",
      "Epoch 11838/20000: Train Loss = 0.439032, Test Loss = 0.252976, Learning Rate = 1.112463e-05\n",
      "Epoch 11839/20000: Train Loss = 0.440040, Test Loss = 0.249653, Learning Rate = 1.112040e-05\n",
      "Epoch 11840/20000: Train Loss = 0.439995, Test Loss = 0.248002, Learning Rate = 1.111617e-05\n",
      "Epoch 11841/20000: Train Loss = 0.439224, Test Loss = 0.257349, Learning Rate = 1.111195e-05\n",
      "Epoch 11842/20000: Train Loss = 0.439014, Test Loss = 0.248074, Learning Rate = 1.110773e-05\n",
      "Epoch 11843/20000: Train Loss = 0.438709, Test Loss = 0.251227, Learning Rate = 1.110351e-05\n",
      "Epoch 11844/20000: Train Loss = 0.438905, Test Loss = 0.250645, Learning Rate = 1.109929e-05\n",
      "Epoch 11845/20000: Train Loss = 0.438667, Test Loss = 0.249997, Learning Rate = 1.109507e-05\n",
      "Epoch 11846/20000: Train Loss = 0.438746, Test Loss = 0.254368, Learning Rate = 1.109085e-05\n",
      "Epoch 11847/20000: Train Loss = 0.438848, Test Loss = 0.247575, Learning Rate = 1.108664e-05\n",
      "Epoch 11848/20000: Train Loss = 0.438859, Test Loss = 0.251094, Learning Rate = 1.108243e-05\n",
      "Epoch 11849/20000: Train Loss = 0.439043, Test Loss = 0.250344, Learning Rate = 1.107822e-05\n",
      "Epoch 11850/20000: Train Loss = 0.438902, Test Loss = 0.248768, Learning Rate = 1.107401e-05\n",
      "Epoch 11851/20000: Train Loss = 0.438598, Test Loss = 0.253754, Learning Rate = 1.106980e-05\n",
      "Epoch 11852/20000: Train Loss = 0.438830, Test Loss = 0.251398, Learning Rate = 1.106559e-05\n",
      "Epoch 11853/20000: Train Loss = 0.438593, Test Loss = 0.250729, Learning Rate = 1.106139e-05\n",
      "Epoch 11854/20000: Train Loss = 0.438728, Test Loss = 0.249704, Learning Rate = 1.105718e-05\n",
      "Epoch 11855/20000: Train Loss = 0.438949, Test Loss = 0.246305, Learning Rate = 1.105298e-05\n",
      "Epoch 11856/20000: Train Loss = 0.438830, Test Loss = 0.247889, Learning Rate = 1.104878e-05\n",
      "Epoch 11857/20000: Train Loss = 0.438981, Test Loss = 0.247921, Learning Rate = 1.104459e-05\n",
      "Epoch 11858/20000: Train Loss = 0.438751, Test Loss = 0.248125, Learning Rate = 1.104039e-05\n",
      "Epoch 11859/20000: Train Loss = 0.438725, Test Loss = 0.250058, Learning Rate = 1.103619e-05\n",
      "Epoch 11860/20000: Train Loss = 0.438826, Test Loss = 0.249881, Learning Rate = 1.103200e-05\n",
      "Epoch 11861/20000: Train Loss = 0.438724, Test Loss = 0.252483, Learning Rate = 1.102781e-05\n",
      "Epoch 11862/20000: Train Loss = 0.439291, Test Loss = 0.247707, Learning Rate = 1.102362e-05\n",
      "Epoch 11863/20000: Train Loss = 0.438775, Test Loss = 0.252992, Learning Rate = 1.101943e-05\n",
      "Epoch 11864/20000: Train Loss = 0.438949, Test Loss = 0.250810, Learning Rate = 1.101524e-05\n",
      "Epoch 11865/20000: Train Loss = 0.438712, Test Loss = 0.247272, Learning Rate = 1.101106e-05\n",
      "Epoch 11866/20000: Train Loss = 0.438819, Test Loss = 0.246933, Learning Rate = 1.100687e-05\n",
      "Epoch 11867/20000: Train Loss = 0.438899, Test Loss = 0.246032, Learning Rate = 1.100269e-05\n",
      "Epoch 11868/20000: Train Loss = 0.438792, Test Loss = 0.247370, Learning Rate = 1.099851e-05\n",
      "Epoch 11869/20000: Train Loss = 0.438804, Test Loss = 0.248595, Learning Rate = 1.099433e-05\n",
      "Epoch 11870/20000: Train Loss = 0.438713, Test Loss = 0.248132, Learning Rate = 1.099015e-05\n",
      "Epoch 11871/20000: Train Loss = 0.439710, Test Loss = 0.251928, Learning Rate = 1.098598e-05\n",
      "Epoch 11872/20000: Train Loss = 0.439353, Test Loss = 0.246979, Learning Rate = 1.098180e-05\n",
      "Epoch 11873/20000: Train Loss = 0.438935, Test Loss = 0.249577, Learning Rate = 1.097763e-05\n",
      "Epoch 11874/20000: Train Loss = 0.438681, Test Loss = 0.250076, Learning Rate = 1.097346e-05\n",
      "Epoch 11875/20000: Train Loss = 0.438812, Test Loss = 0.250699, Learning Rate = 1.096929e-05\n",
      "Epoch 11876/20000: Train Loss = 0.439140, Test Loss = 0.251345, Learning Rate = 1.096512e-05\n",
      "Epoch 11877/20000: Train Loss = 0.439245, Test Loss = 0.248646, Learning Rate = 1.096095e-05\n",
      "Epoch 11878/20000: Train Loss = 0.438517, Test Loss = 0.249206, Learning Rate = 1.095679e-05\n",
      "Epoch 11879/20000: Train Loss = 0.438624, Test Loss = 0.248035, Learning Rate = 1.095263e-05\n",
      "Epoch 11880/20000: Train Loss = 0.438936, Test Loss = 0.250603, Learning Rate = 1.094846e-05\n",
      "Epoch 11881/20000: Train Loss = 0.438878, Test Loss = 0.251584, Learning Rate = 1.094430e-05\n",
      "Epoch 11882/20000: Train Loss = 0.438665, Test Loss = 0.253548, Learning Rate = 1.094015e-05\n",
      "Epoch 11883/20000: Train Loss = 0.438499, Test Loss = 0.250410, Learning Rate = 1.093599e-05\n",
      "Epoch 11884/20000: Train Loss = 0.438790, Test Loss = 0.252877, Learning Rate = 1.093183e-05\n",
      "Epoch 11885/20000: Train Loss = 0.438852, Test Loss = 0.249036, Learning Rate = 1.092768e-05\n",
      "Epoch 11886/20000: Train Loss = 0.439149, Test Loss = 0.248976, Learning Rate = 1.092353e-05\n",
      "Epoch 11887/20000: Train Loss = 0.438764, Test Loss = 0.250469, Learning Rate = 1.091938e-05\n",
      "Epoch 11888/20000: Train Loss = 0.438967, Test Loss = 0.255277, Learning Rate = 1.091523e-05\n",
      "Epoch 11889/20000: Train Loss = 0.438485, Test Loss = 0.251070, Learning Rate = 1.091108e-05\n",
      "Epoch 11890/20000: Train Loss = 0.438665, Test Loss = 0.248946, Learning Rate = 1.090693e-05\n",
      "Epoch 11891/20000: Train Loss = 0.438721, Test Loss = 0.252043, Learning Rate = 1.090279e-05\n",
      "Epoch 11892/20000: Train Loss = 0.438585, Test Loss = 0.249689, Learning Rate = 1.089865e-05\n",
      "Epoch 11893/20000: Train Loss = 0.438625, Test Loss = 0.248887, Learning Rate = 1.089451e-05\n",
      "Epoch 11894/20000: Train Loss = 0.438953, Test Loss = 0.249140, Learning Rate = 1.089037e-05\n",
      "Epoch 11895/20000: Train Loss = 0.438687, Test Loss = 0.255254, Learning Rate = 1.088623e-05\n",
      "Epoch 11896/20000: Train Loss = 0.438826, Test Loss = 0.249367, Learning Rate = 1.088209e-05\n",
      "Epoch 11897/20000: Train Loss = 0.438611, Test Loss = 0.251391, Learning Rate = 1.087796e-05\n",
      "Epoch 11898/20000: Train Loss = 0.438564, Test Loss = 0.250413, Learning Rate = 1.087382e-05\n",
      "Epoch 11899/20000: Train Loss = 0.439022, Test Loss = 0.250804, Learning Rate = 1.086969e-05\n",
      "Epoch 11900/20000: Train Loss = 0.438459, Test Loss = 0.250237, Learning Rate = 1.086556e-05\n",
      "Epoch 11901/20000: Train Loss = 0.438926, Test Loss = 0.253525, Learning Rate = 1.086143e-05\n",
      "Epoch 11902/20000: Train Loss = 0.438797, Test Loss = 0.253991, Learning Rate = 1.085731e-05\n",
      "Epoch 11903/20000: Train Loss = 0.438565, Test Loss = 0.249555, Learning Rate = 1.085318e-05\n",
      "Epoch 11904/20000: Train Loss = 0.438508, Test Loss = 0.252109, Learning Rate = 1.084906e-05\n",
      "Epoch 11905/20000: Train Loss = 0.439044, Test Loss = 0.248850, Learning Rate = 1.084493e-05\n",
      "Epoch 11906/20000: Train Loss = 0.438933, Test Loss = 0.244914, Learning Rate = 1.084081e-05\n",
      "Epoch 11907/20000: Train Loss = 0.438688, Test Loss = 0.251929, Learning Rate = 1.083669e-05\n",
      "Epoch 11908/20000: Train Loss = 0.438462, Test Loss = 0.251017, Learning Rate = 1.083258e-05\n",
      "Epoch 11909/20000: Train Loss = 0.438487, Test Loss = 0.249192, Learning Rate = 1.082846e-05\n",
      "Epoch 11910/20000: Train Loss = 0.438857, Test Loss = 0.252711, Learning Rate = 1.082435e-05\n",
      "Epoch 11911/20000: Train Loss = 0.438343, Test Loss = 0.246681, Learning Rate = 1.082023e-05\n",
      "Epoch 11912/20000: Train Loss = 0.438765, Test Loss = 0.249052, Learning Rate = 1.081612e-05\n",
      "Epoch 11913/20000: Train Loss = 0.438839, Test Loss = 0.247710, Learning Rate = 1.081201e-05\n",
      "Epoch 11914/20000: Train Loss = 0.438861, Test Loss = 0.251058, Learning Rate = 1.080790e-05\n",
      "Epoch 11915/20000: Train Loss = 0.438576, Test Loss = 0.250190, Learning Rate = 1.080380e-05\n",
      "Epoch 11916/20000: Train Loss = 0.438847, Test Loss = 0.249356, Learning Rate = 1.079969e-05\n",
      "Epoch 11917/20000: Train Loss = 0.439445, Test Loss = 0.246437, Learning Rate = 1.079559e-05\n",
      "Epoch 11918/20000: Train Loss = 0.439528, Test Loss = 0.250433, Learning Rate = 1.079149e-05\n",
      "Epoch 11919/20000: Train Loss = 0.438701, Test Loss = 0.252090, Learning Rate = 1.078739e-05\n",
      "Epoch 11920/20000: Train Loss = 0.439024, Test Loss = 0.247982, Learning Rate = 1.078329e-05\n",
      "Epoch 11921/20000: Train Loss = 0.439578, Test Loss = 0.252516, Learning Rate = 1.077919e-05\n",
      "Epoch 11922/20000: Train Loss = 0.439130, Test Loss = 0.245002, Learning Rate = 1.077509e-05\n",
      "Epoch 11923/20000: Train Loss = 0.439229, Test Loss = 0.251161, Learning Rate = 1.077100e-05\n",
      "Epoch 11924/20000: Train Loss = 0.438774, Test Loss = 0.247777, Learning Rate = 1.076691e-05\n",
      "Epoch 11925/20000: Train Loss = 0.439122, Test Loss = 0.249174, Learning Rate = 1.076282e-05\n",
      "Epoch 11926/20000: Train Loss = 0.438733, Test Loss = 0.253609, Learning Rate = 1.075873e-05\n",
      "Epoch 11927/20000: Train Loss = 0.438461, Test Loss = 0.250814, Learning Rate = 1.075464e-05\n",
      "Epoch 11928/20000: Train Loss = 0.439140, Test Loss = 0.246874, Learning Rate = 1.075055e-05\n",
      "Epoch 11929/20000: Train Loss = 0.439349, Test Loss = 0.250687, Learning Rate = 1.074647e-05\n",
      "Epoch 11930/20000: Train Loss = 0.438649, Test Loss = 0.247227, Learning Rate = 1.074238e-05\n",
      "Epoch 11931/20000: Train Loss = 0.438913, Test Loss = 0.249269, Learning Rate = 1.073830e-05\n",
      "Epoch 11932/20000: Train Loss = 0.438566, Test Loss = 0.250173, Learning Rate = 1.073422e-05\n",
      "Epoch 11933/20000: Train Loss = 0.438840, Test Loss = 0.252255, Learning Rate = 1.073014e-05\n",
      "Epoch 11934/20000: Train Loss = 0.438574, Test Loss = 0.251421, Learning Rate = 1.072607e-05\n",
      "Epoch 11935/20000: Train Loss = 0.438656, Test Loss = 0.247679, Learning Rate = 1.072199e-05\n",
      "Epoch 11936/20000: Train Loss = 0.438745, Test Loss = 0.249058, Learning Rate = 1.071792e-05\n",
      "Epoch 11937/20000: Train Loss = 0.438847, Test Loss = 0.250990, Learning Rate = 1.071384e-05\n",
      "Epoch 11938/20000: Train Loss = 0.438302, Test Loss = 0.252903, Learning Rate = 1.070977e-05\n",
      "Epoch 11939/20000: Train Loss = 0.438421, Test Loss = 0.251253, Learning Rate = 1.070570e-05\n",
      "Epoch 11940/20000: Train Loss = 0.438600, Test Loss = 0.251981, Learning Rate = 1.070164e-05\n",
      "Epoch 11941/20000: Train Loss = 0.438616, Test Loss = 0.252652, Learning Rate = 1.069757e-05\n",
      "Epoch 11942/20000: Train Loss = 0.438951, Test Loss = 0.248684, Learning Rate = 1.069350e-05\n",
      "Epoch 11943/20000: Train Loss = 0.438336, Test Loss = 0.252144, Learning Rate = 1.068944e-05\n",
      "Epoch 11944/20000: Train Loss = 0.438604, Test Loss = 0.251569, Learning Rate = 1.068538e-05\n",
      "Epoch 11945/20000: Train Loss = 0.438500, Test Loss = 0.250051, Learning Rate = 1.068132e-05\n",
      "Epoch 11946/20000: Train Loss = 0.438573, Test Loss = 0.249571, Learning Rate = 1.067726e-05\n",
      "Epoch 11947/20000: Train Loss = 0.438476, Test Loss = 0.251632, Learning Rate = 1.067320e-05\n",
      "Epoch 11948/20000: Train Loss = 0.438554, Test Loss = 0.245919, Learning Rate = 1.066915e-05\n",
      "Epoch 11949/20000: Train Loss = 0.438496, Test Loss = 0.251060, Learning Rate = 1.066509e-05\n",
      "Epoch 11950/20000: Train Loss = 0.438496, Test Loss = 0.247681, Learning Rate = 1.066104e-05\n",
      "Epoch 11951/20000: Train Loss = 0.438359, Test Loss = 0.250269, Learning Rate = 1.065699e-05\n",
      "Epoch 11952/20000: Train Loss = 0.438464, Test Loss = 0.251470, Learning Rate = 1.065294e-05\n",
      "Epoch 11953/20000: Train Loss = 0.438686, Test Loss = 0.248812, Learning Rate = 1.064889e-05\n",
      "Epoch 11954/20000: Train Loss = 0.438487, Test Loss = 0.250187, Learning Rate = 1.064485e-05\n",
      "Epoch 11955/20000: Train Loss = 0.438729, Test Loss = 0.249631, Learning Rate = 1.064080e-05\n",
      "Epoch 11956/20000: Train Loss = 0.438408, Test Loss = 0.245207, Learning Rate = 1.063676e-05\n",
      "Epoch 11957/20000: Train Loss = 0.439137, Test Loss = 0.248485, Learning Rate = 1.063272e-05\n",
      "Epoch 11958/20000: Train Loss = 0.438506, Test Loss = 0.245515, Learning Rate = 1.062868e-05\n",
      "Epoch 11959/20000: Train Loss = 0.438632, Test Loss = 0.247737, Learning Rate = 1.062464e-05\n",
      "Epoch 11960/20000: Train Loss = 0.438318, Test Loss = 0.247338, Learning Rate = 1.062060e-05\n",
      "Epoch 11961/20000: Train Loss = 0.438909, Test Loss = 0.248226, Learning Rate = 1.061657e-05\n",
      "Epoch 11962/20000: Train Loss = 0.438727, Test Loss = 0.252378, Learning Rate = 1.061253e-05\n",
      "Epoch 11963/20000: Train Loss = 0.438421, Test Loss = 0.254517, Learning Rate = 1.060850e-05\n",
      "Epoch 11964/20000: Train Loss = 0.438714, Test Loss = 0.253724, Learning Rate = 1.060447e-05\n",
      "Epoch 11965/20000: Train Loss = 0.439123, Test Loss = 0.253642, Learning Rate = 1.060044e-05\n",
      "Epoch 11966/20000: Train Loss = 0.438690, Test Loss = 0.250521, Learning Rate = 1.059641e-05\n",
      "Epoch 11967/20000: Train Loss = 0.438623, Test Loss = 0.251992, Learning Rate = 1.059238e-05\n",
      "Epoch 11968/20000: Train Loss = 0.438394, Test Loss = 0.248017, Learning Rate = 1.058836e-05\n",
      "Epoch 11969/20000: Train Loss = 0.439102, Test Loss = 0.252721, Learning Rate = 1.058434e-05\n",
      "Epoch 11970/20000: Train Loss = 0.438876, Test Loss = 0.252559, Learning Rate = 1.058032e-05\n",
      "Epoch 11971/20000: Train Loss = 0.438572, Test Loss = 0.251775, Learning Rate = 1.057629e-05\n",
      "Epoch 11972/20000: Train Loss = 0.438296, Test Loss = 0.247162, Learning Rate = 1.057228e-05\n",
      "Epoch 11973/20000: Train Loss = 0.439303, Test Loss = 0.247553, Learning Rate = 1.056826e-05\n",
      "Epoch 11974/20000: Train Loss = 0.438449, Test Loss = 0.254964, Learning Rate = 1.056424e-05\n",
      "Epoch 11975/20000: Train Loss = 0.438514, Test Loss = 0.254068, Learning Rate = 1.056023e-05\n",
      "Epoch 11976/20000: Train Loss = 0.438915, Test Loss = 0.250412, Learning Rate = 1.055622e-05\n",
      "Epoch 11977/20000: Train Loss = 0.438968, Test Loss = 0.252863, Learning Rate = 1.055221e-05\n",
      "Epoch 11978/20000: Train Loss = 0.438539, Test Loss = 0.251138, Learning Rate = 1.054820e-05\n",
      "Epoch 11979/20000: Train Loss = 0.438793, Test Loss = 0.256534, Learning Rate = 1.054419e-05\n",
      "Epoch 11980/20000: Train Loss = 0.438357, Test Loss = 0.252111, Learning Rate = 1.054018e-05\n",
      "Epoch 11981/20000: Train Loss = 0.438312, Test Loss = 0.249804, Learning Rate = 1.053618e-05\n",
      "Epoch 11982/20000: Train Loss = 0.438709, Test Loss = 0.251883, Learning Rate = 1.053217e-05\n",
      "Epoch 11983/20000: Train Loss = 0.438843, Test Loss = 0.249120, Learning Rate = 1.052817e-05\n",
      "Epoch 11984/20000: Train Loss = 0.438626, Test Loss = 0.250181, Learning Rate = 1.052417e-05\n",
      "Epoch 11985/20000: Train Loss = 0.438476, Test Loss = 0.245699, Learning Rate = 1.052017e-05\n",
      "Epoch 11986/20000: Train Loss = 0.438713, Test Loss = 0.249548, Learning Rate = 1.051617e-05\n",
      "Epoch 11987/20000: Train Loss = 0.438560, Test Loss = 0.250045, Learning Rate = 1.051218e-05\n",
      "Epoch 11988/20000: Train Loss = 0.438510, Test Loss = 0.251922, Learning Rate = 1.050818e-05\n",
      "Epoch 11989/20000: Train Loss = 0.438672, Test Loss = 0.252844, Learning Rate = 1.050419e-05\n",
      "Epoch 11990/20000: Train Loss = 0.439066, Test Loss = 0.252404, Learning Rate = 1.050020e-05\n",
      "Epoch 11991/20000: Train Loss = 0.438354, Test Loss = 0.251475, Learning Rate = 1.049621e-05\n",
      "Epoch 11992/20000: Train Loss = 0.438332, Test Loss = 0.251233, Learning Rate = 1.049222e-05\n",
      "Epoch 11993/20000: Train Loss = 0.438632, Test Loss = 0.252440, Learning Rate = 1.048824e-05\n",
      "Epoch 11994/20000: Train Loss = 0.438425, Test Loss = 0.254005, Learning Rate = 1.048425e-05\n",
      "Epoch 11995/20000: Train Loss = 0.438329, Test Loss = 0.251341, Learning Rate = 1.048027e-05\n",
      "Epoch 11996/20000: Train Loss = 0.438517, Test Loss = 0.246292, Learning Rate = 1.047628e-05\n",
      "Epoch 11997/20000: Train Loss = 0.438399, Test Loss = 0.251362, Learning Rate = 1.047230e-05\n",
      "Epoch 11998/20000: Train Loss = 0.438562, Test Loss = 0.245219, Learning Rate = 1.046832e-05\n",
      "Epoch 11999/20000: Train Loss = 0.438858, Test Loss = 0.249223, Learning Rate = 1.046435e-05\n",
      "Epoch 12000/20000: Train Loss = 0.438494, Test Loss = 0.249607, Learning Rate = 1.046037e-05\n",
      "Epoch 12001/20000: Train Loss = 0.438377, Test Loss = 0.252610, Learning Rate = 1.045640e-05\n",
      "Epoch 12002/20000: Train Loss = 0.438448, Test Loss = 0.249236, Learning Rate = 1.045242e-05\n",
      "Epoch 12003/20000: Train Loss = 0.438499, Test Loss = 0.245918, Learning Rate = 1.044845e-05\n",
      "Epoch 12004/20000: Train Loss = 0.438659, Test Loss = 0.254658, Learning Rate = 1.044448e-05\n",
      "Epoch 12005/20000: Train Loss = 0.438423, Test Loss = 0.249571, Learning Rate = 1.044051e-05\n",
      "Epoch 12006/20000: Train Loss = 0.438783, Test Loss = 0.250125, Learning Rate = 1.043654e-05\n",
      "Epoch 12007/20000: Train Loss = 0.438401, Test Loss = 0.250080, Learning Rate = 1.043258e-05\n",
      "Epoch 12008/20000: Train Loss = 0.438712, Test Loss = 0.254498, Learning Rate = 1.042862e-05\n",
      "Epoch 12009/20000: Train Loss = 0.438514, Test Loss = 0.257460, Learning Rate = 1.042465e-05\n",
      "Epoch 12010/20000: Train Loss = 0.438561, Test Loss = 0.253403, Learning Rate = 1.042069e-05\n",
      "Epoch 12011/20000: Train Loss = 0.438619, Test Loss = 0.252334, Learning Rate = 1.041673e-05\n",
      "Epoch 12012/20000: Train Loss = 0.438721, Test Loss = 0.251085, Learning Rate = 1.041277e-05\n",
      "Epoch 12013/20000: Train Loss = 0.438788, Test Loss = 0.250342, Learning Rate = 1.040882e-05\n",
      "Epoch 12014/20000: Train Loss = 0.438792, Test Loss = 0.251636, Learning Rate = 1.040486e-05\n",
      "Epoch 12015/20000: Train Loss = 0.438629, Test Loss = 0.250005, Learning Rate = 1.040091e-05\n",
      "Epoch 12016/20000: Train Loss = 0.439387, Test Loss = 0.251363, Learning Rate = 1.039696e-05\n",
      "Epoch 12017/20000: Train Loss = 0.438165, Test Loss = 0.248684, Learning Rate = 1.039301e-05\n",
      "Epoch 12018/20000: Train Loss = 0.438283, Test Loss = 0.249002, Learning Rate = 1.038906e-05\n",
      "Epoch 12019/20000: Train Loss = 0.438742, Test Loss = 0.249291, Learning Rate = 1.038511e-05\n",
      "Epoch 12020/20000: Train Loss = 0.438653, Test Loss = 0.247408, Learning Rate = 1.038116e-05\n",
      "Epoch 12021/20000: Train Loss = 0.438375, Test Loss = 0.247700, Learning Rate = 1.037722e-05\n",
      "Epoch 12022/20000: Train Loss = 0.438596, Test Loss = 0.247703, Learning Rate = 1.037328e-05\n",
      "Epoch 12023/20000: Train Loss = 0.438493, Test Loss = 0.250317, Learning Rate = 1.036933e-05\n",
      "Epoch 12024/20000: Train Loss = 0.439017, Test Loss = 0.250947, Learning Rate = 1.036539e-05\n",
      "Epoch 12025/20000: Train Loss = 0.438290, Test Loss = 0.253232, Learning Rate = 1.036146e-05\n",
      "Epoch 12026/20000: Train Loss = 0.439021, Test Loss = 0.249071, Learning Rate = 1.035752e-05\n",
      "Epoch 12027/20000: Train Loss = 0.438658, Test Loss = 0.249491, Learning Rate = 1.035358e-05\n",
      "Epoch 12028/20000: Train Loss = 0.438935, Test Loss = 0.250086, Learning Rate = 1.034965e-05\n",
      "Epoch 12029/20000: Train Loss = 0.438678, Test Loss = 0.251844, Learning Rate = 1.034572e-05\n",
      "Epoch 12030/20000: Train Loss = 0.438695, Test Loss = 0.251321, Learning Rate = 1.034179e-05\n",
      "Epoch 12031/20000: Train Loss = 0.439104, Test Loss = 0.245410, Learning Rate = 1.033786e-05\n",
      "Epoch 12032/20000: Train Loss = 0.438725, Test Loss = 0.249537, Learning Rate = 1.033393e-05\n",
      "Epoch 12033/20000: Train Loss = 0.438347, Test Loss = 0.246649, Learning Rate = 1.033000e-05\n",
      "Epoch 12034/20000: Train Loss = 0.438533, Test Loss = 0.248019, Learning Rate = 1.032608e-05\n",
      "Epoch 12035/20000: Train Loss = 0.438874, Test Loss = 0.253159, Learning Rate = 1.032215e-05\n",
      "Epoch 12036/20000: Train Loss = 0.438629, Test Loss = 0.254291, Learning Rate = 1.031823e-05\n",
      "Epoch 12037/20000: Train Loss = 0.438533, Test Loss = 0.245389, Learning Rate = 1.031431e-05\n",
      "Epoch 12038/20000: Train Loss = 0.438486, Test Loss = 0.250848, Learning Rate = 1.031039e-05\n",
      "Epoch 12039/20000: Train Loss = 0.438122, Test Loss = 0.249567, Learning Rate = 1.030647e-05\n",
      "Epoch 12040/20000: Train Loss = 0.438412, Test Loss = 0.251755, Learning Rate = 1.030256e-05\n",
      "Epoch 12041/20000: Train Loss = 0.438569, Test Loss = 0.251857, Learning Rate = 1.029864e-05\n",
      "Epoch 12042/20000: Train Loss = 0.439140, Test Loss = 0.247975, Learning Rate = 1.029473e-05\n",
      "Epoch 12043/20000: Train Loss = 0.438813, Test Loss = 0.251826, Learning Rate = 1.029082e-05\n",
      "Epoch 12044/20000: Train Loss = 0.438315, Test Loss = 0.250986, Learning Rate = 1.028691e-05\n",
      "Epoch 12045/20000: Train Loss = 0.438385, Test Loss = 0.251942, Learning Rate = 1.028300e-05\n",
      "Epoch 12046/20000: Train Loss = 0.438686, Test Loss = 0.249001, Learning Rate = 1.027909e-05\n",
      "Epoch 12047/20000: Train Loss = 0.438276, Test Loss = 0.251304, Learning Rate = 1.027518e-05\n",
      "Epoch 12048/20000: Train Loss = 0.438373, Test Loss = 0.250434, Learning Rate = 1.027128e-05\n",
      "Epoch 12049/20000: Train Loss = 0.438570, Test Loss = 0.248745, Learning Rate = 1.026738e-05\n",
      "Epoch 12050/20000: Train Loss = 0.438332, Test Loss = 0.251083, Learning Rate = 1.026348e-05\n",
      "Epoch 12051/20000: Train Loss = 0.438481, Test Loss = 0.246257, Learning Rate = 1.025958e-05\n",
      "Epoch 12052/20000: Train Loss = 0.438797, Test Loss = 0.253015, Learning Rate = 1.025568e-05\n",
      "Epoch 12053/20000: Train Loss = 0.438767, Test Loss = 0.248544, Learning Rate = 1.025178e-05\n",
      "Epoch 12054/20000: Train Loss = 0.438287, Test Loss = 0.253140, Learning Rate = 1.024789e-05\n",
      "Epoch 12055/20000: Train Loss = 0.438283, Test Loss = 0.253479, Learning Rate = 1.024399e-05\n",
      "Epoch 12056/20000: Train Loss = 0.438914, Test Loss = 0.248063, Learning Rate = 1.024010e-05\n",
      "Epoch 12057/20000: Train Loss = 0.438882, Test Loss = 0.250427, Learning Rate = 1.023621e-05\n",
      "Epoch 12058/20000: Train Loss = 0.438986, Test Loss = 0.248130, Learning Rate = 1.023232e-05\n",
      "Epoch 12059/20000: Train Loss = 0.438002, Test Loss = 0.253695, Learning Rate = 1.022843e-05\n",
      "Epoch 12060/20000: Train Loss = 0.438263, Test Loss = 0.254991, Learning Rate = 1.022454e-05\n",
      "Epoch 12061/20000: Train Loss = 0.438409, Test Loss = 0.250094, Learning Rate = 1.022066e-05\n",
      "Epoch 12062/20000: Train Loss = 0.438322, Test Loss = 0.248349, Learning Rate = 1.021678e-05\n",
      "Epoch 12063/20000: Train Loss = 0.438301, Test Loss = 0.251325, Learning Rate = 1.021289e-05\n",
      "Epoch 12064/20000: Train Loss = 0.438476, Test Loss = 0.250899, Learning Rate = 1.020901e-05\n",
      "Epoch 12065/20000: Train Loss = 0.438527, Test Loss = 0.249957, Learning Rate = 1.020513e-05\n",
      "Epoch 12066/20000: Train Loss = 0.438457, Test Loss = 0.248746, Learning Rate = 1.020126e-05\n",
      "Epoch 12067/20000: Train Loss = 0.438473, Test Loss = 0.247275, Learning Rate = 1.019738e-05\n",
      "Epoch 12068/20000: Train Loss = 0.438323, Test Loss = 0.245374, Learning Rate = 1.019351e-05\n",
      "Epoch 12069/20000: Train Loss = 0.438357, Test Loss = 0.248312, Learning Rate = 1.018963e-05\n",
      "Epoch 12070/20000: Train Loss = 0.439146, Test Loss = 0.249404, Learning Rate = 1.018576e-05\n",
      "Epoch 12071/20000: Train Loss = 0.438422, Test Loss = 0.245028, Learning Rate = 1.018189e-05\n",
      "Epoch 12072/20000: Train Loss = 0.438701, Test Loss = 0.251364, Learning Rate = 1.017802e-05\n",
      "Epoch 12073/20000: Train Loss = 0.438082, Test Loss = 0.251311, Learning Rate = 1.017415e-05\n",
      "Epoch 12074/20000: Train Loss = 0.438494, Test Loss = 0.249687, Learning Rate = 1.017029e-05\n",
      "Epoch 12075/20000: Train Loss = 0.438423, Test Loss = 0.259209, Learning Rate = 1.016642e-05\n",
      "Epoch 12076/20000: Train Loss = 0.438862, Test Loss = 0.251615, Learning Rate = 1.016256e-05\n",
      "Epoch 12077/20000: Train Loss = 0.438246, Test Loss = 0.253856, Learning Rate = 1.015870e-05\n",
      "Epoch 12078/20000: Train Loss = 0.439670, Test Loss = 0.256323, Learning Rate = 1.015484e-05\n",
      "Epoch 12079/20000: Train Loss = 0.437991, Test Loss = 0.249490, Learning Rate = 1.015098e-05\n",
      "Epoch 12080/20000: Train Loss = 0.439069, Test Loss = 0.251417, Learning Rate = 1.014712e-05\n",
      "Epoch 12081/20000: Train Loss = 0.438626, Test Loss = 0.257850, Learning Rate = 1.014327e-05\n",
      "Epoch 12082/20000: Train Loss = 0.438353, Test Loss = 0.248239, Learning Rate = 1.013941e-05\n",
      "Epoch 12083/20000: Train Loss = 0.438485, Test Loss = 0.248670, Learning Rate = 1.013556e-05\n",
      "Epoch 12084/20000: Train Loss = 0.438198, Test Loss = 0.246395, Learning Rate = 1.013171e-05\n",
      "Epoch 12085/20000: Train Loss = 0.438504, Test Loss = 0.242576, Learning Rate = 1.012786e-05\n",
      "Epoch 12086/20000: Train Loss = 0.439127, Test Loss = 0.241198, Learning Rate = 1.012401e-05\n",
      "Epoch 12087/20000: Train Loss = 0.438569, Test Loss = 0.246568, Learning Rate = 1.012016e-05\n",
      "Epoch 12088/20000: Train Loss = 0.438308, Test Loss = 0.248784, Learning Rate = 1.011632e-05\n",
      "Epoch 12089/20000: Train Loss = 0.438328, Test Loss = 0.248529, Learning Rate = 1.011248e-05\n",
      "Epoch 12090/20000: Train Loss = 0.438391, Test Loss = 0.243774, Learning Rate = 1.010863e-05\n",
      "Epoch 12091/20000: Train Loss = 0.438355, Test Loss = 0.250295, Learning Rate = 1.010479e-05\n",
      "Epoch 12092/20000: Train Loss = 0.438092, Test Loss = 0.246407, Learning Rate = 1.010095e-05\n",
      "Epoch 12093/20000: Train Loss = 0.438357, Test Loss = 0.250834, Learning Rate = 1.009711e-05\n",
      "Epoch 12094/20000: Train Loss = 0.438209, Test Loss = 0.253158, Learning Rate = 1.009328e-05\n",
      "Epoch 12095/20000: Train Loss = 0.438431, Test Loss = 0.252420, Learning Rate = 1.008944e-05\n",
      "Epoch 12096/20000: Train Loss = 0.438368, Test Loss = 0.255101, Learning Rate = 1.008561e-05\n",
      "Epoch 12097/20000: Train Loss = 0.438310, Test Loss = 0.250885, Learning Rate = 1.008178e-05\n",
      "Epoch 12098/20000: Train Loss = 0.438258, Test Loss = 0.255572, Learning Rate = 1.007795e-05\n",
      "Epoch 12099/20000: Train Loss = 0.438701, Test Loss = 0.251382, Learning Rate = 1.007412e-05\n",
      "Epoch 12100/20000: Train Loss = 0.438057, Test Loss = 0.252603, Learning Rate = 1.007029e-05\n",
      "Epoch 12101/20000: Train Loss = 0.438324, Test Loss = 0.252750, Learning Rate = 1.006646e-05\n",
      "Epoch 12102/20000: Train Loss = 0.438470, Test Loss = 0.251172, Learning Rate = 1.006264e-05\n",
      "Epoch 12103/20000: Train Loss = 0.438221, Test Loss = 0.252344, Learning Rate = 1.005881e-05\n",
      "Epoch 12104/20000: Train Loss = 0.439035, Test Loss = 0.250811, Learning Rate = 1.005499e-05\n",
      "Epoch 12105/20000: Train Loss = 0.438275, Test Loss = 0.250384, Learning Rate = 1.005117e-05\n",
      "Epoch 12106/20000: Train Loss = 0.438320, Test Loss = 0.249111, Learning Rate = 1.004735e-05\n",
      "Epoch 12107/20000: Train Loss = 0.438338, Test Loss = 0.248967, Learning Rate = 1.004353e-05\n",
      "Epoch 12108/20000: Train Loss = 0.438024, Test Loss = 0.254062, Learning Rate = 1.003972e-05\n",
      "Epoch 12109/20000: Train Loss = 0.438220, Test Loss = 0.251975, Learning Rate = 1.003590e-05\n",
      "Epoch 12110/20000: Train Loss = 0.438842, Test Loss = 0.247251, Learning Rate = 1.003209e-05\n",
      "Epoch 12111/20000: Train Loss = 0.438601, Test Loss = 0.250404, Learning Rate = 1.002828e-05\n",
      "Epoch 12112/20000: Train Loss = 0.438272, Test Loss = 0.254416, Learning Rate = 1.002447e-05\n",
      "Epoch 12113/20000: Train Loss = 0.438312, Test Loss = 0.249353, Learning Rate = 1.002066e-05\n",
      "Epoch 12114/20000: Train Loss = 0.438360, Test Loss = 0.252151, Learning Rate = 1.001685e-05\n",
      "Epoch 12115/20000: Train Loss = 0.438322, Test Loss = 0.249400, Learning Rate = 1.001304e-05\n",
      "Epoch 12116/20000: Train Loss = 0.438623, Test Loss = 0.251292, Learning Rate = 1.000924e-05\n",
      "Epoch 12117/20000: Train Loss = 0.439011, Test Loss = 0.250452, Learning Rate = 1.000544e-05\n",
      "Epoch 12118/20000: Train Loss = 0.438269, Test Loss = 0.251135, Learning Rate = 1.000163e-05\n",
      "Epoch 12119/20000: Train Loss = 0.438074, Test Loss = 0.248483, Learning Rate = 9.997834e-06\n",
      "Epoch 12120/20000: Train Loss = 0.438440, Test Loss = 0.246948, Learning Rate = 9.994035e-06\n",
      "Epoch 12121/20000: Train Loss = 0.438188, Test Loss = 0.252235, Learning Rate = 9.990238e-06\n",
      "Epoch 12122/20000: Train Loss = 0.438449, Test Loss = 0.251294, Learning Rate = 9.986442e-06\n",
      "Epoch 12123/20000: Train Loss = 0.438210, Test Loss = 0.254052, Learning Rate = 9.982647e-06\n",
      "Epoch 12124/20000: Train Loss = 0.438056, Test Loss = 0.248450, Learning Rate = 9.978854e-06\n",
      "Epoch 12125/20000: Train Loss = 0.438996, Test Loss = 0.245805, Learning Rate = 9.975062e-06\n",
      "Epoch 12126/20000: Train Loss = 0.438814, Test Loss = 0.254445, Learning Rate = 9.971272e-06\n",
      "Epoch 12127/20000: Train Loss = 0.438233, Test Loss = 0.250845, Learning Rate = 9.967483e-06\n",
      "Epoch 12128/20000: Train Loss = 0.438452, Test Loss = 0.252202, Learning Rate = 9.963696e-06\n",
      "Epoch 12129/20000: Train Loss = 0.438335, Test Loss = 0.247275, Learning Rate = 9.959910e-06\n",
      "Epoch 12130/20000: Train Loss = 0.438149, Test Loss = 0.248975, Learning Rate = 9.956125e-06\n",
      "Epoch 12131/20000: Train Loss = 0.438187, Test Loss = 0.252174, Learning Rate = 9.952342e-06\n",
      "Epoch 12132/20000: Train Loss = 0.438183, Test Loss = 0.247146, Learning Rate = 9.948561e-06\n",
      "Epoch 12133/20000: Train Loss = 0.438299, Test Loss = 0.247877, Learning Rate = 9.944781e-06\n",
      "Epoch 12134/20000: Train Loss = 0.438394, Test Loss = 0.250092, Learning Rate = 9.941002e-06\n",
      "Epoch 12135/20000: Train Loss = 0.438368, Test Loss = 0.253774, Learning Rate = 9.937225e-06\n",
      "Epoch 12136/20000: Train Loss = 0.438099, Test Loss = 0.257411, Learning Rate = 9.933449e-06\n",
      "Epoch 12137/20000: Train Loss = 0.439078, Test Loss = 0.252581, Learning Rate = 9.929674e-06\n",
      "Epoch 12138/20000: Train Loss = 0.438221, Test Loss = 0.255131, Learning Rate = 9.925901e-06\n",
      "Epoch 12139/20000: Train Loss = 0.437953, Test Loss = 0.252108, Learning Rate = 9.922130e-06\n",
      "Epoch 12140/20000: Train Loss = 0.438131, Test Loss = 0.249968, Learning Rate = 9.918359e-06\n",
      "Epoch 12141/20000: Train Loss = 0.438452, Test Loss = 0.247624, Learning Rate = 9.914591e-06\n",
      "Epoch 12142/20000: Train Loss = 0.438033, Test Loss = 0.248042, Learning Rate = 9.910823e-06\n",
      "Epoch 12143/20000: Train Loss = 0.438175, Test Loss = 0.250058, Learning Rate = 9.907058e-06\n",
      "Epoch 12144/20000: Train Loss = 0.438241, Test Loss = 0.248733, Learning Rate = 9.903293e-06\n",
      "Epoch 12145/20000: Train Loss = 0.438646, Test Loss = 0.253879, Learning Rate = 9.899530e-06\n",
      "Epoch 12146/20000: Train Loss = 0.438431, Test Loss = 0.248995, Learning Rate = 9.895769e-06\n",
      "Epoch 12147/20000: Train Loss = 0.438329, Test Loss = 0.251928, Learning Rate = 9.892009e-06\n",
      "Epoch 12148/20000: Train Loss = 0.438475, Test Loss = 0.251982, Learning Rate = 9.888250e-06\n",
      "Epoch 12149/20000: Train Loss = 0.438617, Test Loss = 0.247276, Learning Rate = 9.884493e-06\n",
      "Epoch 12150/20000: Train Loss = 0.438187, Test Loss = 0.248596, Learning Rate = 9.880737e-06\n",
      "Epoch 12151/20000: Train Loss = 0.438639, Test Loss = 0.243929, Learning Rate = 9.876982e-06\n",
      "Epoch 12152/20000: Train Loss = 0.438928, Test Loss = 0.246111, Learning Rate = 9.873229e-06\n",
      "Epoch 12153/20000: Train Loss = 0.438448, Test Loss = 0.242639, Learning Rate = 9.869478e-06\n",
      "Epoch 12154/20000: Train Loss = 0.438124, Test Loss = 0.248936, Learning Rate = 9.865728e-06\n",
      "Epoch 12155/20000: Train Loss = 0.438239, Test Loss = 0.246068, Learning Rate = 9.861979e-06\n",
      "Epoch 12156/20000: Train Loss = 0.438574, Test Loss = 0.250980, Learning Rate = 9.858232e-06\n",
      "Epoch 12157/20000: Train Loss = 0.438019, Test Loss = 0.247735, Learning Rate = 9.854486e-06\n",
      "Epoch 12158/20000: Train Loss = 0.438542, Test Loss = 0.247767, Learning Rate = 9.850741e-06\n",
      "Epoch 12159/20000: Train Loss = 0.438272, Test Loss = 0.247540, Learning Rate = 9.846998e-06\n",
      "Epoch 12160/20000: Train Loss = 0.438222, Test Loss = 0.248004, Learning Rate = 9.843257e-06\n",
      "Epoch 12161/20000: Train Loss = 0.439283, Test Loss = 0.247141, Learning Rate = 9.839517e-06\n",
      "Epoch 12162/20000: Train Loss = 0.438686, Test Loss = 0.250412, Learning Rate = 9.835778e-06\n",
      "Epoch 12163/20000: Train Loss = 0.438385, Test Loss = 0.248998, Learning Rate = 9.832041e-06\n",
      "Epoch 12164/20000: Train Loss = 0.438348, Test Loss = 0.251009, Learning Rate = 9.828305e-06\n",
      "Epoch 12165/20000: Train Loss = 0.437947, Test Loss = 0.251454, Learning Rate = 9.824570e-06\n",
      "Epoch 12166/20000: Train Loss = 0.438744, Test Loss = 0.248195, Learning Rate = 9.820837e-06\n",
      "Epoch 12167/20000: Train Loss = 0.438688, Test Loss = 0.245714, Learning Rate = 9.817105e-06\n",
      "Epoch 12168/20000: Train Loss = 0.438657, Test Loss = 0.247229, Learning Rate = 9.813375e-06\n",
      "Epoch 12169/20000: Train Loss = 0.438388, Test Loss = 0.249857, Learning Rate = 9.809646e-06\n",
      "Epoch 12170/20000: Train Loss = 0.438091, Test Loss = 0.253875, Learning Rate = 9.805919e-06\n",
      "Epoch 12171/20000: Train Loss = 0.438551, Test Loss = 0.248992, Learning Rate = 9.802193e-06\n",
      "Epoch 12172/20000: Train Loss = 0.438056, Test Loss = 0.253908, Learning Rate = 9.798468e-06\n",
      "Epoch 12173/20000: Train Loss = 0.438178, Test Loss = 0.246236, Learning Rate = 9.794745e-06\n",
      "Epoch 12174/20000: Train Loss = 0.438489, Test Loss = 0.245682, Learning Rate = 9.791024e-06\n",
      "Epoch 12175/20000: Train Loss = 0.438133, Test Loss = 0.245093, Learning Rate = 9.787303e-06\n",
      "Epoch 12176/20000: Train Loss = 0.437975, Test Loss = 0.247276, Learning Rate = 9.783584e-06\n",
      "Epoch 12177/20000: Train Loss = 0.438154, Test Loss = 0.251923, Learning Rate = 9.779867e-06\n",
      "Epoch 12178/20000: Train Loss = 0.438633, Test Loss = 0.250557, Learning Rate = 9.776151e-06\n",
      "Epoch 12179/20000: Train Loss = 0.437971, Test Loss = 0.251295, Learning Rate = 9.772436e-06\n",
      "Epoch 12180/20000: Train Loss = 0.438451, Test Loss = 0.252155, Learning Rate = 9.768723e-06\n",
      "Epoch 12181/20000: Train Loss = 0.438022, Test Loss = 0.250083, Learning Rate = 9.765011e-06\n",
      "Epoch 12182/20000: Train Loss = 0.438155, Test Loss = 0.248972, Learning Rate = 9.761300e-06\n",
      "Epoch 12183/20000: Train Loss = 0.438426, Test Loss = 0.245493, Learning Rate = 9.757591e-06\n",
      "Epoch 12184/20000: Train Loss = 0.438417, Test Loss = 0.245683, Learning Rate = 9.753884e-06\n",
      "Epoch 12185/20000: Train Loss = 0.438477, Test Loss = 0.252160, Learning Rate = 9.750178e-06\n",
      "Epoch 12186/20000: Train Loss = 0.438543, Test Loss = 0.248941, Learning Rate = 9.746473e-06\n",
      "Epoch 12187/20000: Train Loss = 0.438252, Test Loss = 0.254380, Learning Rate = 9.742769e-06\n",
      "Epoch 12188/20000: Train Loss = 0.437957, Test Loss = 0.255985, Learning Rate = 9.739067e-06\n",
      "Epoch 12189/20000: Train Loss = 0.438386, Test Loss = 0.249893, Learning Rate = 9.735367e-06\n",
      "Epoch 12190/20000: Train Loss = 0.437975, Test Loss = 0.254184, Learning Rate = 9.731668e-06\n",
      "Epoch 12191/20000: Train Loss = 0.438506, Test Loss = 0.250047, Learning Rate = 9.727970e-06\n",
      "Epoch 12192/20000: Train Loss = 0.438269, Test Loss = 0.252861, Learning Rate = 9.724274e-06\n",
      "Epoch 12193/20000: Train Loss = 0.438271, Test Loss = 0.249657, Learning Rate = 9.720579e-06\n",
      "Epoch 12194/20000: Train Loss = 0.438312, Test Loss = 0.253959, Learning Rate = 9.716885e-06\n",
      "Epoch 12195/20000: Train Loss = 0.438540, Test Loss = 0.252668, Learning Rate = 9.713193e-06\n",
      "Epoch 12196/20000: Train Loss = 0.438554, Test Loss = 0.254950, Learning Rate = 9.709502e-06\n",
      "Epoch 12197/20000: Train Loss = 0.438179, Test Loss = 0.251590, Learning Rate = 9.705813e-06\n",
      "Epoch 12198/20000: Train Loss = 0.438427, Test Loss = 0.257122, Learning Rate = 9.702125e-06\n",
      "Epoch 12199/20000: Train Loss = 0.438003, Test Loss = 0.253872, Learning Rate = 9.698438e-06\n",
      "Epoch 12200/20000: Train Loss = 0.438160, Test Loss = 0.251493, Learning Rate = 9.694753e-06\n",
      "Epoch 12201/20000: Train Loss = 0.437947, Test Loss = 0.251303, Learning Rate = 9.691069e-06\n",
      "Epoch 12202/20000: Train Loss = 0.438347, Test Loss = 0.252604, Learning Rate = 9.687387e-06\n",
      "Epoch 12203/20000: Train Loss = 0.438355, Test Loss = 0.247818, Learning Rate = 9.683706e-06\n",
      "Epoch 12204/20000: Train Loss = 0.438051, Test Loss = 0.253333, Learning Rate = 9.680027e-06\n",
      "Epoch 12205/20000: Train Loss = 0.437958, Test Loss = 0.250106, Learning Rate = 9.676348e-06\n",
      "Epoch 12206/20000: Train Loss = 0.438657, Test Loss = 0.251311, Learning Rate = 9.672672e-06\n",
      "Epoch 12207/20000: Train Loss = 0.438258, Test Loss = 0.250820, Learning Rate = 9.668996e-06\n",
      "Epoch 12208/20000: Train Loss = 0.438498, Test Loss = 0.248396, Learning Rate = 9.665322e-06\n",
      "Epoch 12209/20000: Train Loss = 0.438208, Test Loss = 0.251385, Learning Rate = 9.661650e-06\n",
      "Epoch 12210/20000: Train Loss = 0.437953, Test Loss = 0.254233, Learning Rate = 9.657979e-06\n",
      "Epoch 12211/20000: Train Loss = 0.438508, Test Loss = 0.250242, Learning Rate = 9.654309e-06\n",
      "Epoch 12212/20000: Train Loss = 0.438547, Test Loss = 0.252082, Learning Rate = 9.650640e-06\n",
      "Epoch 12213/20000: Train Loss = 0.437932, Test Loss = 0.254421, Learning Rate = 9.646973e-06\n",
      "Epoch 12214/20000: Train Loss = 0.439344, Test Loss = 0.248051, Learning Rate = 9.643308e-06\n",
      "Epoch 12215/20000: Train Loss = 0.437925, Test Loss = 0.248280, Learning Rate = 9.639644e-06\n",
      "Epoch 12216/20000: Train Loss = 0.437994, Test Loss = 0.253062, Learning Rate = 9.635981e-06\n",
      "Epoch 12217/20000: Train Loss = 0.438024, Test Loss = 0.249325, Learning Rate = 9.632319e-06\n",
      "Epoch 12218/20000: Train Loss = 0.438824, Test Loss = 0.248938, Learning Rate = 9.628659e-06\n",
      "Epoch 12219/20000: Train Loss = 0.438048, Test Loss = 0.251874, Learning Rate = 9.625001e-06\n",
      "Epoch 12220/20000: Train Loss = 0.438529, Test Loss = 0.256468, Learning Rate = 9.621344e-06\n",
      "Epoch 12221/20000: Train Loss = 0.438393, Test Loss = 0.252673, Learning Rate = 9.617688e-06\n",
      "Epoch 12222/20000: Train Loss = 0.437989, Test Loss = 0.253856, Learning Rate = 9.614033e-06\n",
      "Epoch 12223/20000: Train Loss = 0.438406, Test Loss = 0.252171, Learning Rate = 9.610380e-06\n",
      "Epoch 12224/20000: Train Loss = 0.439604, Test Loss = 0.253507, Learning Rate = 9.606729e-06\n",
      "Epoch 12225/20000: Train Loss = 0.438572, Test Loss = 0.249531, Learning Rate = 9.603078e-06\n",
      "Epoch 12226/20000: Train Loss = 0.439301, Test Loss = 0.250380, Learning Rate = 9.599429e-06\n",
      "Epoch 12227/20000: Train Loss = 0.438589, Test Loss = 0.247712, Learning Rate = 9.595782e-06\n",
      "Epoch 12228/20000: Train Loss = 0.438254, Test Loss = 0.248567, Learning Rate = 9.592136e-06\n",
      "Epoch 12229/20000: Train Loss = 0.438551, Test Loss = 0.252097, Learning Rate = 9.588491e-06\n",
      "Epoch 12230/20000: Train Loss = 0.437988, Test Loss = 0.250783, Learning Rate = 9.584848e-06\n",
      "Epoch 12231/20000: Train Loss = 0.438251, Test Loss = 0.251524, Learning Rate = 9.581206e-06\n",
      "Epoch 12232/20000: Train Loss = 0.438170, Test Loss = 0.247782, Learning Rate = 9.577565e-06\n",
      "Epoch 12233/20000: Train Loss = 0.437910, Test Loss = 0.246348, Learning Rate = 9.573926e-06\n",
      "Epoch 12234/20000: Train Loss = 0.437938, Test Loss = 0.250139, Learning Rate = 9.570288e-06\n",
      "Epoch 12235/20000: Train Loss = 0.438034, Test Loss = 0.247793, Learning Rate = 9.566651e-06\n",
      "Epoch 12236/20000: Train Loss = 0.438525, Test Loss = 0.253754, Learning Rate = 9.563016e-06\n",
      "Epoch 12237/20000: Train Loss = 0.438066, Test Loss = 0.252406, Learning Rate = 9.559383e-06\n",
      "Epoch 12238/20000: Train Loss = 0.438447, Test Loss = 0.251657, Learning Rate = 9.555750e-06\n",
      "Epoch 12239/20000: Train Loss = 0.438779, Test Loss = 0.248945, Learning Rate = 9.552119e-06\n",
      "Epoch 12240/20000: Train Loss = 0.438022, Test Loss = 0.251805, Learning Rate = 9.548490e-06\n",
      "Epoch 12241/20000: Train Loss = 0.438157, Test Loss = 0.248766, Learning Rate = 9.544862e-06\n",
      "Epoch 12242/20000: Train Loss = 0.438318, Test Loss = 0.249956, Learning Rate = 9.541235e-06\n",
      "Epoch 12243/20000: Train Loss = 0.438054, Test Loss = 0.246447, Learning Rate = 9.537610e-06\n",
      "Epoch 12244/20000: Train Loss = 0.438561, Test Loss = 0.250387, Learning Rate = 9.533986e-06\n",
      "Epoch 12245/20000: Train Loss = 0.438231, Test Loss = 0.252499, Learning Rate = 9.530363e-06\n",
      "Epoch 12246/20000: Train Loss = 0.438172, Test Loss = 0.248963, Learning Rate = 9.526742e-06\n",
      "Epoch 12247/20000: Train Loss = 0.438103, Test Loss = 0.249055, Learning Rate = 9.523122e-06\n",
      "Epoch 12248/20000: Train Loss = 0.438276, Test Loss = 0.253142, Learning Rate = 9.519503e-06\n",
      "Epoch 12249/20000: Train Loss = 0.438165, Test Loss = 0.247973, Learning Rate = 9.515886e-06\n",
      "Epoch 12250/20000: Train Loss = 0.438066, Test Loss = 0.245341, Learning Rate = 9.512270e-06\n",
      "Epoch 12251/20000: Train Loss = 0.437858, Test Loss = 0.252061, Learning Rate = 9.508656e-06\n",
      "Epoch 12252/20000: Train Loss = 0.438360, Test Loss = 0.248381, Learning Rate = 9.505043e-06\n",
      "Epoch 12253/20000: Train Loss = 0.438277, Test Loss = 0.246446, Learning Rate = 9.501431e-06\n",
      "Epoch 12254/20000: Train Loss = 0.437988, Test Loss = 0.249884, Learning Rate = 9.497821e-06\n",
      "Epoch 12255/20000: Train Loss = 0.438036, Test Loss = 0.255141, Learning Rate = 9.494212e-06\n",
      "Epoch 12256/20000: Train Loss = 0.438035, Test Loss = 0.252085, Learning Rate = 9.490604e-06\n",
      "Epoch 12257/20000: Train Loss = 0.438073, Test Loss = 0.250737, Learning Rate = 9.486998e-06\n",
      "Epoch 12258/20000: Train Loss = 0.438257, Test Loss = 0.246818, Learning Rate = 9.483393e-06\n",
      "Epoch 12259/20000: Train Loss = 0.438589, Test Loss = 0.245681, Learning Rate = 9.479790e-06\n",
      "Epoch 12260/20000: Train Loss = 0.437883, Test Loss = 0.249119, Learning Rate = 9.476188e-06\n",
      "Epoch 12261/20000: Train Loss = 0.438238, Test Loss = 0.250545, Learning Rate = 9.472587e-06\n",
      "Epoch 12262/20000: Train Loss = 0.437836, Test Loss = 0.252916, Learning Rate = 9.468988e-06\n",
      "Epoch 12263/20000: Train Loss = 0.438108, Test Loss = 0.256681, Learning Rate = 9.465390e-06\n",
      "Epoch 12264/20000: Train Loss = 0.438210, Test Loss = 0.249468, Learning Rate = 9.461793e-06\n",
      "Epoch 12265/20000: Train Loss = 0.438255, Test Loss = 0.251122, Learning Rate = 9.458198e-06\n",
      "Epoch 12266/20000: Train Loss = 0.438264, Test Loss = 0.251640, Learning Rate = 9.454604e-06\n",
      "Epoch 12267/20000: Train Loss = 0.438269, Test Loss = 0.252184, Learning Rate = 9.451012e-06\n",
      "Epoch 12268/20000: Train Loss = 0.438050, Test Loss = 0.256874, Learning Rate = 9.447421e-06\n",
      "Epoch 12269/20000: Train Loss = 0.438215, Test Loss = 0.251312, Learning Rate = 9.443831e-06\n",
      "Epoch 12270/20000: Train Loss = 0.437962, Test Loss = 0.256665, Learning Rate = 9.440242e-06\n",
      "Epoch 12271/20000: Train Loss = 0.437930, Test Loss = 0.250998, Learning Rate = 9.436655e-06\n",
      "Epoch 12272/20000: Train Loss = 0.438200, Test Loss = 0.244518, Learning Rate = 9.433070e-06\n",
      "Epoch 12273/20000: Train Loss = 0.438049, Test Loss = 0.247279, Learning Rate = 9.429485e-06\n",
      "Epoch 12274/20000: Train Loss = 0.438794, Test Loss = 0.252502, Learning Rate = 9.425902e-06\n",
      "Epoch 12275/20000: Train Loss = 0.438543, Test Loss = 0.254906, Learning Rate = 9.422321e-06\n",
      "Epoch 12276/20000: Train Loss = 0.438262, Test Loss = 0.249075, Learning Rate = 9.418741e-06\n",
      "Epoch 12277/20000: Train Loss = 0.438298, Test Loss = 0.246007, Learning Rate = 9.415162e-06\n",
      "Epoch 12278/20000: Train Loss = 0.437752, Test Loss = 0.251479, Learning Rate = 9.411584e-06\n",
      "Epoch 12279/20000: Train Loss = 0.438096, Test Loss = 0.252408, Learning Rate = 9.408008e-06\n",
      "Epoch 12280/20000: Train Loss = 0.438071, Test Loss = 0.251619, Learning Rate = 9.404433e-06\n",
      "Epoch 12281/20000: Train Loss = 0.438015, Test Loss = 0.254757, Learning Rate = 9.400860e-06\n",
      "Epoch 12282/20000: Train Loss = 0.439604, Test Loss = 0.253667, Learning Rate = 9.397288e-06\n",
      "Epoch 12283/20000: Train Loss = 0.438452, Test Loss = 0.251471, Learning Rate = 9.393717e-06\n",
      "Epoch 12284/20000: Train Loss = 0.437984, Test Loss = 0.250904, Learning Rate = 9.390148e-06\n",
      "Epoch 12285/20000: Train Loss = 0.438938, Test Loss = 0.250648, Learning Rate = 9.386580e-06\n",
      "Epoch 12286/20000: Train Loss = 0.438189, Test Loss = 0.248598, Learning Rate = 9.383013e-06\n",
      "Epoch 12287/20000: Train Loss = 0.437970, Test Loss = 0.251439, Learning Rate = 9.379448e-06\n",
      "Epoch 12288/20000: Train Loss = 0.437767, Test Loss = 0.248627, Learning Rate = 9.375884e-06\n",
      "Epoch 12289/20000: Train Loss = 0.437965, Test Loss = 0.252247, Learning Rate = 9.372321e-06\n",
      "Epoch 12290/20000: Train Loss = 0.437927, Test Loss = 0.249943, Learning Rate = 9.368760e-06\n",
      "Epoch 12291/20000: Train Loss = 0.438212, Test Loss = 0.254117, Learning Rate = 9.365200e-06\n",
      "Epoch 12292/20000: Train Loss = 0.438311, Test Loss = 0.249135, Learning Rate = 9.361642e-06\n",
      "Epoch 12293/20000: Train Loss = 0.437894, Test Loss = 0.248164, Learning Rate = 9.358085e-06\n",
      "Epoch 12294/20000: Train Loss = 0.437784, Test Loss = 0.251158, Learning Rate = 9.354529e-06\n",
      "Epoch 12295/20000: Train Loss = 0.438107, Test Loss = 0.250337, Learning Rate = 9.350974e-06\n",
      "Epoch 12296/20000: Train Loss = 0.437882, Test Loss = 0.246462, Learning Rate = 9.347421e-06\n",
      "Epoch 12297/20000: Train Loss = 0.438055, Test Loss = 0.247831, Learning Rate = 9.343869e-06\n",
      "Epoch 12298/20000: Train Loss = 0.438389, Test Loss = 0.248157, Learning Rate = 9.340319e-06\n",
      "Epoch 12299/20000: Train Loss = 0.438753, Test Loss = 0.246341, Learning Rate = 9.336770e-06\n",
      "Epoch 12300/20000: Train Loss = 0.437738, Test Loss = 0.246474, Learning Rate = 9.333222e-06\n",
      "Epoch 12301/20000: Train Loss = 0.437909, Test Loss = 0.248885, Learning Rate = 9.329676e-06\n",
      "Epoch 12302/20000: Train Loss = 0.437872, Test Loss = 0.248820, Learning Rate = 9.326131e-06\n",
      "Epoch 12303/20000: Train Loss = 0.437882, Test Loss = 0.249768, Learning Rate = 9.322587e-06\n",
      "Epoch 12304/20000: Train Loss = 0.437954, Test Loss = 0.251790, Learning Rate = 9.319045e-06\n",
      "Epoch 12305/20000: Train Loss = 0.438058, Test Loss = 0.250356, Learning Rate = 9.315504e-06\n",
      "Epoch 12306/20000: Train Loss = 0.438268, Test Loss = 0.253774, Learning Rate = 9.311964e-06\n",
      "Epoch 12307/20000: Train Loss = 0.437740, Test Loss = 0.249142, Learning Rate = 9.308426e-06\n",
      "Epoch 12308/20000: Train Loss = 0.437878, Test Loss = 0.251220, Learning Rate = 9.304889e-06\n",
      "Epoch 12309/20000: Train Loss = 0.438330, Test Loss = 0.255708, Learning Rate = 9.301353e-06\n",
      "Epoch 12310/20000: Train Loss = 0.438058, Test Loss = 0.250499, Learning Rate = 9.297819e-06\n",
      "Epoch 12311/20000: Train Loss = 0.437645, Test Loss = 0.254826, Learning Rate = 9.294286e-06\n",
      "Epoch 12312/20000: Train Loss = 0.437834, Test Loss = 0.252342, Learning Rate = 9.290755e-06\n",
      "Epoch 12313/20000: Train Loss = 0.437738, Test Loss = 0.248936, Learning Rate = 9.287224e-06\n",
      "Epoch 12314/20000: Train Loss = 0.438768, Test Loss = 0.249337, Learning Rate = 9.283695e-06\n",
      "Epoch 12315/20000: Train Loss = 0.438331, Test Loss = 0.247832, Learning Rate = 9.280168e-06\n",
      "Epoch 12316/20000: Train Loss = 0.438071, Test Loss = 0.251392, Learning Rate = 9.276642e-06\n",
      "Epoch 12317/20000: Train Loss = 0.437853, Test Loss = 0.252975, Learning Rate = 9.273117e-06\n",
      "Epoch 12318/20000: Train Loss = 0.437695, Test Loss = 0.249730, Learning Rate = 9.269593e-06\n",
      "Epoch 12319/20000: Train Loss = 0.437938, Test Loss = 0.248587, Learning Rate = 9.266071e-06\n",
      "Epoch 12320/20000: Train Loss = 0.437972, Test Loss = 0.251247, Learning Rate = 9.262550e-06\n",
      "Epoch 12321/20000: Train Loss = 0.438458, Test Loss = 0.251849, Learning Rate = 9.259031e-06\n",
      "Epoch 12322/20000: Train Loss = 0.438262, Test Loss = 0.249240, Learning Rate = 9.255512e-06\n",
      "Epoch 12323/20000: Train Loss = 0.438067, Test Loss = 0.252034, Learning Rate = 9.251996e-06\n",
      "Epoch 12324/20000: Train Loss = 0.438695, Test Loss = 0.247163, Learning Rate = 9.248480e-06\n",
      "Epoch 12325/20000: Train Loss = 0.437727, Test Loss = 0.251944, Learning Rate = 9.244966e-06\n",
      "Epoch 12326/20000: Train Loss = 0.438308, Test Loss = 0.256496, Learning Rate = 9.241453e-06\n",
      "Epoch 12327/20000: Train Loss = 0.437925, Test Loss = 0.252863, Learning Rate = 9.237942e-06\n",
      "Epoch 12328/20000: Train Loss = 0.437955, Test Loss = 0.252637, Learning Rate = 9.234431e-06\n",
      "Epoch 12329/20000: Train Loss = 0.438095, Test Loss = 0.254996, Learning Rate = 9.230923e-06\n",
      "Epoch 12330/20000: Train Loss = 0.437927, Test Loss = 0.255570, Learning Rate = 9.227415e-06\n",
      "Epoch 12331/20000: Train Loss = 0.437937, Test Loss = 0.252138, Learning Rate = 9.223909e-06\n",
      "Epoch 12332/20000: Train Loss = 0.438519, Test Loss = 0.254473, Learning Rate = 9.220404e-06\n",
      "Epoch 12333/20000: Train Loss = 0.438162, Test Loss = 0.251560, Learning Rate = 9.216901e-06\n",
      "Epoch 12334/20000: Train Loss = 0.437755, Test Loss = 0.251155, Learning Rate = 9.213398e-06\n",
      "Epoch 12335/20000: Train Loss = 0.438235, Test Loss = 0.252272, Learning Rate = 9.209898e-06\n",
      "Epoch 12336/20000: Train Loss = 0.437485, Test Loss = 0.248019, Learning Rate = 9.206398e-06\n",
      "Epoch 12337/20000: Train Loss = 0.438458, Test Loss = 0.249506, Learning Rate = 9.202900e-06\n",
      "Epoch 12338/20000: Train Loss = 0.437984, Test Loss = 0.250859, Learning Rate = 9.199403e-06\n",
      "Epoch 12339/20000: Train Loss = 0.437768, Test Loss = 0.248934, Learning Rate = 9.195908e-06\n",
      "Epoch 12340/20000: Train Loss = 0.438125, Test Loss = 0.247373, Learning Rate = 9.192413e-06\n",
      "Epoch 12341/20000: Train Loss = 0.439385, Test Loss = 0.252010, Learning Rate = 9.188920e-06\n",
      "Epoch 12342/20000: Train Loss = 0.437812, Test Loss = 0.248424, Learning Rate = 9.185429e-06\n",
      "Epoch 12343/20000: Train Loss = 0.437961, Test Loss = 0.253342, Learning Rate = 9.181939e-06\n",
      "Epoch 12344/20000: Train Loss = 0.438083, Test Loss = 0.255372, Learning Rate = 9.178450e-06\n",
      "Epoch 12345/20000: Train Loss = 0.437681, Test Loss = 0.251348, Learning Rate = 9.174962e-06\n",
      "Epoch 12346/20000: Train Loss = 0.437768, Test Loss = 0.251453, Learning Rate = 9.171476e-06\n",
      "Epoch 12347/20000: Train Loss = 0.437839, Test Loss = 0.251160, Learning Rate = 9.167991e-06\n",
      "Epoch 12348/20000: Train Loss = 0.437873, Test Loss = 0.252155, Learning Rate = 9.164508e-06\n",
      "Epoch 12349/20000: Train Loss = 0.437732, Test Loss = 0.252375, Learning Rate = 9.161025e-06\n",
      "Epoch 12350/20000: Train Loss = 0.438051, Test Loss = 0.253426, Learning Rate = 9.157544e-06\n",
      "Epoch 12351/20000: Train Loss = 0.438007, Test Loss = 0.255130, Learning Rate = 9.154065e-06\n",
      "Epoch 12352/20000: Train Loss = 0.437711, Test Loss = 0.252821, Learning Rate = 9.150586e-06\n",
      "Epoch 12353/20000: Train Loss = 0.438079, Test Loss = 0.255775, Learning Rate = 9.147109e-06\n",
      "Epoch 12354/20000: Train Loss = 0.437844, Test Loss = 0.251682, Learning Rate = 9.143634e-06\n",
      "Epoch 12355/20000: Train Loss = 0.438187, Test Loss = 0.254031, Learning Rate = 9.140159e-06\n",
      "Epoch 12356/20000: Train Loss = 0.438593, Test Loss = 0.250345, Learning Rate = 9.136686e-06\n",
      "Epoch 12357/20000: Train Loss = 0.438646, Test Loss = 0.251436, Learning Rate = 9.133215e-06\n",
      "Epoch 12358/20000: Train Loss = 0.438288, Test Loss = 0.252584, Learning Rate = 9.129744e-06\n",
      "Epoch 12359/20000: Train Loss = 0.437835, Test Loss = 0.249934, Learning Rate = 9.126275e-06\n",
      "Epoch 12360/20000: Train Loss = 0.437727, Test Loss = 0.252650, Learning Rate = 9.122808e-06\n",
      "Epoch 12361/20000: Train Loss = 0.437827, Test Loss = 0.246025, Learning Rate = 9.119341e-06\n",
      "Epoch 12362/20000: Train Loss = 0.438180, Test Loss = 0.249032, Learning Rate = 9.115876e-06\n",
      "Epoch 12363/20000: Train Loss = 0.438035, Test Loss = 0.254320, Learning Rate = 9.112412e-06\n",
      "Epoch 12364/20000: Train Loss = 0.438173, Test Loss = 0.252941, Learning Rate = 9.108950e-06\n",
      "Epoch 12365/20000: Train Loss = 0.438099, Test Loss = 0.251784, Learning Rate = 9.105489e-06\n",
      "Epoch 12366/20000: Train Loss = 0.438083, Test Loss = 0.253624, Learning Rate = 9.102029e-06\n",
      "Epoch 12367/20000: Train Loss = 0.437943, Test Loss = 0.248141, Learning Rate = 9.098570e-06\n",
      "Epoch 12368/20000: Train Loss = 0.438008, Test Loss = 0.251323, Learning Rate = 9.095113e-06\n",
      "Epoch 12369/20000: Train Loss = 0.437794, Test Loss = 0.249689, Learning Rate = 9.091657e-06\n",
      "Epoch 12370/20000: Train Loss = 0.437530, Test Loss = 0.255119, Learning Rate = 9.088203e-06\n",
      "Epoch 12371/20000: Train Loss = 0.438003, Test Loss = 0.249894, Learning Rate = 9.084749e-06\n",
      "Epoch 12372/20000: Train Loss = 0.437683, Test Loss = 0.251981, Learning Rate = 9.081297e-06\n",
      "Epoch 12373/20000: Train Loss = 0.437692, Test Loss = 0.252686, Learning Rate = 9.077847e-06\n",
      "Epoch 12374/20000: Train Loss = 0.437724, Test Loss = 0.247959, Learning Rate = 9.074397e-06\n",
      "Epoch 12375/20000: Train Loss = 0.438210, Test Loss = 0.248355, Learning Rate = 9.070949e-06\n",
      "Epoch 12376/20000: Train Loss = 0.438275, Test Loss = 0.247933, Learning Rate = 9.067503e-06\n",
      "Epoch 12377/20000: Train Loss = 0.438086, Test Loss = 0.248474, Learning Rate = 9.064057e-06\n",
      "Epoch 12378/20000: Train Loss = 0.437631, Test Loss = 0.247124, Learning Rate = 9.060613e-06\n",
      "Epoch 12379/20000: Train Loss = 0.437921, Test Loss = 0.249737, Learning Rate = 9.057170e-06\n",
      "Epoch 12380/20000: Train Loss = 0.438145, Test Loss = 0.253024, Learning Rate = 9.053729e-06\n",
      "Epoch 12381/20000: Train Loss = 0.437599, Test Loss = 0.251807, Learning Rate = 9.050289e-06\n",
      "Epoch 12382/20000: Train Loss = 0.437789, Test Loss = 0.250747, Learning Rate = 9.046850e-06\n",
      "Epoch 12383/20000: Train Loss = 0.437707, Test Loss = 0.253840, Learning Rate = 9.043412e-06\n",
      "Epoch 12384/20000: Train Loss = 0.438260, Test Loss = 0.253388, Learning Rate = 9.039976e-06\n",
      "Epoch 12385/20000: Train Loss = 0.438155, Test Loss = 0.248670, Learning Rate = 9.036541e-06\n",
      "Epoch 12386/20000: Train Loss = 0.437562, Test Loss = 0.253055, Learning Rate = 9.033107e-06\n",
      "Epoch 12387/20000: Train Loss = 0.437603, Test Loss = 0.248679, Learning Rate = 9.029675e-06\n",
      "Epoch 12388/20000: Train Loss = 0.437933, Test Loss = 0.253205, Learning Rate = 9.026244e-06\n",
      "Epoch 12389/20000: Train Loss = 0.438030, Test Loss = 0.252153, Learning Rate = 9.022814e-06\n",
      "Epoch 12390/20000: Train Loss = 0.438372, Test Loss = 0.253877, Learning Rate = 9.019386e-06\n",
      "Epoch 12391/20000: Train Loss = 0.438321, Test Loss = 0.253357, Learning Rate = 9.015959e-06\n",
      "Epoch 12392/20000: Train Loss = 0.437937, Test Loss = 0.249185, Learning Rate = 9.012533e-06\n",
      "Epoch 12393/20000: Train Loss = 0.438220, Test Loss = 0.249119, Learning Rate = 9.009108e-06\n",
      "Epoch 12394/20000: Train Loss = 0.437771, Test Loss = 0.252494, Learning Rate = 9.005685e-06\n",
      "Epoch 12395/20000: Train Loss = 0.437895, Test Loss = 0.249081, Learning Rate = 9.002263e-06\n",
      "Epoch 12396/20000: Train Loss = 0.438417, Test Loss = 0.252924, Learning Rate = 8.998843e-06\n",
      "Epoch 12397/20000: Train Loss = 0.437993, Test Loss = 0.257356, Learning Rate = 8.995423e-06\n",
      "Epoch 12398/20000: Train Loss = 0.437686, Test Loss = 0.252752, Learning Rate = 8.992005e-06\n",
      "Epoch 12399/20000: Train Loss = 0.437719, Test Loss = 0.251140, Learning Rate = 8.988589e-06\n",
      "Epoch 12400/20000: Train Loss = 0.437846, Test Loss = 0.250077, Learning Rate = 8.985173e-06\n",
      "Epoch 12401/20000: Train Loss = 0.437694, Test Loss = 0.249325, Learning Rate = 8.981759e-06\n",
      "Epoch 12402/20000: Train Loss = 0.437652, Test Loss = 0.251903, Learning Rate = 8.978346e-06\n",
      "Epoch 12403/20000: Train Loss = 0.437931, Test Loss = 0.251763, Learning Rate = 8.974935e-06\n",
      "Epoch 12404/20000: Train Loss = 0.438041, Test Loss = 0.249725, Learning Rate = 8.971525e-06\n",
      "Epoch 12405/20000: Train Loss = 0.438383, Test Loss = 0.247320, Learning Rate = 8.968116e-06\n",
      "Epoch 12406/20000: Train Loss = 0.437852, Test Loss = 0.249606, Learning Rate = 8.964708e-06\n",
      "Epoch 12407/20000: Train Loss = 0.437847, Test Loss = 0.245120, Learning Rate = 8.961302e-06\n",
      "Epoch 12408/20000: Train Loss = 0.438044, Test Loss = 0.248704, Learning Rate = 8.957897e-06\n",
      "Epoch 12409/20000: Train Loss = 0.437678, Test Loss = 0.250866, Learning Rate = 8.954493e-06\n",
      "Epoch 12410/20000: Train Loss = 0.437802, Test Loss = 0.251576, Learning Rate = 8.951090e-06\n",
      "Epoch 12411/20000: Train Loss = 0.438176, Test Loss = 0.254354, Learning Rate = 8.947689e-06\n",
      "Epoch 12412/20000: Train Loss = 0.437544, Test Loss = 0.251899, Learning Rate = 8.944289e-06\n",
      "Epoch 12413/20000: Train Loss = 0.437928, Test Loss = 0.251865, Learning Rate = 8.940891e-06\n",
      "Epoch 12414/20000: Train Loss = 0.437749, Test Loss = 0.251143, Learning Rate = 8.937493e-06\n",
      "Epoch 12415/20000: Train Loss = 0.437626, Test Loss = 0.251888, Learning Rate = 8.934097e-06\n",
      "Epoch 12416/20000: Train Loss = 0.437581, Test Loss = 0.254555, Learning Rate = 8.930703e-06\n",
      "Epoch 12417/20000: Train Loss = 0.437737, Test Loss = 0.251329, Learning Rate = 8.927309e-06\n",
      "Epoch 12418/20000: Train Loss = 0.437888, Test Loss = 0.252056, Learning Rate = 8.923917e-06\n",
      "Epoch 12419/20000: Train Loss = 0.438339, Test Loss = 0.251365, Learning Rate = 8.920526e-06\n",
      "Epoch 12420/20000: Train Loss = 0.437659, Test Loss = 0.252532, Learning Rate = 8.917137e-06\n",
      "Epoch 12421/20000: Train Loss = 0.438010, Test Loss = 0.252544, Learning Rate = 8.913748e-06\n",
      "Epoch 12422/20000: Train Loss = 0.437885, Test Loss = 0.253192, Learning Rate = 8.910361e-06\n",
      "Epoch 12423/20000: Train Loss = 0.438159, Test Loss = 0.253585, Learning Rate = 8.906976e-06\n",
      "Epoch 12424/20000: Train Loss = 0.437853, Test Loss = 0.254680, Learning Rate = 8.903591e-06\n",
      "Epoch 12425/20000: Train Loss = 0.437986, Test Loss = 0.254269, Learning Rate = 8.900208e-06\n",
      "Epoch 12426/20000: Train Loss = 0.437899, Test Loss = 0.249196, Learning Rate = 8.896826e-06\n",
      "Epoch 12427/20000: Train Loss = 0.438399, Test Loss = 0.256034, Learning Rate = 8.893446e-06\n",
      "Epoch 12428/20000: Train Loss = 0.437820, Test Loss = 0.250360, Learning Rate = 8.890067e-06\n",
      "Epoch 12429/20000: Train Loss = 0.438875, Test Loss = 0.250484, Learning Rate = 8.886689e-06\n",
      "Epoch 12430/20000: Train Loss = 0.437729, Test Loss = 0.250471, Learning Rate = 8.883312e-06\n",
      "Epoch 12431/20000: Train Loss = 0.437936, Test Loss = 0.245325, Learning Rate = 8.879936e-06\n",
      "Epoch 12432/20000: Train Loss = 0.437646, Test Loss = 0.247155, Learning Rate = 8.876562e-06\n",
      "Epoch 12433/20000: Train Loss = 0.437710, Test Loss = 0.252819, Learning Rate = 8.873189e-06\n",
      "Epoch 12434/20000: Train Loss = 0.438017, Test Loss = 0.249759, Learning Rate = 8.869818e-06\n",
      "Epoch 12435/20000: Train Loss = 0.437646, Test Loss = 0.250074, Learning Rate = 8.866448e-06\n",
      "Epoch 12436/20000: Train Loss = 0.437953, Test Loss = 0.247149, Learning Rate = 8.863079e-06\n",
      "Epoch 12437/20000: Train Loss = 0.437721, Test Loss = 0.248882, Learning Rate = 8.859711e-06\n",
      "Epoch 12438/20000: Train Loss = 0.437852, Test Loss = 0.248800, Learning Rate = 8.856344e-06\n",
      "Epoch 12439/20000: Train Loss = 0.437649, Test Loss = 0.251422, Learning Rate = 8.852979e-06\n",
      "Epoch 12440/20000: Train Loss = 0.438028, Test Loss = 0.251681, Learning Rate = 8.849615e-06\n",
      "Epoch 12441/20000: Train Loss = 0.438180, Test Loss = 0.249556, Learning Rate = 8.846253e-06\n",
      "Epoch 12442/20000: Train Loss = 0.437817, Test Loss = 0.251080, Learning Rate = 8.842891e-06\n",
      "Epoch 12443/20000: Train Loss = 0.437511, Test Loss = 0.248466, Learning Rate = 8.839531e-06\n",
      "Epoch 12444/20000: Train Loss = 0.437520, Test Loss = 0.250478, Learning Rate = 8.836173e-06\n",
      "Epoch 12445/20000: Train Loss = 0.437970, Test Loss = 0.256561, Learning Rate = 8.832815e-06\n",
      "Epoch 12446/20000: Train Loss = 0.437880, Test Loss = 0.250316, Learning Rate = 8.829459e-06\n",
      "Epoch 12447/20000: Train Loss = 0.438481, Test Loss = 0.251075, Learning Rate = 8.826104e-06\n",
      "Epoch 12448/20000: Train Loss = 0.437630, Test Loss = 0.246600, Learning Rate = 8.822750e-06\n",
      "Epoch 12449/20000: Train Loss = 0.437514, Test Loss = 0.249701, Learning Rate = 8.819398e-06\n",
      "Epoch 12450/20000: Train Loss = 0.437799, Test Loss = 0.245916, Learning Rate = 8.816047e-06\n",
      "Epoch 12451/20000: Train Loss = 0.437901, Test Loss = 0.247475, Learning Rate = 8.812697e-06\n",
      "Epoch 12452/20000: Train Loss = 0.437653, Test Loss = 0.245710, Learning Rate = 8.809348e-06\n",
      "Epoch 12453/20000: Train Loss = 0.437536, Test Loss = 0.247022, Learning Rate = 8.806001e-06\n",
      "Epoch 12454/20000: Train Loss = 0.437864, Test Loss = 0.247805, Learning Rate = 8.802655e-06\n",
      "Epoch 12455/20000: Train Loss = 0.437912, Test Loss = 0.254905, Learning Rate = 8.799310e-06\n",
      "Epoch 12456/20000: Train Loss = 0.438102, Test Loss = 0.250598, Learning Rate = 8.795967e-06\n",
      "Epoch 12457/20000: Train Loss = 0.437826, Test Loss = 0.252329, Learning Rate = 8.792624e-06\n",
      "Epoch 12458/20000: Train Loss = 0.437865, Test Loss = 0.251428, Learning Rate = 8.789283e-06\n",
      "Epoch 12459/20000: Train Loss = 0.437803, Test Loss = 0.249704, Learning Rate = 8.785944e-06\n",
      "Epoch 12460/20000: Train Loss = 0.437694, Test Loss = 0.250376, Learning Rate = 8.782605e-06\n",
      "Epoch 12461/20000: Train Loss = 0.437341, Test Loss = 0.246906, Learning Rate = 8.779268e-06\n",
      "Epoch 12462/20000: Train Loss = 0.438085, Test Loss = 0.252779, Learning Rate = 8.775932e-06\n",
      "Epoch 12463/20000: Train Loss = 0.437842, Test Loss = 0.245977, Learning Rate = 8.772598e-06\n",
      "Epoch 12464/20000: Train Loss = 0.437644, Test Loss = 0.246938, Learning Rate = 8.769264e-06\n",
      "Epoch 12465/20000: Train Loss = 0.437536, Test Loss = 0.249502, Learning Rate = 8.765932e-06\n",
      "Epoch 12466/20000: Train Loss = 0.437666, Test Loss = 0.251188, Learning Rate = 8.762601e-06\n",
      "Epoch 12467/20000: Train Loss = 0.437331, Test Loss = 0.247812, Learning Rate = 8.759272e-06\n",
      "Epoch 12468/20000: Train Loss = 0.437897, Test Loss = 0.249650, Learning Rate = 8.755944e-06\n",
      "Epoch 12469/20000: Train Loss = 0.437770, Test Loss = 0.251438, Learning Rate = 8.752617e-06\n",
      "Epoch 12470/20000: Train Loss = 0.438012, Test Loss = 0.249289, Learning Rate = 8.749291e-06\n",
      "Epoch 12471/20000: Train Loss = 0.437524, Test Loss = 0.254899, Learning Rate = 8.745966e-06\n",
      "Epoch 12472/20000: Train Loss = 0.437568, Test Loss = 0.254455, Learning Rate = 8.742643e-06\n",
      "Epoch 12473/20000: Train Loss = 0.437891, Test Loss = 0.252810, Learning Rate = 8.739321e-06\n",
      "Epoch 12474/20000: Train Loss = 0.437486, Test Loss = 0.251933, Learning Rate = 8.736000e-06\n",
      "Epoch 12475/20000: Train Loss = 0.437768, Test Loss = 0.256141, Learning Rate = 8.732681e-06\n",
      "Epoch 12476/20000: Train Loss = 0.437713, Test Loss = 0.251976, Learning Rate = 8.729363e-06\n",
      "Epoch 12477/20000: Train Loss = 0.437735, Test Loss = 0.251856, Learning Rate = 8.726046e-06\n",
      "Epoch 12478/20000: Train Loss = 0.437563, Test Loss = 0.253998, Learning Rate = 8.722730e-06\n",
      "Epoch 12479/20000: Train Loss = 0.437738, Test Loss = 0.252306, Learning Rate = 8.719416e-06\n",
      "Epoch 12480/20000: Train Loss = 0.437851, Test Loss = 0.255979, Learning Rate = 8.716103e-06\n",
      "Epoch 12481/20000: Train Loss = 0.437424, Test Loss = 0.248911, Learning Rate = 8.712791e-06\n",
      "Epoch 12482/20000: Train Loss = 0.437525, Test Loss = 0.250950, Learning Rate = 8.709480e-06\n",
      "Epoch 12483/20000: Train Loss = 0.437797, Test Loss = 0.250650, Learning Rate = 8.706171e-06\n",
      "Epoch 12484/20000: Train Loss = 0.437513, Test Loss = 0.245602, Learning Rate = 8.702863e-06\n",
      "Epoch 12485/20000: Train Loss = 0.438436, Test Loss = 0.249818, Learning Rate = 8.699556e-06\n",
      "Epoch 12486/20000: Train Loss = 0.437337, Test Loss = 0.243973, Learning Rate = 8.696250e-06\n",
      "Epoch 12487/20000: Train Loss = 0.438437, Test Loss = 0.247532, Learning Rate = 8.692946e-06\n",
      "Epoch 12488/20000: Train Loss = 0.438137, Test Loss = 0.251393, Learning Rate = 8.689643e-06\n",
      "Epoch 12489/20000: Train Loss = 0.437839, Test Loss = 0.249168, Learning Rate = 8.686341e-06\n",
      "Epoch 12490/20000: Train Loss = 0.437796, Test Loss = 0.252281, Learning Rate = 8.683040e-06\n",
      "Epoch 12491/20000: Train Loss = 0.437687, Test Loss = 0.251781, Learning Rate = 8.679741e-06\n",
      "Epoch 12492/20000: Train Loss = 0.437524, Test Loss = 0.251997, Learning Rate = 8.676443e-06\n",
      "Epoch 12493/20000: Train Loss = 0.437506, Test Loss = 0.256493, Learning Rate = 8.673146e-06\n",
      "Epoch 12494/20000: Train Loss = 0.437798, Test Loss = 0.252089, Learning Rate = 8.669851e-06\n",
      "Epoch 12495/20000: Train Loss = 0.437908, Test Loss = 0.251054, Learning Rate = 8.666556e-06\n",
      "Epoch 12496/20000: Train Loss = 0.437667, Test Loss = 0.252153, Learning Rate = 8.663263e-06\n",
      "Epoch 12497/20000: Train Loss = 0.437867, Test Loss = 0.255292, Learning Rate = 8.659971e-06\n",
      "Epoch 12498/20000: Train Loss = 0.440534, Test Loss = 0.261219, Learning Rate = 8.656681e-06\n",
      "Epoch 12499/20000: Train Loss = 0.437991, Test Loss = 0.249593, Learning Rate = 8.653392e-06\n",
      "Epoch 12500/20000: Train Loss = 0.437799, Test Loss = 0.250660, Learning Rate = 8.650104e-06\n",
      "Epoch 12501/20000: Train Loss = 0.437834, Test Loss = 0.251768, Learning Rate = 8.646817e-06\n",
      "Epoch 12502/20000: Train Loss = 0.437442, Test Loss = 0.251454, Learning Rate = 8.643531e-06\n",
      "Epoch 12503/20000: Train Loss = 0.437470, Test Loss = 0.250608, Learning Rate = 8.640247e-06\n",
      "Epoch 12504/20000: Train Loss = 0.437611, Test Loss = 0.248766, Learning Rate = 8.636964e-06\n",
      "Epoch 12505/20000: Train Loss = 0.437727, Test Loss = 0.255816, Learning Rate = 8.633682e-06\n",
      "Epoch 12506/20000: Train Loss = 0.438576, Test Loss = 0.245172, Learning Rate = 8.630401e-06\n",
      "Epoch 12507/20000: Train Loss = 0.437415, Test Loss = 0.252512, Learning Rate = 8.627122e-06\n",
      "Epoch 12508/20000: Train Loss = 0.437544, Test Loss = 0.253420, Learning Rate = 8.623844e-06\n",
      "Epoch 12509/20000: Train Loss = 0.437278, Test Loss = 0.250199, Learning Rate = 8.620567e-06\n",
      "Epoch 12510/20000: Train Loss = 0.437497, Test Loss = 0.250144, Learning Rate = 8.617292e-06\n",
      "Epoch 12511/20000: Train Loss = 0.437368, Test Loss = 0.248374, Learning Rate = 8.614017e-06\n",
      "Epoch 12512/20000: Train Loss = 0.437575, Test Loss = 0.247445, Learning Rate = 8.610744e-06\n",
      "Epoch 12513/20000: Train Loss = 0.437592, Test Loss = 0.246236, Learning Rate = 8.607472e-06\n",
      "Epoch 12514/20000: Train Loss = 0.437634, Test Loss = 0.252558, Learning Rate = 8.604202e-06\n",
      "Epoch 12515/20000: Train Loss = 0.437622, Test Loss = 0.251466, Learning Rate = 8.600932e-06\n",
      "Epoch 12516/20000: Train Loss = 0.438384, Test Loss = 0.252540, Learning Rate = 8.597664e-06\n",
      "Epoch 12517/20000: Train Loss = 0.438216, Test Loss = 0.248340, Learning Rate = 8.594397e-06\n",
      "Epoch 12518/20000: Train Loss = 0.437632, Test Loss = 0.248008, Learning Rate = 8.591132e-06\n",
      "Epoch 12519/20000: Train Loss = 0.437444, Test Loss = 0.250427, Learning Rate = 8.587867e-06\n",
      "Epoch 12520/20000: Train Loss = 0.437704, Test Loss = 0.253086, Learning Rate = 8.584604e-06\n",
      "Epoch 12521/20000: Train Loss = 0.437979, Test Loss = 0.256815, Learning Rate = 8.581342e-06\n",
      "Epoch 12522/20000: Train Loss = 0.437383, Test Loss = 0.248065, Learning Rate = 8.578082e-06\n",
      "Epoch 12523/20000: Train Loss = 0.437452, Test Loss = 0.251791, Learning Rate = 8.574822e-06\n",
      "Epoch 12524/20000: Train Loss = 0.437677, Test Loss = 0.254552, Learning Rate = 8.571564e-06\n",
      "Epoch 12525/20000: Train Loss = 0.437604, Test Loss = 0.256677, Learning Rate = 8.568307e-06\n",
      "Epoch 12526/20000: Train Loss = 0.437448, Test Loss = 0.255746, Learning Rate = 8.565051e-06\n",
      "Epoch 12527/20000: Train Loss = 0.437561, Test Loss = 0.251732, Learning Rate = 8.561797e-06\n",
      "Epoch 12528/20000: Train Loss = 0.437389, Test Loss = 0.250798, Learning Rate = 8.558543e-06\n",
      "Epoch 12529/20000: Train Loss = 0.437551, Test Loss = 0.251961, Learning Rate = 8.555291e-06\n",
      "Epoch 12530/20000: Train Loss = 0.437597, Test Loss = 0.250535, Learning Rate = 8.552041e-06\n",
      "Epoch 12531/20000: Train Loss = 0.437598, Test Loss = 0.246432, Learning Rate = 8.548791e-06\n",
      "Epoch 12532/20000: Train Loss = 0.437359, Test Loss = 0.246769, Learning Rate = 8.545543e-06\n",
      "Epoch 12533/20000: Train Loss = 0.437723, Test Loss = 0.248874, Learning Rate = 8.542296e-06\n",
      "Epoch 12534/20000: Train Loss = 0.437500, Test Loss = 0.247534, Learning Rate = 8.539050e-06\n",
      "Epoch 12535/20000: Train Loss = 0.437925, Test Loss = 0.252500, Learning Rate = 8.535805e-06\n",
      "Epoch 12536/20000: Train Loss = 0.437580, Test Loss = 0.247650, Learning Rate = 8.532562e-06\n",
      "Epoch 12537/20000: Train Loss = 0.437338, Test Loss = 0.248329, Learning Rate = 8.529320e-06\n",
      "Epoch 12538/20000: Train Loss = 0.437475, Test Loss = 0.248184, Learning Rate = 8.526079e-06\n",
      "Epoch 12539/20000: Train Loss = 0.437383, Test Loss = 0.249375, Learning Rate = 8.522839e-06\n",
      "Epoch 12540/20000: Train Loss = 0.437496, Test Loss = 0.248971, Learning Rate = 8.519601e-06\n",
      "Epoch 12541/20000: Train Loss = 0.437532, Test Loss = 0.252176, Learning Rate = 8.516364e-06\n",
      "Epoch 12542/20000: Train Loss = 0.437382, Test Loss = 0.249975, Learning Rate = 8.513128e-06\n",
      "Epoch 12543/20000: Train Loss = 0.437651, Test Loss = 0.246526, Learning Rate = 8.509893e-06\n",
      "Epoch 12544/20000: Train Loss = 0.437208, Test Loss = 0.252055, Learning Rate = 8.506659e-06\n",
      "Epoch 12545/20000: Train Loss = 0.437387, Test Loss = 0.250540, Learning Rate = 8.503427e-06\n",
      "Epoch 12546/20000: Train Loss = 0.437480, Test Loss = 0.251132, Learning Rate = 8.500196e-06\n",
      "Epoch 12547/20000: Train Loss = 0.437696, Test Loss = 0.246750, Learning Rate = 8.496966e-06\n",
      "Epoch 12548/20000: Train Loss = 0.437839, Test Loss = 0.247638, Learning Rate = 8.493737e-06\n",
      "Epoch 12549/20000: Train Loss = 0.438629, Test Loss = 0.256102, Learning Rate = 8.490510e-06\n",
      "Epoch 12550/20000: Train Loss = 0.437394, Test Loss = 0.250321, Learning Rate = 8.487284e-06\n",
      "Epoch 12551/20000: Train Loss = 0.437622, Test Loss = 0.250298, Learning Rate = 8.484059e-06\n",
      "Epoch 12552/20000: Train Loss = 0.437401, Test Loss = 0.250732, Learning Rate = 8.480835e-06\n",
      "Epoch 12553/20000: Train Loss = 0.437956, Test Loss = 0.255682, Learning Rate = 8.477613e-06\n",
      "Epoch 12554/20000: Train Loss = 0.438980, Test Loss = 0.252283, Learning Rate = 8.474391e-06\n",
      "Epoch 12555/20000: Train Loss = 0.437725, Test Loss = 0.252488, Learning Rate = 8.471171e-06\n",
      "Epoch 12556/20000: Train Loss = 0.437622, Test Loss = 0.258689, Learning Rate = 8.467953e-06\n",
      "Epoch 12557/20000: Train Loss = 0.437591, Test Loss = 0.256894, Learning Rate = 8.464735e-06\n",
      "Epoch 12558/20000: Train Loss = 0.437454, Test Loss = 0.254287, Learning Rate = 8.461519e-06\n",
      "Epoch 12559/20000: Train Loss = 0.437330, Test Loss = 0.252622, Learning Rate = 8.458304e-06\n",
      "Epoch 12560/20000: Train Loss = 0.437927, Test Loss = 0.251367, Learning Rate = 8.455090e-06\n",
      "Epoch 12561/20000: Train Loss = 0.437299, Test Loss = 0.252559, Learning Rate = 8.451877e-06\n",
      "Epoch 12562/20000: Train Loss = 0.437638, Test Loss = 0.250279, Learning Rate = 8.448665e-06\n",
      "Epoch 12563/20000: Train Loss = 0.437617, Test Loss = 0.258561, Learning Rate = 8.445455e-06\n",
      "Epoch 12564/20000: Train Loss = 0.437558, Test Loss = 0.253931, Learning Rate = 8.442246e-06\n",
      "Epoch 12565/20000: Train Loss = 0.437964, Test Loss = 0.258332, Learning Rate = 8.439038e-06\n",
      "Epoch 12566/20000: Train Loss = 0.437661, Test Loss = 0.254009, Learning Rate = 8.435832e-06\n",
      "Epoch 12567/20000: Train Loss = 0.437766, Test Loss = 0.255519, Learning Rate = 8.432626e-06\n",
      "Epoch 12568/20000: Train Loss = 0.437145, Test Loss = 0.249791, Learning Rate = 8.429422e-06\n",
      "Epoch 12569/20000: Train Loss = 0.437589, Test Loss = 0.248808, Learning Rate = 8.426219e-06\n",
      "Epoch 12570/20000: Train Loss = 0.437552, Test Loss = 0.251728, Learning Rate = 8.423017e-06\n",
      "Epoch 12571/20000: Train Loss = 0.437324, Test Loss = 0.250248, Learning Rate = 8.419817e-06\n",
      "Epoch 12572/20000: Train Loss = 0.437285, Test Loss = 0.256094, Learning Rate = 8.416618e-06\n",
      "Epoch 12573/20000: Train Loss = 0.437498, Test Loss = 0.251209, Learning Rate = 8.413420e-06\n",
      "Epoch 12574/20000: Train Loss = 0.437457, Test Loss = 0.251777, Learning Rate = 8.410223e-06\n",
      "Epoch 12575/20000: Train Loss = 0.437952, Test Loss = 0.248695, Learning Rate = 8.407027e-06\n",
      "Epoch 12576/20000: Train Loss = 0.437456, Test Loss = 0.250028, Learning Rate = 8.403833e-06\n",
      "Epoch 12577/20000: Train Loss = 0.437249, Test Loss = 0.252418, Learning Rate = 8.400639e-06\n",
      "Epoch 12578/20000: Train Loss = 0.437395, Test Loss = 0.250179, Learning Rate = 8.397447e-06\n",
      "Epoch 12579/20000: Train Loss = 0.438041, Test Loss = 0.252379, Learning Rate = 8.394257e-06\n",
      "Epoch 12580/20000: Train Loss = 0.437961, Test Loss = 0.249622, Learning Rate = 8.391067e-06\n",
      "Epoch 12581/20000: Train Loss = 0.437371, Test Loss = 0.249145, Learning Rate = 8.387879e-06\n",
      "Epoch 12582/20000: Train Loss = 0.437673, Test Loss = 0.252181, Learning Rate = 8.384691e-06\n",
      "Epoch 12583/20000: Train Loss = 0.437609, Test Loss = 0.254211, Learning Rate = 8.381505e-06\n",
      "Epoch 12584/20000: Train Loss = 0.438125, Test Loss = 0.256935, Learning Rate = 8.378321e-06\n",
      "Epoch 12585/20000: Train Loss = 0.437932, Test Loss = 0.253902, Learning Rate = 8.375137e-06\n",
      "Epoch 12586/20000: Train Loss = 0.437821, Test Loss = 0.252793, Learning Rate = 8.371955e-06\n",
      "Epoch 12587/20000: Train Loss = 0.437307, Test Loss = 0.250396, Learning Rate = 8.368774e-06\n",
      "Epoch 12588/20000: Train Loss = 0.437327, Test Loss = 0.248061, Learning Rate = 8.365594e-06\n",
      "Epoch 12589/20000: Train Loss = 0.437426, Test Loss = 0.246332, Learning Rate = 8.362415e-06\n",
      "Epoch 12590/20000: Train Loss = 0.437481, Test Loss = 0.248929, Learning Rate = 8.359238e-06\n",
      "Epoch 12591/20000: Train Loss = 0.437896, Test Loss = 0.255083, Learning Rate = 8.356061e-06\n",
      "Epoch 12592/20000: Train Loss = 0.437285, Test Loss = 0.249337, Learning Rate = 8.352886e-06\n",
      "Epoch 12593/20000: Train Loss = 0.437547, Test Loss = 0.253881, Learning Rate = 8.349712e-06\n",
      "Epoch 12594/20000: Train Loss = 0.437767, Test Loss = 0.255828, Learning Rate = 8.346540e-06\n",
      "Epoch 12595/20000: Train Loss = 0.437141, Test Loss = 0.249964, Learning Rate = 8.343368e-06\n",
      "Epoch 12596/20000: Train Loss = 0.437459, Test Loss = 0.254424, Learning Rate = 8.340198e-06\n",
      "Epoch 12597/20000: Train Loss = 0.437697, Test Loss = 0.249920, Learning Rate = 8.337029e-06\n",
      "Epoch 12598/20000: Train Loss = 0.437580, Test Loss = 0.252468, Learning Rate = 8.333861e-06\n",
      "Epoch 12599/20000: Train Loss = 0.437516, Test Loss = 0.253205, Learning Rate = 8.330694e-06\n",
      "Epoch 12600/20000: Train Loss = 0.437477, Test Loss = 0.257612, Learning Rate = 8.327529e-06\n",
      "Epoch 12601/20000: Train Loss = 0.437562, Test Loss = 0.253595, Learning Rate = 8.324365e-06\n",
      "Epoch 12602/20000: Train Loss = 0.437590, Test Loss = 0.247664, Learning Rate = 8.321202e-06\n",
      "Epoch 12603/20000: Train Loss = 0.437678, Test Loss = 0.255058, Learning Rate = 8.318040e-06\n",
      "Epoch 12604/20000: Train Loss = 0.437838, Test Loss = 0.253201, Learning Rate = 8.314879e-06\n",
      "Epoch 12605/20000: Train Loss = 0.437584, Test Loss = 0.256000, Learning Rate = 8.311720e-06\n",
      "Epoch 12606/20000: Train Loss = 0.437091, Test Loss = 0.251981, Learning Rate = 8.308562e-06\n",
      "Epoch 12607/20000: Train Loss = 0.437191, Test Loss = 0.254659, Learning Rate = 8.305405e-06\n",
      "Epoch 12608/20000: Train Loss = 0.437524, Test Loss = 0.252938, Learning Rate = 8.302249e-06\n",
      "Epoch 12609/20000: Train Loss = 0.437879, Test Loss = 0.254970, Learning Rate = 8.299094e-06\n",
      "Epoch 12610/20000: Train Loss = 0.437935, Test Loss = 0.255905, Learning Rate = 8.295941e-06\n",
      "Epoch 12611/20000: Train Loss = 0.437360, Test Loss = 0.254265, Learning Rate = 8.292788e-06\n",
      "Epoch 12612/20000: Train Loss = 0.437765, Test Loss = 0.251547, Learning Rate = 8.289637e-06\n",
      "Epoch 12613/20000: Train Loss = 0.437684, Test Loss = 0.253648, Learning Rate = 8.286488e-06\n",
      "Epoch 12614/20000: Train Loss = 0.437350, Test Loss = 0.251632, Learning Rate = 8.283339e-06\n",
      "Epoch 12615/20000: Train Loss = 0.437541, Test Loss = 0.249263, Learning Rate = 8.280192e-06\n",
      "Epoch 12616/20000: Train Loss = 0.437703, Test Loss = 0.253191, Learning Rate = 8.277045e-06\n",
      "Epoch 12617/20000: Train Loss = 0.437220, Test Loss = 0.252924, Learning Rate = 8.273900e-06\n",
      "Epoch 12618/20000: Train Loss = 0.437195, Test Loss = 0.253113, Learning Rate = 8.270756e-06\n",
      "Epoch 12619/20000: Train Loss = 0.437394, Test Loss = 0.255486, Learning Rate = 8.267614e-06\n",
      "Epoch 12620/20000: Train Loss = 0.437421, Test Loss = 0.253959, Learning Rate = 8.264472e-06\n",
      "Epoch 12621/20000: Train Loss = 0.437287, Test Loss = 0.252104, Learning Rate = 8.261332e-06\n",
      "Epoch 12622/20000: Train Loss = 0.437258, Test Loss = 0.251794, Learning Rate = 8.258193e-06\n",
      "Epoch 12623/20000: Train Loss = 0.437456, Test Loss = 0.250460, Learning Rate = 8.255055e-06\n",
      "Epoch 12624/20000: Train Loss = 0.437288, Test Loss = 0.253540, Learning Rate = 8.251918e-06\n",
      "Epoch 12625/20000: Train Loss = 0.437483, Test Loss = 0.252260, Learning Rate = 8.248783e-06\n",
      "Epoch 12626/20000: Train Loss = 0.437313, Test Loss = 0.251394, Learning Rate = 8.245648e-06\n",
      "Epoch 12627/20000: Train Loss = 0.437412, Test Loss = 0.253388, Learning Rate = 8.242515e-06\n",
      "Epoch 12628/20000: Train Loss = 0.437665, Test Loss = 0.252843, Learning Rate = 8.239383e-06\n",
      "Epoch 12629/20000: Train Loss = 0.438153, Test Loss = 0.252850, Learning Rate = 8.236253e-06\n",
      "Epoch 12630/20000: Train Loss = 0.437705, Test Loss = 0.252660, Learning Rate = 8.233123e-06\n",
      "Epoch 12631/20000: Train Loss = 0.437362, Test Loss = 0.254031, Learning Rate = 8.229995e-06\n",
      "Epoch 12632/20000: Train Loss = 0.437490, Test Loss = 0.252087, Learning Rate = 8.226868e-06\n",
      "Epoch 12633/20000: Train Loss = 0.437317, Test Loss = 0.247793, Learning Rate = 8.223742e-06\n",
      "Epoch 12634/20000: Train Loss = 0.437746, Test Loss = 0.253005, Learning Rate = 8.220617e-06\n",
      "Epoch 12635/20000: Train Loss = 0.437669, Test Loss = 0.248681, Learning Rate = 8.217493e-06\n",
      "Epoch 12636/20000: Train Loss = 0.437302, Test Loss = 0.249642, Learning Rate = 8.214371e-06\n",
      "Epoch 12637/20000: Train Loss = 0.437311, Test Loss = 0.250731, Learning Rate = 8.211250e-06\n",
      "Epoch 12638/20000: Train Loss = 0.437093, Test Loss = 0.250450, Learning Rate = 8.208129e-06\n",
      "Epoch 12639/20000: Train Loss = 0.437548, Test Loss = 0.250236, Learning Rate = 8.205011e-06\n",
      "Epoch 12640/20000: Train Loss = 0.437394, Test Loss = 0.254456, Learning Rate = 8.201893e-06\n",
      "Epoch 12641/20000: Train Loss = 0.437418, Test Loss = 0.251097, Learning Rate = 8.198776e-06\n",
      "Epoch 12642/20000: Train Loss = 0.437260, Test Loss = 0.252151, Learning Rate = 8.195661e-06\n",
      "Epoch 12643/20000: Train Loss = 0.437650, Test Loss = 0.254409, Learning Rate = 8.192547e-06\n",
      "Epoch 12644/20000: Train Loss = 0.437290, Test Loss = 0.256958, Learning Rate = 8.189434e-06\n",
      "Epoch 12645/20000: Train Loss = 0.437329, Test Loss = 0.256213, Learning Rate = 8.186322e-06\n",
      "Epoch 12646/20000: Train Loss = 0.437571, Test Loss = 0.252200, Learning Rate = 8.183212e-06\n",
      "Epoch 12647/20000: Train Loss = 0.437377, Test Loss = 0.252147, Learning Rate = 8.180102e-06\n",
      "Epoch 12648/20000: Train Loss = 0.437600, Test Loss = 0.251393, Learning Rate = 8.176994e-06\n",
      "Epoch 12649/20000: Train Loss = 0.437165, Test Loss = 0.250516, Learning Rate = 8.173887e-06\n",
      "Epoch 12650/20000: Train Loss = 0.437793, Test Loss = 0.254896, Learning Rate = 8.170781e-06\n",
      "Epoch 12651/20000: Train Loss = 0.438714, Test Loss = 0.252743, Learning Rate = 8.167676e-06\n",
      "Epoch 12652/20000: Train Loss = 0.437699, Test Loss = 0.253516, Learning Rate = 8.164573e-06\n",
      "Epoch 12653/20000: Train Loss = 0.437364, Test Loss = 0.250432, Learning Rate = 8.161471e-06\n",
      "Epoch 12654/20000: Train Loss = 0.437270, Test Loss = 0.249676, Learning Rate = 8.158370e-06\n",
      "Epoch 12655/20000: Train Loss = 0.437210, Test Loss = 0.249720, Learning Rate = 8.155270e-06\n",
      "Epoch 12656/20000: Train Loss = 0.437202, Test Loss = 0.250094, Learning Rate = 8.152171e-06\n",
      "Epoch 12657/20000: Train Loss = 0.437796, Test Loss = 0.249866, Learning Rate = 8.149073e-06\n",
      "Epoch 12658/20000: Train Loss = 0.437771, Test Loss = 0.250858, Learning Rate = 8.145977e-06\n",
      "Epoch 12659/20000: Train Loss = 0.437468, Test Loss = 0.251522, Learning Rate = 8.142882e-06\n",
      "Epoch 12660/20000: Train Loss = 0.437499, Test Loss = 0.248971, Learning Rate = 8.139787e-06\n",
      "Epoch 12661/20000: Train Loss = 0.437583, Test Loss = 0.255898, Learning Rate = 8.136695e-06\n",
      "Epoch 12662/20000: Train Loss = 0.437792, Test Loss = 0.252122, Learning Rate = 8.133603e-06\n",
      "Epoch 12663/20000: Train Loss = 0.437305, Test Loss = 0.252925, Learning Rate = 8.130512e-06\n",
      "Epoch 12664/20000: Train Loss = 0.437335, Test Loss = 0.251485, Learning Rate = 8.127423e-06\n",
      "Epoch 12665/20000: Train Loss = 0.437207, Test Loss = 0.251213, Learning Rate = 8.124335e-06\n",
      "Epoch 12666/20000: Train Loss = 0.437386, Test Loss = 0.250267, Learning Rate = 8.121248e-06\n",
      "Epoch 12667/20000: Train Loss = 0.437355, Test Loss = 0.254147, Learning Rate = 8.118162e-06\n",
      "Epoch 12668/20000: Train Loss = 0.437211, Test Loss = 0.251652, Learning Rate = 8.115077e-06\n",
      "Epoch 12669/20000: Train Loss = 0.437206, Test Loss = 0.254658, Learning Rate = 8.111994e-06\n",
      "Epoch 12670/20000: Train Loss = 0.437251, Test Loss = 0.252457, Learning Rate = 8.108911e-06\n",
      "Epoch 12671/20000: Train Loss = 0.437509, Test Loss = 0.253381, Learning Rate = 8.105830e-06\n",
      "Epoch 12672/20000: Train Loss = 0.437719, Test Loss = 0.256080, Learning Rate = 8.102750e-06\n",
      "Epoch 12673/20000: Train Loss = 0.436766, Test Loss = 0.248885, Learning Rate = 8.099671e-06\n",
      "Epoch 12674/20000: Train Loss = 0.437961, Test Loss = 0.246084, Learning Rate = 8.096594e-06\n",
      "Epoch 12675/20000: Train Loss = 0.437630, Test Loss = 0.249502, Learning Rate = 8.093517e-06\n",
      "Epoch 12676/20000: Train Loss = 0.437475, Test Loss = 0.253169, Learning Rate = 8.090442e-06\n",
      "Epoch 12677/20000: Train Loss = 0.437615, Test Loss = 0.248705, Learning Rate = 8.087368e-06\n",
      "Epoch 12678/20000: Train Loss = 0.437114, Test Loss = 0.253149, Learning Rate = 8.084295e-06\n",
      "Epoch 12679/20000: Train Loss = 0.437189, Test Loss = 0.256834, Learning Rate = 8.081223e-06\n",
      "Epoch 12680/20000: Train Loss = 0.437889, Test Loss = 0.255115, Learning Rate = 8.078152e-06\n",
      "Epoch 12681/20000: Train Loss = 0.437931, Test Loss = 0.250113, Learning Rate = 8.075083e-06\n",
      "Epoch 12682/20000: Train Loss = 0.437952, Test Loss = 0.252012, Learning Rate = 8.072014e-06\n",
      "Epoch 12683/20000: Train Loss = 0.437205, Test Loss = 0.251240, Learning Rate = 8.068947e-06\n",
      "Epoch 12684/20000: Train Loss = 0.437520, Test Loss = 0.253111, Learning Rate = 8.065881e-06\n",
      "Epoch 12685/20000: Train Loss = 0.437586, Test Loss = 0.251161, Learning Rate = 8.062817e-06\n",
      "Epoch 12686/20000: Train Loss = 0.438356, Test Loss = 0.255447, Learning Rate = 8.059753e-06\n",
      "Epoch 12687/20000: Train Loss = 0.437542, Test Loss = 0.256678, Learning Rate = 8.056690e-06\n",
      "Epoch 12688/20000: Train Loss = 0.437463, Test Loss = 0.253585, Learning Rate = 8.053629e-06\n",
      "Epoch 12689/20000: Train Loss = 0.437241, Test Loss = 0.251731, Learning Rate = 8.050569e-06\n",
      "Epoch 12690/20000: Train Loss = 0.438038, Test Loss = 0.255149, Learning Rate = 8.047510e-06\n",
      "Epoch 12691/20000: Train Loss = 0.437327, Test Loss = 0.251056, Learning Rate = 8.044452e-06\n",
      "Epoch 12692/20000: Train Loss = 0.437399, Test Loss = 0.251907, Learning Rate = 8.041395e-06\n",
      "Epoch 12693/20000: Train Loss = 0.437124, Test Loss = 0.254267, Learning Rate = 8.038340e-06\n",
      "Epoch 12694/20000: Train Loss = 0.437893, Test Loss = 0.253577, Learning Rate = 8.035286e-06\n",
      "Epoch 12695/20000: Train Loss = 0.437440, Test Loss = 0.252400, Learning Rate = 8.032232e-06\n",
      "Epoch 12696/20000: Train Loss = 0.437990, Test Loss = 0.250893, Learning Rate = 8.029180e-06\n",
      "Epoch 12697/20000: Train Loss = 0.437192, Test Loss = 0.249309, Learning Rate = 8.026129e-06\n",
      "Epoch 12698/20000: Train Loss = 0.437457, Test Loss = 0.248370, Learning Rate = 8.023080e-06\n",
      "Epoch 12699/20000: Train Loss = 0.437636, Test Loss = 0.253044, Learning Rate = 8.020031e-06\n",
      "Epoch 12700/20000: Train Loss = 0.437131, Test Loss = 0.253526, Learning Rate = 8.016984e-06\n",
      "Epoch 12701/20000: Train Loss = 0.437182, Test Loss = 0.255117, Learning Rate = 8.013938e-06\n",
      "Epoch 12702/20000: Train Loss = 0.437044, Test Loss = 0.252957, Learning Rate = 8.010892e-06\n",
      "Epoch 12703/20000: Train Loss = 0.437349, Test Loss = 0.253685, Learning Rate = 8.007849e-06\n",
      "Epoch 12704/20000: Train Loss = 0.438159, Test Loss = 0.255647, Learning Rate = 8.004806e-06\n",
      "Epoch 12705/20000: Train Loss = 0.437676, Test Loss = 0.255369, Learning Rate = 8.001764e-06\n",
      "Epoch 12706/20000: Train Loss = 0.437342, Test Loss = 0.254553, Learning Rate = 7.998724e-06\n",
      "Epoch 12707/20000: Train Loss = 0.437920, Test Loss = 0.254108, Learning Rate = 7.995684e-06\n",
      "Epoch 12708/20000: Train Loss = 0.437432, Test Loss = 0.251864, Learning Rate = 7.992646e-06\n",
      "Epoch 12709/20000: Train Loss = 0.437384, Test Loss = 0.252439, Learning Rate = 7.989609e-06\n",
      "Epoch 12710/20000: Train Loss = 0.437156, Test Loss = 0.257520, Learning Rate = 7.986573e-06\n",
      "Epoch 12711/20000: Train Loss = 0.436982, Test Loss = 0.254581, Learning Rate = 7.983539e-06\n",
      "Epoch 12712/20000: Train Loss = 0.437827, Test Loss = 0.255209, Learning Rate = 7.980505e-06\n",
      "Epoch 12713/20000: Train Loss = 0.437262, Test Loss = 0.251014, Learning Rate = 7.977473e-06\n",
      "Epoch 12714/20000: Train Loss = 0.437273, Test Loss = 0.248639, Learning Rate = 7.974442e-06\n",
      "Epoch 12715/20000: Train Loss = 0.437561, Test Loss = 0.248736, Learning Rate = 7.971412e-06\n",
      "Epoch 12716/20000: Train Loss = 0.437714, Test Loss = 0.252143, Learning Rate = 7.968383e-06\n",
      "Epoch 12717/20000: Train Loss = 0.437380, Test Loss = 0.252990, Learning Rate = 7.965355e-06\n",
      "Epoch 12718/20000: Train Loss = 0.437436, Test Loss = 0.252771, Learning Rate = 7.962328e-06\n",
      "Epoch 12719/20000: Train Loss = 0.437949, Test Loss = 0.252064, Learning Rate = 7.959303e-06\n",
      "Epoch 12720/20000: Train Loss = 0.437482, Test Loss = 0.248089, Learning Rate = 7.956278e-06\n",
      "Epoch 12721/20000: Train Loss = 0.437305, Test Loss = 0.244682, Learning Rate = 7.953255e-06\n",
      "Epoch 12722/20000: Train Loss = 0.437059, Test Loss = 0.250110, Learning Rate = 7.950233e-06\n",
      "Epoch 12723/20000: Train Loss = 0.437431, Test Loss = 0.249537, Learning Rate = 7.947212e-06\n",
      "Epoch 12724/20000: Train Loss = 0.437212, Test Loss = 0.251981, Learning Rate = 7.944193e-06\n",
      "Epoch 12725/20000: Train Loss = 0.437199, Test Loss = 0.248782, Learning Rate = 7.941174e-06\n",
      "Epoch 12726/20000: Train Loss = 0.437922, Test Loss = 0.254681, Learning Rate = 7.938157e-06\n",
      "Epoch 12727/20000: Train Loss = 0.437007, Test Loss = 0.251117, Learning Rate = 7.935140e-06\n",
      "Epoch 12728/20000: Train Loss = 0.437337, Test Loss = 0.251176, Learning Rate = 7.932125e-06\n",
      "Epoch 12729/20000: Train Loss = 0.437550, Test Loss = 0.248694, Learning Rate = 7.929111e-06\n",
      "Epoch 12730/20000: Train Loss = 0.437357, Test Loss = 0.248433, Learning Rate = 7.926098e-06\n",
      "Epoch 12731/20000: Train Loss = 0.437645, Test Loss = 0.252658, Learning Rate = 7.923087e-06\n",
      "Epoch 12732/20000: Train Loss = 0.437378, Test Loss = 0.252349, Learning Rate = 7.920076e-06\n",
      "Epoch 12733/20000: Train Loss = 0.437483, Test Loss = 0.250834, Learning Rate = 7.917067e-06\n",
      "Epoch 12734/20000: Train Loss = 0.437635, Test Loss = 0.252083, Learning Rate = 7.914058e-06\n",
      "Epoch 12735/20000: Train Loss = 0.437898, Test Loss = 0.257301, Learning Rate = 7.911051e-06\n",
      "Epoch 12736/20000: Train Loss = 0.437475, Test Loss = 0.256959, Learning Rate = 7.908045e-06\n",
      "Epoch 12737/20000: Train Loss = 0.437222, Test Loss = 0.253130, Learning Rate = 7.905040e-06\n",
      "Epoch 12738/20000: Train Loss = 0.437301, Test Loss = 0.255835, Learning Rate = 7.902037e-06\n",
      "Epoch 12739/20000: Train Loss = 0.437561, Test Loss = 0.255560, Learning Rate = 7.899034e-06\n",
      "Epoch 12740/20000: Train Loss = 0.437376, Test Loss = 0.251407, Learning Rate = 7.896033e-06\n",
      "Epoch 12741/20000: Train Loss = 0.437334, Test Loss = 0.253969, Learning Rate = 7.893033e-06\n",
      "Epoch 12742/20000: Train Loss = 0.437155, Test Loss = 0.253919, Learning Rate = 7.890033e-06\n",
      "Epoch 12743/20000: Train Loss = 0.437078, Test Loss = 0.249493, Learning Rate = 7.887035e-06\n",
      "Epoch 12744/20000: Train Loss = 0.437326, Test Loss = 0.255474, Learning Rate = 7.884039e-06\n",
      "Epoch 12745/20000: Train Loss = 0.437274, Test Loss = 0.248155, Learning Rate = 7.881043e-06\n",
      "Epoch 12746/20000: Train Loss = 0.437467, Test Loss = 0.258685, Learning Rate = 7.878048e-06\n",
      "Epoch 12747/20000: Train Loss = 0.437524, Test Loss = 0.249091, Learning Rate = 7.875055e-06\n",
      "Epoch 12748/20000: Train Loss = 0.437183, Test Loss = 0.254635, Learning Rate = 7.872062e-06\n",
      "Epoch 12749/20000: Train Loss = 0.436950, Test Loss = 0.248689, Learning Rate = 7.869071e-06\n",
      "Epoch 12750/20000: Train Loss = 0.437072, Test Loss = 0.249496, Learning Rate = 7.866081e-06\n",
      "Epoch 12751/20000: Train Loss = 0.437810, Test Loss = 0.251897, Learning Rate = 7.863092e-06\n",
      "Epoch 12752/20000: Train Loss = 0.437013, Test Loss = 0.249291, Learning Rate = 7.860105e-06\n",
      "Epoch 12753/20000: Train Loss = 0.437410, Test Loss = 0.250728, Learning Rate = 7.857118e-06\n",
      "Epoch 12754/20000: Train Loss = 0.437249, Test Loss = 0.253937, Learning Rate = 7.854132e-06\n",
      "Epoch 12755/20000: Train Loss = 0.437512, Test Loss = 0.253817, Learning Rate = 7.851148e-06\n",
      "Epoch 12756/20000: Train Loss = 0.437209, Test Loss = 0.254496, Learning Rate = 7.848165e-06\n",
      "Epoch 12757/20000: Train Loss = 0.437351, Test Loss = 0.252953, Learning Rate = 7.845183e-06\n",
      "Epoch 12758/20000: Train Loss = 0.437347, Test Loss = 0.255341, Learning Rate = 7.842202e-06\n",
      "Epoch 12759/20000: Train Loss = 0.437695, Test Loss = 0.253679, Learning Rate = 7.839222e-06\n",
      "Epoch 12760/20000: Train Loss = 0.437357, Test Loss = 0.250338, Learning Rate = 7.836243e-06\n",
      "Epoch 12761/20000: Train Loss = 0.437367, Test Loss = 0.256980, Learning Rate = 7.833266e-06\n",
      "Epoch 12762/20000: Train Loss = 0.437238, Test Loss = 0.250273, Learning Rate = 7.830289e-06\n",
      "Epoch 12763/20000: Train Loss = 0.437212, Test Loss = 0.250135, Learning Rate = 7.827314e-06\n",
      "Epoch 12764/20000: Train Loss = 0.437585, Test Loss = 0.255901, Learning Rate = 7.824340e-06\n",
      "Epoch 12765/20000: Train Loss = 0.437576, Test Loss = 0.253695, Learning Rate = 7.821367e-06\n",
      "Epoch 12766/20000: Train Loss = 0.437450, Test Loss = 0.253236, Learning Rate = 7.818395e-06\n",
      "Epoch 12767/20000: Train Loss = 0.437406, Test Loss = 0.252414, Learning Rate = 7.815424e-06\n",
      "Epoch 12768/20000: Train Loss = 0.437395, Test Loss = 0.253478, Learning Rate = 7.812455e-06\n",
      "Epoch 12769/20000: Train Loss = 0.437278, Test Loss = 0.254122, Learning Rate = 7.809486e-06\n",
      "Epoch 12770/20000: Train Loss = 0.437887, Test Loss = 0.248410, Learning Rate = 7.806519e-06\n",
      "Epoch 12771/20000: Train Loss = 0.437797, Test Loss = 0.249104, Learning Rate = 7.803552e-06\n",
      "Epoch 12772/20000: Train Loss = 0.437315, Test Loss = 0.251726, Learning Rate = 7.800587e-06\n",
      "Epoch 12773/20000: Train Loss = 0.437150, Test Loss = 0.249009, Learning Rate = 7.797623e-06\n",
      "Epoch 12774/20000: Train Loss = 0.437071, Test Loss = 0.250469, Learning Rate = 7.794660e-06\n",
      "Epoch 12775/20000: Train Loss = 0.437206, Test Loss = 0.251506, Learning Rate = 7.791699e-06\n",
      "Epoch 12776/20000: Train Loss = 0.437148, Test Loss = 0.250439, Learning Rate = 7.788738e-06\n",
      "Epoch 12777/20000: Train Loss = 0.437446, Test Loss = 0.253499, Learning Rate = 7.785778e-06\n",
      "Epoch 12778/20000: Train Loss = 0.437463, Test Loss = 0.254106, Learning Rate = 7.782820e-06\n",
      "Epoch 12779/20000: Train Loss = 0.438265, Test Loss = 0.258051, Learning Rate = 7.779863e-06\n",
      "Epoch 12780/20000: Train Loss = 0.437851, Test Loss = 0.253136, Learning Rate = 7.776907e-06\n",
      "Epoch 12781/20000: Train Loss = 0.437454, Test Loss = 0.253406, Learning Rate = 7.773952e-06\n",
      "Epoch 12782/20000: Train Loss = 0.437119, Test Loss = 0.256460, Learning Rate = 7.770998e-06\n",
      "Epoch 12783/20000: Train Loss = 0.437357, Test Loss = 0.253907, Learning Rate = 7.768045e-06\n",
      "Epoch 12784/20000: Train Loss = 0.437081, Test Loss = 0.254720, Learning Rate = 7.765093e-06\n",
      "Epoch 12785/20000: Train Loss = 0.437190, Test Loss = 0.255366, Learning Rate = 7.762143e-06\n",
      "Epoch 12786/20000: Train Loss = 0.437132, Test Loss = 0.251385, Learning Rate = 7.759193e-06\n",
      "Epoch 12787/20000: Train Loss = 0.437260, Test Loss = 0.252129, Learning Rate = 7.756245e-06\n",
      "Epoch 12788/20000: Train Loss = 0.437682, Test Loss = 0.252722, Learning Rate = 7.753298e-06\n",
      "Epoch 12789/20000: Train Loss = 0.437289, Test Loss = 0.250423, Learning Rate = 7.750352e-06\n",
      "Epoch 12790/20000: Train Loss = 0.437069, Test Loss = 0.250296, Learning Rate = 7.747407e-06\n",
      "Epoch 12791/20000: Train Loss = 0.437232, Test Loss = 0.252924, Learning Rate = 7.744463e-06\n",
      "Epoch 12792/20000: Train Loss = 0.437274, Test Loss = 0.252577, Learning Rate = 7.741520e-06\n",
      "Epoch 12793/20000: Train Loss = 0.437213, Test Loss = 0.254994, Learning Rate = 7.738579e-06\n",
      "Epoch 12794/20000: Train Loss = 0.437383, Test Loss = 0.255616, Learning Rate = 7.735638e-06\n",
      "Epoch 12795/20000: Train Loss = 0.437152, Test Loss = 0.254250, Learning Rate = 7.732699e-06\n",
      "Epoch 12796/20000: Train Loss = 0.437033, Test Loss = 0.251130, Learning Rate = 7.729761e-06\n",
      "Epoch 12797/20000: Train Loss = 0.439283, Test Loss = 0.255124, Learning Rate = 7.726824e-06\n",
      "Epoch 12798/20000: Train Loss = 0.437834, Test Loss = 0.244610, Learning Rate = 7.723888e-06\n",
      "Epoch 12799/20000: Train Loss = 0.436841, Test Loss = 0.253327, Learning Rate = 7.720953e-06\n",
      "Epoch 12800/20000: Train Loss = 0.437529, Test Loss = 0.250952, Learning Rate = 7.718019e-06\n",
      "Epoch 12801/20000: Train Loss = 0.437356, Test Loss = 0.251192, Learning Rate = 7.715087e-06\n",
      "Epoch 12802/20000: Train Loss = 0.437101, Test Loss = 0.251913, Learning Rate = 7.712155e-06\n",
      "Epoch 12803/20000: Train Loss = 0.436927, Test Loss = 0.253436, Learning Rate = 7.709225e-06\n",
      "Epoch 12804/20000: Train Loss = 0.437174, Test Loss = 0.249394, Learning Rate = 7.706295e-06\n",
      "Epoch 12805/20000: Train Loss = 0.437032, Test Loss = 0.255904, Learning Rate = 7.703367e-06\n",
      "Epoch 12806/20000: Train Loss = 0.437839, Test Loss = 0.258836, Learning Rate = 7.700440e-06\n",
      "Epoch 12807/20000: Train Loss = 0.436731, Test Loss = 0.253407, Learning Rate = 7.697514e-06\n",
      "Epoch 12808/20000: Train Loss = 0.437388, Test Loss = 0.257387, Learning Rate = 7.694589e-06\n",
      "Epoch 12809/20000: Train Loss = 0.437390, Test Loss = 0.255624, Learning Rate = 7.691666e-06\n",
      "Epoch 12810/20000: Train Loss = 0.437204, Test Loss = 0.252997, Learning Rate = 7.688743e-06\n",
      "Epoch 12811/20000: Train Loss = 0.437068, Test Loss = 0.251945, Learning Rate = 7.685821e-06\n",
      "Epoch 12812/20000: Train Loss = 0.436937, Test Loss = 0.252641, Learning Rate = 7.682901e-06\n",
      "Epoch 12813/20000: Train Loss = 0.436875, Test Loss = 0.254283, Learning Rate = 7.679982e-06\n",
      "Epoch 12814/20000: Train Loss = 0.436817, Test Loss = 0.253334, Learning Rate = 7.677064e-06\n",
      "Epoch 12815/20000: Train Loss = 0.437484, Test Loss = 0.250430, Learning Rate = 7.674146e-06\n",
      "Epoch 12816/20000: Train Loss = 0.437006, Test Loss = 0.251139, Learning Rate = 7.671230e-06\n",
      "Epoch 12817/20000: Train Loss = 0.436940, Test Loss = 0.253226, Learning Rate = 7.668316e-06\n",
      "Epoch 12818/20000: Train Loss = 0.437324, Test Loss = 0.252936, Learning Rate = 7.665402e-06\n",
      "Epoch 12819/20000: Train Loss = 0.437112, Test Loss = 0.252059, Learning Rate = 7.662489e-06\n",
      "Epoch 12820/20000: Train Loss = 0.436877, Test Loss = 0.253615, Learning Rate = 7.659578e-06\n",
      "Epoch 12821/20000: Train Loss = 0.437137, Test Loss = 0.249935, Learning Rate = 7.656667e-06\n",
      "Epoch 12822/20000: Train Loss = 0.437115, Test Loss = 0.249294, Learning Rate = 7.653758e-06\n",
      "Epoch 12823/20000: Train Loss = 0.437455, Test Loss = 0.250033, Learning Rate = 7.650850e-06\n",
      "Epoch 12824/20000: Train Loss = 0.437179, Test Loss = 0.255816, Learning Rate = 7.647943e-06\n",
      "Epoch 12825/20000: Train Loss = 0.436903, Test Loss = 0.251720, Learning Rate = 7.645037e-06\n",
      "Epoch 12826/20000: Train Loss = 0.437263, Test Loss = 0.252211, Learning Rate = 7.642132e-06\n",
      "Epoch 12827/20000: Train Loss = 0.437138, Test Loss = 0.253855, Learning Rate = 7.639228e-06\n",
      "Epoch 12828/20000: Train Loss = 0.437592, Test Loss = 0.255655, Learning Rate = 7.636325e-06\n",
      "Epoch 12829/20000: Train Loss = 0.437315, Test Loss = 0.255677, Learning Rate = 7.633424e-06\n",
      "Epoch 12830/20000: Train Loss = 0.438579, Test Loss = 0.252927, Learning Rate = 7.630523e-06\n",
      "Epoch 12831/20000: Train Loss = 0.437217, Test Loss = 0.255492, Learning Rate = 7.627624e-06\n",
      "Epoch 12832/20000: Train Loss = 0.438348, Test Loss = 0.255280, Learning Rate = 7.624725e-06\n",
      "Epoch 12833/20000: Train Loss = 0.436980, Test Loss = 0.254313, Learning Rate = 7.621828e-06\n",
      "Epoch 12834/20000: Train Loss = 0.437099, Test Loss = 0.254595, Learning Rate = 7.618932e-06\n",
      "Epoch 12835/20000: Train Loss = 0.436935, Test Loss = 0.252366, Learning Rate = 7.616037e-06\n",
      "Epoch 12836/20000: Train Loss = 0.437323, Test Loss = 0.254574, Learning Rate = 7.613143e-06\n",
      "Epoch 12837/20000: Train Loss = 0.436944, Test Loss = 0.251041, Learning Rate = 7.610250e-06\n",
      "Epoch 12838/20000: Train Loss = 0.437838, Test Loss = 0.250856, Learning Rate = 7.607359e-06\n",
      "Epoch 12839/20000: Train Loss = 0.437157, Test Loss = 0.251185, Learning Rate = 7.604468e-06\n",
      "Epoch 12840/20000: Train Loss = 0.436992, Test Loss = 0.250377, Learning Rate = 7.601579e-06\n",
      "Epoch 12841/20000: Train Loss = 0.436936, Test Loss = 0.250664, Learning Rate = 7.598690e-06\n",
      "Epoch 12842/20000: Train Loss = 0.437072, Test Loss = 0.246443, Learning Rate = 7.595803e-06\n",
      "Epoch 12843/20000: Train Loss = 0.437442, Test Loss = 0.246585, Learning Rate = 7.592917e-06\n",
      "Epoch 12844/20000: Train Loss = 0.437294, Test Loss = 0.246962, Learning Rate = 7.590032e-06\n",
      "Epoch 12845/20000: Train Loss = 0.437513, Test Loss = 0.253267, Learning Rate = 7.587148e-06\n",
      "Epoch 12846/20000: Train Loss = 0.437288, Test Loss = 0.254004, Learning Rate = 7.584265e-06\n",
      "Epoch 12847/20000: Train Loss = 0.436876, Test Loss = 0.252780, Learning Rate = 7.581383e-06\n",
      "Epoch 12848/20000: Train Loss = 0.437272, Test Loss = 0.256139, Learning Rate = 7.578502e-06\n",
      "Epoch 12849/20000: Train Loss = 0.437142, Test Loss = 0.252625, Learning Rate = 7.575623e-06\n",
      "Epoch 12850/20000: Train Loss = 0.437575, Test Loss = 0.253113, Learning Rate = 7.572744e-06\n",
      "Epoch 12851/20000: Train Loss = 0.437024, Test Loss = 0.252808, Learning Rate = 7.569867e-06\n",
      "Epoch 12852/20000: Train Loss = 0.437120, Test Loss = 0.248651, Learning Rate = 7.566990e-06\n",
      "Epoch 12853/20000: Train Loss = 0.437007, Test Loss = 0.250301, Learning Rate = 7.564115e-06\n",
      "Epoch 12854/20000: Train Loss = 0.436849, Test Loss = 0.250367, Learning Rate = 7.561241e-06\n",
      "Epoch 12855/20000: Train Loss = 0.437439, Test Loss = 0.256907, Learning Rate = 7.558368e-06\n",
      "Epoch 12856/20000: Train Loss = 0.437106, Test Loss = 0.253630, Learning Rate = 7.555496e-06\n",
      "Epoch 12857/20000: Train Loss = 0.437522, Test Loss = 0.257121, Learning Rate = 7.552625e-06\n",
      "Epoch 12858/20000: Train Loss = 0.437014, Test Loss = 0.252747, Learning Rate = 7.549755e-06\n",
      "Epoch 12859/20000: Train Loss = 0.436936, Test Loss = 0.249879, Learning Rate = 7.546886e-06\n",
      "Epoch 12860/20000: Train Loss = 0.437570, Test Loss = 0.246171, Learning Rate = 7.544019e-06\n",
      "Epoch 12861/20000: Train Loss = 0.436986, Test Loss = 0.251266, Learning Rate = 7.541152e-06\n",
      "Epoch 12862/20000: Train Loss = 0.436818, Test Loss = 0.250043, Learning Rate = 7.538287e-06\n",
      "Epoch 12863/20000: Train Loss = 0.437114, Test Loss = 0.249709, Learning Rate = 7.535423e-06\n",
      "Epoch 12864/20000: Train Loss = 0.437610, Test Loss = 0.251679, Learning Rate = 7.532559e-06\n",
      "Epoch 12865/20000: Train Loss = 0.437038, Test Loss = 0.250608, Learning Rate = 7.529697e-06\n",
      "Epoch 12866/20000: Train Loss = 0.437047, Test Loss = 0.248556, Learning Rate = 7.526836e-06\n",
      "Epoch 12867/20000: Train Loss = 0.436946, Test Loss = 0.255677, Learning Rate = 7.523976e-06\n",
      "Epoch 12868/20000: Train Loss = 0.437233, Test Loss = 0.257735, Learning Rate = 7.521117e-06\n",
      "Epoch 12869/20000: Train Loss = 0.437041, Test Loss = 0.253235, Learning Rate = 7.518259e-06\n",
      "Epoch 12870/20000: Train Loss = 0.436884, Test Loss = 0.255504, Learning Rate = 7.515403e-06\n",
      "Epoch 12871/20000: Train Loss = 0.437389, Test Loss = 0.256055, Learning Rate = 7.512547e-06\n",
      "Epoch 12872/20000: Train Loss = 0.437272, Test Loss = 0.255283, Learning Rate = 7.509692e-06\n",
      "Epoch 12873/20000: Train Loss = 0.437054, Test Loss = 0.251386, Learning Rate = 7.506839e-06\n",
      "Epoch 12874/20000: Train Loss = 0.437412, Test Loss = 0.254148, Learning Rate = 7.503986e-06\n",
      "Epoch 12875/20000: Train Loss = 0.436976, Test Loss = 0.249534, Learning Rate = 7.501135e-06\n",
      "Epoch 12876/20000: Train Loss = 0.437099, Test Loss = 0.250635, Learning Rate = 7.498285e-06\n",
      "Epoch 12877/20000: Train Loss = 0.437153, Test Loss = 0.253492, Learning Rate = 7.495436e-06\n",
      "Epoch 12878/20000: Train Loss = 0.436840, Test Loss = 0.249554, Learning Rate = 7.492588e-06\n",
      "Epoch 12879/20000: Train Loss = 0.436834, Test Loss = 0.254115, Learning Rate = 7.489741e-06\n",
      "Epoch 12880/20000: Train Loss = 0.437111, Test Loss = 0.255558, Learning Rate = 7.486895e-06\n",
      "Epoch 12881/20000: Train Loss = 0.437318, Test Loss = 0.250356, Learning Rate = 7.484050e-06\n",
      "Epoch 12882/20000: Train Loss = 0.437055, Test Loss = 0.251511, Learning Rate = 7.481206e-06\n",
      "Epoch 12883/20000: Train Loss = 0.438608, Test Loss = 0.255053, Learning Rate = 7.478364e-06\n",
      "Epoch 12884/20000: Train Loss = 0.436738, Test Loss = 0.253105, Learning Rate = 7.475522e-06\n",
      "Epoch 12885/20000: Train Loss = 0.437026, Test Loss = 0.255303, Learning Rate = 7.472682e-06\n",
      "Epoch 12886/20000: Train Loss = 0.437251, Test Loss = 0.257513, Learning Rate = 7.469842e-06\n",
      "Epoch 12887/20000: Train Loss = 0.437273, Test Loss = 0.253236, Learning Rate = 7.467004e-06\n",
      "Epoch 12888/20000: Train Loss = 0.437904, Test Loss = 0.254070, Learning Rate = 7.464167e-06\n",
      "Epoch 12889/20000: Train Loss = 0.437012, Test Loss = 0.253984, Learning Rate = 7.461330e-06\n",
      "Epoch 12890/20000: Train Loss = 0.437316, Test Loss = 0.254727, Learning Rate = 7.458495e-06\n",
      "Epoch 12891/20000: Train Loss = 0.436798, Test Loss = 0.255254, Learning Rate = 7.455661e-06\n",
      "Epoch 12892/20000: Train Loss = 0.436905, Test Loss = 0.250119, Learning Rate = 7.452828e-06\n",
      "Epoch 12893/20000: Train Loss = 0.436850, Test Loss = 0.251887, Learning Rate = 7.449996e-06\n",
      "Epoch 12894/20000: Train Loss = 0.436947, Test Loss = 0.251002, Learning Rate = 7.447166e-06\n",
      "Epoch 12895/20000: Train Loss = 0.437057, Test Loss = 0.255529, Learning Rate = 7.444336e-06\n",
      "Epoch 12896/20000: Train Loss = 0.436894, Test Loss = 0.254921, Learning Rate = 7.441507e-06\n",
      "Epoch 12897/20000: Train Loss = 0.437807, Test Loss = 0.259830, Learning Rate = 7.438680e-06\n",
      "Epoch 12898/20000: Train Loss = 0.436911, Test Loss = 0.254456, Learning Rate = 7.435853e-06\n",
      "Epoch 12899/20000: Train Loss = 0.436873, Test Loss = 0.254485, Learning Rate = 7.433028e-06\n",
      "Epoch 12900/20000: Train Loss = 0.437597, Test Loss = 0.253985, Learning Rate = 7.430203e-06\n",
      "Epoch 12901/20000: Train Loss = 0.437440, Test Loss = 0.253023, Learning Rate = 7.427380e-06\n",
      "Epoch 12902/20000: Train Loss = 0.437009, Test Loss = 0.252496, Learning Rate = 7.424558e-06\n",
      "Epoch 12903/20000: Train Loss = 0.437922, Test Loss = 0.254087, Learning Rate = 7.421737e-06\n",
      "Epoch 12904/20000: Train Loss = 0.436944, Test Loss = 0.254899, Learning Rate = 7.418917e-06\n",
      "Epoch 12905/20000: Train Loss = 0.437217, Test Loss = 0.251839, Learning Rate = 7.416098e-06\n",
      "Epoch 12906/20000: Train Loss = 0.437005, Test Loss = 0.254334, Learning Rate = 7.413280e-06\n",
      "Epoch 12907/20000: Train Loss = 0.436830, Test Loss = 0.255433, Learning Rate = 7.410463e-06\n",
      "Epoch 12908/20000: Train Loss = 0.437116, Test Loss = 0.257572, Learning Rate = 7.407647e-06\n",
      "Epoch 12909/20000: Train Loss = 0.436876, Test Loss = 0.252703, Learning Rate = 7.404833e-06\n",
      "Epoch 12910/20000: Train Loss = 0.436871, Test Loss = 0.254013, Learning Rate = 7.402019e-06\n",
      "Epoch 12911/20000: Train Loss = 0.436936, Test Loss = 0.253090, Learning Rate = 7.399206e-06\n",
      "Epoch 12912/20000: Train Loss = 0.436902, Test Loss = 0.252811, Learning Rate = 7.396395e-06\n",
      "Epoch 12913/20000: Train Loss = 0.437891, Test Loss = 0.255867, Learning Rate = 7.393584e-06\n",
      "Epoch 12914/20000: Train Loss = 0.436949, Test Loss = 0.257459, Learning Rate = 7.390775e-06\n",
      "Epoch 12915/20000: Train Loss = 0.436812, Test Loss = 0.254665, Learning Rate = 7.387967e-06\n",
      "Epoch 12916/20000: Train Loss = 0.437010, Test Loss = 0.251363, Learning Rate = 7.385160e-06\n",
      "Epoch 12917/20000: Train Loss = 0.436798, Test Loss = 0.253736, Learning Rate = 7.382353e-06\n",
      "Epoch 12918/20000: Train Loss = 0.436993, Test Loss = 0.254549, Learning Rate = 7.379548e-06\n",
      "Epoch 12919/20000: Train Loss = 0.437563, Test Loss = 0.255926, Learning Rate = 7.376744e-06\n",
      "Epoch 12920/20000: Train Loss = 0.436727, Test Loss = 0.254202, Learning Rate = 7.373941e-06\n",
      "Epoch 12921/20000: Train Loss = 0.437076, Test Loss = 0.253538, Learning Rate = 7.371139e-06\n",
      "Epoch 12922/20000: Train Loss = 0.437353, Test Loss = 0.245715, Learning Rate = 7.368339e-06\n",
      "Epoch 12923/20000: Train Loss = 0.437094, Test Loss = 0.248211, Learning Rate = 7.365539e-06\n",
      "Epoch 12924/20000: Train Loss = 0.436818, Test Loss = 0.246754, Learning Rate = 7.362740e-06\n",
      "Epoch 12925/20000: Train Loss = 0.436937, Test Loss = 0.250780, Learning Rate = 7.359942e-06\n",
      "Epoch 12926/20000: Train Loss = 0.436817, Test Loss = 0.252803, Learning Rate = 7.357146e-06\n",
      "Epoch 12927/20000: Train Loss = 0.437426, Test Loss = 0.253080, Learning Rate = 7.354350e-06\n",
      "Epoch 12928/20000: Train Loss = 0.437652, Test Loss = 0.255971, Learning Rate = 7.351556e-06\n",
      "Epoch 12929/20000: Train Loss = 0.437355, Test Loss = 0.253122, Learning Rate = 7.348762e-06\n",
      "Epoch 12930/20000: Train Loss = 0.436634, Test Loss = 0.257702, Learning Rate = 7.345970e-06\n",
      "Epoch 12931/20000: Train Loss = 0.436876, Test Loss = 0.252206, Learning Rate = 7.343179e-06\n",
      "Epoch 12932/20000: Train Loss = 0.437050, Test Loss = 0.250650, Learning Rate = 7.340389e-06\n",
      "Epoch 12933/20000: Train Loss = 0.437400, Test Loss = 0.253818, Learning Rate = 7.337600e-06\n",
      "Epoch 12934/20000: Train Loss = 0.437256, Test Loss = 0.253752, Learning Rate = 7.334811e-06\n",
      "Epoch 12935/20000: Train Loss = 0.436900, Test Loss = 0.248332, Learning Rate = 7.332024e-06\n",
      "Epoch 12936/20000: Train Loss = 0.436867, Test Loss = 0.250986, Learning Rate = 7.329238e-06\n",
      "Epoch 12937/20000: Train Loss = 0.436708, Test Loss = 0.254418, Learning Rate = 7.326454e-06\n",
      "Epoch 12938/20000: Train Loss = 0.437130, Test Loss = 0.254439, Learning Rate = 7.323670e-06\n",
      "Epoch 12939/20000: Train Loss = 0.436836, Test Loss = 0.252352, Learning Rate = 7.320887e-06\n",
      "Epoch 12940/20000: Train Loss = 0.437097, Test Loss = 0.247862, Learning Rate = 7.318105e-06\n",
      "Epoch 12941/20000: Train Loss = 0.437066, Test Loss = 0.250225, Learning Rate = 7.315324e-06\n",
      "Epoch 12942/20000: Train Loss = 0.436992, Test Loss = 0.251738, Learning Rate = 7.312545e-06\n",
      "Epoch 12943/20000: Train Loss = 0.437172, Test Loss = 0.250232, Learning Rate = 7.309766e-06\n",
      "Epoch 12944/20000: Train Loss = 0.437000, Test Loss = 0.255844, Learning Rate = 7.306989e-06\n",
      "Epoch 12945/20000: Train Loss = 0.436805, Test Loss = 0.250606, Learning Rate = 7.304212e-06\n",
      "Epoch 12946/20000: Train Loss = 0.436651, Test Loss = 0.251531, Learning Rate = 7.301437e-06\n",
      "Epoch 12947/20000: Train Loss = 0.436907, Test Loss = 0.252409, Learning Rate = 7.298663e-06\n",
      "Epoch 12948/20000: Train Loss = 0.437245, Test Loss = 0.250458, Learning Rate = 7.295889e-06\n",
      "Epoch 12949/20000: Train Loss = 0.436848, Test Loss = 0.250270, Learning Rate = 7.293117e-06\n",
      "Epoch 12950/20000: Train Loss = 0.436817, Test Loss = 0.250779, Learning Rate = 7.290346e-06\n",
      "Epoch 12951/20000: Train Loss = 0.437037, Test Loss = 0.250547, Learning Rate = 7.287576e-06\n",
      "Epoch 12952/20000: Train Loss = 0.437240, Test Loss = 0.249754, Learning Rate = 7.284807e-06\n",
      "Epoch 12953/20000: Train Loss = 0.437304, Test Loss = 0.250719, Learning Rate = 7.282039e-06\n",
      "Epoch 12954/20000: Train Loss = 0.436863, Test Loss = 0.251297, Learning Rate = 7.279272e-06\n",
      "Epoch 12955/20000: Train Loss = 0.437118, Test Loss = 0.249470, Learning Rate = 7.276506e-06\n",
      "Epoch 12956/20000: Train Loss = 0.436708, Test Loss = 0.247242, Learning Rate = 7.273741e-06\n",
      "Epoch 12957/20000: Train Loss = 0.437766, Test Loss = 0.251322, Learning Rate = 7.270977e-06\n",
      "Epoch 12958/20000: Train Loss = 0.436834, Test Loss = 0.250475, Learning Rate = 7.268214e-06\n",
      "Epoch 12959/20000: Train Loss = 0.437323, Test Loss = 0.251177, Learning Rate = 7.265452e-06\n",
      "Epoch 12960/20000: Train Loss = 0.436995, Test Loss = 0.248244, Learning Rate = 7.262692e-06\n",
      "Epoch 12961/20000: Train Loss = 0.436839, Test Loss = 0.249806, Learning Rate = 7.259932e-06\n",
      "Epoch 12962/20000: Train Loss = 0.437576, Test Loss = 0.253465, Learning Rate = 7.257174e-06\n",
      "Epoch 12963/20000: Train Loss = 0.436993, Test Loss = 0.251639, Learning Rate = 7.254416e-06\n",
      "Epoch 12964/20000: Train Loss = 0.436830, Test Loss = 0.245796, Learning Rate = 7.251660e-06\n",
      "Epoch 12965/20000: Train Loss = 0.437232, Test Loss = 0.250640, Learning Rate = 7.248904e-06\n",
      "Epoch 12966/20000: Train Loss = 0.436972, Test Loss = 0.248281, Learning Rate = 7.246150e-06\n",
      "Epoch 12967/20000: Train Loss = 0.436979, Test Loss = 0.247752, Learning Rate = 7.243396e-06\n",
      "Epoch 12968/20000: Train Loss = 0.437065, Test Loss = 0.249697, Learning Rate = 7.240644e-06\n",
      "Epoch 12969/20000: Train Loss = 0.437502, Test Loss = 0.246291, Learning Rate = 7.237893e-06\n",
      "Epoch 12970/20000: Train Loss = 0.437224, Test Loss = 0.253118, Learning Rate = 7.235143e-06\n",
      "Epoch 12971/20000: Train Loss = 0.437250, Test Loss = 0.254437, Learning Rate = 7.232394e-06\n",
      "Epoch 12972/20000: Train Loss = 0.437273, Test Loss = 0.250277, Learning Rate = 7.229645e-06\n",
      "Epoch 12973/20000: Train Loss = 0.436820, Test Loss = 0.256703, Learning Rate = 7.226898e-06\n",
      "Epoch 12974/20000: Train Loss = 0.436738, Test Loss = 0.250729, Learning Rate = 7.224152e-06\n",
      "Epoch 12975/20000: Train Loss = 0.436809, Test Loss = 0.249039, Learning Rate = 7.221407e-06\n",
      "Epoch 12976/20000: Train Loss = 0.436701, Test Loss = 0.250043, Learning Rate = 7.218663e-06\n",
      "Epoch 12977/20000: Train Loss = 0.437141, Test Loss = 0.250931, Learning Rate = 7.215920e-06\n",
      "Epoch 12978/20000: Train Loss = 0.436951, Test Loss = 0.253231, Learning Rate = 7.213179e-06\n",
      "Epoch 12979/20000: Train Loss = 0.436684, Test Loss = 0.251953, Learning Rate = 7.210438e-06\n",
      "Epoch 12980/20000: Train Loss = 0.437042, Test Loss = 0.250570, Learning Rate = 7.207698e-06\n",
      "Epoch 12981/20000: Train Loss = 0.437654, Test Loss = 0.254379, Learning Rate = 7.204959e-06\n",
      "Epoch 12982/20000: Train Loss = 0.436675, Test Loss = 0.247831, Learning Rate = 7.202222e-06\n",
      "Epoch 12983/20000: Train Loss = 0.436815, Test Loss = 0.253385, Learning Rate = 7.199485e-06\n",
      "Epoch 12984/20000: Train Loss = 0.437142, Test Loss = 0.252406, Learning Rate = 7.196749e-06\n",
      "Epoch 12985/20000: Train Loss = 0.436705, Test Loss = 0.250387, Learning Rate = 7.194015e-06\n",
      "Epoch 12986/20000: Train Loss = 0.437044, Test Loss = 0.250028, Learning Rate = 7.191281e-06\n",
      "Epoch 12987/20000: Train Loss = 0.437009, Test Loss = 0.247515, Learning Rate = 7.188549e-06\n",
      "Epoch 12988/20000: Train Loss = 0.436716, Test Loss = 0.248569, Learning Rate = 7.185817e-06\n",
      "Epoch 12989/20000: Train Loss = 0.437008, Test Loss = 0.251651, Learning Rate = 7.183087e-06\n",
      "Epoch 12990/20000: Train Loss = 0.436863, Test Loss = 0.252456, Learning Rate = 7.180358e-06\n",
      "Epoch 12991/20000: Train Loss = 0.437013, Test Loss = 0.250822, Learning Rate = 7.177629e-06\n",
      "Epoch 12992/20000: Train Loss = 0.437136, Test Loss = 0.248483, Learning Rate = 7.174902e-06\n",
      "Epoch 12993/20000: Train Loss = 0.436613, Test Loss = 0.250386, Learning Rate = 7.172176e-06\n",
      "Epoch 12994/20000: Train Loss = 0.436986, Test Loss = 0.248287, Learning Rate = 7.169450e-06\n",
      "Epoch 12995/20000: Train Loss = 0.436799, Test Loss = 0.256290, Learning Rate = 7.166726e-06\n",
      "Epoch 12996/20000: Train Loss = 0.437071, Test Loss = 0.247936, Learning Rate = 7.164003e-06\n",
      "Epoch 12997/20000: Train Loss = 0.437184, Test Loss = 0.248252, Learning Rate = 7.161281e-06\n",
      "Epoch 12998/20000: Train Loss = 0.436955, Test Loss = 0.249020, Learning Rate = 7.158560e-06\n",
      "Epoch 12999/20000: Train Loss = 0.436936, Test Loss = 0.253447, Learning Rate = 7.155840e-06\n",
      "Epoch 13000/20000: Train Loss = 0.436609, Test Loss = 0.254927, Learning Rate = 7.153121e-06\n",
      "Epoch 13001/20000: Train Loss = 0.436925, Test Loss = 0.253809, Learning Rate = 7.150403e-06\n",
      "Epoch 13002/20000: Train Loss = 0.436956, Test Loss = 0.255971, Learning Rate = 7.147686e-06\n",
      "Epoch 13003/20000: Train Loss = 0.436975, Test Loss = 0.252912, Learning Rate = 7.144970e-06\n",
      "Epoch 13004/20000: Train Loss = 0.436995, Test Loss = 0.250339, Learning Rate = 7.142255e-06\n",
      "Epoch 13005/20000: Train Loss = 0.436962, Test Loss = 0.250635, Learning Rate = 7.139541e-06\n",
      "Epoch 13006/20000: Train Loss = 0.437306, Test Loss = 0.255477, Learning Rate = 7.136828e-06\n",
      "Epoch 13007/20000: Train Loss = 0.437469, Test Loss = 0.246018, Learning Rate = 7.134116e-06\n",
      "Epoch 13008/20000: Train Loss = 0.436992, Test Loss = 0.245649, Learning Rate = 7.131406e-06\n",
      "Epoch 13009/20000: Train Loss = 0.437014, Test Loss = 0.247068, Learning Rate = 7.128696e-06\n",
      "Epoch 13010/20000: Train Loss = 0.437744, Test Loss = 0.255973, Learning Rate = 7.125987e-06\n",
      "Epoch 13011/20000: Train Loss = 0.437314, Test Loss = 0.253644, Learning Rate = 7.123280e-06\n",
      "Epoch 13012/20000: Train Loss = 0.436885, Test Loss = 0.253880, Learning Rate = 7.120573e-06\n",
      "Epoch 13013/20000: Train Loss = 0.436697, Test Loss = 0.252975, Learning Rate = 7.117867e-06\n",
      "Epoch 13014/20000: Train Loss = 0.437117, Test Loss = 0.251886, Learning Rate = 7.115163e-06\n",
      "Epoch 13015/20000: Train Loss = 0.437283, Test Loss = 0.256792, Learning Rate = 7.112459e-06\n",
      "Epoch 13016/20000: Train Loss = 0.436787, Test Loss = 0.252035, Learning Rate = 7.109757e-06\n",
      "Epoch 13017/20000: Train Loss = 0.437206, Test Loss = 0.254705, Learning Rate = 7.107055e-06\n",
      "Epoch 13018/20000: Train Loss = 0.439125, Test Loss = 0.253862, Learning Rate = 7.104355e-06\n",
      "Epoch 13019/20000: Train Loss = 0.436979, Test Loss = 0.247392, Learning Rate = 7.101655e-06\n",
      "Epoch 13020/20000: Train Loss = 0.437965, Test Loss = 0.255599, Learning Rate = 7.098957e-06\n",
      "Epoch 13021/20000: Train Loss = 0.437135, Test Loss = 0.249046, Learning Rate = 7.096259e-06\n",
      "Epoch 13022/20000: Train Loss = 0.437544, Test Loss = 0.249229, Learning Rate = 7.093563e-06\n",
      "Epoch 13023/20000: Train Loss = 0.437061, Test Loss = 0.252744, Learning Rate = 7.090867e-06\n",
      "Epoch 13024/20000: Train Loss = 0.437685, Test Loss = 0.255194, Learning Rate = 7.088173e-06\n",
      "Epoch 13025/20000: Train Loss = 0.436902, Test Loss = 0.250952, Learning Rate = 7.085480e-06\n",
      "Epoch 13026/20000: Train Loss = 0.436793, Test Loss = 0.252111, Learning Rate = 7.082788e-06\n",
      "Epoch 13027/20000: Train Loss = 0.436566, Test Loss = 0.258447, Learning Rate = 7.080096e-06\n",
      "Epoch 13028/20000: Train Loss = 0.437811, Test Loss = 0.252930, Learning Rate = 7.077406e-06\n",
      "Epoch 13029/20000: Train Loss = 0.436849, Test Loss = 0.254080, Learning Rate = 7.074717e-06\n",
      "Epoch 13030/20000: Train Loss = 0.436883, Test Loss = 0.248441, Learning Rate = 7.072029e-06\n",
      "Epoch 13031/20000: Train Loss = 0.438130, Test Loss = 0.252142, Learning Rate = 7.069341e-06\n",
      "Epoch 13032/20000: Train Loss = 0.436591, Test Loss = 0.246768, Learning Rate = 7.066655e-06\n",
      "Epoch 13033/20000: Train Loss = 0.436887, Test Loss = 0.251120, Learning Rate = 7.063970e-06\n",
      "Epoch 13034/20000: Train Loss = 0.436860, Test Loss = 0.247315, Learning Rate = 7.061286e-06\n",
      "Epoch 13035/20000: Train Loss = 0.436806, Test Loss = 0.252010, Learning Rate = 7.058603e-06\n",
      "Epoch 13036/20000: Train Loss = 0.437180, Test Loss = 0.250633, Learning Rate = 7.055921e-06\n",
      "Epoch 13037/20000: Train Loss = 0.436702, Test Loss = 0.248621, Learning Rate = 7.053240e-06\n",
      "Epoch 13038/20000: Train Loss = 0.436640, Test Loss = 0.249167, Learning Rate = 7.050560e-06\n",
      "Epoch 13039/20000: Train Loss = 0.436807, Test Loss = 0.254950, Learning Rate = 7.047881e-06\n",
      "Epoch 13040/20000: Train Loss = 0.436959, Test Loss = 0.251453, Learning Rate = 7.045203e-06\n",
      "Epoch 13041/20000: Train Loss = 0.436513, Test Loss = 0.254532, Learning Rate = 7.042526e-06\n",
      "Epoch 13042/20000: Train Loss = 0.436915, Test Loss = 0.256103, Learning Rate = 7.039850e-06\n",
      "Epoch 13043/20000: Train Loss = 0.436654, Test Loss = 0.255770, Learning Rate = 7.037175e-06\n",
      "Epoch 13044/20000: Train Loss = 0.436786, Test Loss = 0.256865, Learning Rate = 7.034501e-06\n",
      "Epoch 13045/20000: Train Loss = 0.437044, Test Loss = 0.254438, Learning Rate = 7.031828e-06\n",
      "Epoch 13046/20000: Train Loss = 0.437113, Test Loss = 0.250179, Learning Rate = 7.029156e-06\n",
      "Epoch 13047/20000: Train Loss = 0.436952, Test Loss = 0.254451, Learning Rate = 7.026485e-06\n",
      "Epoch 13048/20000: Train Loss = 0.436545, Test Loss = 0.253918, Learning Rate = 7.023815e-06\n",
      "Epoch 13049/20000: Train Loss = 0.436794, Test Loss = 0.251644, Learning Rate = 7.021146e-06\n",
      "Epoch 13050/20000: Train Loss = 0.439379, Test Loss = 0.256751, Learning Rate = 7.018479e-06\n",
      "Epoch 13051/20000: Train Loss = 0.437927, Test Loss = 0.253737, Learning Rate = 7.015812e-06\n",
      "Epoch 13052/20000: Train Loss = 0.437093, Test Loss = 0.249385, Learning Rate = 7.013146e-06\n",
      "Epoch 13053/20000: Train Loss = 0.437215, Test Loss = 0.250091, Learning Rate = 7.010481e-06\n",
      "Epoch 13054/20000: Train Loss = 0.436646, Test Loss = 0.253356, Learning Rate = 7.007817e-06\n",
      "Epoch 13055/20000: Train Loss = 0.436938, Test Loss = 0.255046, Learning Rate = 7.005155e-06\n",
      "Epoch 13056/20000: Train Loss = 0.436979, Test Loss = 0.254010, Learning Rate = 7.002493e-06\n",
      "Epoch 13057/20000: Train Loss = 0.436641, Test Loss = 0.252547, Learning Rate = 6.999832e-06\n",
      "Epoch 13058/20000: Train Loss = 0.437080, Test Loss = 0.249026, Learning Rate = 6.997172e-06\n",
      "Epoch 13059/20000: Train Loss = 0.436701, Test Loss = 0.251020, Learning Rate = 6.994514e-06\n",
      "Epoch 13060/20000: Train Loss = 0.436823, Test Loss = 0.252640, Learning Rate = 6.991856e-06\n",
      "Epoch 13061/20000: Train Loss = 0.436536, Test Loss = 0.248731, Learning Rate = 6.989199e-06\n",
      "Epoch 13062/20000: Train Loss = 0.437016, Test Loss = 0.251934, Learning Rate = 6.986543e-06\n",
      "Epoch 13063/20000: Train Loss = 0.437439, Test Loss = 0.251956, Learning Rate = 6.983889e-06\n",
      "Epoch 13064/20000: Train Loss = 0.436750, Test Loss = 0.251016, Learning Rate = 6.981235e-06\n",
      "Epoch 13065/20000: Train Loss = 0.436918, Test Loss = 0.253034, Learning Rate = 6.978582e-06\n",
      "Epoch 13066/20000: Train Loss = 0.437031, Test Loss = 0.246174, Learning Rate = 6.975931e-06\n",
      "Epoch 13067/20000: Train Loss = 0.436733, Test Loss = 0.255197, Learning Rate = 6.973280e-06\n",
      "Epoch 13068/20000: Train Loss = 0.436834, Test Loss = 0.253172, Learning Rate = 6.970630e-06\n",
      "Epoch 13069/20000: Train Loss = 0.436976, Test Loss = 0.249225, Learning Rate = 6.967982e-06\n",
      "Epoch 13070/20000: Train Loss = 0.436933, Test Loss = 0.250123, Learning Rate = 6.965334e-06\n",
      "Epoch 13071/20000: Train Loss = 0.436511, Test Loss = 0.250151, Learning Rate = 6.962687e-06\n",
      "Epoch 13072/20000: Train Loss = 0.436818, Test Loss = 0.250003, Learning Rate = 6.960042e-06\n",
      "Epoch 13073/20000: Train Loss = 0.436707, Test Loss = 0.251593, Learning Rate = 6.957397e-06\n",
      "Epoch 13074/20000: Train Loss = 0.436973, Test Loss = 0.253179, Learning Rate = 6.954754e-06\n",
      "Epoch 13075/20000: Train Loss = 0.437090, Test Loss = 0.249334, Learning Rate = 6.952111e-06\n",
      "Epoch 13076/20000: Train Loss = 0.436739, Test Loss = 0.256426, Learning Rate = 6.949469e-06\n",
      "Epoch 13077/20000: Train Loss = 0.437631, Test Loss = 0.255702, Learning Rate = 6.946829e-06\n",
      "Epoch 13078/20000: Train Loss = 0.436620, Test Loss = 0.251037, Learning Rate = 6.944189e-06\n",
      "Epoch 13079/20000: Train Loss = 0.436578, Test Loss = 0.251971, Learning Rate = 6.941550e-06\n",
      "Epoch 13080/20000: Train Loss = 0.436620, Test Loss = 0.254334, Learning Rate = 6.938913e-06\n",
      "Epoch 13081/20000: Train Loss = 0.436584, Test Loss = 0.253710, Learning Rate = 6.936276e-06\n",
      "Epoch 13082/20000: Train Loss = 0.437091, Test Loss = 0.253223, Learning Rate = 6.933641e-06\n",
      "Epoch 13083/20000: Train Loss = 0.436903, Test Loss = 0.255958, Learning Rate = 6.931006e-06\n",
      "Epoch 13084/20000: Train Loss = 0.436640, Test Loss = 0.250751, Learning Rate = 6.928372e-06\n",
      "Epoch 13085/20000: Train Loss = 0.437076, Test Loss = 0.253570, Learning Rate = 6.925740e-06\n",
      "Epoch 13086/20000: Train Loss = 0.436586, Test Loss = 0.253230, Learning Rate = 6.923108e-06\n",
      "Epoch 13087/20000: Train Loss = 0.437311, Test Loss = 0.258574, Learning Rate = 6.920478e-06\n",
      "Epoch 13088/20000: Train Loss = 0.436847, Test Loss = 0.256846, Learning Rate = 6.917848e-06\n",
      "Epoch 13089/20000: Train Loss = 0.436529, Test Loss = 0.251928, Learning Rate = 6.915220e-06\n",
      "Epoch 13090/20000: Train Loss = 0.436496, Test Loss = 0.255379, Learning Rate = 6.912592e-06\n",
      "Epoch 13091/20000: Train Loss = 0.436892, Test Loss = 0.252208, Learning Rate = 6.909965e-06\n",
      "Epoch 13092/20000: Train Loss = 0.436916, Test Loss = 0.251286, Learning Rate = 6.907340e-06\n",
      "Epoch 13093/20000: Train Loss = 0.436656, Test Loss = 0.255307, Learning Rate = 6.904715e-06\n",
      "Epoch 13094/20000: Train Loss = 0.436548, Test Loss = 0.251894, Learning Rate = 6.902092e-06\n",
      "Epoch 13095/20000: Train Loss = 0.436413, Test Loss = 0.253106, Learning Rate = 6.899469e-06\n",
      "Epoch 13096/20000: Train Loss = 0.436763, Test Loss = 0.252467, Learning Rate = 6.896847e-06\n",
      "Epoch 13097/20000: Train Loss = 0.436501, Test Loss = 0.252541, Learning Rate = 6.894227e-06\n",
      "Epoch 13098/20000: Train Loss = 0.436925, Test Loss = 0.257155, Learning Rate = 6.891607e-06\n",
      "Epoch 13099/20000: Train Loss = 0.436585, Test Loss = 0.252679, Learning Rate = 6.888988e-06\n",
      "Epoch 13100/20000: Train Loss = 0.436588, Test Loss = 0.254162, Learning Rate = 6.886371e-06\n",
      "Epoch 13101/20000: Train Loss = 0.436638, Test Loss = 0.252628, Learning Rate = 6.883754e-06\n",
      "Epoch 13102/20000: Train Loss = 0.436685, Test Loss = 0.252702, Learning Rate = 6.881139e-06\n",
      "Epoch 13103/20000: Train Loss = 0.436626, Test Loss = 0.253104, Learning Rate = 6.878524e-06\n",
      "Epoch 13104/20000: Train Loss = 0.437185, Test Loss = 0.254233, Learning Rate = 6.875910e-06\n",
      "Epoch 13105/20000: Train Loss = 0.436738, Test Loss = 0.251667, Learning Rate = 6.873298e-06\n",
      "Epoch 13106/20000: Train Loss = 0.436788, Test Loss = 0.250764, Learning Rate = 6.870686e-06\n",
      "Epoch 13107/20000: Train Loss = 0.436495, Test Loss = 0.251426, Learning Rate = 6.868075e-06\n",
      "Epoch 13108/20000: Train Loss = 0.436515, Test Loss = 0.252527, Learning Rate = 6.865466e-06\n",
      "Epoch 13109/20000: Train Loss = 0.436561, Test Loss = 0.252187, Learning Rate = 6.862857e-06\n",
      "Epoch 13110/20000: Train Loss = 0.436453, Test Loss = 0.252154, Learning Rate = 6.860249e-06\n",
      "Epoch 13111/20000: Train Loss = 0.436515, Test Loss = 0.254533, Learning Rate = 6.857642e-06\n",
      "Epoch 13112/20000: Train Loss = 0.436882, Test Loss = 0.252395, Learning Rate = 6.855037e-06\n",
      "Epoch 13113/20000: Train Loss = 0.436850, Test Loss = 0.248905, Learning Rate = 6.852432e-06\n",
      "Epoch 13114/20000: Train Loss = 0.436405, Test Loss = 0.253161, Learning Rate = 6.849828e-06\n",
      "Epoch 13115/20000: Train Loss = 0.436529, Test Loss = 0.253030, Learning Rate = 6.847226e-06\n",
      "Epoch 13116/20000: Train Loss = 0.436655, Test Loss = 0.252951, Learning Rate = 6.844624e-06\n",
      "Epoch 13117/20000: Train Loss = 0.436802, Test Loss = 0.250176, Learning Rate = 6.842023e-06\n",
      "Epoch 13118/20000: Train Loss = 0.436671, Test Loss = 0.250355, Learning Rate = 6.839423e-06\n",
      "Epoch 13119/20000: Train Loss = 0.436584, Test Loss = 0.251806, Learning Rate = 6.836824e-06\n",
      "Epoch 13120/20000: Train Loss = 0.436511, Test Loss = 0.249622, Learning Rate = 6.834227e-06\n",
      "Epoch 13121/20000: Train Loss = 0.436681, Test Loss = 0.252273, Learning Rate = 6.831630e-06\n",
      "Epoch 13122/20000: Train Loss = 0.436649, Test Loss = 0.248113, Learning Rate = 6.829034e-06\n",
      "Epoch 13123/20000: Train Loss = 0.436993, Test Loss = 0.253410, Learning Rate = 6.826439e-06\n",
      "Epoch 13124/20000: Train Loss = 0.436793, Test Loss = 0.250782, Learning Rate = 6.823845e-06\n",
      "Epoch 13125/20000: Train Loss = 0.436534, Test Loss = 0.248433, Learning Rate = 6.821252e-06\n",
      "Epoch 13126/20000: Train Loss = 0.436645, Test Loss = 0.251397, Learning Rate = 6.818660e-06\n",
      "Epoch 13127/20000: Train Loss = 0.437019, Test Loss = 0.253695, Learning Rate = 6.816070e-06\n",
      "Epoch 13128/20000: Train Loss = 0.437349, Test Loss = 0.252260, Learning Rate = 6.813480e-06\n",
      "Epoch 13129/20000: Train Loss = 0.437087, Test Loss = 0.252336, Learning Rate = 6.810891e-06\n",
      "Epoch 13130/20000: Train Loss = 0.436782, Test Loss = 0.248555, Learning Rate = 6.808303e-06\n",
      "Epoch 13131/20000: Train Loss = 0.436796, Test Loss = 0.251865, Learning Rate = 6.805716e-06\n",
      "Epoch 13132/20000: Train Loss = 0.436880, Test Loss = 0.252188, Learning Rate = 6.803130e-06\n",
      "Epoch 13133/20000: Train Loss = 0.436970, Test Loss = 0.251756, Learning Rate = 6.800545e-06\n",
      "Epoch 13134/20000: Train Loss = 0.436474, Test Loss = 0.252262, Learning Rate = 6.797961e-06\n",
      "Epoch 13135/20000: Train Loss = 0.436591, Test Loss = 0.253569, Learning Rate = 6.795378e-06\n",
      "Epoch 13136/20000: Train Loss = 0.437119, Test Loss = 0.250903, Learning Rate = 6.792796e-06\n",
      "Epoch 13137/20000: Train Loss = 0.436621, Test Loss = 0.253732, Learning Rate = 6.790215e-06\n",
      "Epoch 13138/20000: Train Loss = 0.436843, Test Loss = 0.251223, Learning Rate = 6.787634e-06\n",
      "Epoch 13139/20000: Train Loss = 0.436755, Test Loss = 0.253698, Learning Rate = 6.785055e-06\n",
      "Epoch 13140/20000: Train Loss = 0.436804, Test Loss = 0.255961, Learning Rate = 6.782477e-06\n",
      "Epoch 13141/20000: Train Loss = 0.436954, Test Loss = 0.254555, Learning Rate = 6.779900e-06\n",
      "Epoch 13142/20000: Train Loss = 0.436630, Test Loss = 0.253380, Learning Rate = 6.777324e-06\n",
      "Epoch 13143/20000: Train Loss = 0.436444, Test Loss = 0.255684, Learning Rate = 6.774749e-06\n",
      "Epoch 13144/20000: Train Loss = 0.436600, Test Loss = 0.251879, Learning Rate = 6.772174e-06\n",
      "Epoch 13145/20000: Train Loss = 0.437191, Test Loss = 0.249065, Learning Rate = 6.769601e-06\n",
      "Epoch 13146/20000: Train Loss = 0.436320, Test Loss = 0.254392, Learning Rate = 6.767029e-06\n",
      "Epoch 13147/20000: Train Loss = 0.436509, Test Loss = 0.253823, Learning Rate = 6.764458e-06\n",
      "Epoch 13148/20000: Train Loss = 0.436862, Test Loss = 0.247697, Learning Rate = 6.761887e-06\n",
      "Epoch 13149/20000: Train Loss = 0.436971, Test Loss = 0.252368, Learning Rate = 6.759318e-06\n",
      "Epoch 13150/20000: Train Loss = 0.436525, Test Loss = 0.252945, Learning Rate = 6.756750e-06\n",
      "Epoch 13151/20000: Train Loss = 0.436465, Test Loss = 0.248253, Learning Rate = 6.754182e-06\n",
      "Epoch 13152/20000: Train Loss = 0.436612, Test Loss = 0.251403, Learning Rate = 6.751616e-06\n",
      "Epoch 13153/20000: Train Loss = 0.436494, Test Loss = 0.249200, Learning Rate = 6.749050e-06\n",
      "Epoch 13154/20000: Train Loss = 0.436499, Test Loss = 0.247607, Learning Rate = 6.746486e-06\n",
      "Epoch 13155/20000: Train Loss = 0.436731, Test Loss = 0.251775, Learning Rate = 6.743923e-06\n",
      "Epoch 13156/20000: Train Loss = 0.436597, Test Loss = 0.245576, Learning Rate = 6.741360e-06\n",
      "Epoch 13157/20000: Train Loss = 0.436518, Test Loss = 0.252371, Learning Rate = 6.738798e-06\n",
      "Epoch 13158/20000: Train Loss = 0.436461, Test Loss = 0.254091, Learning Rate = 6.736238e-06\n",
      "Epoch 13159/20000: Train Loss = 0.437196, Test Loss = 0.255954, Learning Rate = 6.733678e-06\n",
      "Epoch 13160/20000: Train Loss = 0.436618, Test Loss = 0.253725, Learning Rate = 6.731120e-06\n",
      "Epoch 13161/20000: Train Loss = 0.436452, Test Loss = 0.257292, Learning Rate = 6.728562e-06\n",
      "Epoch 13162/20000: Train Loss = 0.437348, Test Loss = 0.255418, Learning Rate = 6.726005e-06\n",
      "Epoch 13163/20000: Train Loss = 0.436723, Test Loss = 0.250148, Learning Rate = 6.723450e-06\n",
      "Epoch 13164/20000: Train Loss = 0.436646, Test Loss = 0.251510, Learning Rate = 6.720895e-06\n",
      "Epoch 13165/20000: Train Loss = 0.437076, Test Loss = 0.251967, Learning Rate = 6.718341e-06\n",
      "Epoch 13166/20000: Train Loss = 0.437260, Test Loss = 0.253555, Learning Rate = 6.715788e-06\n",
      "Epoch 13167/20000: Train Loss = 0.436595, Test Loss = 0.248754, Learning Rate = 6.713237e-06\n",
      "Epoch 13168/20000: Train Loss = 0.436573, Test Loss = 0.249621, Learning Rate = 6.710686e-06\n",
      "Epoch 13169/20000: Train Loss = 0.436558, Test Loss = 0.253588, Learning Rate = 6.708136e-06\n",
      "Epoch 13170/20000: Train Loss = 0.436723, Test Loss = 0.251654, Learning Rate = 6.705587e-06\n",
      "Epoch 13171/20000: Train Loss = 0.436813, Test Loss = 0.251647, Learning Rate = 6.703039e-06\n",
      "Epoch 13172/20000: Train Loss = 0.436720, Test Loss = 0.256109, Learning Rate = 6.700492e-06\n",
      "Epoch 13173/20000: Train Loss = 0.436388, Test Loss = 0.252293, Learning Rate = 6.697946e-06\n",
      "Epoch 13174/20000: Train Loss = 0.437022, Test Loss = 0.253486, Learning Rate = 6.695401e-06\n",
      "Epoch 13175/20000: Train Loss = 0.436495, Test Loss = 0.250579, Learning Rate = 6.692857e-06\n",
      "Epoch 13176/20000: Train Loss = 0.436412, Test Loss = 0.251805, Learning Rate = 6.690314e-06\n",
      "Epoch 13177/20000: Train Loss = 0.436432, Test Loss = 0.249888, Learning Rate = 6.687772e-06\n",
      "Epoch 13178/20000: Train Loss = 0.436767, Test Loss = 0.248045, Learning Rate = 6.685231e-06\n",
      "Epoch 13179/20000: Train Loss = 0.436762, Test Loss = 0.252291, Learning Rate = 6.682690e-06\n",
      "Epoch 13180/20000: Train Loss = 0.436713, Test Loss = 0.249667, Learning Rate = 6.680151e-06\n",
      "Epoch 13181/20000: Train Loss = 0.436273, Test Loss = 0.255324, Learning Rate = 6.677613e-06\n",
      "Epoch 13182/20000: Train Loss = 0.436657, Test Loss = 0.255759, Learning Rate = 6.675075e-06\n",
      "Epoch 13183/20000: Train Loss = 0.436427, Test Loss = 0.248202, Learning Rate = 6.672539e-06\n",
      "Epoch 13184/20000: Train Loss = 0.436483, Test Loss = 0.251330, Learning Rate = 6.670004e-06\n",
      "Epoch 13185/20000: Train Loss = 0.437588, Test Loss = 0.251100, Learning Rate = 6.667469e-06\n",
      "Epoch 13186/20000: Train Loss = 0.437992, Test Loss = 0.254100, Learning Rate = 6.664936e-06\n",
      "Epoch 13187/20000: Train Loss = 0.437258, Test Loss = 0.250831, Learning Rate = 6.662403e-06\n",
      "Epoch 13188/20000: Train Loss = 0.438199, Test Loss = 0.253388, Learning Rate = 6.659872e-06\n",
      "Epoch 13189/20000: Train Loss = 0.436662, Test Loss = 0.247501, Learning Rate = 6.657341e-06\n",
      "Epoch 13190/20000: Train Loss = 0.436839, Test Loss = 0.247815, Learning Rate = 6.654812e-06\n",
      "Epoch 13191/20000: Train Loss = 0.436800, Test Loss = 0.250080, Learning Rate = 6.652283e-06\n",
      "Epoch 13192/20000: Train Loss = 0.436441, Test Loss = 0.250712, Learning Rate = 6.649755e-06\n",
      "Epoch 13193/20000: Train Loss = 0.436658, Test Loss = 0.250855, Learning Rate = 6.647229e-06\n",
      "Epoch 13194/20000: Train Loss = 0.437145, Test Loss = 0.252628, Learning Rate = 6.644703e-06\n",
      "Epoch 13195/20000: Train Loss = 0.436783, Test Loss = 0.250473, Learning Rate = 6.642178e-06\n",
      "Epoch 13196/20000: Train Loss = 0.436476, Test Loss = 0.255706, Learning Rate = 6.639654e-06\n",
      "Epoch 13197/20000: Train Loss = 0.436640, Test Loss = 0.252496, Learning Rate = 6.637131e-06\n",
      "Epoch 13198/20000: Train Loss = 0.437012, Test Loss = 0.255516, Learning Rate = 6.634609e-06\n",
      "Epoch 13199/20000: Train Loss = 0.436921, Test Loss = 0.256283, Learning Rate = 6.632088e-06\n",
      "Epoch 13200/20000: Train Loss = 0.436567, Test Loss = 0.254612, Learning Rate = 6.629568e-06\n",
      "Epoch 13201/20000: Train Loss = 0.436560, Test Loss = 0.254040, Learning Rate = 6.627049e-06\n",
      "Epoch 13202/20000: Train Loss = 0.436260, Test Loss = 0.253675, Learning Rate = 6.624531e-06\n",
      "Epoch 13203/20000: Train Loss = 0.436770, Test Loss = 0.253826, Learning Rate = 6.622014e-06\n",
      "Epoch 13204/20000: Train Loss = 0.436864, Test Loss = 0.253620, Learning Rate = 6.619498e-06\n",
      "Epoch 13205/20000: Train Loss = 0.436945, Test Loss = 0.255098, Learning Rate = 6.616983e-06\n",
      "Epoch 13206/20000: Train Loss = 0.436709, Test Loss = 0.247605, Learning Rate = 6.614468e-06\n",
      "Epoch 13207/20000: Train Loss = 0.436895, Test Loss = 0.247475, Learning Rate = 6.611955e-06\n",
      "Epoch 13208/20000: Train Loss = 0.436397, Test Loss = 0.250904, Learning Rate = 6.609443e-06\n",
      "Epoch 13209/20000: Train Loss = 0.436242, Test Loss = 0.251606, Learning Rate = 6.606931e-06\n",
      "Epoch 13210/20000: Train Loss = 0.436493, Test Loss = 0.251474, Learning Rate = 6.604421e-06\n",
      "Epoch 13211/20000: Train Loss = 0.436589, Test Loss = 0.249387, Learning Rate = 6.601911e-06\n",
      "Epoch 13212/20000: Train Loss = 0.436616, Test Loss = 0.250916, Learning Rate = 6.599403e-06\n",
      "Epoch 13213/20000: Train Loss = 0.436352, Test Loss = 0.251325, Learning Rate = 6.596895e-06\n",
      "Epoch 13214/20000: Train Loss = 0.436508, Test Loss = 0.255438, Learning Rate = 6.594389e-06\n",
      "Epoch 13215/20000: Train Loss = 0.436630, Test Loss = 0.252552, Learning Rate = 6.591883e-06\n",
      "Epoch 13216/20000: Train Loss = 0.436731, Test Loss = 0.253041, Learning Rate = 6.589378e-06\n",
      "Epoch 13217/20000: Train Loss = 0.436624, Test Loss = 0.251944, Learning Rate = 6.586874e-06\n",
      "Epoch 13218/20000: Train Loss = 0.436447, Test Loss = 0.249273, Learning Rate = 6.584372e-06\n",
      "Epoch 13219/20000: Train Loss = 0.436636, Test Loss = 0.249579, Learning Rate = 6.581870e-06\n",
      "Epoch 13220/20000: Train Loss = 0.436365, Test Loss = 0.251570, Learning Rate = 6.579369e-06\n",
      "Epoch 13221/20000: Train Loss = 0.436521, Test Loss = 0.250194, Learning Rate = 6.576869e-06\n",
      "Epoch 13222/20000: Train Loss = 0.436184, Test Loss = 0.249117, Learning Rate = 6.574370e-06\n",
      "Epoch 13223/20000: Train Loss = 0.436089, Test Loss = 0.256591, Learning Rate = 6.571872e-06\n",
      "Epoch 13224/20000: Train Loss = 0.437893, Test Loss = 0.257305, Learning Rate = 6.569374e-06\n",
      "Epoch 13225/20000: Train Loss = 0.435982, Test Loss = 0.251197, Learning Rate = 6.566878e-06\n",
      "Epoch 13226/20000: Train Loss = 0.437705, Test Loss = 0.259040, Learning Rate = 6.564383e-06\n",
      "Epoch 13227/20000: Train Loss = 0.437138, Test Loss = 0.255123, Learning Rate = 6.561889e-06\n",
      "Epoch 13228/20000: Train Loss = 0.436577, Test Loss = 0.255858, Learning Rate = 6.559395e-06\n",
      "Epoch 13229/20000: Train Loss = 0.436536, Test Loss = 0.251934, Learning Rate = 6.556903e-06\n",
      "Epoch 13230/20000: Train Loss = 0.436875, Test Loss = 0.256471, Learning Rate = 6.554412e-06\n",
      "Epoch 13231/20000: Train Loss = 0.436866, Test Loss = 0.253863, Learning Rate = 6.551921e-06\n",
      "Epoch 13232/20000: Train Loss = 0.436337, Test Loss = 0.253743, Learning Rate = 6.549432e-06\n",
      "Epoch 13233/20000: Train Loss = 0.436419, Test Loss = 0.252535, Learning Rate = 6.546943e-06\n",
      "Epoch 13234/20000: Train Loss = 0.436420, Test Loss = 0.253277, Learning Rate = 6.544455e-06\n",
      "Epoch 13235/20000: Train Loss = 0.436491, Test Loss = 0.256386, Learning Rate = 6.541969e-06\n",
      "Epoch 13236/20000: Train Loss = 0.436625, Test Loss = 0.254374, Learning Rate = 6.539483e-06\n",
      "Epoch 13237/20000: Train Loss = 0.436358, Test Loss = 0.254299, Learning Rate = 6.536998e-06\n",
      "Epoch 13238/20000: Train Loss = 0.436373, Test Loss = 0.256851, Learning Rate = 6.534514e-06\n",
      "Epoch 13239/20000: Train Loss = 0.436482, Test Loss = 0.251837, Learning Rate = 6.532031e-06\n",
      "Epoch 13240/20000: Train Loss = 0.438185, Test Loss = 0.249258, Learning Rate = 6.529549e-06\n",
      "Epoch 13241/20000: Train Loss = 0.436684, Test Loss = 0.251035, Learning Rate = 6.527068e-06\n",
      "Epoch 13242/20000: Train Loss = 0.436272, Test Loss = 0.254566, Learning Rate = 6.524588e-06\n",
      "Epoch 13243/20000: Train Loss = 0.436239, Test Loss = 0.256282, Learning Rate = 6.522109e-06\n",
      "Epoch 13244/20000: Train Loss = 0.437343, Test Loss = 0.253587, Learning Rate = 6.519631e-06\n",
      "Epoch 13245/20000: Train Loss = 0.436276, Test Loss = 0.254746, Learning Rate = 6.517153e-06\n",
      "Epoch 13246/20000: Train Loss = 0.436826, Test Loss = 0.255172, Learning Rate = 6.514677e-06\n",
      "Epoch 13247/20000: Train Loss = 0.436677, Test Loss = 0.247472, Learning Rate = 6.512202e-06\n",
      "Epoch 13248/20000: Train Loss = 0.436704, Test Loss = 0.253747, Learning Rate = 6.509727e-06\n",
      "Epoch 13249/20000: Train Loss = 0.436346, Test Loss = 0.248266, Learning Rate = 6.507254e-06\n",
      "Epoch 13250/20000: Train Loss = 0.436410, Test Loss = 0.249118, Learning Rate = 6.504781e-06\n",
      "Epoch 13251/20000: Train Loss = 0.436902, Test Loss = 0.249735, Learning Rate = 6.502309e-06\n",
      "Epoch 13252/20000: Train Loss = 0.436531, Test Loss = 0.246537, Learning Rate = 6.499839e-06\n",
      "Epoch 13253/20000: Train Loss = 0.437057, Test Loss = 0.251037, Learning Rate = 6.497369e-06\n",
      "Epoch 13254/20000: Train Loss = 0.437466, Test Loss = 0.260654, Learning Rate = 6.494900e-06\n",
      "Epoch 13255/20000: Train Loss = 0.436686, Test Loss = 0.252271, Learning Rate = 6.492432e-06\n",
      "Epoch 13256/20000: Train Loss = 0.436378, Test Loss = 0.250991, Learning Rate = 6.489965e-06\n",
      "Epoch 13257/20000: Train Loss = 0.436527, Test Loss = 0.256241, Learning Rate = 6.487499e-06\n",
      "Epoch 13258/20000: Train Loss = 0.436374, Test Loss = 0.252340, Learning Rate = 6.485034e-06\n",
      "Epoch 13259/20000: Train Loss = 0.436516, Test Loss = 0.248084, Learning Rate = 6.482570e-06\n",
      "Epoch 13260/20000: Train Loss = 0.436777, Test Loss = 0.251126, Learning Rate = 6.480107e-06\n",
      "Epoch 13261/20000: Train Loss = 0.436471, Test Loss = 0.252188, Learning Rate = 6.477645e-06\n",
      "Epoch 13262/20000: Train Loss = 0.436733, Test Loss = 0.254886, Learning Rate = 6.475183e-06\n",
      "Epoch 13263/20000: Train Loss = 0.436491, Test Loss = 0.247683, Learning Rate = 6.472723e-06\n",
      "Epoch 13264/20000: Train Loss = 0.436863, Test Loss = 0.249737, Learning Rate = 6.470263e-06\n",
      "Epoch 13265/20000: Train Loss = 0.437113, Test Loss = 0.252472, Learning Rate = 6.467805e-06\n",
      "Epoch 13266/20000: Train Loss = 0.436553, Test Loss = 0.250113, Learning Rate = 6.465347e-06\n",
      "Epoch 13267/20000: Train Loss = 0.436433, Test Loss = 0.252261, Learning Rate = 6.462891e-06\n",
      "Epoch 13268/20000: Train Loss = 0.436427, Test Loss = 0.252346, Learning Rate = 6.460435e-06\n",
      "Epoch 13269/20000: Train Loss = 0.436780, Test Loss = 0.254649, Learning Rate = 6.457980e-06\n",
      "Epoch 13270/20000: Train Loss = 0.436442, Test Loss = 0.249135, Learning Rate = 6.455526e-06\n",
      "Epoch 13271/20000: Train Loss = 0.436573, Test Loss = 0.254281, Learning Rate = 6.453073e-06\n",
      "Epoch 13272/20000: Train Loss = 0.436245, Test Loss = 0.251803, Learning Rate = 6.450621e-06\n",
      "Epoch 13273/20000: Train Loss = 0.436895, Test Loss = 0.256040, Learning Rate = 6.448170e-06\n",
      "Epoch 13274/20000: Train Loss = 0.436344, Test Loss = 0.253560, Learning Rate = 6.445720e-06\n",
      "Epoch 13275/20000: Train Loss = 0.436331, Test Loss = 0.254653, Learning Rate = 6.443271e-06\n",
      "Epoch 13276/20000: Train Loss = 0.436559, Test Loss = 0.250192, Learning Rate = 6.440823e-06\n",
      "Epoch 13277/20000: Train Loss = 0.436640, Test Loss = 0.254679, Learning Rate = 6.438375e-06\n",
      "Epoch 13278/20000: Train Loss = 0.436306, Test Loss = 0.252292, Learning Rate = 6.435929e-06\n",
      "Epoch 13279/20000: Train Loss = 0.436637, Test Loss = 0.251526, Learning Rate = 6.433483e-06\n",
      "Epoch 13280/20000: Train Loss = 0.436536, Test Loss = 0.254757, Learning Rate = 6.431039e-06\n",
      "Epoch 13281/20000: Train Loss = 0.436517, Test Loss = 0.255004, Learning Rate = 6.428595e-06\n",
      "Epoch 13282/20000: Train Loss = 0.436901, Test Loss = 0.252001, Learning Rate = 6.426153e-06\n",
      "Epoch 13283/20000: Train Loss = 0.436241, Test Loss = 0.247648, Learning Rate = 6.423711e-06\n",
      "Epoch 13284/20000: Train Loss = 0.436566, Test Loss = 0.253993, Learning Rate = 6.421270e-06\n",
      "Epoch 13285/20000: Train Loss = 0.436308, Test Loss = 0.254261, Learning Rate = 6.418830e-06\n",
      "Epoch 13286/20000: Train Loss = 0.436800, Test Loss = 0.253244, Learning Rate = 6.416391e-06\n",
      "Epoch 13287/20000: Train Loss = 0.436357, Test Loss = 0.252624, Learning Rate = 6.413953e-06\n",
      "Epoch 13288/20000: Train Loss = 0.436455, Test Loss = 0.254571, Learning Rate = 6.411516e-06\n",
      "Epoch 13289/20000: Train Loss = 0.436640, Test Loss = 0.254351, Learning Rate = 6.409080e-06\n",
      "Epoch 13290/20000: Train Loss = 0.436941, Test Loss = 0.256303, Learning Rate = 6.406644e-06\n",
      "Epoch 13291/20000: Train Loss = 0.436305, Test Loss = 0.255818, Learning Rate = 6.404210e-06\n",
      "Epoch 13292/20000: Train Loss = 0.436908, Test Loss = 0.262329, Learning Rate = 6.401777e-06\n",
      "Epoch 13293/20000: Train Loss = 0.437507, Test Loss = 0.253705, Learning Rate = 6.399344e-06\n",
      "Epoch 13294/20000: Train Loss = 0.436721, Test Loss = 0.250748, Learning Rate = 6.396913e-06\n",
      "Epoch 13295/20000: Train Loss = 0.436253, Test Loss = 0.252141, Learning Rate = 6.394482e-06\n",
      "Epoch 13296/20000: Train Loss = 0.436440, Test Loss = 0.248514, Learning Rate = 6.392052e-06\n",
      "Epoch 13297/20000: Train Loss = 0.436683, Test Loss = 0.252997, Learning Rate = 6.389623e-06\n",
      "Epoch 13298/20000: Train Loss = 0.436135, Test Loss = 0.251146, Learning Rate = 6.387195e-06\n",
      "Epoch 13299/20000: Train Loss = 0.437366, Test Loss = 0.252099, Learning Rate = 6.384769e-06\n",
      "Epoch 13300/20000: Train Loss = 0.436546, Test Loss = 0.256762, Learning Rate = 6.382342e-06\n",
      "Epoch 13301/20000: Train Loss = 0.437089, Test Loss = 0.253357, Learning Rate = 6.379917e-06\n",
      "Epoch 13302/20000: Train Loss = 0.436367, Test Loss = 0.253702, Learning Rate = 6.377493e-06\n",
      "Epoch 13303/20000: Train Loss = 0.436346, Test Loss = 0.251445, Learning Rate = 6.375070e-06\n",
      "Epoch 13304/20000: Train Loss = 0.436296, Test Loss = 0.251724, Learning Rate = 6.372648e-06\n",
      "Epoch 13305/20000: Train Loss = 0.436577, Test Loss = 0.252869, Learning Rate = 6.370226e-06\n",
      "Epoch 13306/20000: Train Loss = 0.436398, Test Loss = 0.254655, Learning Rate = 6.367806e-06\n",
      "Epoch 13307/20000: Train Loss = 0.436452, Test Loss = 0.250492, Learning Rate = 6.365386e-06\n",
      "Epoch 13308/20000: Train Loss = 0.436395, Test Loss = 0.251528, Learning Rate = 6.362967e-06\n",
      "Epoch 13309/20000: Train Loss = 0.436542, Test Loss = 0.251865, Learning Rate = 6.360550e-06\n",
      "Epoch 13310/20000: Train Loss = 0.436705, Test Loss = 0.254123, Learning Rate = 6.358133e-06\n",
      "Epoch 13311/20000: Train Loss = 0.436600, Test Loss = 0.248912, Learning Rate = 6.355717e-06\n",
      "Epoch 13312/20000: Train Loss = 0.436288, Test Loss = 0.249383, Learning Rate = 6.353302e-06\n",
      "Epoch 13313/20000: Train Loss = 0.436177, Test Loss = 0.248048, Learning Rate = 6.350888e-06\n",
      "Epoch 13314/20000: Train Loss = 0.436365, Test Loss = 0.251098, Learning Rate = 6.348475e-06\n",
      "Epoch 13315/20000: Train Loss = 0.436561, Test Loss = 0.250601, Learning Rate = 6.346062e-06\n",
      "Epoch 13316/20000: Train Loss = 0.436578, Test Loss = 0.247960, Learning Rate = 6.343651e-06\n",
      "Epoch 13317/20000: Train Loss = 0.438108, Test Loss = 0.249822, Learning Rate = 6.341241e-06\n",
      "Epoch 13318/20000: Train Loss = 0.436396, Test Loss = 0.248837, Learning Rate = 6.338831e-06\n",
      "Epoch 13319/20000: Train Loss = 0.436274, Test Loss = 0.249104, Learning Rate = 6.336422e-06\n",
      "Epoch 13320/20000: Train Loss = 0.436265, Test Loss = 0.252633, Learning Rate = 6.334015e-06\n",
      "Epoch 13321/20000: Train Loss = 0.436688, Test Loss = 0.253367, Learning Rate = 6.331608e-06\n",
      "Epoch 13322/20000: Train Loss = 0.437008, Test Loss = 0.248456, Learning Rate = 6.329202e-06\n",
      "Epoch 13323/20000: Train Loss = 0.436117, Test Loss = 0.250365, Learning Rate = 6.326797e-06\n",
      "Epoch 13324/20000: Train Loss = 0.436123, Test Loss = 0.250948, Learning Rate = 6.324393e-06\n",
      "Epoch 13325/20000: Train Loss = 0.436654, Test Loss = 0.251582, Learning Rate = 6.321990e-06\n",
      "Epoch 13326/20000: Train Loss = 0.436324, Test Loss = 0.252258, Learning Rate = 6.319588e-06\n",
      "Epoch 13327/20000: Train Loss = 0.436868, Test Loss = 0.251679, Learning Rate = 6.317187e-06\n",
      "Epoch 13328/20000: Train Loss = 0.437022, Test Loss = 0.252900, Learning Rate = 6.314786e-06\n",
      "Epoch 13329/20000: Train Loss = 0.436473, Test Loss = 0.250607, Learning Rate = 6.312387e-06\n",
      "Epoch 13330/20000: Train Loss = 0.436776, Test Loss = 0.253342, Learning Rate = 6.309988e-06\n",
      "Epoch 13331/20000: Train Loss = 0.437438, Test Loss = 0.255371, Learning Rate = 6.307591e-06\n",
      "Epoch 13332/20000: Train Loss = 0.437325, Test Loss = 0.252405, Learning Rate = 6.305194e-06\n",
      "Epoch 13333/20000: Train Loss = 0.437039, Test Loss = 0.251599, Learning Rate = 6.302798e-06\n",
      "Epoch 13334/20000: Train Loss = 0.437714, Test Loss = 0.254858, Learning Rate = 6.300403e-06\n",
      "Epoch 13335/20000: Train Loss = 0.437050, Test Loss = 0.249772, Learning Rate = 6.298009e-06\n",
      "Epoch 13336/20000: Train Loss = 0.436843, Test Loss = 0.254888, Learning Rate = 6.295616e-06\n",
      "Epoch 13337/20000: Train Loss = 0.436510, Test Loss = 0.252546, Learning Rate = 6.293224e-06\n",
      "Epoch 13338/20000: Train Loss = 0.436477, Test Loss = 0.253282, Learning Rate = 6.290833e-06\n",
      "Epoch 13339/20000: Train Loss = 0.436440, Test Loss = 0.253640, Learning Rate = 6.288443e-06\n",
      "Epoch 13340/20000: Train Loss = 0.436390, Test Loss = 0.252372, Learning Rate = 6.286053e-06\n",
      "Epoch 13341/20000: Train Loss = 0.436424, Test Loss = 0.249956, Learning Rate = 6.283665e-06\n",
      "Epoch 13342/20000: Train Loss = 0.436503, Test Loss = 0.255552, Learning Rate = 6.281277e-06\n",
      "Epoch 13343/20000: Train Loss = 0.436096, Test Loss = 0.250534, Learning Rate = 6.278890e-06\n",
      "Epoch 13344/20000: Train Loss = 0.436863, Test Loss = 0.253324, Learning Rate = 6.276504e-06\n",
      "Epoch 13345/20000: Train Loss = 0.436008, Test Loss = 0.252619, Learning Rate = 6.274120e-06\n",
      "Epoch 13346/20000: Train Loss = 0.437696, Test Loss = 0.254390, Learning Rate = 6.271736e-06\n",
      "Epoch 13347/20000: Train Loss = 0.436228, Test Loss = 0.252363, Learning Rate = 6.269352e-06\n",
      "Epoch 13348/20000: Train Loss = 0.436629, Test Loss = 0.255303, Learning Rate = 6.266970e-06\n",
      "Epoch 13349/20000: Train Loss = 0.436323, Test Loss = 0.250550, Learning Rate = 6.264589e-06\n",
      "Epoch 13350/20000: Train Loss = 0.436042, Test Loss = 0.251166, Learning Rate = 6.262209e-06\n",
      "Epoch 13351/20000: Train Loss = 0.436092, Test Loss = 0.250312, Learning Rate = 6.259829e-06\n",
      "Epoch 13352/20000: Train Loss = 0.436210, Test Loss = 0.252987, Learning Rate = 6.257451e-06\n",
      "Epoch 13353/20000: Train Loss = 0.436257, Test Loss = 0.254451, Learning Rate = 6.255073e-06\n",
      "Epoch 13354/20000: Train Loss = 0.436410, Test Loss = 0.251353, Learning Rate = 6.252696e-06\n",
      "Epoch 13355/20000: Train Loss = 0.436543, Test Loss = 0.257767, Learning Rate = 6.250320e-06\n",
      "Epoch 13356/20000: Train Loss = 0.436617, Test Loss = 0.255007, Learning Rate = 6.247945e-06\n",
      "Epoch 13357/20000: Train Loss = 0.436030, Test Loss = 0.250178, Learning Rate = 6.245571e-06\n",
      "Epoch 13358/20000: Train Loss = 0.436298, Test Loss = 0.251743, Learning Rate = 6.243198e-06\n",
      "Epoch 13359/20000: Train Loss = 0.436496, Test Loss = 0.251803, Learning Rate = 6.240826e-06\n",
      "Epoch 13360/20000: Train Loss = 0.436432, Test Loss = 0.253654, Learning Rate = 6.238455e-06\n",
      "Epoch 13361/20000: Train Loss = 0.436592, Test Loss = 0.250402, Learning Rate = 6.236084e-06\n",
      "Epoch 13362/20000: Train Loss = 0.437261, Test Loss = 0.252297, Learning Rate = 6.233715e-06\n",
      "Epoch 13363/20000: Train Loss = 0.436524, Test Loss = 0.251913, Learning Rate = 6.231346e-06\n",
      "Epoch 13364/20000: Train Loss = 0.436317, Test Loss = 0.250236, Learning Rate = 6.228978e-06\n",
      "Epoch 13365/20000: Train Loss = 0.436376, Test Loss = 0.250630, Learning Rate = 6.226611e-06\n",
      "Epoch 13366/20000: Train Loss = 0.436287, Test Loss = 0.250792, Learning Rate = 6.224245e-06\n",
      "Epoch 13367/20000: Train Loss = 0.436260, Test Loss = 0.249613, Learning Rate = 6.221880e-06\n",
      "Epoch 13368/20000: Train Loss = 0.436073, Test Loss = 0.249395, Learning Rate = 6.219516e-06\n",
      "Epoch 13369/20000: Train Loss = 0.436808, Test Loss = 0.255449, Learning Rate = 6.217153e-06\n",
      "Epoch 13370/20000: Train Loss = 0.436284, Test Loss = 0.252970, Learning Rate = 6.214791e-06\n",
      "Epoch 13371/20000: Train Loss = 0.436163, Test Loss = 0.250839, Learning Rate = 6.212429e-06\n",
      "Epoch 13372/20000: Train Loss = 0.436038, Test Loss = 0.247201, Learning Rate = 6.210069e-06\n",
      "Epoch 13373/20000: Train Loss = 0.437416, Test Loss = 0.247400, Learning Rate = 6.207709e-06\n",
      "Epoch 13374/20000: Train Loss = 0.436339, Test Loss = 0.246875, Learning Rate = 6.205350e-06\n",
      "Epoch 13375/20000: Train Loss = 0.436558, Test Loss = 0.252648, Learning Rate = 6.202992e-06\n",
      "Epoch 13376/20000: Train Loss = 0.436616, Test Loss = 0.249326, Learning Rate = 6.200635e-06\n",
      "Epoch 13377/20000: Train Loss = 0.436305, Test Loss = 0.252710, Learning Rate = 6.198279e-06\n",
      "Epoch 13378/20000: Train Loss = 0.436120, Test Loss = 0.254436, Learning Rate = 6.195924e-06\n",
      "Epoch 13379/20000: Train Loss = 0.436623, Test Loss = 0.253706, Learning Rate = 6.193570e-06\n",
      "Epoch 13380/20000: Train Loss = 0.436065, Test Loss = 0.253168, Learning Rate = 6.191216e-06\n",
      "Epoch 13381/20000: Train Loss = 0.436575, Test Loss = 0.254729, Learning Rate = 6.188864e-06\n",
      "Epoch 13382/20000: Train Loss = 0.436120, Test Loss = 0.254453, Learning Rate = 6.186512e-06\n",
      "Epoch 13383/20000: Train Loss = 0.436489, Test Loss = 0.252853, Learning Rate = 6.184162e-06\n",
      "Epoch 13384/20000: Train Loss = 0.436129, Test Loss = 0.251928, Learning Rate = 6.181812e-06\n",
      "Epoch 13385/20000: Train Loss = 0.437150, Test Loss = 0.253651, Learning Rate = 6.179463e-06\n",
      "Epoch 13386/20000: Train Loss = 0.436163, Test Loss = 0.254351, Learning Rate = 6.177115e-06\n",
      "Epoch 13387/20000: Train Loss = 0.436364, Test Loss = 0.258574, Learning Rate = 6.174768e-06\n",
      "Epoch 13388/20000: Train Loss = 0.436553, Test Loss = 0.250625, Learning Rate = 6.172421e-06\n",
      "Epoch 13389/20000: Train Loss = 0.436470, Test Loss = 0.257112, Learning Rate = 6.170076e-06\n",
      "Epoch 13390/20000: Train Loss = 0.436171, Test Loss = 0.254856, Learning Rate = 6.167732e-06\n",
      "Epoch 13391/20000: Train Loss = 0.437024, Test Loss = 0.256684, Learning Rate = 6.165388e-06\n",
      "Epoch 13392/20000: Train Loss = 0.436607, Test Loss = 0.256091, Learning Rate = 6.163045e-06\n",
      "Epoch 13393/20000: Train Loss = 0.436639, Test Loss = 0.254924, Learning Rate = 6.160704e-06\n",
      "Epoch 13394/20000: Train Loss = 0.436725, Test Loss = 0.257265, Learning Rate = 6.158363e-06\n",
      "Epoch 13395/20000: Train Loss = 0.436267, Test Loss = 0.253953, Learning Rate = 6.156023e-06\n",
      "Epoch 13396/20000: Train Loss = 0.436305, Test Loss = 0.252619, Learning Rate = 6.153684e-06\n",
      "Epoch 13397/20000: Train Loss = 0.436291, Test Loss = 0.252267, Learning Rate = 6.151345e-06\n",
      "Epoch 13398/20000: Train Loss = 0.436123, Test Loss = 0.251936, Learning Rate = 6.149008e-06\n",
      "Epoch 13399/20000: Train Loss = 0.436592, Test Loss = 0.252176, Learning Rate = 6.146672e-06\n",
      "Epoch 13400/20000: Train Loss = 0.436554, Test Loss = 0.251059, Learning Rate = 6.144336e-06\n",
      "Epoch 13401/20000: Train Loss = 0.436276, Test Loss = 0.251018, Learning Rate = 6.142001e-06\n",
      "Epoch 13402/20000: Train Loss = 0.436291, Test Loss = 0.248885, Learning Rate = 6.139667e-06\n",
      "Epoch 13403/20000: Train Loss = 0.436335, Test Loss = 0.249817, Learning Rate = 6.137335e-06\n",
      "Epoch 13404/20000: Train Loss = 0.436218, Test Loss = 0.249486, Learning Rate = 6.135003e-06\n",
      "Epoch 13405/20000: Train Loss = 0.436076, Test Loss = 0.253229, Learning Rate = 6.132671e-06\n",
      "Epoch 13406/20000: Train Loss = 0.436693, Test Loss = 0.256059, Learning Rate = 6.130341e-06\n",
      "Epoch 13407/20000: Train Loss = 0.437781, Test Loss = 0.256344, Learning Rate = 6.128012e-06\n",
      "Epoch 13408/20000: Train Loss = 0.436262, Test Loss = 0.256632, Learning Rate = 6.125683e-06\n",
      "Epoch 13409/20000: Train Loss = 0.436187, Test Loss = 0.256107, Learning Rate = 6.123356e-06\n",
      "Epoch 13410/20000: Train Loss = 0.436064, Test Loss = 0.255315, Learning Rate = 6.121029e-06\n",
      "Epoch 13411/20000: Train Loss = 0.436008, Test Loss = 0.250974, Learning Rate = 6.118703e-06\n",
      "Epoch 13412/20000: Train Loss = 0.436219, Test Loss = 0.257170, Learning Rate = 6.116378e-06\n",
      "Epoch 13413/20000: Train Loss = 0.436399, Test Loss = 0.250648, Learning Rate = 6.114054e-06\n",
      "Epoch 13414/20000: Train Loss = 0.436236, Test Loss = 0.251160, Learning Rate = 6.111731e-06\n",
      "Epoch 13415/20000: Train Loss = 0.438541, Test Loss = 0.252049, Learning Rate = 6.109409e-06\n",
      "Epoch 13416/20000: Train Loss = 0.436261, Test Loss = 0.249603, Learning Rate = 6.107087e-06\n",
      "Epoch 13417/20000: Train Loss = 0.436087, Test Loss = 0.251834, Learning Rate = 6.104767e-06\n",
      "Epoch 13418/20000: Train Loss = 0.436076, Test Loss = 0.252450, Learning Rate = 6.102447e-06\n",
      "Epoch 13419/20000: Train Loss = 0.436203, Test Loss = 0.258094, Learning Rate = 6.100128e-06\n",
      "Epoch 13420/20000: Train Loss = 0.436851, Test Loss = 0.261107, Learning Rate = 6.097811e-06\n",
      "Epoch 13421/20000: Train Loss = 0.436455, Test Loss = 0.251163, Learning Rate = 6.095494e-06\n",
      "Epoch 13422/20000: Train Loss = 0.436266, Test Loss = 0.253150, Learning Rate = 6.093177e-06\n",
      "Epoch 13423/20000: Train Loss = 0.436188, Test Loss = 0.254186, Learning Rate = 6.090862e-06\n",
      "Epoch 13424/20000: Train Loss = 0.436382, Test Loss = 0.256597, Learning Rate = 6.088548e-06\n",
      "Epoch 13425/20000: Train Loss = 0.436480, Test Loss = 0.252063, Learning Rate = 6.086234e-06\n",
      "Epoch 13426/20000: Train Loss = 0.436190, Test Loss = 0.252547, Learning Rate = 6.083922e-06\n",
      "Epoch 13427/20000: Train Loss = 0.436309, Test Loss = 0.251376, Learning Rate = 6.081610e-06\n",
      "Epoch 13428/20000: Train Loss = 0.436253, Test Loss = 0.249738, Learning Rate = 6.079299e-06\n",
      "Epoch 13429/20000: Train Loss = 0.436850, Test Loss = 0.251078, Learning Rate = 6.076989e-06\n",
      "Epoch 13430/20000: Train Loss = 0.436275, Test Loss = 0.249869, Learning Rate = 6.074680e-06\n",
      "Epoch 13431/20000: Train Loss = 0.435913, Test Loss = 0.254145, Learning Rate = 6.072372e-06\n",
      "Epoch 13432/20000: Train Loss = 0.435983, Test Loss = 0.254048, Learning Rate = 6.070065e-06\n",
      "Epoch 13433/20000: Train Loss = 0.436437, Test Loss = 0.256132, Learning Rate = 6.067758e-06\n",
      "Epoch 13434/20000: Train Loss = 0.436212, Test Loss = 0.251431, Learning Rate = 6.065452e-06\n",
      "Epoch 13435/20000: Train Loss = 0.436793, Test Loss = 0.252020, Learning Rate = 6.063148e-06\n",
      "Epoch 13436/20000: Train Loss = 0.436135, Test Loss = 0.249556, Learning Rate = 6.060844e-06\n",
      "Epoch 13437/20000: Train Loss = 0.436629, Test Loss = 0.247852, Learning Rate = 6.058541e-06\n",
      "Epoch 13438/20000: Train Loss = 0.436153, Test Loss = 0.248995, Learning Rate = 6.056239e-06\n",
      "Epoch 13439/20000: Train Loss = 0.436526, Test Loss = 0.250052, Learning Rate = 6.053938e-06\n",
      "Epoch 13440/20000: Train Loss = 0.436407, Test Loss = 0.247978, Learning Rate = 6.051637e-06\n",
      "Epoch 13441/20000: Train Loss = 0.436395, Test Loss = 0.254110, Learning Rate = 6.049338e-06\n",
      "Epoch 13442/20000: Train Loss = 0.436412, Test Loss = 0.255464, Learning Rate = 6.047039e-06\n",
      "Epoch 13443/20000: Train Loss = 0.436105, Test Loss = 0.254324, Learning Rate = 6.044742e-06\n",
      "Epoch 13444/20000: Train Loss = 0.436301, Test Loss = 0.251184, Learning Rate = 6.042445e-06\n",
      "Epoch 13445/20000: Train Loss = 0.436542, Test Loss = 0.249009, Learning Rate = 6.040149e-06\n",
      "Epoch 13446/20000: Train Loss = 0.436548, Test Loss = 0.248564, Learning Rate = 6.037854e-06\n",
      "Epoch 13447/20000: Train Loss = 0.436394, Test Loss = 0.247983, Learning Rate = 6.035559e-06\n",
      "Epoch 13448/20000: Train Loss = 0.436104, Test Loss = 0.251501, Learning Rate = 6.033266e-06\n",
      "Epoch 13449/20000: Train Loss = 0.436336, Test Loss = 0.251256, Learning Rate = 6.030974e-06\n",
      "Epoch 13450/20000: Train Loss = 0.436473, Test Loss = 0.250594, Learning Rate = 6.028682e-06\n",
      "Epoch 13451/20000: Train Loss = 0.436162, Test Loss = 0.251618, Learning Rate = 6.026391e-06\n",
      "Epoch 13452/20000: Train Loss = 0.436114, Test Loss = 0.251588, Learning Rate = 6.024101e-06\n",
      "Epoch 13453/20000: Train Loss = 0.436455, Test Loss = 0.255070, Learning Rate = 6.021812e-06\n",
      "Epoch 13454/20000: Train Loss = 0.436169, Test Loss = 0.250544, Learning Rate = 6.019524e-06\n",
      "Epoch 13455/20000: Train Loss = 0.436674, Test Loss = 0.255906, Learning Rate = 6.017237e-06\n",
      "Epoch 13456/20000: Train Loss = 0.436097, Test Loss = 0.249524, Learning Rate = 6.014951e-06\n",
      "Epoch 13457/20000: Train Loss = 0.436516, Test Loss = 0.253683, Learning Rate = 6.012665e-06\n",
      "Epoch 13458/20000: Train Loss = 0.436142, Test Loss = 0.251793, Learning Rate = 6.010381e-06\n",
      "Epoch 13459/20000: Train Loss = 0.436082, Test Loss = 0.250849, Learning Rate = 6.008097e-06\n",
      "Epoch 13460/20000: Train Loss = 0.436242, Test Loss = 0.252322, Learning Rate = 6.005814e-06\n",
      "Epoch 13461/20000: Train Loss = 0.436270, Test Loss = 0.252248, Learning Rate = 6.003532e-06\n",
      "Epoch 13462/20000: Train Loss = 0.436360, Test Loss = 0.251939, Learning Rate = 6.001251e-06\n",
      "Epoch 13463/20000: Train Loss = 0.436865, Test Loss = 0.246531, Learning Rate = 5.998970e-06\n",
      "Epoch 13464/20000: Train Loss = 0.436518, Test Loss = 0.252325, Learning Rate = 5.996691e-06\n",
      "Epoch 13465/20000: Train Loss = 0.436342, Test Loss = 0.251969, Learning Rate = 5.994412e-06\n",
      "Epoch 13466/20000: Train Loss = 0.436111, Test Loss = 0.250577, Learning Rate = 5.992135e-06\n",
      "Epoch 13467/20000: Train Loss = 0.436025, Test Loss = 0.250146, Learning Rate = 5.989858e-06\n",
      "Epoch 13468/20000: Train Loss = 0.436203, Test Loss = 0.249199, Learning Rate = 5.987582e-06\n",
      "Epoch 13469/20000: Train Loss = 0.436122, Test Loss = 0.248514, Learning Rate = 5.985307e-06\n",
      "Epoch 13470/20000: Train Loss = 0.436406, Test Loss = 0.253148, Learning Rate = 5.983032e-06\n",
      "Epoch 13471/20000: Train Loss = 0.436189, Test Loss = 0.250470, Learning Rate = 5.980759e-06\n",
      "Epoch 13472/20000: Train Loss = 0.435967, Test Loss = 0.248695, Learning Rate = 5.978486e-06\n",
      "Epoch 13473/20000: Train Loss = 0.436023, Test Loss = 0.252458, Learning Rate = 5.976215e-06\n",
      "Epoch 13474/20000: Train Loss = 0.436219, Test Loss = 0.250906, Learning Rate = 5.973944e-06\n",
      "Epoch 13475/20000: Train Loss = 0.436602, Test Loss = 0.255388, Learning Rate = 5.971674e-06\n",
      "Epoch 13476/20000: Train Loss = 0.436120, Test Loss = 0.251999, Learning Rate = 5.969405e-06\n",
      "Epoch 13477/20000: Train Loss = 0.436261, Test Loss = 0.251078, Learning Rate = 5.967137e-06\n",
      "Epoch 13478/20000: Train Loss = 0.436585, Test Loss = 0.256182, Learning Rate = 5.964869e-06\n",
      "Epoch 13479/20000: Train Loss = 0.436439, Test Loss = 0.251471, Learning Rate = 5.962603e-06\n",
      "Epoch 13480/20000: Train Loss = 0.436321, Test Loss = 0.250655, Learning Rate = 5.960337e-06\n",
      "Epoch 13481/20000: Train Loss = 0.436328, Test Loss = 0.258210, Learning Rate = 5.958072e-06\n",
      "Epoch 13482/20000: Train Loss = 0.436167, Test Loss = 0.254288, Learning Rate = 5.955809e-06\n",
      "Epoch 13483/20000: Train Loss = 0.436091, Test Loss = 0.253875, Learning Rate = 5.953546e-06\n",
      "Epoch 13484/20000: Train Loss = 0.436236, Test Loss = 0.255606, Learning Rate = 5.951283e-06\n",
      "Epoch 13485/20000: Train Loss = 0.436515, Test Loss = 0.254959, Learning Rate = 5.949022e-06\n",
      "Epoch 13486/20000: Train Loss = 0.436134, Test Loss = 0.253256, Learning Rate = 5.946762e-06\n",
      "Epoch 13487/20000: Train Loss = 0.436318, Test Loss = 0.254058, Learning Rate = 5.944502e-06\n",
      "Epoch 13488/20000: Train Loss = 0.436358, Test Loss = 0.251667, Learning Rate = 5.942243e-06\n",
      "Epoch 13489/20000: Train Loss = 0.435857, Test Loss = 0.252122, Learning Rate = 5.939985e-06\n",
      "Epoch 13490/20000: Train Loss = 0.436133, Test Loss = 0.250890, Learning Rate = 5.937728e-06\n",
      "Epoch 13491/20000: Train Loss = 0.436394, Test Loss = 0.250694, Learning Rate = 5.935472e-06\n",
      "Epoch 13492/20000: Train Loss = 0.436187, Test Loss = 0.252266, Learning Rate = 5.933217e-06\n",
      "Epoch 13493/20000: Train Loss = 0.436084, Test Loss = 0.249020, Learning Rate = 5.930962e-06\n",
      "Epoch 13494/20000: Train Loss = 0.436441, Test Loss = 0.250951, Learning Rate = 5.928709e-06\n",
      "Epoch 13495/20000: Train Loss = 0.436223, Test Loss = 0.252329, Learning Rate = 5.926456e-06\n",
      "Epoch 13496/20000: Train Loss = 0.435915, Test Loss = 0.252319, Learning Rate = 5.924204e-06\n",
      "Epoch 13497/20000: Train Loss = 0.436375, Test Loss = 0.253565, Learning Rate = 5.921953e-06\n",
      "Epoch 13498/20000: Train Loss = 0.436129, Test Loss = 0.255275, Learning Rate = 5.919703e-06\n",
      "Epoch 13499/20000: Train Loss = 0.436358, Test Loss = 0.250821, Learning Rate = 5.917454e-06\n",
      "Epoch 13500/20000: Train Loss = 0.436095, Test Loss = 0.252037, Learning Rate = 5.915205e-06\n",
      "Epoch 13501/20000: Train Loss = 0.435984, Test Loss = 0.251671, Learning Rate = 5.912957e-06\n",
      "Epoch 13502/20000: Train Loss = 0.435963, Test Loss = 0.256235, Learning Rate = 5.910711e-06\n",
      "Epoch 13503/20000: Train Loss = 0.436202, Test Loss = 0.253092, Learning Rate = 5.908465e-06\n",
      "Epoch 13504/20000: Train Loss = 0.437186, Test Loss = 0.252369, Learning Rate = 5.906220e-06\n",
      "Epoch 13505/20000: Train Loss = 0.437981, Test Loss = 0.253464, Learning Rate = 5.903976e-06\n",
      "Epoch 13506/20000: Train Loss = 0.435898, Test Loss = 0.251049, Learning Rate = 5.901732e-06\n",
      "Epoch 13507/20000: Train Loss = 0.436107, Test Loss = 0.251971, Learning Rate = 5.899490e-06\n",
      "Epoch 13508/20000: Train Loss = 0.436131, Test Loss = 0.253985, Learning Rate = 5.897248e-06\n",
      "Epoch 13509/20000: Train Loss = 0.436123, Test Loss = 0.253185, Learning Rate = 5.895007e-06\n",
      "Epoch 13510/20000: Train Loss = 0.436023, Test Loss = 0.250009, Learning Rate = 5.892767e-06\n",
      "Epoch 13511/20000: Train Loss = 0.436352, Test Loss = 0.252416, Learning Rate = 5.890528e-06\n",
      "Epoch 13512/20000: Train Loss = 0.436231, Test Loss = 0.251902, Learning Rate = 5.888290e-06\n",
      "Epoch 13513/20000: Train Loss = 0.436272, Test Loss = 0.249543, Learning Rate = 5.886053e-06\n",
      "Epoch 13514/20000: Train Loss = 0.436087, Test Loss = 0.251178, Learning Rate = 5.883816e-06\n",
      "Epoch 13515/20000: Train Loss = 0.436030, Test Loss = 0.250854, Learning Rate = 5.881580e-06\n",
      "Epoch 13516/20000: Train Loss = 0.436092, Test Loss = 0.250523, Learning Rate = 5.879345e-06\n",
      "Epoch 13517/20000: Train Loss = 0.436023, Test Loss = 0.250146, Learning Rate = 5.877111e-06\n",
      "Epoch 13518/20000: Train Loss = 0.436097, Test Loss = 0.249630, Learning Rate = 5.874878e-06\n",
      "Epoch 13519/20000: Train Loss = 0.436355, Test Loss = 0.254403, Learning Rate = 5.872646e-06\n",
      "Epoch 13520/20000: Train Loss = 0.436127, Test Loss = 0.254533, Learning Rate = 5.870415e-06\n",
      "Epoch 13521/20000: Train Loss = 0.436376, Test Loss = 0.252265, Learning Rate = 5.868184e-06\n",
      "Epoch 13522/20000: Train Loss = 0.436432, Test Loss = 0.250624, Learning Rate = 5.865954e-06\n",
      "Epoch 13523/20000: Train Loss = 0.436101, Test Loss = 0.252620, Learning Rate = 5.863725e-06\n",
      "Epoch 13524/20000: Train Loss = 0.436270, Test Loss = 0.253154, Learning Rate = 5.861497e-06\n",
      "Epoch 13525/20000: Train Loss = 0.436058, Test Loss = 0.250754, Learning Rate = 5.859270e-06\n",
      "Epoch 13526/20000: Train Loss = 0.436487, Test Loss = 0.249815, Learning Rate = 5.857044e-06\n",
      "Epoch 13527/20000: Train Loss = 0.436392, Test Loss = 0.253388, Learning Rate = 5.854818e-06\n",
      "Epoch 13528/20000: Train Loss = 0.435916, Test Loss = 0.252798, Learning Rate = 5.852594e-06\n",
      "Epoch 13529/20000: Train Loss = 0.436520, Test Loss = 0.250381, Learning Rate = 5.850370e-06\n",
      "Epoch 13530/20000: Train Loss = 0.435857, Test Loss = 0.251487, Learning Rate = 5.848147e-06\n",
      "Epoch 13531/20000: Train Loss = 0.436287, Test Loss = 0.253097, Learning Rate = 5.845925e-06\n",
      "Epoch 13532/20000: Train Loss = 0.436238, Test Loss = 0.250487, Learning Rate = 5.843703e-06\n",
      "Epoch 13533/20000: Train Loss = 0.436084, Test Loss = 0.251601, Learning Rate = 5.841483e-06\n",
      "Epoch 13534/20000: Train Loss = 0.435819, Test Loss = 0.248611, Learning Rate = 5.839263e-06\n",
      "Epoch 13535/20000: Train Loss = 0.436520, Test Loss = 0.251835, Learning Rate = 5.837044e-06\n",
      "Epoch 13536/20000: Train Loss = 0.436125, Test Loss = 0.251140, Learning Rate = 5.834827e-06\n",
      "Epoch 13537/20000: Train Loss = 0.436131, Test Loss = 0.254193, Learning Rate = 5.832609e-06\n",
      "Epoch 13538/20000: Train Loss = 0.436814, Test Loss = 0.251453, Learning Rate = 5.830393e-06\n",
      "Epoch 13539/20000: Train Loss = 0.436092, Test Loss = 0.253739, Learning Rate = 5.828178e-06\n",
      "Epoch 13540/20000: Train Loss = 0.436596, Test Loss = 0.254180, Learning Rate = 5.825963e-06\n",
      "Epoch 13541/20000: Train Loss = 0.436380, Test Loss = 0.251877, Learning Rate = 5.823750e-06\n",
      "Epoch 13542/20000: Train Loss = 0.436256, Test Loss = 0.248267, Learning Rate = 5.821537e-06\n",
      "Epoch 13543/20000: Train Loss = 0.435997, Test Loss = 0.251141, Learning Rate = 5.819325e-06\n",
      "Epoch 13544/20000: Train Loss = 0.435865, Test Loss = 0.253040, Learning Rate = 5.817114e-06\n",
      "Epoch 13545/20000: Train Loss = 0.436097, Test Loss = 0.252279, Learning Rate = 5.814903e-06\n",
      "Epoch 13546/20000: Train Loss = 0.436025, Test Loss = 0.253177, Learning Rate = 5.812694e-06\n",
      "Epoch 13547/20000: Train Loss = 0.436075, Test Loss = 0.250974, Learning Rate = 5.810485e-06\n",
      "Epoch 13548/20000: Train Loss = 0.436940, Test Loss = 0.253925, Learning Rate = 5.808277e-06\n",
      "Epoch 13549/20000: Train Loss = 0.436037, Test Loss = 0.252020, Learning Rate = 5.806070e-06\n",
      "Epoch 13550/20000: Train Loss = 0.436314, Test Loss = 0.249636, Learning Rate = 5.803864e-06\n",
      "Epoch 13551/20000: Train Loss = 0.436540, Test Loss = 0.252673, Learning Rate = 5.801659e-06\n",
      "Epoch 13552/20000: Train Loss = 0.436182, Test Loss = 0.251702, Learning Rate = 5.799454e-06\n",
      "Epoch 13553/20000: Train Loss = 0.435927, Test Loss = 0.250032, Learning Rate = 5.797251e-06\n",
      "Epoch 13554/20000: Train Loss = 0.436109, Test Loss = 0.250809, Learning Rate = 5.795048e-06\n",
      "Epoch 13555/20000: Train Loss = 0.435974, Test Loss = 0.251786, Learning Rate = 5.792846e-06\n",
      "Epoch 13556/20000: Train Loss = 0.436502, Test Loss = 0.252066, Learning Rate = 5.790645e-06\n",
      "Epoch 13557/20000: Train Loss = 0.436120, Test Loss = 0.250330, Learning Rate = 5.788444e-06\n",
      "Epoch 13558/20000: Train Loss = 0.436123, Test Loss = 0.253114, Learning Rate = 5.786245e-06\n",
      "Epoch 13559/20000: Train Loss = 0.435999, Test Loss = 0.252577, Learning Rate = 5.784046e-06\n",
      "Epoch 13560/20000: Train Loss = 0.436876, Test Loss = 0.246940, Learning Rate = 5.781849e-06\n",
      "Epoch 13561/20000: Train Loss = 0.436110, Test Loss = 0.253487, Learning Rate = 5.779652e-06\n",
      "Epoch 13562/20000: Train Loss = 0.436337, Test Loss = 0.249792, Learning Rate = 5.777456e-06\n",
      "Epoch 13563/20000: Train Loss = 0.436192, Test Loss = 0.248937, Learning Rate = 5.775260e-06\n",
      "Epoch 13564/20000: Train Loss = 0.436429, Test Loss = 0.258177, Learning Rate = 5.773066e-06\n",
      "Epoch 13565/20000: Train Loss = 0.436306, Test Loss = 0.250847, Learning Rate = 5.770872e-06\n",
      "Epoch 13566/20000: Train Loss = 0.436001, Test Loss = 0.251318, Learning Rate = 5.768679e-06\n",
      "Epoch 13567/20000: Train Loss = 0.436119, Test Loss = 0.249714, Learning Rate = 5.766487e-06\n",
      "Epoch 13568/20000: Train Loss = 0.436151, Test Loss = 0.252448, Learning Rate = 5.764296e-06\n",
      "Epoch 13569/20000: Train Loss = 0.435980, Test Loss = 0.253100, Learning Rate = 5.762106e-06\n",
      "Epoch 13570/20000: Train Loss = 0.436170, Test Loss = 0.252978, Learning Rate = 5.759917e-06\n",
      "Epoch 13571/20000: Train Loss = 0.435812, Test Loss = 0.253512, Learning Rate = 5.757728e-06\n",
      "Epoch 13572/20000: Train Loss = 0.435709, Test Loss = 0.251014, Learning Rate = 5.755540e-06\n",
      "Epoch 13573/20000: Train Loss = 0.436070, Test Loss = 0.250614, Learning Rate = 5.753353e-06\n",
      "Epoch 13574/20000: Train Loss = 0.436142, Test Loss = 0.250916, Learning Rate = 5.751167e-06\n",
      "Epoch 13575/20000: Train Loss = 0.436091, Test Loss = 0.249319, Learning Rate = 5.748982e-06\n",
      "Epoch 13576/20000: Train Loss = 0.435987, Test Loss = 0.252560, Learning Rate = 5.746797e-06\n",
      "Epoch 13577/20000: Train Loss = 0.436117, Test Loss = 0.255175, Learning Rate = 5.744614e-06\n",
      "Epoch 13578/20000: Train Loss = 0.435959, Test Loss = 0.249433, Learning Rate = 5.742431e-06\n",
      "Epoch 13579/20000: Train Loss = 0.436051, Test Loss = 0.250174, Learning Rate = 5.740249e-06\n",
      "Epoch 13580/20000: Train Loss = 0.436087, Test Loss = 0.255006, Learning Rate = 5.738068e-06\n",
      "Epoch 13581/20000: Train Loss = 0.436102, Test Loss = 0.249078, Learning Rate = 5.735888e-06\n",
      "Epoch 13582/20000: Train Loss = 0.435999, Test Loss = 0.250339, Learning Rate = 5.733708e-06\n",
      "Epoch 13583/20000: Train Loss = 0.436237, Test Loss = 0.247081, Learning Rate = 5.731529e-06\n",
      "Epoch 13584/20000: Train Loss = 0.436346, Test Loss = 0.251677, Learning Rate = 5.729352e-06\n",
      "Epoch 13585/20000: Train Loss = 0.436108, Test Loss = 0.251395, Learning Rate = 5.727175e-06\n",
      "Epoch 13586/20000: Train Loss = 0.436242, Test Loss = 0.248545, Learning Rate = 5.724998e-06\n",
      "Epoch 13587/20000: Train Loss = 0.436527, Test Loss = 0.253881, Learning Rate = 5.722823e-06\n",
      "Epoch 13588/20000: Train Loss = 0.436110, Test Loss = 0.248319, Learning Rate = 5.720649e-06\n",
      "Epoch 13589/20000: Train Loss = 0.436399, Test Loss = 0.249464, Learning Rate = 5.718475e-06\n",
      "Epoch 13590/20000: Train Loss = 0.435955, Test Loss = 0.251926, Learning Rate = 5.716302e-06\n",
      "Epoch 13591/20000: Train Loss = 0.436788, Test Loss = 0.247456, Learning Rate = 5.714130e-06\n",
      "Epoch 13592/20000: Train Loss = 0.436201, Test Loss = 0.253287, Learning Rate = 5.711959e-06\n",
      "Epoch 13593/20000: Train Loss = 0.436079, Test Loss = 0.251096, Learning Rate = 5.709788e-06\n",
      "Epoch 13594/20000: Train Loss = 0.436286, Test Loss = 0.253283, Learning Rate = 5.707619e-06\n",
      "Epoch 13595/20000: Train Loss = 0.436731, Test Loss = 0.253166, Learning Rate = 5.705450e-06\n",
      "Epoch 13596/20000: Train Loss = 0.436390, Test Loss = 0.252586, Learning Rate = 5.703282e-06\n",
      "Epoch 13597/20000: Train Loss = 0.436203, Test Loss = 0.251861, Learning Rate = 5.701115e-06\n",
      "Epoch 13598/20000: Train Loss = 0.435998, Test Loss = 0.252110, Learning Rate = 5.698949e-06\n",
      "Epoch 13599/20000: Train Loss = 0.436040, Test Loss = 0.251591, Learning Rate = 5.696783e-06\n",
      "Epoch 13600/20000: Train Loss = 0.435891, Test Loss = 0.251658, Learning Rate = 5.694619e-06\n",
      "Epoch 13601/20000: Train Loss = 0.436425, Test Loss = 0.252328, Learning Rate = 5.692455e-06\n",
      "Epoch 13602/20000: Train Loss = 0.436009, Test Loss = 0.251334, Learning Rate = 5.690292e-06\n",
      "Epoch 13603/20000: Train Loss = 0.436006, Test Loss = 0.249567, Learning Rate = 5.688130e-06\n",
      "Epoch 13604/20000: Train Loss = 0.435869, Test Loss = 0.249862, Learning Rate = 5.685968e-06\n",
      "Epoch 13605/20000: Train Loss = 0.436037, Test Loss = 0.251351, Learning Rate = 5.683808e-06\n",
      "Epoch 13606/20000: Train Loss = 0.436107, Test Loss = 0.250124, Learning Rate = 5.681648e-06\n",
      "Epoch 13607/20000: Train Loss = 0.436088, Test Loss = 0.252397, Learning Rate = 5.679489e-06\n",
      "Epoch 13608/20000: Train Loss = 0.436010, Test Loss = 0.251940, Learning Rate = 5.677331e-06\n",
      "Epoch 13609/20000: Train Loss = 0.435888, Test Loss = 0.251021, Learning Rate = 5.675174e-06\n",
      "Epoch 13610/20000: Train Loss = 0.436117, Test Loss = 0.251831, Learning Rate = 5.673018e-06\n",
      "Epoch 13611/20000: Train Loss = 0.436164, Test Loss = 0.252456, Learning Rate = 5.670862e-06\n",
      "Epoch 13612/20000: Train Loss = 0.437039, Test Loss = 0.248895, Learning Rate = 5.668707e-06\n",
      "Epoch 13613/20000: Train Loss = 0.436451, Test Loss = 0.251747, Learning Rate = 5.666553e-06\n",
      "Epoch 13614/20000: Train Loss = 0.435996, Test Loss = 0.253248, Learning Rate = 5.664400e-06\n",
      "Epoch 13615/20000: Train Loss = 0.436692, Test Loss = 0.254925, Learning Rate = 5.662248e-06\n",
      "Epoch 13616/20000: Train Loss = 0.436323, Test Loss = 0.252457, Learning Rate = 5.660096e-06\n",
      "Epoch 13617/20000: Train Loss = 0.436208, Test Loss = 0.253137, Learning Rate = 5.657946e-06\n",
      "Epoch 13618/20000: Train Loss = 0.437093, Test Loss = 0.252157, Learning Rate = 5.655796e-06\n",
      "Epoch 13619/20000: Train Loss = 0.436239, Test Loss = 0.250892, Learning Rate = 5.653647e-06\n",
      "Epoch 13620/20000: Train Loss = 0.436034, Test Loss = 0.253245, Learning Rate = 5.651499e-06\n",
      "Epoch 13621/20000: Train Loss = 0.436070, Test Loss = 0.249754, Learning Rate = 5.649351e-06\n",
      "Epoch 13622/20000: Train Loss = 0.437434, Test Loss = 0.253655, Learning Rate = 5.647205e-06\n",
      "Epoch 13623/20000: Train Loss = 0.436208, Test Loss = 0.250042, Learning Rate = 5.645059e-06\n",
      "Epoch 13624/20000: Train Loss = 0.435980, Test Loss = 0.255045, Learning Rate = 5.642914e-06\n",
      "Epoch 13625/20000: Train Loss = 0.435953, Test Loss = 0.252353, Learning Rate = 5.640770e-06\n",
      "Epoch 13626/20000: Train Loss = 0.436298, Test Loss = 0.251959, Learning Rate = 5.638626e-06\n",
      "Epoch 13627/20000: Train Loss = 0.436373, Test Loss = 0.252075, Learning Rate = 5.636484e-06\n",
      "Epoch 13628/20000: Train Loss = 0.435924, Test Loss = 0.250728, Learning Rate = 5.634342e-06\n",
      "Epoch 13629/20000: Train Loss = 0.435830, Test Loss = 0.248600, Learning Rate = 5.632201e-06\n",
      "Epoch 13630/20000: Train Loss = 0.436107, Test Loss = 0.250432, Learning Rate = 5.630061e-06\n",
      "Epoch 13631/20000: Train Loss = 0.436508, Test Loss = 0.255220, Learning Rate = 5.627922e-06\n",
      "Epoch 13632/20000: Train Loss = 0.436296, Test Loss = 0.251961, Learning Rate = 5.625783e-06\n",
      "Epoch 13633/20000: Train Loss = 0.436732, Test Loss = 0.253322, Learning Rate = 5.623646e-06\n",
      "Epoch 13634/20000: Train Loss = 0.436675, Test Loss = 0.250058, Learning Rate = 5.621509e-06\n",
      "Epoch 13635/20000: Train Loss = 0.435940, Test Loss = 0.248274, Learning Rate = 5.619373e-06\n",
      "Epoch 13636/20000: Train Loss = 0.436234, Test Loss = 0.250604, Learning Rate = 5.617238e-06\n",
      "Epoch 13637/20000: Train Loss = 0.436417, Test Loss = 0.249968, Learning Rate = 5.615103e-06\n",
      "Epoch 13638/20000: Train Loss = 0.435882, Test Loss = 0.250597, Learning Rate = 5.612970e-06\n",
      "Epoch 13639/20000: Train Loss = 0.436024, Test Loss = 0.256227, Learning Rate = 5.610837e-06\n",
      "Epoch 13640/20000: Train Loss = 0.436094, Test Loss = 0.258384, Learning Rate = 5.608705e-06\n",
      "Epoch 13641/20000: Train Loss = 0.436091, Test Loss = 0.251922, Learning Rate = 5.606574e-06\n",
      "Epoch 13642/20000: Train Loss = 0.436218, Test Loss = 0.251292, Learning Rate = 5.604443e-06\n",
      "Epoch 13643/20000: Train Loss = 0.436142, Test Loss = 0.250901, Learning Rate = 5.602314e-06\n",
      "Epoch 13644/20000: Train Loss = 0.436012, Test Loss = 0.251595, Learning Rate = 5.600185e-06\n",
      "Epoch 13645/20000: Train Loss = 0.435852, Test Loss = 0.252912, Learning Rate = 5.598057e-06\n",
      "Epoch 13646/20000: Train Loss = 0.435791, Test Loss = 0.252786, Learning Rate = 5.595930e-06\n",
      "Epoch 13647/20000: Train Loss = 0.435989, Test Loss = 0.250615, Learning Rate = 5.593804e-06\n",
      "Epoch 13648/20000: Train Loss = 0.435937, Test Loss = 0.248448, Learning Rate = 5.591678e-06\n",
      "Epoch 13649/20000: Train Loss = 0.435940, Test Loss = 0.247956, Learning Rate = 5.589554e-06\n",
      "Epoch 13650/20000: Train Loss = 0.436106, Test Loss = 0.253007, Learning Rate = 5.587430e-06\n",
      "Epoch 13651/20000: Train Loss = 0.436614, Test Loss = 0.255286, Learning Rate = 5.585307e-06\n",
      "Epoch 13652/20000: Train Loss = 0.436807, Test Loss = 0.254191, Learning Rate = 5.583184e-06\n",
      "Epoch 13653/20000: Train Loss = 0.436141, Test Loss = 0.252162, Learning Rate = 5.581063e-06\n",
      "Epoch 13654/20000: Train Loss = 0.436921, Test Loss = 0.256247, Learning Rate = 5.578942e-06\n",
      "Epoch 13655/20000: Train Loss = 0.436285, Test Loss = 0.254373, Learning Rate = 5.576823e-06\n",
      "Epoch 13656/20000: Train Loss = 0.435838, Test Loss = 0.252837, Learning Rate = 5.574703e-06\n",
      "Epoch 13657/20000: Train Loss = 0.435873, Test Loss = 0.249229, Learning Rate = 5.572585e-06\n",
      "Epoch 13658/20000: Train Loss = 0.436128, Test Loss = 0.251871, Learning Rate = 5.570468e-06\n",
      "Epoch 13659/20000: Train Loss = 0.436284, Test Loss = 0.252795, Learning Rate = 5.568351e-06\n",
      "Epoch 13660/20000: Train Loss = 0.436007, Test Loss = 0.253420, Learning Rate = 5.566235e-06\n",
      "Epoch 13661/20000: Train Loss = 0.435798, Test Loss = 0.254788, Learning Rate = 5.564120e-06\n",
      "Epoch 13662/20000: Train Loss = 0.436305, Test Loss = 0.251972, Learning Rate = 5.562006e-06\n",
      "Epoch 13663/20000: Train Loss = 0.435851, Test Loss = 0.254696, Learning Rate = 5.559893e-06\n",
      "Epoch 13664/20000: Train Loss = 0.437192, Test Loss = 0.252983, Learning Rate = 5.557780e-06\n",
      "Epoch 13665/20000: Train Loss = 0.436301, Test Loss = 0.253151, Learning Rate = 5.555668e-06\n",
      "Epoch 13666/20000: Train Loss = 0.436269, Test Loss = 0.254205, Learning Rate = 5.553557e-06\n",
      "Epoch 13667/20000: Train Loss = 0.436117, Test Loss = 0.254993, Learning Rate = 5.551447e-06\n",
      "Epoch 13668/20000: Train Loss = 0.435933, Test Loss = 0.251552, Learning Rate = 5.549338e-06\n",
      "Epoch 13669/20000: Train Loss = 0.435847, Test Loss = 0.252966, Learning Rate = 5.547229e-06\n",
      "Epoch 13670/20000: Train Loss = 0.435852, Test Loss = 0.252985, Learning Rate = 5.545121e-06\n",
      "Epoch 13671/20000: Train Loss = 0.435915, Test Loss = 0.254068, Learning Rate = 5.543014e-06\n",
      "Epoch 13672/20000: Train Loss = 0.435888, Test Loss = 0.253572, Learning Rate = 5.540908e-06\n",
      "Epoch 13673/20000: Train Loss = 0.436281, Test Loss = 0.253176, Learning Rate = 5.538803e-06\n",
      "Epoch 13674/20000: Train Loss = 0.436132, Test Loss = 0.256698, Learning Rate = 5.536698e-06\n",
      "Epoch 13675/20000: Train Loss = 0.435968, Test Loss = 0.252175, Learning Rate = 5.534594e-06\n",
      "Epoch 13676/20000: Train Loss = 0.436566, Test Loss = 0.251370, Learning Rate = 5.532491e-06\n",
      "Epoch 13677/20000: Train Loss = 0.435926, Test Loss = 0.253613, Learning Rate = 5.530389e-06\n",
      "Epoch 13678/20000: Train Loss = 0.436248, Test Loss = 0.256793, Learning Rate = 5.528288e-06\n",
      "Epoch 13679/20000: Train Loss = 0.435817, Test Loss = 0.251904, Learning Rate = 5.526187e-06\n",
      "Epoch 13680/20000: Train Loss = 0.435986, Test Loss = 0.253321, Learning Rate = 5.524087e-06\n",
      "Epoch 13681/20000: Train Loss = 0.436648, Test Loss = 0.251199, Learning Rate = 5.521988e-06\n",
      "Epoch 13682/20000: Train Loss = 0.436112, Test Loss = 0.251403, Learning Rate = 5.519890e-06\n",
      "Epoch 13683/20000: Train Loss = 0.435768, Test Loss = 0.251116, Learning Rate = 5.517793e-06\n",
      "Epoch 13684/20000: Train Loss = 0.435983, Test Loss = 0.251659, Learning Rate = 5.515696e-06\n",
      "Epoch 13685/20000: Train Loss = 0.436217, Test Loss = 0.253342, Learning Rate = 5.513600e-06\n",
      "Epoch 13686/20000: Train Loss = 0.436091, Test Loss = 0.247082, Learning Rate = 5.511505e-06\n",
      "Epoch 13687/20000: Train Loss = 0.436070, Test Loss = 0.252570, Learning Rate = 5.509411e-06\n",
      "Epoch 13688/20000: Train Loss = 0.435843, Test Loss = 0.247801, Learning Rate = 5.507318e-06\n",
      "Epoch 13689/20000: Train Loss = 0.436042, Test Loss = 0.249615, Learning Rate = 5.505225e-06\n",
      "Epoch 13690/20000: Train Loss = 0.436224, Test Loss = 0.252608, Learning Rate = 5.503133e-06\n",
      "Epoch 13691/20000: Train Loss = 0.436028, Test Loss = 0.254092, Learning Rate = 5.501042e-06\n",
      "Epoch 13692/20000: Train Loss = 0.436170, Test Loss = 0.253142, Learning Rate = 5.498952e-06\n",
      "Epoch 13693/20000: Train Loss = 0.436238, Test Loss = 0.253502, Learning Rate = 5.496862e-06\n",
      "Epoch 13694/20000: Train Loss = 0.435988, Test Loss = 0.251343, Learning Rate = 5.494774e-06\n",
      "Epoch 13695/20000: Train Loss = 0.435742, Test Loss = 0.252120, Learning Rate = 5.492686e-06\n",
      "Epoch 13696/20000: Train Loss = 0.435999, Test Loss = 0.251753, Learning Rate = 5.490599e-06\n",
      "Epoch 13697/20000: Train Loss = 0.436145, Test Loss = 0.256216, Learning Rate = 5.488513e-06\n",
      "Epoch 13698/20000: Train Loss = 0.436435, Test Loss = 0.251380, Learning Rate = 5.486427e-06\n",
      "Epoch 13699/20000: Train Loss = 0.435943, Test Loss = 0.253288, Learning Rate = 5.484342e-06\n",
      "Epoch 13700/20000: Train Loss = 0.436151, Test Loss = 0.258736, Learning Rate = 5.482258e-06\n",
      "Epoch 13701/20000: Train Loss = 0.435804, Test Loss = 0.256500, Learning Rate = 5.480175e-06\n",
      "Epoch 13702/20000: Train Loss = 0.436231, Test Loss = 0.253617, Learning Rate = 5.478093e-06\n",
      "Epoch 13703/20000: Train Loss = 0.435854, Test Loss = 0.256305, Learning Rate = 5.476011e-06\n",
      "Epoch 13704/20000: Train Loss = 0.436340, Test Loss = 0.253118, Learning Rate = 5.473931e-06\n",
      "Epoch 13705/20000: Train Loss = 0.436120, Test Loss = 0.255794, Learning Rate = 5.471851e-06\n",
      "Epoch 13706/20000: Train Loss = 0.436057, Test Loss = 0.254562, Learning Rate = 5.469772e-06\n",
      "Epoch 13707/20000: Train Loss = 0.437297, Test Loss = 0.254764, Learning Rate = 5.467693e-06\n",
      "Epoch 13708/20000: Train Loss = 0.436053, Test Loss = 0.248157, Learning Rate = 5.465616e-06\n",
      "Epoch 13709/20000: Train Loss = 0.435787, Test Loss = 0.249519, Learning Rate = 5.463539e-06\n",
      "Epoch 13710/20000: Train Loss = 0.435916, Test Loss = 0.248576, Learning Rate = 5.461463e-06\n",
      "Epoch 13711/20000: Train Loss = 0.435949, Test Loss = 0.248494, Learning Rate = 5.459388e-06\n",
      "Epoch 13712/20000: Train Loss = 0.436043, Test Loss = 0.248941, Learning Rate = 5.457313e-06\n",
      "Epoch 13713/20000: Train Loss = 0.435895, Test Loss = 0.248460, Learning Rate = 5.455240e-06\n",
      "Epoch 13714/20000: Train Loss = 0.436039, Test Loss = 0.251744, Learning Rate = 5.453167e-06\n",
      "Epoch 13715/20000: Train Loss = 0.436518, Test Loss = 0.252407, Learning Rate = 5.451095e-06\n",
      "Epoch 13716/20000: Train Loss = 0.436390, Test Loss = 0.251686, Learning Rate = 5.449023e-06\n",
      "Epoch 13717/20000: Train Loss = 0.435794, Test Loss = 0.250389, Learning Rate = 5.446953e-06\n",
      "Epoch 13718/20000: Train Loss = 0.435668, Test Loss = 0.252429, Learning Rate = 5.444883e-06\n",
      "Epoch 13719/20000: Train Loss = 0.435813, Test Loss = 0.251513, Learning Rate = 5.442814e-06\n",
      "Epoch 13720/20000: Train Loss = 0.435955, Test Loss = 0.251100, Learning Rate = 5.440746e-06\n",
      "Epoch 13721/20000: Train Loss = 0.435725, Test Loss = 0.249572, Learning Rate = 5.438679e-06\n",
      "Epoch 13722/20000: Train Loss = 0.435871, Test Loss = 0.250176, Learning Rate = 5.436612e-06\n",
      "Epoch 13723/20000: Train Loss = 0.436238, Test Loss = 0.248302, Learning Rate = 5.434547e-06\n",
      "Epoch 13724/20000: Train Loss = 0.436194, Test Loss = 0.251739, Learning Rate = 5.432482e-06\n",
      "Epoch 13725/20000: Train Loss = 0.436000, Test Loss = 0.252600, Learning Rate = 5.430417e-06\n",
      "Epoch 13726/20000: Train Loss = 0.436159, Test Loss = 0.250050, Learning Rate = 5.428354e-06\n",
      "Epoch 13727/20000: Train Loss = 0.435970, Test Loss = 0.247546, Learning Rate = 5.426291e-06\n",
      "Epoch 13728/20000: Train Loss = 0.436521, Test Loss = 0.249591, Learning Rate = 5.424230e-06\n",
      "Epoch 13729/20000: Train Loss = 0.435958, Test Loss = 0.250921, Learning Rate = 5.422169e-06\n",
      "Epoch 13730/20000: Train Loss = 0.435784, Test Loss = 0.252738, Learning Rate = 5.420108e-06\n",
      "Epoch 13731/20000: Train Loss = 0.436073, Test Loss = 0.251752, Learning Rate = 5.418049e-06\n",
      "Epoch 13732/20000: Train Loss = 0.435946, Test Loss = 0.249640, Learning Rate = 5.415990e-06\n",
      "Epoch 13733/20000: Train Loss = 0.435857, Test Loss = 0.256771, Learning Rate = 5.413932e-06\n",
      "Epoch 13734/20000: Train Loss = 0.435881, Test Loss = 0.253668, Learning Rate = 5.411875e-06\n",
      "Epoch 13735/20000: Train Loss = 0.436151, Test Loss = 0.256926, Learning Rate = 5.409819e-06\n",
      "Epoch 13736/20000: Train Loss = 0.436075, Test Loss = 0.255601, Learning Rate = 5.407763e-06\n",
      "Epoch 13737/20000: Train Loss = 0.435960, Test Loss = 0.251234, Learning Rate = 5.405708e-06\n",
      "Epoch 13738/20000: Train Loss = 0.436279, Test Loss = 0.251393, Learning Rate = 5.403654e-06\n",
      "Epoch 13739/20000: Train Loss = 0.436191, Test Loss = 0.251298, Learning Rate = 5.401601e-06\n",
      "Epoch 13740/20000: Train Loss = 0.435939, Test Loss = 0.250524, Learning Rate = 5.399548e-06\n",
      "Epoch 13741/20000: Train Loss = 0.436100, Test Loss = 0.251132, Learning Rate = 5.397497e-06\n",
      "Epoch 13742/20000: Train Loss = 0.436220, Test Loss = 0.249362, Learning Rate = 5.395446e-06\n",
      "Epoch 13743/20000: Train Loss = 0.436011, Test Loss = 0.249134, Learning Rate = 5.393396e-06\n",
      "Epoch 13744/20000: Train Loss = 0.436033, Test Loss = 0.252001, Learning Rate = 5.391346e-06\n",
      "Epoch 13745/20000: Train Loss = 0.435797, Test Loss = 0.253086, Learning Rate = 5.389298e-06\n",
      "Epoch 13746/20000: Train Loss = 0.435792, Test Loss = 0.251703, Learning Rate = 5.387250e-06\n",
      "Epoch 13747/20000: Train Loss = 0.435934, Test Loss = 0.250923, Learning Rate = 5.385203e-06\n",
      "Epoch 13748/20000: Train Loss = 0.436298, Test Loss = 0.252987, Learning Rate = 5.383157e-06\n",
      "Epoch 13749/20000: Train Loss = 0.436394, Test Loss = 0.250357, Learning Rate = 5.381111e-06\n",
      "Epoch 13750/20000: Train Loss = 0.436296, Test Loss = 0.252888, Learning Rate = 5.379067e-06\n",
      "Epoch 13751/20000: Train Loss = 0.435840, Test Loss = 0.248063, Learning Rate = 5.377023e-06\n",
      "Epoch 13752/20000: Train Loss = 0.436149, Test Loss = 0.253182, Learning Rate = 5.374980e-06\n",
      "Epoch 13753/20000: Train Loss = 0.435744, Test Loss = 0.255265, Learning Rate = 5.372937e-06\n",
      "Epoch 13754/20000: Train Loss = 0.435935, Test Loss = 0.254282, Learning Rate = 5.370896e-06\n",
      "Epoch 13755/20000: Train Loss = 0.436077, Test Loss = 0.254654, Learning Rate = 5.368855e-06\n",
      "Epoch 13756/20000: Train Loss = 0.436093, Test Loss = 0.251659, Learning Rate = 5.366815e-06\n",
      "Epoch 13757/20000: Train Loss = 0.436203, Test Loss = 0.254034, Learning Rate = 5.364776e-06\n",
      "Epoch 13758/20000: Train Loss = 0.436035, Test Loss = 0.250361, Learning Rate = 5.362737e-06\n",
      "Epoch 13759/20000: Train Loss = 0.436065, Test Loss = 0.252860, Learning Rate = 5.360700e-06\n",
      "Epoch 13760/20000: Train Loss = 0.435637, Test Loss = 0.253424, Learning Rate = 5.358663e-06\n",
      "Epoch 13761/20000: Train Loss = 0.435996, Test Loss = 0.252839, Learning Rate = 5.356626e-06\n",
      "Epoch 13762/20000: Train Loss = 0.435630, Test Loss = 0.253635, Learning Rate = 5.354591e-06\n",
      "Epoch 13763/20000: Train Loss = 0.436020, Test Loss = 0.253459, Learning Rate = 5.352556e-06\n",
      "Epoch 13764/20000: Train Loss = 0.435834, Test Loss = 0.253816, Learning Rate = 5.350523e-06\n",
      "Epoch 13765/20000: Train Loss = 0.436174, Test Loss = 0.251897, Learning Rate = 5.348490e-06\n",
      "Epoch 13766/20000: Train Loss = 0.435931, Test Loss = 0.252152, Learning Rate = 5.346457e-06\n",
      "Epoch 13767/20000: Train Loss = 0.435937, Test Loss = 0.253285, Learning Rate = 5.344426e-06\n",
      "Epoch 13768/20000: Train Loss = 0.436013, Test Loss = 0.252359, Learning Rate = 5.342395e-06\n",
      "Epoch 13769/20000: Train Loss = 0.436446, Test Loss = 0.254137, Learning Rate = 5.340365e-06\n",
      "Epoch 13770/20000: Train Loss = 0.436225, Test Loss = 0.253545, Learning Rate = 5.338336e-06\n",
      "Epoch 13771/20000: Train Loss = 0.435921, Test Loss = 0.255410, Learning Rate = 5.336308e-06\n",
      "Epoch 13772/20000: Train Loss = 0.435820, Test Loss = 0.253087, Learning Rate = 5.334280e-06\n",
      "Epoch 13773/20000: Train Loss = 0.436001, Test Loss = 0.252535, Learning Rate = 5.332253e-06\n",
      "Epoch 13774/20000: Train Loss = 0.436224, Test Loss = 0.253475, Learning Rate = 5.330227e-06\n",
      "Epoch 13775/20000: Train Loss = 0.435678, Test Loss = 0.253397, Learning Rate = 5.328202e-06\n",
      "Epoch 13776/20000: Train Loss = 0.435676, Test Loss = 0.253838, Learning Rate = 5.326177e-06\n",
      "Epoch 13777/20000: Train Loss = 0.436106, Test Loss = 0.253891, Learning Rate = 5.324153e-06\n",
      "Epoch 13778/20000: Train Loss = 0.435648, Test Loss = 0.252557, Learning Rate = 5.322130e-06\n",
      "Epoch 13779/20000: Train Loss = 0.436108, Test Loss = 0.257243, Learning Rate = 5.320108e-06\n",
      "Epoch 13780/20000: Train Loss = 0.435677, Test Loss = 0.253958, Learning Rate = 5.318086e-06\n",
      "Epoch 13781/20000: Train Loss = 0.436011, Test Loss = 0.255024, Learning Rate = 5.316066e-06\n",
      "Epoch 13782/20000: Train Loss = 0.436229, Test Loss = 0.253981, Learning Rate = 5.314046e-06\n",
      "Epoch 13783/20000: Train Loss = 0.435926, Test Loss = 0.251863, Learning Rate = 5.312026e-06\n",
      "Epoch 13784/20000: Train Loss = 0.435767, Test Loss = 0.253842, Learning Rate = 5.310008e-06\n",
      "Epoch 13785/20000: Train Loss = 0.435802, Test Loss = 0.250231, Learning Rate = 5.307990e-06\n",
      "Epoch 13786/20000: Train Loss = 0.436008, Test Loss = 0.249103, Learning Rate = 5.305973e-06\n",
      "Epoch 13787/20000: Train Loss = 0.435681, Test Loss = 0.253796, Learning Rate = 5.303957e-06\n",
      "Epoch 13788/20000: Train Loss = 0.435601, Test Loss = 0.255324, Learning Rate = 5.301942e-06\n",
      "Epoch 13789/20000: Train Loss = 0.436026, Test Loss = 0.252749, Learning Rate = 5.299927e-06\n",
      "Epoch 13790/20000: Train Loss = 0.436728, Test Loss = 0.248742, Learning Rate = 5.297914e-06\n",
      "Epoch 13791/20000: Train Loss = 0.436030, Test Loss = 0.252364, Learning Rate = 5.295901e-06\n",
      "Epoch 13792/20000: Train Loss = 0.435883, Test Loss = 0.254784, Learning Rate = 5.293888e-06\n",
      "Epoch 13793/20000: Train Loss = 0.435623, Test Loss = 0.254528, Learning Rate = 5.291877e-06\n",
      "Epoch 13794/20000: Train Loss = 0.435749, Test Loss = 0.254239, Learning Rate = 5.289866e-06\n",
      "Epoch 13795/20000: Train Loss = 0.436152, Test Loss = 0.254667, Learning Rate = 5.287856e-06\n",
      "Epoch 13796/20000: Train Loss = 0.435905, Test Loss = 0.252943, Learning Rate = 5.285847e-06\n",
      "Epoch 13797/20000: Train Loss = 0.435729, Test Loss = 0.251543, Learning Rate = 5.283838e-06\n",
      "Epoch 13798/20000: Train Loss = 0.436026, Test Loss = 0.255788, Learning Rate = 5.281830e-06\n",
      "Epoch 13799/20000: Train Loss = 0.436107, Test Loss = 0.253759, Learning Rate = 5.279824e-06\n",
      "Epoch 13800/20000: Train Loss = 0.436233, Test Loss = 0.252872, Learning Rate = 5.277817e-06\n",
      "Epoch 13801/20000: Train Loss = 0.436174, Test Loss = 0.251560, Learning Rate = 5.275812e-06\n",
      "Epoch 13802/20000: Train Loss = 0.436013, Test Loss = 0.254330, Learning Rate = 5.273807e-06\n",
      "Epoch 13803/20000: Train Loss = 0.436217, Test Loss = 0.253938, Learning Rate = 5.271803e-06\n",
      "Epoch 13804/20000: Train Loss = 0.435866, Test Loss = 0.249997, Learning Rate = 5.269800e-06\n",
      "Epoch 13805/20000: Train Loss = 0.435708, Test Loss = 0.251163, Learning Rate = 5.267798e-06\n",
      "Epoch 13806/20000: Train Loss = 0.436138, Test Loss = 0.251763, Learning Rate = 5.265796e-06\n",
      "Epoch 13807/20000: Train Loss = 0.435671, Test Loss = 0.250042, Learning Rate = 5.263795e-06\n",
      "Epoch 13808/20000: Train Loss = 0.436042, Test Loss = 0.251726, Learning Rate = 5.261795e-06\n",
      "Epoch 13809/20000: Train Loss = 0.435706, Test Loss = 0.252444, Learning Rate = 5.259796e-06\n",
      "Epoch 13810/20000: Train Loss = 0.435738, Test Loss = 0.251814, Learning Rate = 5.257797e-06\n",
      "Epoch 13811/20000: Train Loss = 0.435780, Test Loss = 0.251141, Learning Rate = 5.255799e-06\n",
      "Epoch 13812/20000: Train Loss = 0.435688, Test Loss = 0.246443, Learning Rate = 5.253802e-06\n",
      "Epoch 13813/20000: Train Loss = 0.436591, Test Loss = 0.253339, Learning Rate = 5.251806e-06\n",
      "Epoch 13814/20000: Train Loss = 0.435895, Test Loss = 0.250890, Learning Rate = 5.249811e-06\n",
      "Epoch 13815/20000: Train Loss = 0.435791, Test Loss = 0.253428, Learning Rate = 5.247816e-06\n",
      "Epoch 13816/20000: Train Loss = 0.435837, Test Loss = 0.250793, Learning Rate = 5.245822e-06\n",
      "Epoch 13817/20000: Train Loss = 0.436015, Test Loss = 0.252211, Learning Rate = 5.243828e-06\n",
      "Epoch 13818/20000: Train Loss = 0.435916, Test Loss = 0.250138, Learning Rate = 5.241836e-06\n",
      "Epoch 13819/20000: Train Loss = 0.435947, Test Loss = 0.249920, Learning Rate = 5.239844e-06\n",
      "Epoch 13820/20000: Train Loss = 0.435840, Test Loss = 0.248886, Learning Rate = 5.237853e-06\n",
      "Epoch 13821/20000: Train Loss = 0.436720, Test Loss = 0.250143, Learning Rate = 5.235863e-06\n",
      "Epoch 13822/20000: Train Loss = 0.435741, Test Loss = 0.253117, Learning Rate = 5.233873e-06\n",
      "Epoch 13823/20000: Train Loss = 0.436039, Test Loss = 0.249614, Learning Rate = 5.231885e-06\n",
      "Epoch 13824/20000: Train Loss = 0.435783, Test Loss = 0.249685, Learning Rate = 5.229897e-06\n",
      "Epoch 13825/20000: Train Loss = 0.435797, Test Loss = 0.253697, Learning Rate = 5.227910e-06\n",
      "Epoch 13826/20000: Train Loss = 0.435900, Test Loss = 0.251530, Learning Rate = 5.225923e-06\n",
      "Epoch 13827/20000: Train Loss = 0.436011, Test Loss = 0.255321, Learning Rate = 5.223937e-06\n",
      "Epoch 13828/20000: Train Loss = 0.435788, Test Loss = 0.252820, Learning Rate = 5.221952e-06\n",
      "Epoch 13829/20000: Train Loss = 0.436217, Test Loss = 0.254569, Learning Rate = 5.219968e-06\n",
      "Epoch 13830/20000: Train Loss = 0.435851, Test Loss = 0.253414, Learning Rate = 5.217985e-06\n",
      "Epoch 13831/20000: Train Loss = 0.436020, Test Loss = 0.254211, Learning Rate = 5.216002e-06\n",
      "Epoch 13832/20000: Train Loss = 0.436123, Test Loss = 0.250251, Learning Rate = 5.214020e-06\n",
      "Epoch 13833/20000: Train Loss = 0.435735, Test Loss = 0.249650, Learning Rate = 5.212039e-06\n",
      "Epoch 13834/20000: Train Loss = 0.436045, Test Loss = 0.255137, Learning Rate = 5.210059e-06\n",
      "Epoch 13835/20000: Train Loss = 0.435587, Test Loss = 0.252669, Learning Rate = 5.208079e-06\n",
      "Epoch 13836/20000: Train Loss = 0.435933, Test Loss = 0.254685, Learning Rate = 5.206100e-06\n",
      "Epoch 13837/20000: Train Loss = 0.435675, Test Loss = 0.253155, Learning Rate = 5.204122e-06\n",
      "Epoch 13838/20000: Train Loss = 0.435925, Test Loss = 0.250349, Learning Rate = 5.202144e-06\n",
      "Epoch 13839/20000: Train Loss = 0.435756, Test Loss = 0.253691, Learning Rate = 5.200168e-06\n",
      "Epoch 13840/20000: Train Loss = 0.435761, Test Loss = 0.256517, Learning Rate = 5.198192e-06\n",
      "Epoch 13841/20000: Train Loss = 0.435691, Test Loss = 0.255083, Learning Rate = 5.196217e-06\n",
      "Epoch 13842/20000: Train Loss = 0.436100, Test Loss = 0.250685, Learning Rate = 5.194242e-06\n",
      "Epoch 13843/20000: Train Loss = 0.435932, Test Loss = 0.253434, Learning Rate = 5.192268e-06\n",
      "Epoch 13844/20000: Train Loss = 0.435736, Test Loss = 0.252444, Learning Rate = 5.190296e-06\n",
      "Epoch 13845/20000: Train Loss = 0.435725, Test Loss = 0.251679, Learning Rate = 5.188323e-06\n",
      "Epoch 13846/20000: Train Loss = 0.436064, Test Loss = 0.254776, Learning Rate = 5.186352e-06\n",
      "Epoch 13847/20000: Train Loss = 0.435578, Test Loss = 0.252747, Learning Rate = 5.184381e-06\n",
      "Epoch 13848/20000: Train Loss = 0.436950, Test Loss = 0.254107, Learning Rate = 5.182411e-06\n",
      "Epoch 13849/20000: Train Loss = 0.435739, Test Loss = 0.251312, Learning Rate = 5.180442e-06\n",
      "Epoch 13850/20000: Train Loss = 0.435691, Test Loss = 0.251762, Learning Rate = 5.178474e-06\n",
      "Epoch 13851/20000: Train Loss = 0.435931, Test Loss = 0.253763, Learning Rate = 5.176506e-06\n",
      "Epoch 13852/20000: Train Loss = 0.436382, Test Loss = 0.251282, Learning Rate = 5.174539e-06\n",
      "Epoch 13853/20000: Train Loss = 0.437321, Test Loss = 0.258556, Learning Rate = 5.172573e-06\n",
      "Epoch 13854/20000: Train Loss = 0.436113, Test Loss = 0.256399, Learning Rate = 5.170608e-06\n",
      "Epoch 13855/20000: Train Loss = 0.436020, Test Loss = 0.254435, Learning Rate = 5.168643e-06\n",
      "Epoch 13856/20000: Train Loss = 0.435985, Test Loss = 0.253510, Learning Rate = 5.166679e-06\n",
      "Epoch 13857/20000: Train Loss = 0.435912, Test Loss = 0.253100, Learning Rate = 5.164716e-06\n",
      "Epoch 13858/20000: Train Loss = 0.435849, Test Loss = 0.250352, Learning Rate = 5.162753e-06\n",
      "Epoch 13859/20000: Train Loss = 0.435754, Test Loss = 0.254574, Learning Rate = 5.160792e-06\n",
      "Epoch 13860/20000: Train Loss = 0.436074, Test Loss = 0.254941, Learning Rate = 5.158831e-06\n",
      "Epoch 13861/20000: Train Loss = 0.436273, Test Loss = 0.254290, Learning Rate = 5.156870e-06\n",
      "Epoch 13862/20000: Train Loss = 0.436208, Test Loss = 0.253156, Learning Rate = 5.154911e-06\n",
      "Epoch 13863/20000: Train Loss = 0.435905, Test Loss = 0.251815, Learning Rate = 5.152952e-06\n",
      "Epoch 13864/20000: Train Loss = 0.435824, Test Loss = 0.252502, Learning Rate = 5.150994e-06\n",
      "Epoch 13865/20000: Train Loss = 0.436224, Test Loss = 0.251148, Learning Rate = 5.149037e-06\n",
      "Epoch 13866/20000: Train Loss = 0.436906, Test Loss = 0.252768, Learning Rate = 5.147080e-06\n",
      "Epoch 13867/20000: Train Loss = 0.436803, Test Loss = 0.253876, Learning Rate = 5.145125e-06\n",
      "Epoch 13868/20000: Train Loss = 0.436137, Test Loss = 0.250950, Learning Rate = 5.143170e-06\n",
      "Epoch 13869/20000: Train Loss = 0.435639, Test Loss = 0.253309, Learning Rate = 5.141215e-06\n",
      "Epoch 13870/20000: Train Loss = 0.436272, Test Loss = 0.249304, Learning Rate = 5.139262e-06\n",
      "Epoch 13871/20000: Train Loss = 0.435854, Test Loss = 0.247782, Learning Rate = 5.137309e-06\n",
      "Epoch 13872/20000: Train Loss = 0.435882, Test Loss = 0.247960, Learning Rate = 5.135357e-06\n",
      "Epoch 13873/20000: Train Loss = 0.435746, Test Loss = 0.245993, Learning Rate = 5.133406e-06\n",
      "Epoch 13874/20000: Train Loss = 0.435911, Test Loss = 0.250695, Learning Rate = 5.131455e-06\n",
      "Epoch 13875/20000: Train Loss = 0.436365, Test Loss = 0.249004, Learning Rate = 5.129505e-06\n",
      "Epoch 13876/20000: Train Loss = 0.435905, Test Loss = 0.248058, Learning Rate = 5.127556e-06\n",
      "Epoch 13877/20000: Train Loss = 0.436128, Test Loss = 0.247726, Learning Rate = 5.125608e-06\n",
      "Epoch 13878/20000: Train Loss = 0.436464, Test Loss = 0.252157, Learning Rate = 5.123660e-06\n",
      "Epoch 13879/20000: Train Loss = 0.436720, Test Loss = 0.248580, Learning Rate = 5.121714e-06\n",
      "Epoch 13880/20000: Train Loss = 0.436170, Test Loss = 0.244558, Learning Rate = 5.119767e-06\n",
      "Epoch 13881/20000: Train Loss = 0.435803, Test Loss = 0.246989, Learning Rate = 5.117822e-06\n",
      "Epoch 13882/20000: Train Loss = 0.435808, Test Loss = 0.247351, Learning Rate = 5.115877e-06\n",
      "Epoch 13883/20000: Train Loss = 0.435710, Test Loss = 0.247841, Learning Rate = 5.113934e-06\n",
      "Epoch 13884/20000: Train Loss = 0.435805, Test Loss = 0.251379, Learning Rate = 5.111990e-06\n",
      "Epoch 13885/20000: Train Loss = 0.435777, Test Loss = 0.249386, Learning Rate = 5.110048e-06\n",
      "Epoch 13886/20000: Train Loss = 0.435708, Test Loss = 0.248840, Learning Rate = 5.108106e-06\n",
      "Epoch 13887/20000: Train Loss = 0.435768, Test Loss = 0.248512, Learning Rate = 5.106165e-06\n",
      "Epoch 13888/20000: Train Loss = 0.435903, Test Loss = 0.251181, Learning Rate = 5.104225e-06\n",
      "Epoch 13889/20000: Train Loss = 0.435605, Test Loss = 0.253020, Learning Rate = 5.102286e-06\n",
      "Epoch 13890/20000: Train Loss = 0.435808, Test Loss = 0.250825, Learning Rate = 5.100347e-06\n",
      "Epoch 13891/20000: Train Loss = 0.435935, Test Loss = 0.251434, Learning Rate = 5.098409e-06\n",
      "Epoch 13892/20000: Train Loss = 0.435795, Test Loss = 0.250410, Learning Rate = 5.096472e-06\n",
      "Epoch 13893/20000: Train Loss = 0.435765, Test Loss = 0.249731, Learning Rate = 5.094535e-06\n",
      "Epoch 13894/20000: Train Loss = 0.435868, Test Loss = 0.255029, Learning Rate = 5.092599e-06\n",
      "Epoch 13895/20000: Train Loss = 0.435859, Test Loss = 0.249031, Learning Rate = 5.090664e-06\n",
      "Epoch 13896/20000: Train Loss = 0.435786, Test Loss = 0.251590, Learning Rate = 5.088730e-06\n",
      "Epoch 13897/20000: Train Loss = 0.435515, Test Loss = 0.252931, Learning Rate = 5.086796e-06\n",
      "Epoch 13898/20000: Train Loss = 0.435744, Test Loss = 0.251116, Learning Rate = 5.084864e-06\n",
      "Epoch 13899/20000: Train Loss = 0.435711, Test Loss = 0.248648, Learning Rate = 5.082931e-06\n",
      "Epoch 13900/20000: Train Loss = 0.435888, Test Loss = 0.252725, Learning Rate = 5.081000e-06\n",
      "Epoch 13901/20000: Train Loss = 0.435754, Test Loss = 0.252972, Learning Rate = 5.079069e-06\n",
      "Epoch 13902/20000: Train Loss = 0.435698, Test Loss = 0.254394, Learning Rate = 5.077140e-06\n",
      "Epoch 13903/20000: Train Loss = 0.435751, Test Loss = 0.254570, Learning Rate = 5.075210e-06\n",
      "Epoch 13904/20000: Train Loss = 0.436005, Test Loss = 0.254979, Learning Rate = 5.073282e-06\n",
      "Epoch 13905/20000: Train Loss = 0.435545, Test Loss = 0.251607, Learning Rate = 5.071354e-06\n",
      "Epoch 13906/20000: Train Loss = 0.436217, Test Loss = 0.251729, Learning Rate = 5.069427e-06\n",
      "Epoch 13907/20000: Train Loss = 0.436002, Test Loss = 0.253044, Learning Rate = 5.067501e-06\n",
      "Epoch 13908/20000: Train Loss = 0.436274, Test Loss = 0.252054, Learning Rate = 5.065575e-06\n",
      "Epoch 13909/20000: Train Loss = 0.436237, Test Loss = 0.250305, Learning Rate = 5.063651e-06\n",
      "Epoch 13910/20000: Train Loss = 0.435694, Test Loss = 0.249622, Learning Rate = 5.061727e-06\n",
      "Epoch 13911/20000: Train Loss = 0.435690, Test Loss = 0.251861, Learning Rate = 5.059803e-06\n",
      "Epoch 13912/20000: Train Loss = 0.435931, Test Loss = 0.253165, Learning Rate = 5.057881e-06\n",
      "Epoch 13913/20000: Train Loss = 0.436245, Test Loss = 0.255681, Learning Rate = 5.055959e-06\n",
      "Epoch 13914/20000: Train Loss = 0.435485, Test Loss = 0.251527, Learning Rate = 5.054038e-06\n",
      "Epoch 13915/20000: Train Loss = 0.435802, Test Loss = 0.251599, Learning Rate = 5.052117e-06\n",
      "Epoch 13916/20000: Train Loss = 0.435686, Test Loss = 0.253232, Learning Rate = 5.050198e-06\n",
      "Epoch 13917/20000: Train Loss = 0.436792, Test Loss = 0.248611, Learning Rate = 5.048279e-06\n",
      "Epoch 13918/20000: Train Loss = 0.435790, Test Loss = 0.254974, Learning Rate = 5.046361e-06\n",
      "Epoch 13919/20000: Train Loss = 0.435874, Test Loss = 0.251336, Learning Rate = 5.044443e-06\n",
      "Epoch 13920/20000: Train Loss = 0.435939, Test Loss = 0.250587, Learning Rate = 5.042526e-06\n",
      "Epoch 13921/20000: Train Loss = 0.435922, Test Loss = 0.252932, Learning Rate = 5.040610e-06\n",
      "Epoch 13922/20000: Train Loss = 0.435697, Test Loss = 0.254272, Learning Rate = 5.038695e-06\n",
      "Epoch 13923/20000: Train Loss = 0.435955, Test Loss = 0.251082, Learning Rate = 5.036780e-06\n",
      "Epoch 13924/20000: Train Loss = 0.435866, Test Loss = 0.251120, Learning Rate = 5.034867e-06\n",
      "Epoch 13925/20000: Train Loss = 0.436202, Test Loss = 0.254403, Learning Rate = 5.032953e-06\n",
      "Epoch 13926/20000: Train Loss = 0.436200, Test Loss = 0.252759, Learning Rate = 5.031041e-06\n",
      "Epoch 13927/20000: Train Loss = 0.435941, Test Loss = 0.248659, Learning Rate = 5.029129e-06\n",
      "Epoch 13928/20000: Train Loss = 0.435711, Test Loss = 0.250623, Learning Rate = 5.027219e-06\n",
      "Epoch 13929/20000: Train Loss = 0.435687, Test Loss = 0.251283, Learning Rate = 5.025308e-06\n",
      "Epoch 13930/20000: Train Loss = 0.436408, Test Loss = 0.250668, Learning Rate = 5.023399e-06\n",
      "Epoch 13931/20000: Train Loss = 0.435866, Test Loss = 0.247371, Learning Rate = 5.021490e-06\n",
      "Epoch 13932/20000: Train Loss = 0.435922, Test Loss = 0.248313, Learning Rate = 5.019582e-06\n",
      "Epoch 13933/20000: Train Loss = 0.436107, Test Loss = 0.251895, Learning Rate = 5.017675e-06\n",
      "Epoch 13934/20000: Train Loss = 0.435730, Test Loss = 0.253282, Learning Rate = 5.015768e-06\n",
      "Epoch 13935/20000: Train Loss = 0.435869, Test Loss = 0.249427, Learning Rate = 5.013862e-06\n",
      "Epoch 13936/20000: Train Loss = 0.435972, Test Loss = 0.254656, Learning Rate = 5.011957e-06\n",
      "Epoch 13937/20000: Train Loss = 0.437206, Test Loss = 0.252391, Learning Rate = 5.010053e-06\n",
      "Epoch 13938/20000: Train Loss = 0.436034, Test Loss = 0.253101, Learning Rate = 5.008149e-06\n",
      "Epoch 13939/20000: Train Loss = 0.436570, Test Loss = 0.251376, Learning Rate = 5.006246e-06\n",
      "Epoch 13940/20000: Train Loss = 0.436881, Test Loss = 0.256143, Learning Rate = 5.004344e-06\n",
      "Epoch 13941/20000: Train Loss = 0.435905, Test Loss = 0.248106, Learning Rate = 5.002442e-06\n",
      "Epoch 13942/20000: Train Loss = 0.435896, Test Loss = 0.249771, Learning Rate = 5.000542e-06\n",
      "Epoch 13943/20000: Train Loss = 0.435766, Test Loss = 0.251911, Learning Rate = 4.998641e-06\n",
      "Epoch 13944/20000: Train Loss = 0.435569, Test Loss = 0.252313, Learning Rate = 4.996742e-06\n",
      "Epoch 13945/20000: Train Loss = 0.435628, Test Loss = 0.250772, Learning Rate = 4.994844e-06\n",
      "Epoch 13946/20000: Train Loss = 0.435814, Test Loss = 0.254667, Learning Rate = 4.992946e-06\n",
      "Epoch 13947/20000: Train Loss = 0.435468, Test Loss = 0.252067, Learning Rate = 4.991048e-06\n",
      "Epoch 13948/20000: Train Loss = 0.435699, Test Loss = 0.251687, Learning Rate = 4.989152e-06\n",
      "Epoch 13949/20000: Train Loss = 0.435756, Test Loss = 0.253510, Learning Rate = 4.987256e-06\n",
      "Epoch 13950/20000: Train Loss = 0.436087, Test Loss = 0.250351, Learning Rate = 4.985361e-06\n",
      "Epoch 13951/20000: Train Loss = 0.435793, Test Loss = 0.248184, Learning Rate = 4.983467e-06\n",
      "Epoch 13952/20000: Train Loss = 0.435616, Test Loss = 0.252431, Learning Rate = 4.981573e-06\n",
      "Epoch 13953/20000: Train Loss = 0.435677, Test Loss = 0.252960, Learning Rate = 4.979680e-06\n",
      "Epoch 13954/20000: Train Loss = 0.436534, Test Loss = 0.254034, Learning Rate = 4.977788e-06\n",
      "Epoch 13955/20000: Train Loss = 0.436448, Test Loss = 0.251107, Learning Rate = 4.975897e-06\n",
      "Epoch 13956/20000: Train Loss = 0.435888, Test Loss = 0.252330, Learning Rate = 4.974006e-06\n",
      "Epoch 13957/20000: Train Loss = 0.435951, Test Loss = 0.251392, Learning Rate = 4.972116e-06\n",
      "Epoch 13958/20000: Train Loss = 0.435655, Test Loss = 0.250964, Learning Rate = 4.970227e-06\n",
      "Epoch 13959/20000: Train Loss = 0.435642, Test Loss = 0.250196, Learning Rate = 4.968338e-06\n",
      "Epoch 13960/20000: Train Loss = 0.435917, Test Loss = 0.248133, Learning Rate = 4.966451e-06\n",
      "Epoch 13961/20000: Train Loss = 0.435820, Test Loss = 0.252144, Learning Rate = 4.964563e-06\n",
      "Epoch 13962/20000: Train Loss = 0.436149, Test Loss = 0.252129, Learning Rate = 4.962677e-06\n",
      "Epoch 13963/20000: Train Loss = 0.436068, Test Loss = 0.250551, Learning Rate = 4.960791e-06\n",
      "Epoch 13964/20000: Train Loss = 0.435903, Test Loss = 0.249807, Learning Rate = 4.958906e-06\n",
      "Epoch 13965/20000: Train Loss = 0.437587, Test Loss = 0.251372, Learning Rate = 4.957022e-06\n",
      "Epoch 13966/20000: Train Loss = 0.436058, Test Loss = 0.252795, Learning Rate = 4.955139e-06\n",
      "Epoch 13967/20000: Train Loss = 0.435882, Test Loss = 0.254069, Learning Rate = 4.953256e-06\n",
      "Epoch 13968/20000: Train Loss = 0.435802, Test Loss = 0.253982, Learning Rate = 4.951374e-06\n",
      "Epoch 13969/20000: Train Loss = 0.435565, Test Loss = 0.254517, Learning Rate = 4.949492e-06\n",
      "Epoch 13970/20000: Train Loss = 0.435598, Test Loss = 0.254792, Learning Rate = 4.947612e-06\n",
      "Epoch 13971/20000: Train Loss = 0.435751, Test Loss = 0.252534, Learning Rate = 4.945732e-06\n",
      "Epoch 13972/20000: Train Loss = 0.436264, Test Loss = 0.250771, Learning Rate = 4.943852e-06\n",
      "Epoch 13973/20000: Train Loss = 0.435776, Test Loss = 0.252364, Learning Rate = 4.941974e-06\n",
      "Epoch 13974/20000: Train Loss = 0.435725, Test Loss = 0.251643, Learning Rate = 4.940096e-06\n",
      "Epoch 13975/20000: Train Loss = 0.435895, Test Loss = 0.250944, Learning Rate = 4.938219e-06\n",
      "Epoch 13976/20000: Train Loss = 0.436058, Test Loss = 0.250717, Learning Rate = 4.936343e-06\n",
      "Epoch 13977/20000: Train Loss = 0.436210, Test Loss = 0.253479, Learning Rate = 4.934467e-06\n",
      "Epoch 13978/20000: Train Loss = 0.435706, Test Loss = 0.251665, Learning Rate = 4.932592e-06\n",
      "Epoch 13979/20000: Train Loss = 0.435542, Test Loss = 0.250572, Learning Rate = 4.930718e-06\n",
      "Epoch 13980/20000: Train Loss = 0.435852, Test Loss = 0.253071, Learning Rate = 4.928844e-06\n",
      "Epoch 13981/20000: Train Loss = 0.435829, Test Loss = 0.251998, Learning Rate = 4.926971e-06\n",
      "Epoch 13982/20000: Train Loss = 0.435759, Test Loss = 0.252074, Learning Rate = 4.925099e-06\n",
      "Epoch 13983/20000: Train Loss = 0.435930, Test Loss = 0.251095, Learning Rate = 4.923228e-06\n",
      "Epoch 13984/20000: Train Loss = 0.435689, Test Loss = 0.251401, Learning Rate = 4.921357e-06\n",
      "Epoch 13985/20000: Train Loss = 0.435446, Test Loss = 0.250882, Learning Rate = 4.919487e-06\n",
      "Epoch 13986/20000: Train Loss = 0.435969, Test Loss = 0.251483, Learning Rate = 4.917618e-06\n",
      "Epoch 13987/20000: Train Loss = 0.435768, Test Loss = 0.249818, Learning Rate = 4.915749e-06\n",
      "Epoch 13988/20000: Train Loss = 0.435761, Test Loss = 0.250707, Learning Rate = 4.913881e-06\n",
      "Epoch 13989/20000: Train Loss = 0.435819, Test Loss = 0.253456, Learning Rate = 4.912014e-06\n",
      "Epoch 13990/20000: Train Loss = 0.435766, Test Loss = 0.251055, Learning Rate = 4.910148e-06\n",
      "Epoch 13991/20000: Train Loss = 0.435708, Test Loss = 0.250757, Learning Rate = 4.908282e-06\n",
      "Epoch 13992/20000: Train Loss = 0.435557, Test Loss = 0.249851, Learning Rate = 4.906417e-06\n",
      "Epoch 13993/20000: Train Loss = 0.435911, Test Loss = 0.253300, Learning Rate = 4.904553e-06\n",
      "Epoch 13994/20000: Train Loss = 0.435950, Test Loss = 0.247585, Learning Rate = 4.902689e-06\n",
      "Epoch 13995/20000: Train Loss = 0.435841, Test Loss = 0.248066, Learning Rate = 4.900826e-06\n",
      "Epoch 13996/20000: Train Loss = 0.435626, Test Loss = 0.251201, Learning Rate = 4.898964e-06\n",
      "Epoch 13997/20000: Train Loss = 0.436183, Test Loss = 0.250723, Learning Rate = 4.897103e-06\n",
      "Epoch 13998/20000: Train Loss = 0.435969, Test Loss = 0.252250, Learning Rate = 4.895242e-06\n",
      "Epoch 13999/20000: Train Loss = 0.435687, Test Loss = 0.250764, Learning Rate = 4.893382e-06\n",
      "Epoch 14000/20000: Train Loss = 0.436093, Test Loss = 0.251181, Learning Rate = 4.891522e-06\n",
      "Epoch 14001/20000: Train Loss = 0.436014, Test Loss = 0.250988, Learning Rate = 4.889664e-06\n",
      "Epoch 14002/20000: Train Loss = 0.436035, Test Loss = 0.255014, Learning Rate = 4.887806e-06\n",
      "Epoch 14003/20000: Train Loss = 0.436476, Test Loss = 0.253074, Learning Rate = 4.885949e-06\n",
      "Epoch 14004/20000: Train Loss = 0.436104, Test Loss = 0.254521, Learning Rate = 4.884092e-06\n",
      "Epoch 14005/20000: Train Loss = 0.435575, Test Loss = 0.255253, Learning Rate = 4.882236e-06\n",
      "Epoch 14006/20000: Train Loss = 0.435787, Test Loss = 0.252972, Learning Rate = 4.880381e-06\n",
      "Epoch 14007/20000: Train Loss = 0.435640, Test Loss = 0.253181, Learning Rate = 4.878527e-06\n",
      "Epoch 14008/20000: Train Loss = 0.435637, Test Loss = 0.253943, Learning Rate = 4.876673e-06\n",
      "Epoch 14009/20000: Train Loss = 0.435922, Test Loss = 0.253118, Learning Rate = 4.874820e-06\n",
      "Epoch 14010/20000: Train Loss = 0.436163, Test Loss = 0.251899, Learning Rate = 4.872968e-06\n",
      "Epoch 14011/20000: Train Loss = 0.435639, Test Loss = 0.248418, Learning Rate = 4.871116e-06\n",
      "Epoch 14012/20000: Train Loss = 0.435664, Test Loss = 0.251541, Learning Rate = 4.869265e-06\n",
      "Epoch 14013/20000: Train Loss = 0.435754, Test Loss = 0.253313, Learning Rate = 4.867415e-06\n",
      "Epoch 14014/20000: Train Loss = 0.436395, Test Loss = 0.251393, Learning Rate = 4.865566e-06\n",
      "Epoch 14015/20000: Train Loss = 0.436255, Test Loss = 0.254312, Learning Rate = 4.863717e-06\n",
      "Epoch 14016/20000: Train Loss = 0.437373, Test Loss = 0.250192, Learning Rate = 4.861869e-06\n",
      "Epoch 14017/20000: Train Loss = 0.436203, Test Loss = 0.248300, Learning Rate = 4.860021e-06\n",
      "Epoch 14018/20000: Train Loss = 0.436085, Test Loss = 0.252088, Learning Rate = 4.858175e-06\n",
      "Epoch 14019/20000: Train Loss = 0.435830, Test Loss = 0.250727, Learning Rate = 4.856329e-06\n",
      "Epoch 14020/20000: Train Loss = 0.435845, Test Loss = 0.251441, Learning Rate = 4.854483e-06\n",
      "Epoch 14021/20000: Train Loss = 0.435748, Test Loss = 0.252352, Learning Rate = 4.852639e-06\n",
      "Epoch 14022/20000: Train Loss = 0.435766, Test Loss = 0.251134, Learning Rate = 4.850795e-06\n",
      "Epoch 14023/20000: Train Loss = 0.436177, Test Loss = 0.254392, Learning Rate = 4.848952e-06\n",
      "Epoch 14024/20000: Train Loss = 0.435754, Test Loss = 0.249807, Learning Rate = 4.847109e-06\n",
      "Epoch 14025/20000: Train Loss = 0.435715, Test Loss = 0.249681, Learning Rate = 4.845268e-06\n",
      "Epoch 14026/20000: Train Loss = 0.436210, Test Loss = 0.251008, Learning Rate = 4.843427e-06\n",
      "Epoch 14027/20000: Train Loss = 0.436086, Test Loss = 0.250649, Learning Rate = 4.841586e-06\n",
      "Epoch 14028/20000: Train Loss = 0.435797, Test Loss = 0.254420, Learning Rate = 4.839746e-06\n",
      "Epoch 14029/20000: Train Loss = 0.435829, Test Loss = 0.252414, Learning Rate = 4.837908e-06\n",
      "Epoch 14030/20000: Train Loss = 0.435831, Test Loss = 0.254387, Learning Rate = 4.836069e-06\n",
      "Epoch 14031/20000: Train Loss = 0.435810, Test Loss = 0.252139, Learning Rate = 4.834232e-06\n",
      "Epoch 14032/20000: Train Loss = 0.435811, Test Loss = 0.247976, Learning Rate = 4.832395e-06\n",
      "Epoch 14033/20000: Train Loss = 0.435893, Test Loss = 0.250064, Learning Rate = 4.830559e-06\n",
      "Epoch 14034/20000: Train Loss = 0.435829, Test Loss = 0.254282, Learning Rate = 4.828723e-06\n",
      "Epoch 14035/20000: Train Loss = 0.435564, Test Loss = 0.251633, Learning Rate = 4.826888e-06\n",
      "Epoch 14036/20000: Train Loss = 0.435697, Test Loss = 0.251036, Learning Rate = 4.825054e-06\n",
      "Epoch 14037/20000: Train Loss = 0.436285, Test Loss = 0.255155, Learning Rate = 4.823221e-06\n",
      "Epoch 14038/20000: Train Loss = 0.436787, Test Loss = 0.251360, Learning Rate = 4.821388e-06\n",
      "Epoch 14039/20000: Train Loss = 0.436200, Test Loss = 0.250739, Learning Rate = 4.819556e-06\n",
      "Epoch 14040/20000: Train Loss = 0.435484, Test Loss = 0.256257, Learning Rate = 4.817725e-06\n",
      "Epoch 14041/20000: Train Loss = 0.435937, Test Loss = 0.252729, Learning Rate = 4.815894e-06\n",
      "Epoch 14042/20000: Train Loss = 0.435642, Test Loss = 0.253324, Learning Rate = 4.814064e-06\n",
      "Epoch 14043/20000: Train Loss = 0.435774, Test Loss = 0.249470, Learning Rate = 4.812235e-06\n",
      "Epoch 14044/20000: Train Loss = 0.435660, Test Loss = 0.251283, Learning Rate = 4.810407e-06\n",
      "Epoch 14045/20000: Train Loss = 0.435830, Test Loss = 0.254732, Learning Rate = 4.808579e-06\n",
      "Epoch 14046/20000: Train Loss = 0.435955, Test Loss = 0.251790, Learning Rate = 4.806752e-06\n",
      "Epoch 14047/20000: Train Loss = 0.435584, Test Loss = 0.252583, Learning Rate = 4.804925e-06\n",
      "Epoch 14048/20000: Train Loss = 0.435819, Test Loss = 0.255467, Learning Rate = 4.803099e-06\n",
      "Epoch 14049/20000: Train Loss = 0.435689, Test Loss = 0.253554, Learning Rate = 4.801274e-06\n",
      "Epoch 14050/20000: Train Loss = 0.435640, Test Loss = 0.256470, Learning Rate = 4.799450e-06\n",
      "Epoch 14051/20000: Train Loss = 0.435903, Test Loss = 0.256088, Learning Rate = 4.797626e-06\n",
      "Epoch 14052/20000: Train Loss = 0.435605, Test Loss = 0.253705, Learning Rate = 4.795803e-06\n",
      "Epoch 14053/20000: Train Loss = 0.435900, Test Loss = 0.256518, Learning Rate = 4.793981e-06\n",
      "Epoch 14054/20000: Train Loss = 0.435600, Test Loss = 0.252989, Learning Rate = 4.792160e-06\n",
      "Epoch 14055/20000: Train Loss = 0.435856, Test Loss = 0.253451, Learning Rate = 4.790339e-06\n",
      "Epoch 14056/20000: Train Loss = 0.436068, Test Loss = 0.254028, Learning Rate = 4.788519e-06\n",
      "Epoch 14057/20000: Train Loss = 0.435946, Test Loss = 0.252118, Learning Rate = 4.786699e-06\n",
      "Epoch 14058/20000: Train Loss = 0.435514, Test Loss = 0.252009, Learning Rate = 4.784880e-06\n",
      "Epoch 14059/20000: Train Loss = 0.435822, Test Loss = 0.252407, Learning Rate = 4.783062e-06\n",
      "Epoch 14060/20000: Train Loss = 0.435644, Test Loss = 0.252797, Learning Rate = 4.781245e-06\n",
      "Epoch 14061/20000: Train Loss = 0.435777, Test Loss = 0.252305, Learning Rate = 4.779428e-06\n",
      "Epoch 14062/20000: Train Loss = 0.435737, Test Loss = 0.254022, Learning Rate = 4.777612e-06\n",
      "Epoch 14063/20000: Train Loss = 0.435542, Test Loss = 0.249816, Learning Rate = 4.775796e-06\n",
      "Epoch 14064/20000: Train Loss = 0.435793, Test Loss = 0.253265, Learning Rate = 4.773982e-06\n",
      "Epoch 14065/20000: Train Loss = 0.435728, Test Loss = 0.251969, Learning Rate = 4.772168e-06\n",
      "Epoch 14066/20000: Train Loss = 0.435884, Test Loss = 0.252012, Learning Rate = 4.770355e-06\n",
      "Epoch 14067/20000: Train Loss = 0.435939, Test Loss = 0.252916, Learning Rate = 4.768542e-06\n",
      "Epoch 14068/20000: Train Loss = 0.435784, Test Loss = 0.254092, Learning Rate = 4.766730e-06\n",
      "Epoch 14069/20000: Train Loss = 0.435532, Test Loss = 0.250072, Learning Rate = 4.764919e-06\n",
      "Epoch 14070/20000: Train Loss = 0.436147, Test Loss = 0.251078, Learning Rate = 4.763108e-06\n",
      "Epoch 14071/20000: Train Loss = 0.435817, Test Loss = 0.251641, Learning Rate = 4.761298e-06\n",
      "Epoch 14072/20000: Train Loss = 0.435639, Test Loss = 0.252106, Learning Rate = 4.759489e-06\n",
      "Epoch 14073/20000: Train Loss = 0.436019, Test Loss = 0.251736, Learning Rate = 4.757681e-06\n",
      "Epoch 14074/20000: Train Loss = 0.435859, Test Loss = 0.253140, Learning Rate = 4.755873e-06\n",
      "Epoch 14075/20000: Train Loss = 0.435479, Test Loss = 0.255237, Learning Rate = 4.754066e-06\n",
      "Epoch 14076/20000: Train Loss = 0.435748, Test Loss = 0.252140, Learning Rate = 4.752259e-06\n",
      "Epoch 14077/20000: Train Loss = 0.436001, Test Loss = 0.255036, Learning Rate = 4.750454e-06\n",
      "Epoch 14078/20000: Train Loss = 0.435975, Test Loss = 0.253641, Learning Rate = 4.748649e-06\n",
      "Epoch 14079/20000: Train Loss = 0.435873, Test Loss = 0.250918, Learning Rate = 4.746844e-06\n",
      "Epoch 14080/20000: Train Loss = 0.435809, Test Loss = 0.252781, Learning Rate = 4.745041e-06\n",
      "Epoch 14081/20000: Train Loss = 0.435735, Test Loss = 0.253465, Learning Rate = 4.743238e-06\n",
      "Epoch 14082/20000: Train Loss = 0.435887, Test Loss = 0.251213, Learning Rate = 4.741435e-06\n",
      "Epoch 14083/20000: Train Loss = 0.436376, Test Loss = 0.251860, Learning Rate = 4.739634e-06\n",
      "Epoch 14084/20000: Train Loss = 0.435364, Test Loss = 0.251257, Learning Rate = 4.737833e-06\n",
      "Epoch 14085/20000: Train Loss = 0.435793, Test Loss = 0.252659, Learning Rate = 4.736033e-06\n",
      "Epoch 14086/20000: Train Loss = 0.435687, Test Loss = 0.249425, Learning Rate = 4.734233e-06\n",
      "Epoch 14087/20000: Train Loss = 0.435509, Test Loss = 0.250948, Learning Rate = 4.732434e-06\n",
      "Epoch 14088/20000: Train Loss = 0.435731, Test Loss = 0.251105, Learning Rate = 4.730636e-06\n",
      "Epoch 14089/20000: Train Loss = 0.435681, Test Loss = 0.253825, Learning Rate = 4.728838e-06\n",
      "Epoch 14090/20000: Train Loss = 0.435576, Test Loss = 0.253454, Learning Rate = 4.727042e-06\n",
      "Epoch 14091/20000: Train Loss = 0.435617, Test Loss = 0.250765, Learning Rate = 4.725245e-06\n",
      "Epoch 14092/20000: Train Loss = 0.436051, Test Loss = 0.253725, Learning Rate = 4.723450e-06\n",
      "Epoch 14093/20000: Train Loss = 0.435712, Test Loss = 0.252198, Learning Rate = 4.721655e-06\n",
      "Epoch 14094/20000: Train Loss = 0.435562, Test Loss = 0.253740, Learning Rate = 4.719861e-06\n",
      "Epoch 14095/20000: Train Loss = 0.435548, Test Loss = 0.253197, Learning Rate = 4.718068e-06\n",
      "Epoch 14096/20000: Train Loss = 0.435584, Test Loss = 0.251322, Learning Rate = 4.716275e-06\n",
      "Epoch 14097/20000: Train Loss = 0.435442, Test Loss = 0.251642, Learning Rate = 4.714483e-06\n",
      "Epoch 14098/20000: Train Loss = 0.435477, Test Loss = 0.252086, Learning Rate = 4.712691e-06\n",
      "Epoch 14099/20000: Train Loss = 0.435832, Test Loss = 0.251355, Learning Rate = 4.710901e-06\n",
      "Epoch 14100/20000: Train Loss = 0.436786, Test Loss = 0.255506, Learning Rate = 4.709111e-06\n",
      "Epoch 14101/20000: Train Loss = 0.435719, Test Loss = 0.250866, Learning Rate = 4.707321e-06\n",
      "Epoch 14102/20000: Train Loss = 0.435717, Test Loss = 0.255663, Learning Rate = 4.705533e-06\n",
      "Epoch 14103/20000: Train Loss = 0.436853, Test Loss = 0.255561, Learning Rate = 4.703745e-06\n",
      "Epoch 14104/20000: Train Loss = 0.436043, Test Loss = 0.253462, Learning Rate = 4.701957e-06\n",
      "Epoch 14105/20000: Train Loss = 0.435810, Test Loss = 0.250781, Learning Rate = 4.700171e-06\n",
      "Epoch 14106/20000: Train Loss = 0.435902, Test Loss = 0.255056, Learning Rate = 4.698385e-06\n",
      "Epoch 14107/20000: Train Loss = 0.436294, Test Loss = 0.252060, Learning Rate = 4.696600e-06\n",
      "Epoch 14108/20000: Train Loss = 0.435655, Test Loss = 0.252060, Learning Rate = 4.694815e-06\n",
      "Epoch 14109/20000: Train Loss = 0.435596, Test Loss = 0.254992, Learning Rate = 4.693031e-06\n",
      "Epoch 14110/20000: Train Loss = 0.435591, Test Loss = 0.250593, Learning Rate = 4.691248e-06\n",
      "Epoch 14111/20000: Train Loss = 0.435549, Test Loss = 0.252666, Learning Rate = 4.689465e-06\n",
      "Epoch 14112/20000: Train Loss = 0.435947, Test Loss = 0.251559, Learning Rate = 4.687684e-06\n",
      "Epoch 14113/20000: Train Loss = 0.436009, Test Loss = 0.251557, Learning Rate = 4.685902e-06\n",
      "Epoch 14114/20000: Train Loss = 0.435320, Test Loss = 0.250197, Learning Rate = 4.684122e-06\n",
      "Epoch 14115/20000: Train Loss = 0.435904, Test Loss = 0.252197, Learning Rate = 4.682342e-06\n",
      "Epoch 14116/20000: Train Loss = 0.435631, Test Loss = 0.254067, Learning Rate = 4.680563e-06\n",
      "Epoch 14117/20000: Train Loss = 0.435843, Test Loss = 0.251090, Learning Rate = 4.678784e-06\n",
      "Epoch 14118/20000: Train Loss = 0.435802, Test Loss = 0.252122, Learning Rate = 4.677007e-06\n",
      "Epoch 14119/20000: Train Loss = 0.435499, Test Loss = 0.252336, Learning Rate = 4.675229e-06\n",
      "Epoch 14120/20000: Train Loss = 0.435648, Test Loss = 0.253640, Learning Rate = 4.673453e-06\n",
      "Epoch 14121/20000: Train Loss = 0.435598, Test Loss = 0.255794, Learning Rate = 4.671677e-06\n",
      "Epoch 14122/20000: Train Loss = 0.435831, Test Loss = 0.255675, Learning Rate = 4.669902e-06\n",
      "Epoch 14123/20000: Train Loss = 0.435785, Test Loss = 0.253537, Learning Rate = 4.668128e-06\n",
      "Epoch 14124/20000: Train Loss = 0.435830, Test Loss = 0.254193, Learning Rate = 4.666354e-06\n",
      "Epoch 14125/20000: Train Loss = 0.435549, Test Loss = 0.254703, Learning Rate = 4.664581e-06\n",
      "Epoch 14126/20000: Train Loss = 0.435806, Test Loss = 0.253758, Learning Rate = 4.662808e-06\n",
      "Epoch 14127/20000: Train Loss = 0.435638, Test Loss = 0.255937, Learning Rate = 4.661037e-06\n",
      "Epoch 14128/20000: Train Loss = 0.435632, Test Loss = 0.254492, Learning Rate = 4.659266e-06\n",
      "Epoch 14129/20000: Train Loss = 0.435632, Test Loss = 0.253602, Learning Rate = 4.657495e-06\n",
      "Epoch 14130/20000: Train Loss = 0.435928, Test Loss = 0.250889, Learning Rate = 4.655725e-06\n",
      "Epoch 14131/20000: Train Loss = 0.435534, Test Loss = 0.253209, Learning Rate = 4.653956e-06\n",
      "Epoch 14132/20000: Train Loss = 0.435624, Test Loss = 0.251530, Learning Rate = 4.652188e-06\n",
      "Epoch 14133/20000: Train Loss = 0.437005, Test Loss = 0.251941, Learning Rate = 4.650420e-06\n",
      "Epoch 14134/20000: Train Loss = 0.435625, Test Loss = 0.254234, Learning Rate = 4.648653e-06\n",
      "Epoch 14135/20000: Train Loss = 0.435673, Test Loss = 0.254383, Learning Rate = 4.646887e-06\n",
      "Epoch 14136/20000: Train Loss = 0.435708, Test Loss = 0.255051, Learning Rate = 4.645121e-06\n",
      "Epoch 14137/20000: Train Loss = 0.435409, Test Loss = 0.252885, Learning Rate = 4.643356e-06\n",
      "Epoch 14138/20000: Train Loss = 0.437168, Test Loss = 0.253635, Learning Rate = 4.641592e-06\n",
      "Epoch 14139/20000: Train Loss = 0.435644, Test Loss = 0.253756, Learning Rate = 4.639828e-06\n",
      "Epoch 14140/20000: Train Loss = 0.435581, Test Loss = 0.252207, Learning Rate = 4.638065e-06\n",
      "Epoch 14141/20000: Train Loss = 0.436884, Test Loss = 0.252831, Learning Rate = 4.636303e-06\n",
      "Epoch 14142/20000: Train Loss = 0.435535, Test Loss = 0.250100, Learning Rate = 4.634541e-06\n",
      "Epoch 14143/20000: Train Loss = 0.435578, Test Loss = 0.254047, Learning Rate = 4.632780e-06\n",
      "Epoch 14144/20000: Train Loss = 0.435751, Test Loss = 0.252780, Learning Rate = 4.631020e-06\n",
      "Epoch 14145/20000: Train Loss = 0.435622, Test Loss = 0.252945, Learning Rate = 4.629260e-06\n",
      "Epoch 14146/20000: Train Loss = 0.435625, Test Loss = 0.253255, Learning Rate = 4.627501e-06\n",
      "Epoch 14147/20000: Train Loss = 0.435471, Test Loss = 0.252047, Learning Rate = 4.625743e-06\n",
      "Epoch 14148/20000: Train Loss = 0.435587, Test Loss = 0.248825, Learning Rate = 4.623985e-06\n",
      "Epoch 14149/20000: Train Loss = 0.435715, Test Loss = 0.248946, Learning Rate = 4.622228e-06\n",
      "Epoch 14150/20000: Train Loss = 0.435866, Test Loss = 0.249478, Learning Rate = 4.620472e-06\n",
      "Epoch 14151/20000: Train Loss = 0.435729, Test Loss = 0.249141, Learning Rate = 4.618716e-06\n",
      "Epoch 14152/20000: Train Loss = 0.435382, Test Loss = 0.249828, Learning Rate = 4.616961e-06\n",
      "Epoch 14153/20000: Train Loss = 0.435585, Test Loss = 0.252237, Learning Rate = 4.615207e-06\n",
      "Epoch 14154/20000: Train Loss = 0.435769, Test Loss = 0.251152, Learning Rate = 4.613453e-06\n",
      "Epoch 14155/20000: Train Loss = 0.436714, Test Loss = 0.253848, Learning Rate = 4.611700e-06\n",
      "Epoch 14156/20000: Train Loss = 0.435565, Test Loss = 0.252680, Learning Rate = 4.609948e-06\n",
      "Epoch 14157/20000: Train Loss = 0.435720, Test Loss = 0.252397, Learning Rate = 4.608196e-06\n",
      "Epoch 14158/20000: Train Loss = 0.435887, Test Loss = 0.250641, Learning Rate = 4.606445e-06\n",
      "Epoch 14159/20000: Train Loss = 0.435784, Test Loss = 0.256802, Learning Rate = 4.604695e-06\n",
      "Epoch 14160/20000: Train Loss = 0.435890, Test Loss = 0.254206, Learning Rate = 4.602945e-06\n",
      "Epoch 14161/20000: Train Loss = 0.435553, Test Loss = 0.254848, Learning Rate = 4.601196e-06\n",
      "Epoch 14162/20000: Train Loss = 0.435628, Test Loss = 0.254358, Learning Rate = 4.599448e-06\n",
      "Epoch 14163/20000: Train Loss = 0.435626, Test Loss = 0.254073, Learning Rate = 4.597700e-06\n",
      "Epoch 14164/20000: Train Loss = 0.435494, Test Loss = 0.253581, Learning Rate = 4.595953e-06\n",
      "Epoch 14165/20000: Train Loss = 0.435560, Test Loss = 0.253673, Learning Rate = 4.594207e-06\n",
      "Epoch 14166/20000: Train Loss = 0.436254, Test Loss = 0.254209, Learning Rate = 4.592461e-06\n",
      "Epoch 14167/20000: Train Loss = 0.436950, Test Loss = 0.254573, Learning Rate = 4.590716e-06\n",
      "Epoch 14168/20000: Train Loss = 0.435664, Test Loss = 0.253536, Learning Rate = 4.588972e-06\n",
      "Epoch 14169/20000: Train Loss = 0.435700, Test Loss = 0.254329, Learning Rate = 4.587228e-06\n",
      "Epoch 14170/20000: Train Loss = 0.436421, Test Loss = 0.251748, Learning Rate = 4.585485e-06\n",
      "Epoch 14171/20000: Train Loss = 0.436053, Test Loss = 0.250319, Learning Rate = 4.583743e-06\n",
      "Epoch 14172/20000: Train Loss = 0.436726, Test Loss = 0.251262, Learning Rate = 4.582001e-06\n",
      "Epoch 14173/20000: Train Loss = 0.435469, Test Loss = 0.250992, Learning Rate = 4.580260e-06\n",
      "Epoch 14174/20000: Train Loss = 0.435507, Test Loss = 0.250289, Learning Rate = 4.578520e-06\n",
      "Epoch 14175/20000: Train Loss = 0.435512, Test Loss = 0.250746, Learning Rate = 4.576780e-06\n",
      "Epoch 14176/20000: Train Loss = 0.435922, Test Loss = 0.250073, Learning Rate = 4.575041e-06\n",
      "Epoch 14177/20000: Train Loss = 0.435682, Test Loss = 0.248056, Learning Rate = 4.573303e-06\n",
      "Epoch 14178/20000: Train Loss = 0.435713, Test Loss = 0.248732, Learning Rate = 4.571565e-06\n",
      "Epoch 14179/20000: Train Loss = 0.435686, Test Loss = 0.247014, Learning Rate = 4.569828e-06\n",
      "Epoch 14180/20000: Train Loss = 0.435325, Test Loss = 0.251106, Learning Rate = 4.568091e-06\n",
      "Epoch 14181/20000: Train Loss = 0.435609, Test Loss = 0.250080, Learning Rate = 4.566356e-06\n",
      "Epoch 14182/20000: Train Loss = 0.435359, Test Loss = 0.251651, Learning Rate = 4.564621e-06\n",
      "Epoch 14183/20000: Train Loss = 0.435899, Test Loss = 0.251248, Learning Rate = 4.562886e-06\n",
      "Epoch 14184/20000: Train Loss = 0.436042, Test Loss = 0.255370, Learning Rate = 4.561152e-06\n",
      "Epoch 14185/20000: Train Loss = 0.435509, Test Loss = 0.254707, Learning Rate = 4.559419e-06\n",
      "Epoch 14186/20000: Train Loss = 0.435768, Test Loss = 0.248681, Learning Rate = 4.557687e-06\n",
      "Epoch 14187/20000: Train Loss = 0.435431, Test Loss = 0.253143, Learning Rate = 4.555955e-06\n",
      "Epoch 14188/20000: Train Loss = 0.435886, Test Loss = 0.249397, Learning Rate = 4.554224e-06\n",
      "Epoch 14189/20000: Train Loss = 0.435994, Test Loss = 0.249920, Learning Rate = 4.552493e-06\n",
      "Epoch 14190/20000: Train Loss = 0.435687, Test Loss = 0.250672, Learning Rate = 4.550764e-06\n",
      "Epoch 14191/20000: Train Loss = 0.435642, Test Loss = 0.252359, Learning Rate = 4.549034e-06\n",
      "Epoch 14192/20000: Train Loss = 0.435638, Test Loss = 0.251981, Learning Rate = 4.547306e-06\n",
      "Epoch 14193/20000: Train Loss = 0.435938, Test Loss = 0.253949, Learning Rate = 4.545578e-06\n",
      "Epoch 14194/20000: Train Loss = 0.435428, Test Loss = 0.252506, Learning Rate = 4.543851e-06\n",
      "Epoch 14195/20000: Train Loss = 0.435977, Test Loss = 0.251762, Learning Rate = 4.542124e-06\n",
      "Epoch 14196/20000: Train Loss = 0.435727, Test Loss = 0.255190, Learning Rate = 4.540398e-06\n",
      "Epoch 14197/20000: Train Loss = 0.435780, Test Loss = 0.253269, Learning Rate = 4.538673e-06\n",
      "Epoch 14198/20000: Train Loss = 0.435739, Test Loss = 0.256562, Learning Rate = 4.536949e-06\n",
      "Epoch 14199/20000: Train Loss = 0.435659, Test Loss = 0.253962, Learning Rate = 4.535225e-06\n",
      "Epoch 14200/20000: Train Loss = 0.435686, Test Loss = 0.253288, Learning Rate = 4.533501e-06\n",
      "Epoch 14201/20000: Train Loss = 0.435628, Test Loss = 0.254506, Learning Rate = 4.531779e-06\n",
      "Epoch 14202/20000: Train Loss = 0.435540, Test Loss = 0.253363, Learning Rate = 4.530057e-06\n",
      "Epoch 14203/20000: Train Loss = 0.435833, Test Loss = 0.248239, Learning Rate = 4.528336e-06\n",
      "Epoch 14204/20000: Train Loss = 0.435541, Test Loss = 0.251849, Learning Rate = 4.526615e-06\n",
      "Epoch 14205/20000: Train Loss = 0.436052, Test Loss = 0.253967, Learning Rate = 4.524895e-06\n",
      "Epoch 14206/20000: Train Loss = 0.435600, Test Loss = 0.253014, Learning Rate = 4.523176e-06\n",
      "Epoch 14207/20000: Train Loss = 0.435422, Test Loss = 0.250282, Learning Rate = 4.521457e-06\n",
      "Epoch 14208/20000: Train Loss = 0.435530, Test Loss = 0.251146, Learning Rate = 4.519739e-06\n",
      "Epoch 14209/20000: Train Loss = 0.435449, Test Loss = 0.253309, Learning Rate = 4.518021e-06\n",
      "Epoch 14210/20000: Train Loss = 0.435688, Test Loss = 0.250269, Learning Rate = 4.516305e-06\n",
      "Epoch 14211/20000: Train Loss = 0.436053, Test Loss = 0.251220, Learning Rate = 4.514589e-06\n",
      "Epoch 14212/20000: Train Loss = 0.435503, Test Loss = 0.250794, Learning Rate = 4.512873e-06\n",
      "Epoch 14213/20000: Train Loss = 0.435717, Test Loss = 0.252870, Learning Rate = 4.511158e-06\n",
      "Epoch 14214/20000: Train Loss = 0.435970, Test Loss = 0.253623, Learning Rate = 4.509444e-06\n",
      "Epoch 14215/20000: Train Loss = 0.435689, Test Loss = 0.253738, Learning Rate = 4.507731e-06\n",
      "Epoch 14216/20000: Train Loss = 0.435453, Test Loss = 0.253393, Learning Rate = 4.506018e-06\n",
      "Epoch 14217/20000: Train Loss = 0.435855, Test Loss = 0.254938, Learning Rate = 4.504306e-06\n",
      "Epoch 14218/20000: Train Loss = 0.435616, Test Loss = 0.252653, Learning Rate = 4.502594e-06\n",
      "Epoch 14219/20000: Train Loss = 0.435520, Test Loss = 0.254164, Learning Rate = 4.500884e-06\n",
      "Epoch 14220/20000: Train Loss = 0.435777, Test Loss = 0.253244, Learning Rate = 4.499173e-06\n",
      "Epoch 14221/20000: Train Loss = 0.436455, Test Loss = 0.251732, Learning Rate = 4.497464e-06\n",
      "Epoch 14222/20000: Train Loss = 0.436172, Test Loss = 0.255568, Learning Rate = 4.495755e-06\n",
      "Epoch 14223/20000: Train Loss = 0.435519, Test Loss = 0.253275, Learning Rate = 4.494047e-06\n",
      "Epoch 14224/20000: Train Loss = 0.435361, Test Loss = 0.254857, Learning Rate = 4.492339e-06\n",
      "Epoch 14225/20000: Train Loss = 0.436332, Test Loss = 0.253273, Learning Rate = 4.490632e-06\n",
      "Epoch 14226/20000: Train Loss = 0.435839, Test Loss = 0.252246, Learning Rate = 4.488926e-06\n",
      "Epoch 14227/20000: Train Loss = 0.436245, Test Loss = 0.254453, Learning Rate = 4.487220e-06\n",
      "Epoch 14228/20000: Train Loss = 0.435984, Test Loss = 0.249360, Learning Rate = 4.485515e-06\n",
      "Epoch 14229/20000: Train Loss = 0.435547, Test Loss = 0.248776, Learning Rate = 4.483811e-06\n",
      "Epoch 14230/20000: Train Loss = 0.435465, Test Loss = 0.250667, Learning Rate = 4.482107e-06\n",
      "Epoch 14231/20000: Train Loss = 0.435441, Test Loss = 0.251159, Learning Rate = 4.480404e-06\n",
      "Epoch 14232/20000: Train Loss = 0.436031, Test Loss = 0.252382, Learning Rate = 4.478701e-06\n",
      "Epoch 14233/20000: Train Loss = 0.435744, Test Loss = 0.249306, Learning Rate = 4.477000e-06\n",
      "Epoch 14234/20000: Train Loss = 0.435688, Test Loss = 0.251235, Learning Rate = 4.475298e-06\n",
      "Epoch 14235/20000: Train Loss = 0.435817, Test Loss = 0.250349, Learning Rate = 4.473598e-06\n",
      "Epoch 14236/20000: Train Loss = 0.435421, Test Loss = 0.253054, Learning Rate = 4.471898e-06\n",
      "Epoch 14237/20000: Train Loss = 0.435469, Test Loss = 0.249765, Learning Rate = 4.470199e-06\n",
      "Epoch 14238/20000: Train Loss = 0.436328, Test Loss = 0.252961, Learning Rate = 4.468500e-06\n",
      "Epoch 14239/20000: Train Loss = 0.436431, Test Loss = 0.252034, Learning Rate = 4.466802e-06\n",
      "Epoch 14240/20000: Train Loss = 0.435397, Test Loss = 0.249981, Learning Rate = 4.465105e-06\n",
      "Epoch 14241/20000: Train Loss = 0.435429, Test Loss = 0.249196, Learning Rate = 4.463409e-06\n",
      "Epoch 14242/20000: Train Loss = 0.435898, Test Loss = 0.252724, Learning Rate = 4.461713e-06\n",
      "Epoch 14243/20000: Train Loss = 0.435429, Test Loss = 0.252255, Learning Rate = 4.460017e-06\n",
      "Epoch 14244/20000: Train Loss = 0.435548, Test Loss = 0.251143, Learning Rate = 4.458323e-06\n",
      "Epoch 14245/20000: Train Loss = 0.435558, Test Loss = 0.251444, Learning Rate = 4.456629e-06\n",
      "Epoch 14246/20000: Train Loss = 0.435416, Test Loss = 0.252205, Learning Rate = 4.454935e-06\n",
      "Epoch 14247/20000: Train Loss = 0.435875, Test Loss = 0.254228, Learning Rate = 4.453242e-06\n",
      "Epoch 14248/20000: Train Loss = 0.435500, Test Loss = 0.249921, Learning Rate = 4.451550e-06\n",
      "Epoch 14249/20000: Train Loss = 0.435599, Test Loss = 0.253118, Learning Rate = 4.449859e-06\n",
      "Epoch 14250/20000: Train Loss = 0.435448, Test Loss = 0.252911, Learning Rate = 4.448168e-06\n",
      "Epoch 14251/20000: Train Loss = 0.435563, Test Loss = 0.250609, Learning Rate = 4.446478e-06\n",
      "Epoch 14252/20000: Train Loss = 0.436103, Test Loss = 0.247664, Learning Rate = 4.444788e-06\n",
      "Epoch 14253/20000: Train Loss = 0.435405, Test Loss = 0.251468, Learning Rate = 4.443099e-06\n",
      "Epoch 14254/20000: Train Loss = 0.435499, Test Loss = 0.250660, Learning Rate = 4.441411e-06\n",
      "Epoch 14255/20000: Train Loss = 0.435699, Test Loss = 0.252390, Learning Rate = 4.439723e-06\n",
      "Epoch 14256/20000: Train Loss = 0.435776, Test Loss = 0.249744, Learning Rate = 4.438037e-06\n",
      "Epoch 14257/20000: Train Loss = 0.435652, Test Loss = 0.251578, Learning Rate = 4.436350e-06\n",
      "Epoch 14258/20000: Train Loss = 0.435668, Test Loss = 0.251538, Learning Rate = 4.434664e-06\n",
      "Epoch 14259/20000: Train Loss = 0.435714, Test Loss = 0.251752, Learning Rate = 4.432979e-06\n",
      "Epoch 14260/20000: Train Loss = 0.435361, Test Loss = 0.253021, Learning Rate = 4.431295e-06\n",
      "Epoch 14261/20000: Train Loss = 0.435495, Test Loss = 0.252717, Learning Rate = 4.429611e-06\n",
      "Epoch 14262/20000: Train Loss = 0.435775, Test Loss = 0.250973, Learning Rate = 4.427928e-06\n",
      "Epoch 14263/20000: Train Loss = 0.435746, Test Loss = 0.250148, Learning Rate = 4.426246e-06\n",
      "Epoch 14264/20000: Train Loss = 0.436053, Test Loss = 0.247589, Learning Rate = 4.424564e-06\n",
      "Epoch 14265/20000: Train Loss = 0.435454, Test Loss = 0.250866, Learning Rate = 4.422883e-06\n",
      "Epoch 14266/20000: Train Loss = 0.435410, Test Loss = 0.252539, Learning Rate = 4.421202e-06\n",
      "Epoch 14267/20000: Train Loss = 0.435605, Test Loss = 0.253460, Learning Rate = 4.419522e-06\n",
      "Epoch 14268/20000: Train Loss = 0.435492, Test Loss = 0.251234, Learning Rate = 4.417843e-06\n",
      "Epoch 14269/20000: Train Loss = 0.435477, Test Loss = 0.253102, Learning Rate = 4.416164e-06\n",
      "Epoch 14270/20000: Train Loss = 0.435698, Test Loss = 0.252615, Learning Rate = 4.414486e-06\n",
      "Epoch 14271/20000: Train Loss = 0.435970, Test Loss = 0.252582, Learning Rate = 4.412809e-06\n",
      "Epoch 14272/20000: Train Loss = 0.435367, Test Loss = 0.253346, Learning Rate = 4.411132e-06\n",
      "Epoch 14273/20000: Train Loss = 0.435833, Test Loss = 0.255256, Learning Rate = 4.409456e-06\n",
      "Epoch 14274/20000: Train Loss = 0.435926, Test Loss = 0.252512, Learning Rate = 4.407780e-06\n",
      "Epoch 14275/20000: Train Loss = 0.435591, Test Loss = 0.249609, Learning Rate = 4.406106e-06\n",
      "Epoch 14276/20000: Train Loss = 0.435777, Test Loss = 0.248966, Learning Rate = 4.404431e-06\n",
      "Epoch 14277/20000: Train Loss = 0.436050, Test Loss = 0.250670, Learning Rate = 4.402758e-06\n",
      "Epoch 14278/20000: Train Loss = 0.435668, Test Loss = 0.252264, Learning Rate = 4.401085e-06\n",
      "Epoch 14279/20000: Train Loss = 0.435776, Test Loss = 0.251807, Learning Rate = 4.399413e-06\n",
      "Epoch 14280/20000: Train Loss = 0.435659, Test Loss = 0.250825, Learning Rate = 4.397741e-06\n",
      "Epoch 14281/20000: Train Loss = 0.436113, Test Loss = 0.252918, Learning Rate = 4.396070e-06\n",
      "Epoch 14282/20000: Train Loss = 0.435388, Test Loss = 0.251649, Learning Rate = 4.394399e-06\n",
      "Epoch 14283/20000: Train Loss = 0.435552, Test Loss = 0.250085, Learning Rate = 4.392730e-06\n",
      "Epoch 14284/20000: Train Loss = 0.435632, Test Loss = 0.252335, Learning Rate = 4.391061e-06\n",
      "Epoch 14285/20000: Train Loss = 0.435559, Test Loss = 0.251112, Learning Rate = 4.389392e-06\n",
      "Epoch 14286/20000: Train Loss = 0.435794, Test Loss = 0.253076, Learning Rate = 4.387724e-06\n",
      "Epoch 14287/20000: Train Loss = 0.435323, Test Loss = 0.250723, Learning Rate = 4.386057e-06\n",
      "Epoch 14288/20000: Train Loss = 0.435769, Test Loss = 0.250838, Learning Rate = 4.384390e-06\n",
      "Epoch 14289/20000: Train Loss = 0.435489, Test Loss = 0.251086, Learning Rate = 4.382724e-06\n",
      "Epoch 14290/20000: Train Loss = 0.435537, Test Loss = 0.252691, Learning Rate = 4.381059e-06\n",
      "Epoch 14291/20000: Train Loss = 0.435779, Test Loss = 0.254027, Learning Rate = 4.379394e-06\n",
      "Epoch 14292/20000: Train Loss = 0.435845, Test Loss = 0.254567, Learning Rate = 4.377730e-06\n",
      "Epoch 14293/20000: Train Loss = 0.435584, Test Loss = 0.253030, Learning Rate = 4.376067e-06\n",
      "Epoch 14294/20000: Train Loss = 0.435452, Test Loss = 0.253855, Learning Rate = 4.374404e-06\n",
      "Epoch 14295/20000: Train Loss = 0.435318, Test Loss = 0.251972, Learning Rate = 4.372742e-06\n",
      "Epoch 14296/20000: Train Loss = 0.435620, Test Loss = 0.253840, Learning Rate = 4.371081e-06\n",
      "Epoch 14297/20000: Train Loss = 0.435628, Test Loss = 0.254515, Learning Rate = 4.369420e-06\n",
      "Epoch 14298/20000: Train Loss = 0.436402, Test Loss = 0.253455, Learning Rate = 4.367759e-06\n",
      "Epoch 14299/20000: Train Loss = 0.435521, Test Loss = 0.252361, Learning Rate = 4.366100e-06\n",
      "Epoch 14300/20000: Train Loss = 0.435711, Test Loss = 0.251950, Learning Rate = 4.364441e-06\n",
      "Epoch 14301/20000: Train Loss = 0.435772, Test Loss = 0.251843, Learning Rate = 4.362782e-06\n",
      "Epoch 14302/20000: Train Loss = 0.435425, Test Loss = 0.250906, Learning Rate = 4.361125e-06\n",
      "Epoch 14303/20000: Train Loss = 0.435863, Test Loss = 0.249413, Learning Rate = 4.359468e-06\n",
      "Epoch 14304/20000: Train Loss = 0.435539, Test Loss = 0.247626, Learning Rate = 4.357811e-06\n",
      "Epoch 14305/20000: Train Loss = 0.435417, Test Loss = 0.248994, Learning Rate = 4.356155e-06\n",
      "Epoch 14306/20000: Train Loss = 0.435460, Test Loss = 0.250609, Learning Rate = 4.354500e-06\n",
      "Epoch 14307/20000: Train Loss = 0.435480, Test Loss = 0.249812, Learning Rate = 4.352845e-06\n",
      "Epoch 14308/20000: Train Loss = 0.435464, Test Loss = 0.249988, Learning Rate = 4.351191e-06\n",
      "Epoch 14309/20000: Train Loss = 0.435582, Test Loss = 0.245710, Learning Rate = 4.349538e-06\n",
      "Epoch 14310/20000: Train Loss = 0.436146, Test Loss = 0.248682, Learning Rate = 4.347885e-06\n",
      "Epoch 14311/20000: Train Loss = 0.435538, Test Loss = 0.250457, Learning Rate = 4.346233e-06\n",
      "Epoch 14312/20000: Train Loss = 0.435607, Test Loss = 0.248808, Learning Rate = 4.344582e-06\n",
      "Epoch 14313/20000: Train Loss = 0.435475, Test Loss = 0.251753, Learning Rate = 4.342931e-06\n",
      "Epoch 14314/20000: Train Loss = 0.435594, Test Loss = 0.248022, Learning Rate = 4.341281e-06\n",
      "Epoch 14315/20000: Train Loss = 0.435455, Test Loss = 0.247522, Learning Rate = 4.339631e-06\n",
      "Epoch 14316/20000: Train Loss = 0.435641, Test Loss = 0.251446, Learning Rate = 4.337982e-06\n",
      "Epoch 14317/20000: Train Loss = 0.435718, Test Loss = 0.251572, Learning Rate = 4.336334e-06\n",
      "Epoch 14318/20000: Train Loss = 0.435506, Test Loss = 0.249421, Learning Rate = 4.334686e-06\n",
      "Epoch 14319/20000: Train Loss = 0.435671, Test Loss = 0.251929, Learning Rate = 4.333039e-06\n",
      "Epoch 14320/20000: Train Loss = 0.435611, Test Loss = 0.248454, Learning Rate = 4.331393e-06\n",
      "Epoch 14321/20000: Train Loss = 0.435543, Test Loss = 0.251779, Learning Rate = 4.329747e-06\n",
      "Epoch 14322/20000: Train Loss = 0.436224, Test Loss = 0.255042, Learning Rate = 4.328102e-06\n",
      "Epoch 14323/20000: Train Loss = 0.436060, Test Loss = 0.254786, Learning Rate = 4.326457e-06\n",
      "Epoch 14324/20000: Train Loss = 0.435700, Test Loss = 0.253297, Learning Rate = 4.324813e-06\n",
      "Epoch 14325/20000: Train Loss = 0.436021, Test Loss = 0.251151, Learning Rate = 4.323170e-06\n",
      "Epoch 14326/20000: Train Loss = 0.435498, Test Loss = 0.253172, Learning Rate = 4.321527e-06\n",
      "Epoch 14327/20000: Train Loss = 0.435490, Test Loss = 0.251582, Learning Rate = 4.319885e-06\n",
      "Epoch 14328/20000: Train Loss = 0.435403, Test Loss = 0.249462, Learning Rate = 4.318244e-06\n",
      "Epoch 14329/20000: Train Loss = 0.435622, Test Loss = 0.250035, Learning Rate = 4.316603e-06\n",
      "Epoch 14330/20000: Train Loss = 0.435433, Test Loss = 0.250153, Learning Rate = 4.314963e-06\n",
      "Epoch 14331/20000: Train Loss = 0.435646, Test Loss = 0.250070, Learning Rate = 4.313323e-06\n",
      "Epoch 14332/20000: Train Loss = 0.436110, Test Loss = 0.251028, Learning Rate = 4.311684e-06\n",
      "Epoch 14333/20000: Train Loss = 0.435675, Test Loss = 0.249472, Learning Rate = 4.310046e-06\n",
      "Epoch 14334/20000: Train Loss = 0.435416, Test Loss = 0.250384, Learning Rate = 4.308408e-06\n",
      "Epoch 14335/20000: Train Loss = 0.436018, Test Loss = 0.250979, Learning Rate = 4.306771e-06\n",
      "Epoch 14336/20000: Train Loss = 0.435516, Test Loss = 0.246041, Learning Rate = 4.305135e-06\n",
      "Epoch 14337/20000: Train Loss = 0.435590, Test Loss = 0.251045, Learning Rate = 4.303499e-06\n",
      "Epoch 14338/20000: Train Loss = 0.435703, Test Loss = 0.248184, Learning Rate = 4.301864e-06\n",
      "Epoch 14339/20000: Train Loss = 0.435523, Test Loss = 0.251236, Learning Rate = 4.300229e-06\n",
      "Epoch 14340/20000: Train Loss = 0.435531, Test Loss = 0.251083, Learning Rate = 4.298595e-06\n",
      "Epoch 14341/20000: Train Loss = 0.435900, Test Loss = 0.253732, Learning Rate = 4.296962e-06\n",
      "Epoch 14342/20000: Train Loss = 0.435773, Test Loss = 0.248910, Learning Rate = 4.295329e-06\n",
      "Epoch 14343/20000: Train Loss = 0.435495, Test Loss = 0.256740, Learning Rate = 4.293697e-06\n",
      "Epoch 14344/20000: Train Loss = 0.435565, Test Loss = 0.252137, Learning Rate = 4.292065e-06\n",
      "Epoch 14345/20000: Train Loss = 0.435964, Test Loss = 0.253098, Learning Rate = 4.290435e-06\n",
      "Epoch 14346/20000: Train Loss = 0.435930, Test Loss = 0.251378, Learning Rate = 4.288804e-06\n",
      "Epoch 14347/20000: Train Loss = 0.435493, Test Loss = 0.249596, Learning Rate = 4.287175e-06\n",
      "Epoch 14348/20000: Train Loss = 0.435946, Test Loss = 0.250483, Learning Rate = 4.285546e-06\n",
      "Epoch 14349/20000: Train Loss = 0.435549, Test Loss = 0.248616, Learning Rate = 4.283917e-06\n",
      "Epoch 14350/20000: Train Loss = 0.435682, Test Loss = 0.248740, Learning Rate = 4.282290e-06\n",
      "Epoch 14351/20000: Train Loss = 0.435446, Test Loss = 0.248708, Learning Rate = 4.280662e-06\n",
      "Epoch 14352/20000: Train Loss = 0.435685, Test Loss = 0.246684, Learning Rate = 4.279036e-06\n",
      "Epoch 14353/20000: Train Loss = 0.435609, Test Loss = 0.248688, Learning Rate = 4.277410e-06\n",
      "Epoch 14354/20000: Train Loss = 0.435427, Test Loss = 0.251560, Learning Rate = 4.275785e-06\n",
      "Epoch 14355/20000: Train Loss = 0.435631, Test Loss = 0.250629, Learning Rate = 4.274160e-06\n",
      "Epoch 14356/20000: Train Loss = 0.435932, Test Loss = 0.251974, Learning Rate = 4.272536e-06\n",
      "Epoch 14357/20000: Train Loss = 0.435577, Test Loss = 0.249423, Learning Rate = 4.270912e-06\n",
      "Epoch 14358/20000: Train Loss = 0.435487, Test Loss = 0.251150, Learning Rate = 4.269290e-06\n",
      "Epoch 14359/20000: Train Loss = 0.435834, Test Loss = 0.253652, Learning Rate = 4.267667e-06\n",
      "Epoch 14360/20000: Train Loss = 0.435942, Test Loss = 0.253928, Learning Rate = 4.266046e-06\n",
      "Epoch 14361/20000: Train Loss = 0.435528, Test Loss = 0.254974, Learning Rate = 4.264425e-06\n",
      "Epoch 14362/20000: Train Loss = 0.435564, Test Loss = 0.255295, Learning Rate = 4.262804e-06\n",
      "Epoch 14363/20000: Train Loss = 0.436089, Test Loss = 0.253552, Learning Rate = 4.261185e-06\n",
      "Epoch 14364/20000: Train Loss = 0.435659, Test Loss = 0.252340, Learning Rate = 4.259566e-06\n",
      "Epoch 14365/20000: Train Loss = 0.435501, Test Loss = 0.254402, Learning Rate = 4.257947e-06\n",
      "Epoch 14366/20000: Train Loss = 0.435540, Test Loss = 0.254251, Learning Rate = 4.256329e-06\n",
      "Epoch 14367/20000: Train Loss = 0.435521, Test Loss = 0.251667, Learning Rate = 4.254712e-06\n",
      "Epoch 14368/20000: Train Loss = 0.435383, Test Loss = 0.248974, Learning Rate = 4.253095e-06\n",
      "Epoch 14369/20000: Train Loss = 0.435643, Test Loss = 0.253775, Learning Rate = 4.251479e-06\n",
      "Epoch 14370/20000: Train Loss = 0.435462, Test Loss = 0.250312, Learning Rate = 4.249864e-06\n",
      "Epoch 14371/20000: Train Loss = 0.435524, Test Loss = 0.249590, Learning Rate = 4.248249e-06\n",
      "Epoch 14372/20000: Train Loss = 0.435729, Test Loss = 0.252333, Learning Rate = 4.246635e-06\n",
      "Epoch 14373/20000: Train Loss = 0.435545, Test Loss = 0.252158, Learning Rate = 4.245021e-06\n",
      "Epoch 14374/20000: Train Loss = 0.435467, Test Loss = 0.253121, Learning Rate = 4.243408e-06\n",
      "Epoch 14375/20000: Train Loss = 0.435434, Test Loss = 0.252391, Learning Rate = 4.241796e-06\n",
      "Epoch 14376/20000: Train Loss = 0.435553, Test Loss = 0.253019, Learning Rate = 4.240184e-06\n",
      "Epoch 14377/20000: Train Loss = 0.435580, Test Loss = 0.252053, Learning Rate = 4.238573e-06\n",
      "Epoch 14378/20000: Train Loss = 0.435415, Test Loss = 0.252121, Learning Rate = 4.236962e-06\n",
      "Epoch 14379/20000: Train Loss = 0.435694, Test Loss = 0.253957, Learning Rate = 4.235352e-06\n",
      "Epoch 14380/20000: Train Loss = 0.435284, Test Loss = 0.253483, Learning Rate = 4.233743e-06\n",
      "Epoch 14381/20000: Train Loss = 0.435477, Test Loss = 0.254616, Learning Rate = 4.232134e-06\n",
      "Epoch 14382/20000: Train Loss = 0.435607, Test Loss = 0.252316, Learning Rate = 4.230526e-06\n",
      "Epoch 14383/20000: Train Loss = 0.435487, Test Loss = 0.253217, Learning Rate = 4.228919e-06\n",
      "Epoch 14384/20000: Train Loss = 0.435916, Test Loss = 0.252703, Learning Rate = 4.227312e-06\n",
      "Epoch 14385/20000: Train Loss = 0.435566, Test Loss = 0.253094, Learning Rate = 4.225706e-06\n",
      "Epoch 14386/20000: Train Loss = 0.435651, Test Loss = 0.251442, Learning Rate = 4.224100e-06\n",
      "Epoch 14387/20000: Train Loss = 0.436010, Test Loss = 0.254495, Learning Rate = 4.222495e-06\n",
      "Epoch 14388/20000: Train Loss = 0.435766, Test Loss = 0.253770, Learning Rate = 4.220890e-06\n",
      "Epoch 14389/20000: Train Loss = 0.435848, Test Loss = 0.252004, Learning Rate = 4.219287e-06\n",
      "Epoch 14390/20000: Train Loss = 0.435843, Test Loss = 0.252662, Learning Rate = 4.217683e-06\n",
      "Epoch 14391/20000: Train Loss = 0.435408, Test Loss = 0.251563, Learning Rate = 4.216081e-06\n",
      "Epoch 14392/20000: Train Loss = 0.435353, Test Loss = 0.253728, Learning Rate = 4.214479e-06\n",
      "Epoch 14393/20000: Train Loss = 0.435580, Test Loss = 0.254943, Learning Rate = 4.212877e-06\n",
      "Epoch 14394/20000: Train Loss = 0.435679, Test Loss = 0.254601, Learning Rate = 4.211277e-06\n",
      "Epoch 14395/20000: Train Loss = 0.435392, Test Loss = 0.253459, Learning Rate = 4.209676e-06\n",
      "Epoch 14396/20000: Train Loss = 0.435566, Test Loss = 0.254086, Learning Rate = 4.208077e-06\n",
      "Epoch 14397/20000: Train Loss = 0.435820, Test Loss = 0.254668, Learning Rate = 4.206478e-06\n",
      "Epoch 14398/20000: Train Loss = 0.436507, Test Loss = 0.249752, Learning Rate = 4.204880e-06\n",
      "Epoch 14399/20000: Train Loss = 0.436306, Test Loss = 0.253591, Learning Rate = 4.203282e-06\n",
      "Epoch 14400/20000: Train Loss = 0.436164, Test Loss = 0.249913, Learning Rate = 4.201685e-06\n",
      "Epoch 14401/20000: Train Loss = 0.435943, Test Loss = 0.254126, Learning Rate = 4.200088e-06\n",
      "Epoch 14402/20000: Train Loss = 0.435798, Test Loss = 0.254189, Learning Rate = 4.198492e-06\n",
      "Epoch 14403/20000: Train Loss = 0.435471, Test Loss = 0.250997, Learning Rate = 4.196897e-06\n",
      "Epoch 14404/20000: Train Loss = 0.435545, Test Loss = 0.253389, Learning Rate = 4.195302e-06\n",
      "Epoch 14405/20000: Train Loss = 0.435691, Test Loss = 0.249145, Learning Rate = 4.193708e-06\n",
      "Epoch 14406/20000: Train Loss = 0.435284, Test Loss = 0.252881, Learning Rate = 4.192115e-06\n",
      "Epoch 14407/20000: Train Loss = 0.435526, Test Loss = 0.255295, Learning Rate = 4.190522e-06\n",
      "Epoch 14408/20000: Train Loss = 0.435528, Test Loss = 0.255917, Learning Rate = 4.188929e-06\n",
      "Epoch 14409/20000: Train Loss = 0.435646, Test Loss = 0.250644, Learning Rate = 4.187338e-06\n",
      "Epoch 14410/20000: Train Loss = 0.435628, Test Loss = 0.256211, Learning Rate = 4.185747e-06\n",
      "Epoch 14411/20000: Train Loss = 0.436052, Test Loss = 0.253365, Learning Rate = 4.184156e-06\n",
      "Epoch 14412/20000: Train Loss = 0.435525, Test Loss = 0.255138, Learning Rate = 4.182566e-06\n",
      "Epoch 14413/20000: Train Loss = 0.435372, Test Loss = 0.253476, Learning Rate = 4.180977e-06\n",
      "Epoch 14414/20000: Train Loss = 0.435589, Test Loss = 0.255003, Learning Rate = 4.179388e-06\n",
      "Epoch 14415/20000: Train Loss = 0.435790, Test Loss = 0.254632, Learning Rate = 4.177800e-06\n",
      "Epoch 14416/20000: Train Loss = 0.435582, Test Loss = 0.253340, Learning Rate = 4.176213e-06\n",
      "Epoch 14417/20000: Train Loss = 0.435398, Test Loss = 0.254826, Learning Rate = 4.174626e-06\n",
      "Epoch 14418/20000: Train Loss = 0.435466, Test Loss = 0.253357, Learning Rate = 4.173040e-06\n",
      "Epoch 14419/20000: Train Loss = 0.435832, Test Loss = 0.252563, Learning Rate = 4.171454e-06\n",
      "Epoch 14420/20000: Train Loss = 0.435397, Test Loss = 0.252485, Learning Rate = 4.169869e-06\n",
      "Epoch 14421/20000: Train Loss = 0.435571, Test Loss = 0.253341, Learning Rate = 4.168285e-06\n",
      "Epoch 14422/20000: Train Loss = 0.435490, Test Loss = 0.251837, Learning Rate = 4.166701e-06\n",
      "Epoch 14423/20000: Train Loss = 0.435363, Test Loss = 0.253659, Learning Rate = 4.165118e-06\n",
      "Epoch 14424/20000: Train Loss = 0.435404, Test Loss = 0.252344, Learning Rate = 4.163535e-06\n",
      "Epoch 14425/20000: Train Loss = 0.435406, Test Loss = 0.252456, Learning Rate = 4.161953e-06\n",
      "Epoch 14426/20000: Train Loss = 0.435519, Test Loss = 0.252134, Learning Rate = 4.160372e-06\n",
      "Epoch 14427/20000: Train Loss = 0.436186, Test Loss = 0.251636, Learning Rate = 4.158791e-06\n",
      "Epoch 14428/20000: Train Loss = 0.435665, Test Loss = 0.250759, Learning Rate = 4.157210e-06\n",
      "Epoch 14429/20000: Train Loss = 0.435479, Test Loss = 0.249060, Learning Rate = 4.155631e-06\n",
      "Epoch 14430/20000: Train Loss = 0.435444, Test Loss = 0.247652, Learning Rate = 4.154052e-06\n",
      "Epoch 14431/20000: Train Loss = 0.435578, Test Loss = 0.250001, Learning Rate = 4.152473e-06\n",
      "Epoch 14432/20000: Train Loss = 0.436004, Test Loss = 0.251639, Learning Rate = 4.150896e-06\n",
      "Epoch 14433/20000: Train Loss = 0.435821, Test Loss = 0.253909, Learning Rate = 4.149318e-06\n",
      "Epoch 14434/20000: Train Loss = 0.436051, Test Loss = 0.253094, Learning Rate = 4.147742e-06\n",
      "Epoch 14435/20000: Train Loss = 0.435571, Test Loss = 0.255443, Learning Rate = 4.146166e-06\n",
      "Epoch 14436/20000: Train Loss = 0.435539, Test Loss = 0.248365, Learning Rate = 4.144590e-06\n",
      "Epoch 14437/20000: Train Loss = 0.436038, Test Loss = 0.252173, Learning Rate = 4.143015e-06\n",
      "Epoch 14438/20000: Train Loss = 0.435784, Test Loss = 0.253226, Learning Rate = 4.141441e-06\n",
      "Epoch 14439/20000: Train Loss = 0.435584, Test Loss = 0.251198, Learning Rate = 4.139868e-06\n",
      "Epoch 14440/20000: Train Loss = 0.435490, Test Loss = 0.251314, Learning Rate = 4.138295e-06\n",
      "Epoch 14441/20000: Train Loss = 0.435397, Test Loss = 0.250991, Learning Rate = 4.136722e-06\n",
      "Epoch 14442/20000: Train Loss = 0.435663, Test Loss = 0.250098, Learning Rate = 4.135150e-06\n",
      "Epoch 14443/20000: Train Loss = 0.435665, Test Loss = 0.252421, Learning Rate = 4.133579e-06\n",
      "Epoch 14444/20000: Train Loss = 0.437822, Test Loss = 0.257193, Learning Rate = 4.132008e-06\n",
      "Epoch 14445/20000: Train Loss = 0.435850, Test Loss = 0.253323, Learning Rate = 4.130438e-06\n",
      "Epoch 14446/20000: Train Loss = 0.435550, Test Loss = 0.253576, Learning Rate = 4.128869e-06\n",
      "Epoch 14447/20000: Train Loss = 0.435424, Test Loss = 0.256367, Learning Rate = 4.127300e-06\n",
      "Epoch 14448/20000: Train Loss = 0.435744, Test Loss = 0.256840, Learning Rate = 4.125732e-06\n",
      "Epoch 14449/20000: Train Loss = 0.435449, Test Loss = 0.254259, Learning Rate = 4.124164e-06\n",
      "Epoch 14450/20000: Train Loss = 0.435396, Test Loss = 0.255976, Learning Rate = 4.122597e-06\n",
      "Epoch 14451/20000: Train Loss = 0.435788, Test Loss = 0.254710, Learning Rate = 4.121030e-06\n",
      "Epoch 14452/20000: Train Loss = 0.435608, Test Loss = 0.259953, Learning Rate = 4.119465e-06\n",
      "Epoch 14453/20000: Train Loss = 0.435871, Test Loss = 0.256453, Learning Rate = 4.117899e-06\n",
      "Epoch 14454/20000: Train Loss = 0.435427, Test Loss = 0.255702, Learning Rate = 4.116335e-06\n",
      "Epoch 14455/20000: Train Loss = 0.435470, Test Loss = 0.254444, Learning Rate = 4.114771e-06\n",
      "Epoch 14456/20000: Train Loss = 0.435394, Test Loss = 0.255703, Learning Rate = 4.113207e-06\n",
      "Epoch 14457/20000: Train Loss = 0.435879, Test Loss = 0.258540, Learning Rate = 4.111644e-06\n",
      "Epoch 14458/20000: Train Loss = 0.435447, Test Loss = 0.258335, Learning Rate = 4.110082e-06\n",
      "Epoch 14459/20000: Train Loss = 0.435656, Test Loss = 0.258764, Learning Rate = 4.108520e-06\n",
      "Epoch 14460/20000: Train Loss = 0.435316, Test Loss = 0.258077, Learning Rate = 4.106959e-06\n",
      "Epoch 14461/20000: Train Loss = 0.435525, Test Loss = 0.256310, Learning Rate = 4.105398e-06\n",
      "Epoch 14462/20000: Train Loss = 0.435418, Test Loss = 0.256305, Learning Rate = 4.103839e-06\n",
      "Epoch 14463/20000: Train Loss = 0.435458, Test Loss = 0.257025, Learning Rate = 4.102279e-06\n",
      "Epoch 14464/20000: Train Loss = 0.435542, Test Loss = 0.255074, Learning Rate = 4.100720e-06\n",
      "Epoch 14465/20000: Train Loss = 0.435407, Test Loss = 0.256637, Learning Rate = 4.099162e-06\n",
      "Epoch 14466/20000: Train Loss = 0.435394, Test Loss = 0.254729, Learning Rate = 4.097605e-06\n",
      "Epoch 14467/20000: Train Loss = 0.435419, Test Loss = 0.256488, Learning Rate = 4.096048e-06\n",
      "Epoch 14468/20000: Train Loss = 0.435720, Test Loss = 0.255534, Learning Rate = 4.094491e-06\n",
      "Epoch 14469/20000: Train Loss = 0.435584, Test Loss = 0.257190, Learning Rate = 4.092936e-06\n",
      "Epoch 14470/20000: Train Loss = 0.435585, Test Loss = 0.253288, Learning Rate = 4.091380e-06\n",
      "Epoch 14471/20000: Train Loss = 0.435447, Test Loss = 0.254704, Learning Rate = 4.089826e-06\n",
      "Epoch 14472/20000: Train Loss = 0.435810, Test Loss = 0.255939, Learning Rate = 4.088272e-06\n",
      "Epoch 14473/20000: Train Loss = 0.435809, Test Loss = 0.255522, Learning Rate = 4.086718e-06\n",
      "Epoch 14474/20000: Train Loss = 0.435982, Test Loss = 0.253375, Learning Rate = 4.085165e-06\n",
      "Epoch 14475/20000: Train Loss = 0.435967, Test Loss = 0.255773, Learning Rate = 4.083613e-06\n",
      "Epoch 14476/20000: Train Loss = 0.435636, Test Loss = 0.252194, Learning Rate = 4.082061e-06\n",
      "Epoch 14477/20000: Train Loss = 0.436148, Test Loss = 0.252086, Learning Rate = 4.080510e-06\n",
      "Epoch 14478/20000: Train Loss = 0.435372, Test Loss = 0.251784, Learning Rate = 4.078960e-06\n",
      "Epoch 14479/20000: Train Loss = 0.436080, Test Loss = 0.256003, Learning Rate = 4.077410e-06\n",
      "Epoch 14480/20000: Train Loss = 0.435376, Test Loss = 0.252108, Learning Rate = 4.075861e-06\n",
      "Epoch 14481/20000: Train Loss = 0.435587, Test Loss = 0.251944, Learning Rate = 4.074312e-06\n",
      "Epoch 14482/20000: Train Loss = 0.435446, Test Loss = 0.250075, Learning Rate = 4.072764e-06\n",
      "Epoch 14483/20000: Train Loss = 0.435426, Test Loss = 0.252542, Learning Rate = 4.071216e-06\n",
      "Epoch 14484/20000: Train Loss = 0.435693, Test Loss = 0.253469, Learning Rate = 4.069669e-06\n",
      "Epoch 14485/20000: Train Loss = 0.435324, Test Loss = 0.254391, Learning Rate = 4.068123e-06\n",
      "Epoch 14486/20000: Train Loss = 0.435398, Test Loss = 0.252289, Learning Rate = 4.066577e-06\n",
      "Epoch 14487/20000: Train Loss = 0.435598, Test Loss = 0.251793, Learning Rate = 4.065032e-06\n",
      "Epoch 14488/20000: Train Loss = 0.435453, Test Loss = 0.250222, Learning Rate = 4.063487e-06\n",
      "Epoch 14489/20000: Train Loss = 0.435451, Test Loss = 0.250685, Learning Rate = 4.061943e-06\n",
      "Epoch 14490/20000: Train Loss = 0.435454, Test Loss = 0.254738, Learning Rate = 4.060400e-06\n",
      "Epoch 14491/20000: Train Loss = 0.435447, Test Loss = 0.251847, Learning Rate = 4.058857e-06\n",
      "Epoch 14492/20000: Train Loss = 0.435655, Test Loss = 0.252918, Learning Rate = 4.057315e-06\n",
      "Epoch 14493/20000: Train Loss = 0.435544, Test Loss = 0.250776, Learning Rate = 4.055773e-06\n",
      "Epoch 14494/20000: Train Loss = 0.435397, Test Loss = 0.252857, Learning Rate = 4.054232e-06\n",
      "Epoch 14495/20000: Train Loss = 0.436420, Test Loss = 0.253155, Learning Rate = 4.052692e-06\n",
      "Epoch 14496/20000: Train Loss = 0.435580, Test Loss = 0.252384, Learning Rate = 4.051152e-06\n",
      "Epoch 14497/20000: Train Loss = 0.435309, Test Loss = 0.250990, Learning Rate = 4.049612e-06\n",
      "Epoch 14498/20000: Train Loss = 0.435439, Test Loss = 0.252139, Learning Rate = 4.048074e-06\n",
      "Epoch 14499/20000: Train Loss = 0.435551, Test Loss = 0.252020, Learning Rate = 4.046536e-06\n",
      "Epoch 14500/20000: Train Loss = 0.435540, Test Loss = 0.252113, Learning Rate = 4.044998e-06\n",
      "Epoch 14501/20000: Train Loss = 0.435714, Test Loss = 0.251170, Learning Rate = 4.043461e-06\n",
      "Epoch 14502/20000: Train Loss = 0.435637, Test Loss = 0.251521, Learning Rate = 4.041925e-06\n",
      "Epoch 14503/20000: Train Loss = 0.435624, Test Loss = 0.250736, Learning Rate = 4.040389e-06\n",
      "Epoch 14504/20000: Train Loss = 0.435395, Test Loss = 0.249383, Learning Rate = 4.038853e-06\n",
      "Epoch 14505/20000: Train Loss = 0.435328, Test Loss = 0.247718, Learning Rate = 4.037319e-06\n",
      "Epoch 14506/20000: Train Loss = 0.435968, Test Loss = 0.246859, Learning Rate = 4.035785e-06\n",
      "Epoch 14507/20000: Train Loss = 0.435236, Test Loss = 0.249988, Learning Rate = 4.034251e-06\n",
      "Epoch 14508/20000: Train Loss = 0.435373, Test Loss = 0.250686, Learning Rate = 4.032718e-06\n",
      "Epoch 14509/20000: Train Loss = 0.435587, Test Loss = 0.254971, Learning Rate = 4.031186e-06\n",
      "Epoch 14510/20000: Train Loss = 0.435692, Test Loss = 0.251602, Learning Rate = 4.029654e-06\n",
      "Epoch 14511/20000: Train Loss = 0.435185, Test Loss = 0.253886, Learning Rate = 4.028123e-06\n",
      "Epoch 14512/20000: Train Loss = 0.435379, Test Loss = 0.255147, Learning Rate = 4.026593e-06\n",
      "Epoch 14513/20000: Train Loss = 0.435639, Test Loss = 0.253166, Learning Rate = 4.025063e-06\n",
      "Epoch 14514/20000: Train Loss = 0.435556, Test Loss = 0.254229, Learning Rate = 4.023533e-06\n",
      "Epoch 14515/20000: Train Loss = 0.435360, Test Loss = 0.254302, Learning Rate = 4.022004e-06\n",
      "Epoch 14516/20000: Train Loss = 0.435330, Test Loss = 0.255276, Learning Rate = 4.020476e-06\n",
      "Epoch 14517/20000: Train Loss = 0.435493, Test Loss = 0.256370, Learning Rate = 4.018948e-06\n",
      "Epoch 14518/20000: Train Loss = 0.435524, Test Loss = 0.253579, Learning Rate = 4.017421e-06\n",
      "Epoch 14519/20000: Train Loss = 0.435551, Test Loss = 0.253450, Learning Rate = 4.015895e-06\n",
      "Epoch 14520/20000: Train Loss = 0.435537, Test Loss = 0.254897, Learning Rate = 4.014369e-06\n",
      "Epoch 14521/20000: Train Loss = 0.435355, Test Loss = 0.253014, Learning Rate = 4.012844e-06\n",
      "Epoch 14522/20000: Train Loss = 0.435423, Test Loss = 0.254355, Learning Rate = 4.011319e-06\n",
      "Epoch 14523/20000: Train Loss = 0.435405, Test Loss = 0.255952, Learning Rate = 4.009795e-06\n",
      "Epoch 14524/20000: Train Loss = 0.435292, Test Loss = 0.255187, Learning Rate = 4.008271e-06\n",
      "Epoch 14525/20000: Train Loss = 0.435359, Test Loss = 0.253597, Learning Rate = 4.006748e-06\n",
      "Epoch 14526/20000: Train Loss = 0.435372, Test Loss = 0.253377, Learning Rate = 4.005225e-06\n",
      "Epoch 14527/20000: Train Loss = 0.435291, Test Loss = 0.252159, Learning Rate = 4.003704e-06\n",
      "Epoch 14528/20000: Train Loss = 0.435398, Test Loss = 0.254432, Learning Rate = 4.002182e-06\n",
      "Epoch 14529/20000: Train Loss = 0.435509, Test Loss = 0.254665, Learning Rate = 4.000662e-06\n",
      "Epoch 14530/20000: Train Loss = 0.436108, Test Loss = 0.250405, Learning Rate = 3.999141e-06\n",
      "Epoch 14531/20000: Train Loss = 0.435426, Test Loss = 0.252026, Learning Rate = 3.997622e-06\n",
      "Epoch 14532/20000: Train Loss = 0.435208, Test Loss = 0.253020, Learning Rate = 3.996103e-06\n",
      "Epoch 14533/20000: Train Loss = 0.435418, Test Loss = 0.251879, Learning Rate = 3.994584e-06\n",
      "Epoch 14534/20000: Train Loss = 0.435342, Test Loss = 0.254737, Learning Rate = 3.993067e-06\n",
      "Epoch 14535/20000: Train Loss = 0.435711, Test Loss = 0.251555, Learning Rate = 3.991549e-06\n",
      "Epoch 14536/20000: Train Loss = 0.435410, Test Loss = 0.251352, Learning Rate = 3.990033e-06\n",
      "Epoch 14537/20000: Train Loss = 0.435396, Test Loss = 0.252123, Learning Rate = 3.988517e-06\n",
      "Epoch 14538/20000: Train Loss = 0.435408, Test Loss = 0.250859, Learning Rate = 3.987001e-06\n",
      "Epoch 14539/20000: Train Loss = 0.435272, Test Loss = 0.252348, Learning Rate = 3.985486e-06\n",
      "Epoch 14540/20000: Train Loss = 0.435323, Test Loss = 0.251433, Learning Rate = 3.983972e-06\n",
      "Epoch 14541/20000: Train Loss = 0.435777, Test Loss = 0.250478, Learning Rate = 3.982458e-06\n",
      "Epoch 14542/20000: Train Loss = 0.435782, Test Loss = 0.251228, Learning Rate = 3.980945e-06\n",
      "Epoch 14543/20000: Train Loss = 0.435471, Test Loss = 0.250729, Learning Rate = 3.979432e-06\n",
      "Epoch 14544/20000: Train Loss = 0.435457, Test Loss = 0.250102, Learning Rate = 3.977920e-06\n",
      "Epoch 14545/20000: Train Loss = 0.435625, Test Loss = 0.248481, Learning Rate = 3.976408e-06\n",
      "Epoch 14546/20000: Train Loss = 0.435409, Test Loss = 0.248724, Learning Rate = 3.974898e-06\n",
      "Epoch 14547/20000: Train Loss = 0.435515, Test Loss = 0.248846, Learning Rate = 3.973387e-06\n",
      "Epoch 14548/20000: Train Loss = 0.435715, Test Loss = 0.250594, Learning Rate = 3.971877e-06\n",
      "Epoch 14549/20000: Train Loss = 0.436154, Test Loss = 0.252607, Learning Rate = 3.970368e-06\n",
      "Epoch 14550/20000: Train Loss = 0.435566, Test Loss = 0.251707, Learning Rate = 3.968860e-06\n",
      "Epoch 14551/20000: Train Loss = 0.435248, Test Loss = 0.249641, Learning Rate = 3.967351e-06\n",
      "Epoch 14552/20000: Train Loss = 0.435190, Test Loss = 0.251933, Learning Rate = 3.965844e-06\n",
      "Epoch 14553/20000: Train Loss = 0.435731, Test Loss = 0.248127, Learning Rate = 3.964337e-06\n",
      "Epoch 14554/20000: Train Loss = 0.435756, Test Loss = 0.247521, Learning Rate = 3.962831e-06\n",
      "Epoch 14555/20000: Train Loss = 0.435328, Test Loss = 0.249931, Learning Rate = 3.961325e-06\n",
      "Epoch 14556/20000: Train Loss = 0.435495, Test Loss = 0.249139, Learning Rate = 3.959820e-06\n",
      "Epoch 14557/20000: Train Loss = 0.435292, Test Loss = 0.251016, Learning Rate = 3.958315e-06\n",
      "Epoch 14558/20000: Train Loss = 0.435619, Test Loss = 0.250304, Learning Rate = 3.956811e-06\n",
      "Epoch 14559/20000: Train Loss = 0.435192, Test Loss = 0.250638, Learning Rate = 3.955308e-06\n",
      "Epoch 14560/20000: Train Loss = 0.435523, Test Loss = 0.250964, Learning Rate = 3.953805e-06\n",
      "Epoch 14561/20000: Train Loss = 0.435529, Test Loss = 0.252666, Learning Rate = 3.952302e-06\n",
      "Epoch 14562/20000: Train Loss = 0.435618, Test Loss = 0.254413, Learning Rate = 3.950801e-06\n",
      "Epoch 14563/20000: Train Loss = 0.435113, Test Loss = 0.250471, Learning Rate = 3.949299e-06\n",
      "Epoch 14564/20000: Train Loss = 0.435954, Test Loss = 0.249532, Learning Rate = 3.947799e-06\n",
      "Epoch 14565/20000: Train Loss = 0.435319, Test Loss = 0.251736, Learning Rate = 3.946299e-06\n",
      "Epoch 14566/20000: Train Loss = 0.435540, Test Loss = 0.250283, Learning Rate = 3.944799e-06\n",
      "Epoch 14567/20000: Train Loss = 0.435424, Test Loss = 0.251920, Learning Rate = 3.943300e-06\n",
      "Epoch 14568/20000: Train Loss = 0.435285, Test Loss = 0.248755, Learning Rate = 3.941802e-06\n",
      "Epoch 14569/20000: Train Loss = 0.435353, Test Loss = 0.252057, Learning Rate = 3.940304e-06\n",
      "Epoch 14570/20000: Train Loss = 0.435569, Test Loss = 0.251837, Learning Rate = 3.938807e-06\n",
      "Epoch 14571/20000: Train Loss = 0.435494, Test Loss = 0.250074, Learning Rate = 3.937310e-06\n",
      "Epoch 14572/20000: Train Loss = 0.435929, Test Loss = 0.248798, Learning Rate = 3.935814e-06\n",
      "Epoch 14573/20000: Train Loss = 0.435710, Test Loss = 0.247858, Learning Rate = 3.934319e-06\n",
      "Epoch 14574/20000: Train Loss = 0.436220, Test Loss = 0.251353, Learning Rate = 3.932824e-06\n",
      "Epoch 14575/20000: Train Loss = 0.435348, Test Loss = 0.253107, Learning Rate = 3.931329e-06\n",
      "Epoch 14576/20000: Train Loss = 0.435467, Test Loss = 0.250276, Learning Rate = 3.929836e-06\n",
      "Epoch 14577/20000: Train Loss = 0.435669, Test Loss = 0.250134, Learning Rate = 3.928342e-06\n",
      "Epoch 14578/20000: Train Loss = 0.435345, Test Loss = 0.250547, Learning Rate = 3.926850e-06\n",
      "Epoch 14579/20000: Train Loss = 0.435421, Test Loss = 0.252173, Learning Rate = 3.925358e-06\n",
      "Epoch 14580/20000: Train Loss = 0.435462, Test Loss = 0.247971, Learning Rate = 3.923866e-06\n",
      "Epoch 14581/20000: Train Loss = 0.435431, Test Loss = 0.248878, Learning Rate = 3.922375e-06\n",
      "Epoch 14582/20000: Train Loss = 0.435184, Test Loss = 0.246476, Learning Rate = 3.920885e-06\n",
      "Epoch 14583/20000: Train Loss = 0.435518, Test Loss = 0.247546, Learning Rate = 3.919395e-06\n",
      "Epoch 14584/20000: Train Loss = 0.436220, Test Loss = 0.247986, Learning Rate = 3.917906e-06\n",
      "Epoch 14585/20000: Train Loss = 0.435505, Test Loss = 0.245269, Learning Rate = 3.916417e-06\n",
      "Epoch 14586/20000: Train Loss = 0.435760, Test Loss = 0.246857, Learning Rate = 3.914929e-06\n",
      "Epoch 14587/20000: Train Loss = 0.435225, Test Loss = 0.248331, Learning Rate = 3.913441e-06\n",
      "Epoch 14588/20000: Train Loss = 0.435461, Test Loss = 0.250881, Learning Rate = 3.911954e-06\n",
      "Epoch 14589/20000: Train Loss = 0.435862, Test Loss = 0.252241, Learning Rate = 3.910468e-06\n",
      "Epoch 14590/20000: Train Loss = 0.435279, Test Loss = 0.251090, Learning Rate = 3.908982e-06\n",
      "Epoch 14591/20000: Train Loss = 0.435271, Test Loss = 0.254088, Learning Rate = 3.907497e-06\n",
      "Epoch 14592/20000: Train Loss = 0.435597, Test Loss = 0.254471, Learning Rate = 3.906012e-06\n",
      "Epoch 14593/20000: Train Loss = 0.435530, Test Loss = 0.249677, Learning Rate = 3.904528e-06\n",
      "Epoch 14594/20000: Train Loss = 0.435257, Test Loss = 0.253219, Learning Rate = 3.903044e-06\n",
      "Epoch 14595/20000: Train Loss = 0.435386, Test Loss = 0.252824, Learning Rate = 3.901561e-06\n",
      "Epoch 14596/20000: Train Loss = 0.435998, Test Loss = 0.253137, Learning Rate = 3.900079e-06\n",
      "Epoch 14597/20000: Train Loss = 0.435361, Test Loss = 0.254496, Learning Rate = 3.898597e-06\n",
      "Epoch 14598/20000: Train Loss = 0.435753, Test Loss = 0.251580, Learning Rate = 3.897115e-06\n",
      "Epoch 14599/20000: Train Loss = 0.435576, Test Loss = 0.250944, Learning Rate = 3.895635e-06\n",
      "Epoch 14600/20000: Train Loss = 0.435265, Test Loss = 0.253051, Learning Rate = 3.894154e-06\n",
      "Epoch 14601/20000: Train Loss = 0.435413, Test Loss = 0.254224, Learning Rate = 3.892675e-06\n",
      "Epoch 14602/20000: Train Loss = 0.435576, Test Loss = 0.251606, Learning Rate = 3.891196e-06\n",
      "Epoch 14603/20000: Train Loss = 0.435289, Test Loss = 0.251547, Learning Rate = 3.889717e-06\n",
      "Epoch 14604/20000: Train Loss = 0.435473, Test Loss = 0.252154, Learning Rate = 3.888239e-06\n",
      "Epoch 14605/20000: Train Loss = 0.435199, Test Loss = 0.253251, Learning Rate = 3.886762e-06\n",
      "Epoch 14606/20000: Train Loss = 0.435501, Test Loss = 0.250895, Learning Rate = 3.885285e-06\n",
      "Epoch 14607/20000: Train Loss = 0.435195, Test Loss = 0.251364, Learning Rate = 3.883808e-06\n",
      "Epoch 14608/20000: Train Loss = 0.435377, Test Loss = 0.252517, Learning Rate = 3.882333e-06\n",
      "Epoch 14609/20000: Train Loss = 0.435247, Test Loss = 0.252960, Learning Rate = 3.880857e-06\n",
      "Epoch 14610/20000: Train Loss = 0.435554, Test Loss = 0.253633, Learning Rate = 3.879383e-06\n",
      "Epoch 14611/20000: Train Loss = 0.435484, Test Loss = 0.253070, Learning Rate = 3.877909e-06\n",
      "Epoch 14612/20000: Train Loss = 0.435221, Test Loss = 0.251748, Learning Rate = 3.876435e-06\n",
      "Epoch 14613/20000: Train Loss = 0.435286, Test Loss = 0.251557, Learning Rate = 3.874962e-06\n",
      "Epoch 14614/20000: Train Loss = 0.435366, Test Loss = 0.249920, Learning Rate = 3.873490e-06\n",
      "Epoch 14615/20000: Train Loss = 0.435209, Test Loss = 0.250604, Learning Rate = 3.872018e-06\n",
      "Epoch 14616/20000: Train Loss = 0.435559, Test Loss = 0.250415, Learning Rate = 3.870547e-06\n",
      "Epoch 14617/20000: Train Loss = 0.435643, Test Loss = 0.253613, Learning Rate = 3.869076e-06\n",
      "Epoch 14618/20000: Train Loss = 0.435067, Test Loss = 0.251580, Learning Rate = 3.867606e-06\n",
      "Epoch 14619/20000: Train Loss = 0.435203, Test Loss = 0.254753, Learning Rate = 3.866136e-06\n",
      "Epoch 14620/20000: Train Loss = 0.435458, Test Loss = 0.254710, Learning Rate = 3.864667e-06\n",
      "Epoch 14621/20000: Train Loss = 0.436137, Test Loss = 0.254376, Learning Rate = 3.863199e-06\n",
      "Epoch 14622/20000: Train Loss = 0.435764, Test Loss = 0.254955, Learning Rate = 3.861731e-06\n",
      "Epoch 14623/20000: Train Loss = 0.435624, Test Loss = 0.253693, Learning Rate = 3.860264e-06\n",
      "Epoch 14624/20000: Train Loss = 0.435691, Test Loss = 0.253691, Learning Rate = 3.858797e-06\n",
      "Epoch 14625/20000: Train Loss = 0.435320, Test Loss = 0.254067, Learning Rate = 3.857331e-06\n",
      "Epoch 14626/20000: Train Loss = 0.435201, Test Loss = 0.251834, Learning Rate = 3.855865e-06\n",
      "Epoch 14627/20000: Train Loss = 0.435194, Test Loss = 0.252498, Learning Rate = 3.854400e-06\n",
      "Epoch 14628/20000: Train Loss = 0.435851, Test Loss = 0.254924, Learning Rate = 3.852935e-06\n",
      "Epoch 14629/20000: Train Loss = 0.435666, Test Loss = 0.253200, Learning Rate = 3.851471e-06\n",
      "Epoch 14630/20000: Train Loss = 0.435453, Test Loss = 0.252792, Learning Rate = 3.850008e-06\n",
      "Epoch 14631/20000: Train Loss = 0.435438, Test Loss = 0.250427, Learning Rate = 3.848545e-06\n",
      "Epoch 14632/20000: Train Loss = 0.435384, Test Loss = 0.252356, Learning Rate = 3.847083e-06\n",
      "Epoch 14633/20000: Train Loss = 0.435795, Test Loss = 0.255960, Learning Rate = 3.845621e-06\n",
      "Epoch 14634/20000: Train Loss = 0.435228, Test Loss = 0.251122, Learning Rate = 3.844160e-06\n",
      "Epoch 14635/20000: Train Loss = 0.435813, Test Loss = 0.253618, Learning Rate = 3.842699e-06\n",
      "Epoch 14636/20000: Train Loss = 0.435326, Test Loss = 0.252383, Learning Rate = 3.841239e-06\n",
      "Epoch 14637/20000: Train Loss = 0.435524, Test Loss = 0.255568, Learning Rate = 3.839779e-06\n",
      "Epoch 14638/20000: Train Loss = 0.435484, Test Loss = 0.252284, Learning Rate = 3.838320e-06\n",
      "Epoch 14639/20000: Train Loss = 0.435278, Test Loss = 0.254215, Learning Rate = 3.836862e-06\n",
      "Epoch 14640/20000: Train Loss = 0.435379, Test Loss = 0.254166, Learning Rate = 3.835404e-06\n",
      "Epoch 14641/20000: Train Loss = 0.435651, Test Loss = 0.251337, Learning Rate = 3.833946e-06\n",
      "Epoch 14642/20000: Train Loss = 0.435159, Test Loss = 0.253825, Learning Rate = 3.832490e-06\n",
      "Epoch 14643/20000: Train Loss = 0.435333, Test Loss = 0.254927, Learning Rate = 3.831033e-06\n",
      "Epoch 14644/20000: Train Loss = 0.436493, Test Loss = 0.252096, Learning Rate = 3.829578e-06\n",
      "Epoch 14645/20000: Train Loss = 0.435574, Test Loss = 0.250367, Learning Rate = 3.828123e-06\n",
      "Epoch 14646/20000: Train Loss = 0.435275, Test Loss = 0.253253, Learning Rate = 3.826668e-06\n",
      "Epoch 14647/20000: Train Loss = 0.435425, Test Loss = 0.252120, Learning Rate = 3.825214e-06\n",
      "Epoch 14648/20000: Train Loss = 0.435417, Test Loss = 0.253283, Learning Rate = 3.823761e-06\n",
      "Epoch 14649/20000: Train Loss = 0.435273, Test Loss = 0.251474, Learning Rate = 3.822308e-06\n",
      "Epoch 14650/20000: Train Loss = 0.435271, Test Loss = 0.252692, Learning Rate = 3.820855e-06\n",
      "Epoch 14651/20000: Train Loss = 0.435140, Test Loss = 0.252834, Learning Rate = 3.819403e-06\n",
      "Epoch 14652/20000: Train Loss = 0.435616, Test Loss = 0.255317, Learning Rate = 3.817952e-06\n",
      "Epoch 14653/20000: Train Loss = 0.435355, Test Loss = 0.250644, Learning Rate = 3.816501e-06\n",
      "Epoch 14654/20000: Train Loss = 0.435108, Test Loss = 0.251276, Learning Rate = 3.815051e-06\n",
      "Epoch 14655/20000: Train Loss = 0.435636, Test Loss = 0.250451, Learning Rate = 3.813602e-06\n",
      "Epoch 14656/20000: Train Loss = 0.435622, Test Loss = 0.250369, Learning Rate = 3.812153e-06\n",
      "Epoch 14657/20000: Train Loss = 0.435349, Test Loss = 0.252620, Learning Rate = 3.810704e-06\n",
      "Epoch 14658/20000: Train Loss = 0.435730, Test Loss = 0.251919, Learning Rate = 3.809256e-06\n",
      "Epoch 14659/20000: Train Loss = 0.436045, Test Loss = 0.251722, Learning Rate = 3.807809e-06\n",
      "Epoch 14660/20000: Train Loss = 0.435363, Test Loss = 0.252611, Learning Rate = 3.806362e-06\n",
      "Epoch 14661/20000: Train Loss = 0.435300, Test Loss = 0.250281, Learning Rate = 3.804915e-06\n",
      "Epoch 14662/20000: Train Loss = 0.435632, Test Loss = 0.252710, Learning Rate = 3.803470e-06\n",
      "Epoch 14663/20000: Train Loss = 0.436712, Test Loss = 0.255520, Learning Rate = 3.802024e-06\n",
      "Epoch 14664/20000: Train Loss = 0.435601, Test Loss = 0.252580, Learning Rate = 3.800580e-06\n",
      "Epoch 14665/20000: Train Loss = 0.435306, Test Loss = 0.252833, Learning Rate = 3.799136e-06\n",
      "Epoch 14666/20000: Train Loss = 0.436076, Test Loss = 0.255669, Learning Rate = 3.797692e-06\n",
      "Epoch 14667/20000: Train Loss = 0.435197, Test Loss = 0.254896, Learning Rate = 3.796249e-06\n",
      "Epoch 14668/20000: Train Loss = 0.435918, Test Loss = 0.254198, Learning Rate = 3.794807e-06\n",
      "Epoch 14669/20000: Train Loss = 0.435170, Test Loss = 0.250190, Learning Rate = 3.793365e-06\n",
      "Epoch 14670/20000: Train Loss = 0.435286, Test Loss = 0.253466, Learning Rate = 3.791923e-06\n",
      "Epoch 14671/20000: Train Loss = 0.435187, Test Loss = 0.253447, Learning Rate = 3.790483e-06\n",
      "Epoch 14672/20000: Train Loss = 0.435476, Test Loss = 0.251439, Learning Rate = 3.789042e-06\n",
      "Epoch 14673/20000: Train Loss = 0.435341, Test Loss = 0.253494, Learning Rate = 3.787603e-06\n",
      "Epoch 14674/20000: Train Loss = 0.435473, Test Loss = 0.252401, Learning Rate = 3.786163e-06\n",
      "Epoch 14675/20000: Train Loss = 0.435461, Test Loss = 0.251202, Learning Rate = 3.784725e-06\n",
      "Epoch 14676/20000: Train Loss = 0.435726, Test Loss = 0.251500, Learning Rate = 3.783287e-06\n",
      "Epoch 14677/20000: Train Loss = 0.435388, Test Loss = 0.250460, Learning Rate = 3.781849e-06\n",
      "Epoch 14678/20000: Train Loss = 0.435204, Test Loss = 0.249722, Learning Rate = 3.780412e-06\n",
      "Epoch 14679/20000: Train Loss = 0.436003, Test Loss = 0.250526, Learning Rate = 3.778976e-06\n",
      "Epoch 14680/20000: Train Loss = 0.435547, Test Loss = 0.248246, Learning Rate = 3.777540e-06\n",
      "Epoch 14681/20000: Train Loss = 0.435921, Test Loss = 0.251657, Learning Rate = 3.776104e-06\n",
      "Epoch 14682/20000: Train Loss = 0.435536, Test Loss = 0.247679, Learning Rate = 3.774669e-06\n",
      "Epoch 14683/20000: Train Loss = 0.435473, Test Loss = 0.251061, Learning Rate = 3.773235e-06\n",
      "Epoch 14684/20000: Train Loss = 0.435229, Test Loss = 0.251264, Learning Rate = 3.771801e-06\n",
      "Epoch 14685/20000: Train Loss = 0.435297, Test Loss = 0.247001, Learning Rate = 3.770368e-06\n",
      "Epoch 14686/20000: Train Loss = 0.435318, Test Loss = 0.248199, Learning Rate = 3.768936e-06\n",
      "Epoch 14687/20000: Train Loss = 0.435615, Test Loss = 0.248106, Learning Rate = 3.767504e-06\n",
      "Epoch 14688/20000: Train Loss = 0.435187, Test Loss = 0.249570, Learning Rate = 3.766072e-06\n",
      "Epoch 14689/20000: Train Loss = 0.435365, Test Loss = 0.250691, Learning Rate = 3.764641e-06\n",
      "Epoch 14690/20000: Train Loss = 0.435297, Test Loss = 0.249742, Learning Rate = 3.763211e-06\n",
      "Epoch 14691/20000: Train Loss = 0.435253, Test Loss = 0.248737, Learning Rate = 3.761781e-06\n",
      "Epoch 14692/20000: Train Loss = 0.435464, Test Loss = 0.249229, Learning Rate = 3.760351e-06\n",
      "Epoch 14693/20000: Train Loss = 0.435557, Test Loss = 0.249827, Learning Rate = 3.758922e-06\n",
      "Epoch 14694/20000: Train Loss = 0.435212, Test Loss = 0.250812, Learning Rate = 3.757494e-06\n",
      "Epoch 14695/20000: Train Loss = 0.435484, Test Loss = 0.249153, Learning Rate = 3.756066e-06\n",
      "Epoch 14696/20000: Train Loss = 0.435383, Test Loss = 0.250001, Learning Rate = 3.754639e-06\n",
      "Epoch 14697/20000: Train Loss = 0.435146, Test Loss = 0.250786, Learning Rate = 3.753213e-06\n",
      "Epoch 14698/20000: Train Loss = 0.435524, Test Loss = 0.251574, Learning Rate = 3.751786e-06\n",
      "Epoch 14699/20000: Train Loss = 0.435454, Test Loss = 0.252922, Learning Rate = 3.750361e-06\n",
      "Epoch 14700/20000: Train Loss = 0.435272, Test Loss = 0.253839, Learning Rate = 3.748936e-06\n",
      "Epoch 14701/20000: Train Loss = 0.435369, Test Loss = 0.252755, Learning Rate = 3.747511e-06\n",
      "Epoch 14702/20000: Train Loss = 0.435199, Test Loss = 0.254421, Learning Rate = 3.746087e-06\n",
      "Epoch 14703/20000: Train Loss = 0.435487, Test Loss = 0.252777, Learning Rate = 3.744664e-06\n",
      "Epoch 14704/20000: Train Loss = 0.435200, Test Loss = 0.251422, Learning Rate = 3.743241e-06\n",
      "Epoch 14705/20000: Train Loss = 0.435478, Test Loss = 0.253220, Learning Rate = 3.741819e-06\n",
      "Epoch 14706/20000: Train Loss = 0.435498, Test Loss = 0.250668, Learning Rate = 3.740397e-06\n",
      "Epoch 14707/20000: Train Loss = 0.435240, Test Loss = 0.250851, Learning Rate = 3.738976e-06\n",
      "Epoch 14708/20000: Train Loss = 0.435153, Test Loss = 0.250966, Learning Rate = 3.737555e-06\n",
      "Epoch 14709/20000: Train Loss = 0.435211, Test Loss = 0.252816, Learning Rate = 3.736135e-06\n",
      "Epoch 14710/20000: Train Loss = 0.435348, Test Loss = 0.251305, Learning Rate = 3.734715e-06\n",
      "Epoch 14711/20000: Train Loss = 0.435305, Test Loss = 0.252366, Learning Rate = 3.733296e-06\n",
      "Epoch 14712/20000: Train Loss = 0.435355, Test Loss = 0.252073, Learning Rate = 3.731878e-06\n",
      "Epoch 14713/20000: Train Loss = 0.435347, Test Loss = 0.250502, Learning Rate = 3.730460e-06\n",
      "Epoch 14714/20000: Train Loss = 0.435322, Test Loss = 0.251680, Learning Rate = 3.729042e-06\n",
      "Epoch 14715/20000: Train Loss = 0.436364, Test Loss = 0.251575, Learning Rate = 3.727625e-06\n",
      "Epoch 14716/20000: Train Loss = 0.435249, Test Loss = 0.252412, Learning Rate = 3.726209e-06\n",
      "Epoch 14717/20000: Train Loss = 0.435444, Test Loss = 0.252702, Learning Rate = 3.724793e-06\n",
      "Epoch 14718/20000: Train Loss = 0.435554, Test Loss = 0.250754, Learning Rate = 3.723378e-06\n",
      "Epoch 14719/20000: Train Loss = 0.435547, Test Loss = 0.255080, Learning Rate = 3.721963e-06\n",
      "Epoch 14720/20000: Train Loss = 0.435426, Test Loss = 0.252359, Learning Rate = 3.720549e-06\n",
      "Epoch 14721/20000: Train Loss = 0.435364, Test Loss = 0.252590, Learning Rate = 3.719135e-06\n",
      "Epoch 14722/20000: Train Loss = 0.435617, Test Loss = 0.250710, Learning Rate = 3.717722e-06\n",
      "Epoch 14723/20000: Train Loss = 0.435141, Test Loss = 0.249023, Learning Rate = 3.716309e-06\n",
      "Epoch 14724/20000: Train Loss = 0.435319, Test Loss = 0.252181, Learning Rate = 3.714897e-06\n",
      "Epoch 14725/20000: Train Loss = 0.435813, Test Loss = 0.250598, Learning Rate = 3.713485e-06\n",
      "Epoch 14726/20000: Train Loss = 0.436394, Test Loss = 0.252494, Learning Rate = 3.712074e-06\n",
      "Epoch 14727/20000: Train Loss = 0.435418, Test Loss = 0.251486, Learning Rate = 3.710664e-06\n",
      "Epoch 14728/20000: Train Loss = 0.435427, Test Loss = 0.249225, Learning Rate = 3.709254e-06\n",
      "Epoch 14729/20000: Train Loss = 0.435661, Test Loss = 0.250746, Learning Rate = 3.707844e-06\n",
      "Epoch 14730/20000: Train Loss = 0.435373, Test Loss = 0.254198, Learning Rate = 3.706436e-06\n",
      "Epoch 14731/20000: Train Loss = 0.435451, Test Loss = 0.252270, Learning Rate = 3.705027e-06\n",
      "Epoch 14732/20000: Train Loss = 0.435390, Test Loss = 0.251652, Learning Rate = 3.703619e-06\n",
      "Epoch 14733/20000: Train Loss = 0.436070, Test Loss = 0.252947, Learning Rate = 3.702212e-06\n",
      "Epoch 14734/20000: Train Loss = 0.435362, Test Loss = 0.254377, Learning Rate = 3.700805e-06\n",
      "Epoch 14735/20000: Train Loss = 0.435401, Test Loss = 0.252465, Learning Rate = 3.699399e-06\n",
      "Epoch 14736/20000: Train Loss = 0.435441, Test Loss = 0.252258, Learning Rate = 3.697994e-06\n",
      "Epoch 14737/20000: Train Loss = 0.435606, Test Loss = 0.255250, Learning Rate = 3.696588e-06\n",
      "Epoch 14738/20000: Train Loss = 0.435141, Test Loss = 0.255191, Learning Rate = 3.695184e-06\n",
      "Epoch 14739/20000: Train Loss = 0.435303, Test Loss = 0.255500, Learning Rate = 3.693780e-06\n",
      "Epoch 14740/20000: Train Loss = 0.435305, Test Loss = 0.255984, Learning Rate = 3.692376e-06\n",
      "Epoch 14741/20000: Train Loss = 0.435328, Test Loss = 0.257892, Learning Rate = 3.690973e-06\n",
      "Epoch 14742/20000: Train Loss = 0.435335, Test Loss = 0.256337, Learning Rate = 3.689571e-06\n",
      "Epoch 14743/20000: Train Loss = 0.435222, Test Loss = 0.257013, Learning Rate = 3.688169e-06\n",
      "Epoch 14744/20000: Train Loss = 0.435238, Test Loss = 0.255657, Learning Rate = 3.686767e-06\n",
      "Epoch 14745/20000: Train Loss = 0.435376, Test Loss = 0.256662, Learning Rate = 3.685367e-06\n",
      "Epoch 14746/20000: Train Loss = 0.435399, Test Loss = 0.254639, Learning Rate = 3.683966e-06\n",
      "Epoch 14747/20000: Train Loss = 0.435354, Test Loss = 0.253198, Learning Rate = 3.682566e-06\n",
      "Epoch 14748/20000: Train Loss = 0.435337, Test Loss = 0.253735, Learning Rate = 3.681167e-06\n",
      "Epoch 14749/20000: Train Loss = 0.435198, Test Loss = 0.252121, Learning Rate = 3.679768e-06\n",
      "Epoch 14750/20000: Train Loss = 0.436013, Test Loss = 0.252071, Learning Rate = 3.678370e-06\n",
      "Epoch 14751/20000: Train Loss = 0.435629, Test Loss = 0.255437, Learning Rate = 3.676972e-06\n",
      "Epoch 14752/20000: Train Loss = 0.435461, Test Loss = 0.253154, Learning Rate = 3.675575e-06\n",
      "Epoch 14753/20000: Train Loss = 0.435236, Test Loss = 0.252505, Learning Rate = 3.674179e-06\n",
      "Epoch 14754/20000: Train Loss = 0.437479, Test Loss = 0.252795, Learning Rate = 3.672783e-06\n",
      "Epoch 14755/20000: Train Loss = 0.435088, Test Loss = 0.251596, Learning Rate = 3.671387e-06\n",
      "Epoch 14756/20000: Train Loss = 0.435560, Test Loss = 0.252919, Learning Rate = 3.669992e-06\n",
      "Epoch 14757/20000: Train Loss = 0.435648, Test Loss = 0.253815, Learning Rate = 3.668598e-06\n",
      "Epoch 14758/20000: Train Loss = 0.435834, Test Loss = 0.250166, Learning Rate = 3.667204e-06\n",
      "Epoch 14759/20000: Train Loss = 0.435503, Test Loss = 0.252392, Learning Rate = 3.665810e-06\n",
      "Epoch 14760/20000: Train Loss = 0.435401, Test Loss = 0.251258, Learning Rate = 3.664417e-06\n",
      "Epoch 14761/20000: Train Loss = 0.435430, Test Loss = 0.251602, Learning Rate = 3.663025e-06\n",
      "Epoch 14762/20000: Train Loss = 0.435567, Test Loss = 0.251681, Learning Rate = 3.661633e-06\n",
      "Epoch 14763/20000: Train Loss = 0.435056, Test Loss = 0.253297, Learning Rate = 3.660242e-06\n",
      "Epoch 14764/20000: Train Loss = 0.435094, Test Loss = 0.256514, Learning Rate = 3.658851e-06\n",
      "Epoch 14765/20000: Train Loss = 0.435133, Test Loss = 0.255782, Learning Rate = 3.657461e-06\n",
      "Epoch 14766/20000: Train Loss = 0.435417, Test Loss = 0.256481, Learning Rate = 3.656071e-06\n",
      "Epoch 14767/20000: Train Loss = 0.435410, Test Loss = 0.255240, Learning Rate = 3.654682e-06\n",
      "Epoch 14768/20000: Train Loss = 0.435248, Test Loss = 0.253761, Learning Rate = 3.653293e-06\n",
      "Epoch 14769/20000: Train Loss = 0.435642, Test Loss = 0.253337, Learning Rate = 3.651905e-06\n",
      "Epoch 14770/20000: Train Loss = 0.435620, Test Loss = 0.253483, Learning Rate = 3.650517e-06\n",
      "Epoch 14771/20000: Train Loss = 0.436053, Test Loss = 0.253890, Learning Rate = 3.649130e-06\n",
      "Epoch 14772/20000: Train Loss = 0.435545, Test Loss = 0.253361, Learning Rate = 3.647744e-06\n",
      "Epoch 14773/20000: Train Loss = 0.435607, Test Loss = 0.250479, Learning Rate = 3.646357e-06\n",
      "Epoch 14774/20000: Train Loss = 0.435339, Test Loss = 0.253981, Learning Rate = 3.644972e-06\n",
      "Epoch 14775/20000: Train Loss = 0.435372, Test Loss = 0.250951, Learning Rate = 3.643587e-06\n",
      "Epoch 14776/20000: Train Loss = 0.435277, Test Loss = 0.252009, Learning Rate = 3.642203e-06\n",
      "Epoch 14777/20000: Train Loss = 0.435206, Test Loss = 0.251567, Learning Rate = 3.640819e-06\n",
      "Epoch 14778/20000: Train Loss = 0.435363, Test Loss = 0.249558, Learning Rate = 3.639435e-06\n",
      "Epoch 14779/20000: Train Loss = 0.435316, Test Loss = 0.250662, Learning Rate = 3.638052e-06\n",
      "Epoch 14780/20000: Train Loss = 0.435257, Test Loss = 0.249945, Learning Rate = 3.636670e-06\n",
      "Epoch 14781/20000: Train Loss = 0.435382, Test Loss = 0.249477, Learning Rate = 3.635288e-06\n",
      "Epoch 14782/20000: Train Loss = 0.435381, Test Loss = 0.249566, Learning Rate = 3.633907e-06\n",
      "Epoch 14783/20000: Train Loss = 0.435341, Test Loss = 0.250593, Learning Rate = 3.632526e-06\n",
      "Epoch 14784/20000: Train Loss = 0.435492, Test Loss = 0.252000, Learning Rate = 3.631146e-06\n",
      "Epoch 14785/20000: Train Loss = 0.435327, Test Loss = 0.253309, Learning Rate = 3.629766e-06\n",
      "Epoch 14786/20000: Train Loss = 0.436051, Test Loss = 0.250095, Learning Rate = 3.628387e-06\n",
      "Epoch 14787/20000: Train Loss = 0.435391, Test Loss = 0.251058, Learning Rate = 3.627008e-06\n",
      "Epoch 14788/20000: Train Loss = 0.435135, Test Loss = 0.249279, Learning Rate = 3.625630e-06\n",
      "Epoch 14789/20000: Train Loss = 0.435314, Test Loss = 0.253020, Learning Rate = 3.624252e-06\n",
      "Epoch 14790/20000: Train Loss = 0.435197, Test Loss = 0.252222, Learning Rate = 3.622875e-06\n",
      "Epoch 14791/20000: Train Loss = 0.435594, Test Loss = 0.252563, Learning Rate = 3.621499e-06\n",
      "Epoch 14792/20000: Train Loss = 0.435413, Test Loss = 0.254444, Learning Rate = 3.620122e-06\n",
      "Epoch 14793/20000: Train Loss = 0.435473, Test Loss = 0.251941, Learning Rate = 3.618747e-06\n",
      "Epoch 14794/20000: Train Loss = 0.435266, Test Loss = 0.255602, Learning Rate = 3.617372e-06\n",
      "Epoch 14795/20000: Train Loss = 0.435237, Test Loss = 0.254310, Learning Rate = 3.615997e-06\n",
      "Epoch 14796/20000: Train Loss = 0.435240, Test Loss = 0.254174, Learning Rate = 3.614623e-06\n",
      "Epoch 14797/20000: Train Loss = 0.435262, Test Loss = 0.255409, Learning Rate = 3.613250e-06\n",
      "Epoch 14798/20000: Train Loss = 0.435222, Test Loss = 0.252764, Learning Rate = 3.611877e-06\n",
      "Epoch 14799/20000: Train Loss = 0.435196, Test Loss = 0.254847, Learning Rate = 3.610505e-06\n",
      "Epoch 14800/20000: Train Loss = 0.435374, Test Loss = 0.252644, Learning Rate = 3.609133e-06\n",
      "Epoch 14801/20000: Train Loss = 0.435142, Test Loss = 0.253195, Learning Rate = 3.607761e-06\n",
      "Epoch 14802/20000: Train Loss = 0.435211, Test Loss = 0.254160, Learning Rate = 3.606391e-06\n",
      "Epoch 14803/20000: Train Loss = 0.435208, Test Loss = 0.254036, Learning Rate = 3.605020e-06\n",
      "Epoch 14804/20000: Train Loss = 0.435361, Test Loss = 0.251422, Learning Rate = 3.603650e-06\n",
      "Epoch 14805/20000: Train Loss = 0.435316, Test Loss = 0.253868, Learning Rate = 3.602281e-06\n",
      "Epoch 14806/20000: Train Loss = 0.435172, Test Loss = 0.251630, Learning Rate = 3.600912e-06\n",
      "Epoch 14807/20000: Train Loss = 0.435259, Test Loss = 0.252690, Learning Rate = 3.599544e-06\n",
      "Epoch 14808/20000: Train Loss = 0.435159, Test Loss = 0.252352, Learning Rate = 3.598176e-06\n",
      "Epoch 14809/20000: Train Loss = 0.435229, Test Loss = 0.253404, Learning Rate = 3.596809e-06\n",
      "Epoch 14810/20000: Train Loss = 0.435326, Test Loss = 0.253541, Learning Rate = 3.595442e-06\n",
      "Epoch 14811/20000: Train Loss = 0.435270, Test Loss = 0.252646, Learning Rate = 3.594076e-06\n",
      "Epoch 14812/20000: Train Loss = 0.435727, Test Loss = 0.252536, Learning Rate = 3.592711e-06\n",
      "Epoch 14813/20000: Train Loss = 0.435432, Test Loss = 0.253146, Learning Rate = 3.591345e-06\n",
      "Epoch 14814/20000: Train Loss = 0.435836, Test Loss = 0.253715, Learning Rate = 3.589981e-06\n",
      "Epoch 14815/20000: Train Loss = 0.435291, Test Loss = 0.253800, Learning Rate = 3.588617e-06\n",
      "Epoch 14816/20000: Train Loss = 0.435460, Test Loss = 0.251199, Learning Rate = 3.587253e-06\n",
      "Epoch 14817/20000: Train Loss = 0.435676, Test Loss = 0.249631, Learning Rate = 3.585890e-06\n",
      "Epoch 14818/20000: Train Loss = 0.435376, Test Loss = 0.250489, Learning Rate = 3.584528e-06\n",
      "Epoch 14819/20000: Train Loss = 0.435378, Test Loss = 0.249616, Learning Rate = 3.583166e-06\n",
      "Epoch 14820/20000: Train Loss = 0.435284, Test Loss = 0.248912, Learning Rate = 3.581804e-06\n",
      "Epoch 14821/20000: Train Loss = 0.435396, Test Loss = 0.249430, Learning Rate = 3.580443e-06\n",
      "Epoch 14822/20000: Train Loss = 0.435250, Test Loss = 0.249932, Learning Rate = 3.579083e-06\n",
      "Epoch 14823/20000: Train Loss = 0.435278, Test Loss = 0.250219, Learning Rate = 3.577723e-06\n",
      "Epoch 14824/20000: Train Loss = 0.435591, Test Loss = 0.251526, Learning Rate = 3.576363e-06\n",
      "Epoch 14825/20000: Train Loss = 0.435046, Test Loss = 0.248223, Learning Rate = 3.575004e-06\n",
      "Epoch 14826/20000: Train Loss = 0.435598, Test Loss = 0.250376, Learning Rate = 3.573646e-06\n",
      "Epoch 14827/20000: Train Loss = 0.435362, Test Loss = 0.250326, Learning Rate = 3.572288e-06\n",
      "Epoch 14828/20000: Train Loss = 0.435477, Test Loss = 0.249559, Learning Rate = 3.570931e-06\n",
      "Epoch 14829/20000: Train Loss = 0.435010, Test Loss = 0.249542, Learning Rate = 3.569574e-06\n",
      "Epoch 14830/20000: Train Loss = 0.435236, Test Loss = 0.252674, Learning Rate = 3.568217e-06\n",
      "Epoch 14831/20000: Train Loss = 0.435190, Test Loss = 0.250983, Learning Rate = 3.566862e-06\n",
      "Epoch 14832/20000: Train Loss = 0.435502, Test Loss = 0.250314, Learning Rate = 3.565506e-06\n",
      "Epoch 14833/20000: Train Loss = 0.435212, Test Loss = 0.249929, Learning Rate = 3.564151e-06\n",
      "Epoch 14834/20000: Train Loss = 0.435503, Test Loss = 0.250188, Learning Rate = 3.562797e-06\n",
      "Epoch 14835/20000: Train Loss = 0.435461, Test Loss = 0.251333, Learning Rate = 3.561443e-06\n",
      "Epoch 14836/20000: Train Loss = 0.435188, Test Loss = 0.252000, Learning Rate = 3.560090e-06\n",
      "Epoch 14837/20000: Train Loss = 0.435439, Test Loss = 0.251872, Learning Rate = 3.558737e-06\n",
      "Epoch 14838/20000: Train Loss = 0.435244, Test Loss = 0.251465, Learning Rate = 3.557385e-06\n",
      "Epoch 14839/20000: Train Loss = 0.435130, Test Loss = 0.251434, Learning Rate = 3.556034e-06\n",
      "Epoch 14840/20000: Train Loss = 0.435218, Test Loss = 0.249892, Learning Rate = 3.554682e-06\n",
      "Epoch 14841/20000: Train Loss = 0.435462, Test Loss = 0.250638, Learning Rate = 3.553332e-06\n",
      "Epoch 14842/20000: Train Loss = 0.435262, Test Loss = 0.253366, Learning Rate = 3.551981e-06\n",
      "Epoch 14843/20000: Train Loss = 0.435385, Test Loss = 0.252847, Learning Rate = 3.550632e-06\n",
      "Epoch 14844/20000: Train Loss = 0.435306, Test Loss = 0.253301, Learning Rate = 3.549283e-06\n",
      "Epoch 14845/20000: Train Loss = 0.435348, Test Loss = 0.253833, Learning Rate = 3.547934e-06\n",
      "Epoch 14846/20000: Train Loss = 0.435614, Test Loss = 0.252848, Learning Rate = 3.546586e-06\n",
      "Epoch 14847/20000: Train Loss = 0.435430, Test Loss = 0.254057, Learning Rate = 3.545238e-06\n",
      "Epoch 14848/20000: Train Loss = 0.435265, Test Loss = 0.254478, Learning Rate = 3.543891e-06\n",
      "Epoch 14849/20000: Train Loss = 0.435146, Test Loss = 0.253849, Learning Rate = 3.542545e-06\n",
      "Epoch 14850/20000: Train Loss = 0.435922, Test Loss = 0.250859, Learning Rate = 3.541199e-06\n",
      "Epoch 14851/20000: Train Loss = 0.435561, Test Loss = 0.251733, Learning Rate = 3.539853e-06\n",
      "Epoch 14852/20000: Train Loss = 0.435383, Test Loss = 0.253722, Learning Rate = 3.538508e-06\n",
      "Epoch 14853/20000: Train Loss = 0.435340, Test Loss = 0.250803, Learning Rate = 3.537163e-06\n",
      "Epoch 14854/20000: Train Loss = 0.435699, Test Loss = 0.251661, Learning Rate = 3.535819e-06\n",
      "Epoch 14855/20000: Train Loss = 0.435253, Test Loss = 0.253374, Learning Rate = 3.534476e-06\n",
      "Epoch 14856/20000: Train Loss = 0.435447, Test Loss = 0.252901, Learning Rate = 3.533133e-06\n",
      "Epoch 14857/20000: Train Loss = 0.435147, Test Loss = 0.254161, Learning Rate = 3.531790e-06\n",
      "Epoch 14858/20000: Train Loss = 0.435672, Test Loss = 0.256793, Learning Rate = 3.530448e-06\n",
      "Epoch 14859/20000: Train Loss = 0.435315, Test Loss = 0.253398, Learning Rate = 3.529107e-06\n",
      "Epoch 14860/20000: Train Loss = 0.435168, Test Loss = 0.251913, Learning Rate = 3.527766e-06\n",
      "Epoch 14861/20000: Train Loss = 0.435141, Test Loss = 0.251824, Learning Rate = 3.526425e-06\n",
      "Epoch 14862/20000: Train Loss = 0.435424, Test Loss = 0.253423, Learning Rate = 3.525086e-06\n",
      "Epoch 14863/20000: Train Loss = 0.435603, Test Loss = 0.252936, Learning Rate = 3.523746e-06\n",
      "Epoch 14864/20000: Train Loss = 0.435421, Test Loss = 0.255183, Learning Rate = 3.522407e-06\n",
      "Epoch 14865/20000: Train Loss = 0.435746, Test Loss = 0.249584, Learning Rate = 3.521069e-06\n",
      "Epoch 14866/20000: Train Loss = 0.436074, Test Loss = 0.253782, Learning Rate = 3.519731e-06\n",
      "Epoch 14867/20000: Train Loss = 0.435534, Test Loss = 0.250262, Learning Rate = 3.518393e-06\n",
      "Epoch 14868/20000: Train Loss = 0.435397, Test Loss = 0.253644, Learning Rate = 3.517057e-06\n",
      "Epoch 14869/20000: Train Loss = 0.435213, Test Loss = 0.252900, Learning Rate = 3.515720e-06\n",
      "Epoch 14870/20000: Train Loss = 0.435178, Test Loss = 0.252907, Learning Rate = 3.514384e-06\n",
      "Epoch 14871/20000: Train Loss = 0.435541, Test Loss = 0.254055, Learning Rate = 3.513049e-06\n",
      "Epoch 14872/20000: Train Loss = 0.435121, Test Loss = 0.251483, Learning Rate = 3.511714e-06\n",
      "Epoch 14873/20000: Train Loss = 0.435468, Test Loss = 0.250801, Learning Rate = 3.510380e-06\n",
      "Epoch 14874/20000: Train Loss = 0.435083, Test Loss = 0.252800, Learning Rate = 3.509046e-06\n",
      "Epoch 14875/20000: Train Loss = 0.435167, Test Loss = 0.253462, Learning Rate = 3.507713e-06\n",
      "Epoch 14876/20000: Train Loss = 0.435590, Test Loss = 0.253387, Learning Rate = 3.506380e-06\n",
      "Epoch 14877/20000: Train Loss = 0.435435, Test Loss = 0.253181, Learning Rate = 3.505047e-06\n",
      "Epoch 14878/20000: Train Loss = 0.435061, Test Loss = 0.251088, Learning Rate = 3.503716e-06\n",
      "Epoch 14879/20000: Train Loss = 0.435685, Test Loss = 0.252491, Learning Rate = 3.502384e-06\n",
      "Epoch 14880/20000: Train Loss = 0.435084, Test Loss = 0.251875, Learning Rate = 3.501053e-06\n",
      "Epoch 14881/20000: Train Loss = 0.435062, Test Loss = 0.251108, Learning Rate = 3.499723e-06\n",
      "Epoch 14882/20000: Train Loss = 0.435237, Test Loss = 0.252828, Learning Rate = 3.498393e-06\n",
      "Epoch 14883/20000: Train Loss = 0.435114, Test Loss = 0.252523, Learning Rate = 3.497064e-06\n",
      "Epoch 14884/20000: Train Loss = 0.435414, Test Loss = 0.251696, Learning Rate = 3.495735e-06\n",
      "Epoch 14885/20000: Train Loss = 0.435439, Test Loss = 0.251197, Learning Rate = 3.494407e-06\n",
      "Epoch 14886/20000: Train Loss = 0.435814, Test Loss = 0.250534, Learning Rate = 3.493079e-06\n",
      "Epoch 14887/20000: Train Loss = 0.435443, Test Loss = 0.251298, Learning Rate = 3.491752e-06\n",
      "Epoch 14888/20000: Train Loss = 0.435131, Test Loss = 0.251602, Learning Rate = 3.490425e-06\n",
      "Epoch 14889/20000: Train Loss = 0.435365, Test Loss = 0.252510, Learning Rate = 3.489099e-06\n",
      "Epoch 14890/20000: Train Loss = 0.435560, Test Loss = 0.252427, Learning Rate = 3.487773e-06\n",
      "Epoch 14891/20000: Train Loss = 0.435577, Test Loss = 0.252675, Learning Rate = 3.486448e-06\n",
      "Epoch 14892/20000: Train Loss = 0.435323, Test Loss = 0.250962, Learning Rate = 3.485123e-06\n",
      "Epoch 14893/20000: Train Loss = 0.435516, Test Loss = 0.253405, Learning Rate = 3.483799e-06\n",
      "Epoch 14894/20000: Train Loss = 0.435598, Test Loss = 0.250652, Learning Rate = 3.482475e-06\n",
      "Epoch 14895/20000: Train Loss = 0.435368, Test Loss = 0.251502, Learning Rate = 3.481152e-06\n",
      "Epoch 14896/20000: Train Loss = 0.435662, Test Loss = 0.249995, Learning Rate = 3.479829e-06\n",
      "Epoch 14897/20000: Train Loss = 0.435666, Test Loss = 0.252385, Learning Rate = 3.478507e-06\n",
      "Epoch 14898/20000: Train Loss = 0.435279, Test Loss = 0.251937, Learning Rate = 3.477185e-06\n",
      "Epoch 14899/20000: Train Loss = 0.435267, Test Loss = 0.251259, Learning Rate = 3.475864e-06\n",
      "Epoch 14900/20000: Train Loss = 0.435308, Test Loss = 0.252738, Learning Rate = 3.474543e-06\n",
      "Epoch 14901/20000: Train Loss = 0.435066, Test Loss = 0.250477, Learning Rate = 3.473223e-06\n",
      "Epoch 14902/20000: Train Loss = 0.435186, Test Loss = 0.250229, Learning Rate = 3.471903e-06\n",
      "Epoch 14903/20000: Train Loss = 0.435023, Test Loss = 0.254071, Learning Rate = 3.470584e-06\n",
      "Epoch 14904/20000: Train Loss = 0.435128, Test Loss = 0.253100, Learning Rate = 3.469265e-06\n",
      "Epoch 14905/20000: Train Loss = 0.435546, Test Loss = 0.251391, Learning Rate = 3.467947e-06\n",
      "Epoch 14906/20000: Train Loss = 0.434983, Test Loss = 0.253973, Learning Rate = 3.466629e-06\n",
      "Epoch 14907/20000: Train Loss = 0.435149, Test Loss = 0.252824, Learning Rate = 3.465312e-06\n",
      "Epoch 14908/20000: Train Loss = 0.435377, Test Loss = 0.252464, Learning Rate = 3.463995e-06\n",
      "Epoch 14909/20000: Train Loss = 0.435670, Test Loss = 0.255925, Learning Rate = 3.462679e-06\n",
      "Epoch 14910/20000: Train Loss = 0.435798, Test Loss = 0.253804, Learning Rate = 3.461363e-06\n",
      "Epoch 14911/20000: Train Loss = 0.435510, Test Loss = 0.251695, Learning Rate = 3.460048e-06\n",
      "Epoch 14912/20000: Train Loss = 0.435245, Test Loss = 0.253362, Learning Rate = 3.458733e-06\n",
      "Epoch 14913/20000: Train Loss = 0.435798, Test Loss = 0.253544, Learning Rate = 3.457419e-06\n",
      "Epoch 14914/20000: Train Loss = 0.435278, Test Loss = 0.252523, Learning Rate = 3.456105e-06\n",
      "Epoch 14915/20000: Train Loss = 0.436150, Test Loss = 0.252320, Learning Rate = 3.454792e-06\n",
      "Epoch 14916/20000: Train Loss = 0.435471, Test Loss = 0.252064, Learning Rate = 3.453479e-06\n",
      "Epoch 14917/20000: Train Loss = 0.435215, Test Loss = 0.252027, Learning Rate = 3.452167e-06\n",
      "Epoch 14918/20000: Train Loss = 0.435162, Test Loss = 0.251287, Learning Rate = 3.450856e-06\n",
      "Epoch 14919/20000: Train Loss = 0.435457, Test Loss = 0.251798, Learning Rate = 3.449544e-06\n",
      "Epoch 14920/20000: Train Loss = 0.435207, Test Loss = 0.250038, Learning Rate = 3.448234e-06\n",
      "Epoch 14921/20000: Train Loss = 0.435484, Test Loss = 0.252391, Learning Rate = 3.446923e-06\n",
      "Epoch 14922/20000: Train Loss = 0.435259, Test Loss = 0.253879, Learning Rate = 3.445614e-06\n",
      "Epoch 14923/20000: Train Loss = 0.435158, Test Loss = 0.255008, Learning Rate = 3.444304e-06\n",
      "Epoch 14924/20000: Train Loss = 0.435162, Test Loss = 0.254916, Learning Rate = 3.442996e-06\n",
      "Epoch 14925/20000: Train Loss = 0.435326, Test Loss = 0.253085, Learning Rate = 3.441687e-06\n",
      "Epoch 14926/20000: Train Loss = 0.435451, Test Loss = 0.252733, Learning Rate = 3.440380e-06\n",
      "Epoch 14927/20000: Train Loss = 0.435003, Test Loss = 0.252421, Learning Rate = 3.439072e-06\n",
      "Epoch 14928/20000: Train Loss = 0.435268, Test Loss = 0.252018, Learning Rate = 3.437766e-06\n",
      "Epoch 14929/20000: Train Loss = 0.435338, Test Loss = 0.253849, Learning Rate = 3.436459e-06\n",
      "Epoch 14930/20000: Train Loss = 0.435142, Test Loss = 0.256072, Learning Rate = 3.435154e-06\n",
      "Epoch 14931/20000: Train Loss = 0.435214, Test Loss = 0.255000, Learning Rate = 3.433848e-06\n",
      "Epoch 14932/20000: Train Loss = 0.435253, Test Loss = 0.255822, Learning Rate = 3.432544e-06\n",
      "Epoch 14933/20000: Train Loss = 0.435209, Test Loss = 0.253134, Learning Rate = 3.431239e-06\n",
      "Epoch 14934/20000: Train Loss = 0.435542, Test Loss = 0.254402, Learning Rate = 3.429936e-06\n",
      "Epoch 14935/20000: Train Loss = 0.435337, Test Loss = 0.255231, Learning Rate = 3.428632e-06\n",
      "Epoch 14936/20000: Train Loss = 0.435263, Test Loss = 0.254678, Learning Rate = 3.427329e-06\n",
      "Epoch 14937/20000: Train Loss = 0.435143, Test Loss = 0.255160, Learning Rate = 3.426027e-06\n",
      "Epoch 14938/20000: Train Loss = 0.435208, Test Loss = 0.253338, Learning Rate = 3.424725e-06\n",
      "Epoch 14939/20000: Train Loss = 0.435282, Test Loss = 0.251300, Learning Rate = 3.423424e-06\n",
      "Epoch 14940/20000: Train Loss = 0.435104, Test Loss = 0.254973, Learning Rate = 3.422123e-06\n",
      "Epoch 14941/20000: Train Loss = 0.435341, Test Loss = 0.253173, Learning Rate = 3.420823e-06\n",
      "Epoch 14942/20000: Train Loss = 0.435264, Test Loss = 0.253254, Learning Rate = 3.419523e-06\n",
      "Epoch 14943/20000: Train Loss = 0.435203, Test Loss = 0.254054, Learning Rate = 3.418224e-06\n",
      "Epoch 14944/20000: Train Loss = 0.435650, Test Loss = 0.254110, Learning Rate = 3.416925e-06\n",
      "Epoch 14945/20000: Train Loss = 0.435324, Test Loss = 0.252586, Learning Rate = 3.415627e-06\n",
      "Epoch 14946/20000: Train Loss = 0.435179, Test Loss = 0.252628, Learning Rate = 3.414329e-06\n",
      "Epoch 14947/20000: Train Loss = 0.435370, Test Loss = 0.253009, Learning Rate = 3.413031e-06\n",
      "Epoch 14948/20000: Train Loss = 0.435475, Test Loss = 0.253468, Learning Rate = 3.411735e-06\n",
      "Epoch 14949/20000: Train Loss = 0.435374, Test Loss = 0.250239, Learning Rate = 3.410438e-06\n",
      "Epoch 14950/20000: Train Loss = 0.435129, Test Loss = 0.251132, Learning Rate = 3.409142e-06\n",
      "Epoch 14951/20000: Train Loss = 0.435308, Test Loss = 0.251729, Learning Rate = 3.407847e-06\n",
      "Epoch 14952/20000: Train Loss = 0.435177, Test Loss = 0.254919, Learning Rate = 3.406552e-06\n",
      "Epoch 14953/20000: Train Loss = 0.435330, Test Loss = 0.253802, Learning Rate = 3.405258e-06\n",
      "Epoch 14954/20000: Train Loss = 0.436402, Test Loss = 0.254894, Learning Rate = 3.403964e-06\n",
      "Epoch 14955/20000: Train Loss = 0.435429, Test Loss = 0.248359, Learning Rate = 3.402670e-06\n",
      "Epoch 14956/20000: Train Loss = 0.435280, Test Loss = 0.249358, Learning Rate = 3.401377e-06\n",
      "Epoch 14957/20000: Train Loss = 0.436049, Test Loss = 0.252038, Learning Rate = 3.400085e-06\n",
      "Epoch 14958/20000: Train Loss = 0.434698, Test Loss = 0.249178, Learning Rate = 3.398793e-06\n",
      "Epoch 14959/20000: Train Loss = 0.435377, Test Loss = 0.250518, Learning Rate = 3.397502e-06\n",
      "Epoch 14960/20000: Train Loss = 0.435231, Test Loss = 0.249578, Learning Rate = 3.396211e-06\n",
      "Epoch 14961/20000: Train Loss = 0.435657, Test Loss = 0.251417, Learning Rate = 3.394920e-06\n",
      "Epoch 14962/20000: Train Loss = 0.435125, Test Loss = 0.250781, Learning Rate = 3.393630e-06\n",
      "Epoch 14963/20000: Train Loss = 0.435893, Test Loss = 0.255743, Learning Rate = 3.392341e-06\n",
      "Epoch 14964/20000: Train Loss = 0.435502, Test Loss = 0.250413, Learning Rate = 3.391052e-06\n",
      "Epoch 14965/20000: Train Loss = 0.435549, Test Loss = 0.250621, Learning Rate = 3.389763e-06\n",
      "Epoch 14966/20000: Train Loss = 0.435451, Test Loss = 0.250063, Learning Rate = 3.388475e-06\n",
      "Epoch 14967/20000: Train Loss = 0.435363, Test Loss = 0.251614, Learning Rate = 3.387188e-06\n",
      "Epoch 14968/20000: Train Loss = 0.435312, Test Loss = 0.250141, Learning Rate = 3.385901e-06\n",
      "Epoch 14969/20000: Train Loss = 0.435043, Test Loss = 0.248904, Learning Rate = 3.384614e-06\n",
      "Epoch 14970/20000: Train Loss = 0.435235, Test Loss = 0.249264, Learning Rate = 3.383328e-06\n",
      "Epoch 14971/20000: Train Loss = 0.436088, Test Loss = 0.252875, Learning Rate = 3.382042e-06\n",
      "Epoch 14972/20000: Train Loss = 0.435046, Test Loss = 0.250171, Learning Rate = 3.380757e-06\n",
      "Epoch 14973/20000: Train Loss = 0.435120, Test Loss = 0.250744, Learning Rate = 3.379473e-06\n",
      "Epoch 14974/20000: Train Loss = 0.435260, Test Loss = 0.248746, Learning Rate = 3.378189e-06\n",
      "Epoch 14975/20000: Train Loss = 0.435058, Test Loss = 0.250911, Learning Rate = 3.376905e-06\n",
      "Epoch 14976/20000: Train Loss = 0.435385, Test Loss = 0.249419, Learning Rate = 3.375622e-06\n",
      "Epoch 14977/20000: Train Loss = 0.435803, Test Loss = 0.252129, Learning Rate = 3.374339e-06\n",
      "Epoch 14978/20000: Train Loss = 0.435164, Test Loss = 0.252499, Learning Rate = 3.373057e-06\n",
      "Epoch 14979/20000: Train Loss = 0.435301, Test Loss = 0.248265, Learning Rate = 3.371775e-06\n",
      "Epoch 14980/20000: Train Loss = 0.435172, Test Loss = 0.250256, Learning Rate = 3.370494e-06\n",
      "Epoch 14981/20000: Train Loss = 0.435223, Test Loss = 0.252942, Learning Rate = 3.369214e-06\n",
      "Epoch 14982/20000: Train Loss = 0.435339, Test Loss = 0.250290, Learning Rate = 3.367933e-06\n",
      "Epoch 14983/20000: Train Loss = 0.435440, Test Loss = 0.251741, Learning Rate = 3.366654e-06\n",
      "Epoch 14984/20000: Train Loss = 0.435158, Test Loss = 0.252328, Learning Rate = 3.365374e-06\n",
      "Epoch 14985/20000: Train Loss = 0.435429, Test Loss = 0.250858, Learning Rate = 3.364096e-06\n",
      "Epoch 14986/20000: Train Loss = 0.435339, Test Loss = 0.250235, Learning Rate = 3.362817e-06\n",
      "Epoch 14987/20000: Train Loss = 0.435335, Test Loss = 0.250503, Learning Rate = 3.361540e-06\n",
      "Epoch 14988/20000: Train Loss = 0.435453, Test Loss = 0.252149, Learning Rate = 3.360262e-06\n",
      "Epoch 14989/20000: Train Loss = 0.435089, Test Loss = 0.251952, Learning Rate = 3.358985e-06\n",
      "Epoch 14990/20000: Train Loss = 0.435109, Test Loss = 0.251705, Learning Rate = 3.357709e-06\n",
      "Epoch 14991/20000: Train Loss = 0.435227, Test Loss = 0.251006, Learning Rate = 3.356433e-06\n",
      "Epoch 14992/20000: Train Loss = 0.435398, Test Loss = 0.253910, Learning Rate = 3.355158e-06\n",
      "Epoch 14993/20000: Train Loss = 0.435710, Test Loss = 0.251185, Learning Rate = 3.353883e-06\n",
      "Epoch 14994/20000: Train Loss = 0.435467, Test Loss = 0.253333, Learning Rate = 3.352609e-06\n",
      "Epoch 14995/20000: Train Loss = 0.435530, Test Loss = 0.252628, Learning Rate = 3.351335e-06\n",
      "Epoch 14996/20000: Train Loss = 0.435462, Test Loss = 0.253632, Learning Rate = 3.350061e-06\n",
      "Epoch 14997/20000: Train Loss = 0.435082, Test Loss = 0.252031, Learning Rate = 3.348788e-06\n",
      "Epoch 14998/20000: Train Loss = 0.435028, Test Loss = 0.250687, Learning Rate = 3.347516e-06\n",
      "Epoch 14999/20000: Train Loss = 0.435273, Test Loss = 0.250847, Learning Rate = 3.346244e-06\n",
      "Epoch 15000/20000: Train Loss = 0.435249, Test Loss = 0.253877, Learning Rate = 3.344973e-06\n",
      "Epoch 15001/20000: Train Loss = 0.435151, Test Loss = 0.251494, Learning Rate = 3.343702e-06\n",
      "Epoch 15002/20000: Train Loss = 0.435140, Test Loss = 0.249644, Learning Rate = 3.342431e-06\n",
      "Epoch 15003/20000: Train Loss = 0.435367, Test Loss = 0.252810, Learning Rate = 3.341161e-06\n",
      "Epoch 15004/20000: Train Loss = 0.435088, Test Loss = 0.251072, Learning Rate = 3.339891e-06\n",
      "Epoch 15005/20000: Train Loss = 0.435348, Test Loss = 0.251081, Learning Rate = 3.338622e-06\n",
      "Epoch 15006/20000: Train Loss = 0.435066, Test Loss = 0.251618, Learning Rate = 3.337354e-06\n",
      "Epoch 15007/20000: Train Loss = 0.435172, Test Loss = 0.251484, Learning Rate = 3.336086e-06\n",
      "Epoch 15008/20000: Train Loss = 0.435347, Test Loss = 0.251553, Learning Rate = 3.334818e-06\n",
      "Epoch 15009/20000: Train Loss = 0.435459, Test Loss = 0.253011, Learning Rate = 3.333551e-06\n",
      "Epoch 15010/20000: Train Loss = 0.435347, Test Loss = 0.253014, Learning Rate = 3.332284e-06\n",
      "Epoch 15011/20000: Train Loss = 0.435485, Test Loss = 0.252801, Learning Rate = 3.331018e-06\n",
      "Epoch 15012/20000: Train Loss = 0.435090, Test Loss = 0.252775, Learning Rate = 3.329752e-06\n",
      "Epoch 15013/20000: Train Loss = 0.435353, Test Loss = 0.250411, Learning Rate = 3.328487e-06\n",
      "Epoch 15014/20000: Train Loss = 0.435117, Test Loss = 0.251224, Learning Rate = 3.327222e-06\n",
      "Epoch 15015/20000: Train Loss = 0.435174, Test Loss = 0.253464, Learning Rate = 3.325958e-06\n",
      "Epoch 15016/20000: Train Loss = 0.435155, Test Loss = 0.251308, Learning Rate = 3.324694e-06\n",
      "Epoch 15017/20000: Train Loss = 0.435407, Test Loss = 0.252764, Learning Rate = 3.323431e-06\n",
      "Epoch 15018/20000: Train Loss = 0.435305, Test Loss = 0.252101, Learning Rate = 3.322168e-06\n",
      "Epoch 15019/20000: Train Loss = 0.434995, Test Loss = 0.252203, Learning Rate = 3.320906e-06\n",
      "Epoch 15020/20000: Train Loss = 0.435158, Test Loss = 0.250881, Learning Rate = 3.319644e-06\n",
      "Epoch 15021/20000: Train Loss = 0.435112, Test Loss = 0.251708, Learning Rate = 3.318383e-06\n",
      "Epoch 15022/20000: Train Loss = 0.435143, Test Loss = 0.253342, Learning Rate = 3.317122e-06\n",
      "Epoch 15023/20000: Train Loss = 0.435320, Test Loss = 0.251992, Learning Rate = 3.315861e-06\n",
      "Epoch 15024/20000: Train Loss = 0.435572, Test Loss = 0.253931, Learning Rate = 3.314601e-06\n",
      "Epoch 15025/20000: Train Loss = 0.435575, Test Loss = 0.251190, Learning Rate = 3.313342e-06\n",
      "Epoch 15026/20000: Train Loss = 0.435082, Test Loss = 0.251752, Learning Rate = 3.312083e-06\n",
      "Epoch 15027/20000: Train Loss = 0.435238, Test Loss = 0.250053, Learning Rate = 3.310825e-06\n",
      "Epoch 15028/20000: Train Loss = 0.434945, Test Loss = 0.253316, Learning Rate = 3.309567e-06\n",
      "Epoch 15029/20000: Train Loss = 0.435154, Test Loss = 0.254970, Learning Rate = 3.308309e-06\n",
      "Epoch 15030/20000: Train Loss = 0.435198, Test Loss = 0.255018, Learning Rate = 3.307052e-06\n",
      "Epoch 15031/20000: Train Loss = 0.435059, Test Loss = 0.252843, Learning Rate = 3.305795e-06\n",
      "Epoch 15032/20000: Train Loss = 0.435194, Test Loss = 0.253580, Learning Rate = 3.304539e-06\n",
      "Epoch 15033/20000: Train Loss = 0.435217, Test Loss = 0.254052, Learning Rate = 3.303284e-06\n",
      "Epoch 15034/20000: Train Loss = 0.435065, Test Loss = 0.251847, Learning Rate = 3.302028e-06\n",
      "Epoch 15035/20000: Train Loss = 0.435426, Test Loss = 0.251789, Learning Rate = 3.300774e-06\n",
      "Epoch 15036/20000: Train Loss = 0.435433, Test Loss = 0.254729, Learning Rate = 3.299520e-06\n",
      "Epoch 15037/20000: Train Loss = 0.435072, Test Loss = 0.252537, Learning Rate = 3.298266e-06\n",
      "Epoch 15038/20000: Train Loss = 0.435336, Test Loss = 0.253827, Learning Rate = 3.297013e-06\n",
      "Epoch 15039/20000: Train Loss = 0.435074, Test Loss = 0.255641, Learning Rate = 3.295760e-06\n",
      "Epoch 15040/20000: Train Loss = 0.435144, Test Loss = 0.253775, Learning Rate = 3.294507e-06\n",
      "Epoch 15041/20000: Train Loss = 0.435223, Test Loss = 0.252051, Learning Rate = 3.293256e-06\n",
      "Epoch 15042/20000: Train Loss = 0.435293, Test Loss = 0.254357, Learning Rate = 3.292004e-06\n",
      "Epoch 15043/20000: Train Loss = 0.435087, Test Loss = 0.252819, Learning Rate = 3.290753e-06\n",
      "Epoch 15044/20000: Train Loss = 0.435232, Test Loss = 0.255319, Learning Rate = 3.289503e-06\n",
      "Epoch 15045/20000: Train Loss = 0.435509, Test Loss = 0.254607, Learning Rate = 3.288253e-06\n",
      "Epoch 15046/20000: Train Loss = 0.436165, Test Loss = 0.253131, Learning Rate = 3.287004e-06\n",
      "Epoch 15047/20000: Train Loss = 0.435146, Test Loss = 0.252370, Learning Rate = 3.285755e-06\n",
      "Epoch 15048/20000: Train Loss = 0.435216, Test Loss = 0.254165, Learning Rate = 3.284506e-06\n",
      "Epoch 15049/20000: Train Loss = 0.435275, Test Loss = 0.255053, Learning Rate = 3.283258e-06\n",
      "Epoch 15050/20000: Train Loss = 0.435030, Test Loss = 0.252693, Learning Rate = 3.282011e-06\n",
      "Epoch 15051/20000: Train Loss = 0.435762, Test Loss = 0.253419, Learning Rate = 3.280764e-06\n",
      "Epoch 15052/20000: Train Loss = 0.435301, Test Loss = 0.251339, Learning Rate = 3.279517e-06\n",
      "Epoch 15053/20000: Train Loss = 0.435568, Test Loss = 0.250934, Learning Rate = 3.278271e-06\n",
      "Epoch 15054/20000: Train Loss = 0.435646, Test Loss = 0.251883, Learning Rate = 3.277025e-06\n",
      "Epoch 15055/20000: Train Loss = 0.435224, Test Loss = 0.250599, Learning Rate = 3.275780e-06\n",
      "Epoch 15056/20000: Train Loss = 0.435092, Test Loss = 0.250179, Learning Rate = 3.274535e-06\n",
      "Epoch 15057/20000: Train Loss = 0.434797, Test Loss = 0.248635, Learning Rate = 3.273291e-06\n",
      "Epoch 15058/20000: Train Loss = 0.435341, Test Loss = 0.249835, Learning Rate = 3.272047e-06\n",
      "Epoch 15059/20000: Train Loss = 0.434987, Test Loss = 0.250565, Learning Rate = 3.270804e-06\n",
      "Epoch 15060/20000: Train Loss = 0.435200, Test Loss = 0.251607, Learning Rate = 3.269561e-06\n",
      "Epoch 15061/20000: Train Loss = 0.434968, Test Loss = 0.250477, Learning Rate = 3.268319e-06\n",
      "Epoch 15062/20000: Train Loss = 0.435282, Test Loss = 0.251548, Learning Rate = 3.267077e-06\n",
      "Epoch 15063/20000: Train Loss = 0.435113, Test Loss = 0.252137, Learning Rate = 3.265836e-06\n",
      "Epoch 15064/20000: Train Loss = 0.435417, Test Loss = 0.252704, Learning Rate = 3.264595e-06\n",
      "Epoch 15065/20000: Train Loss = 0.435126, Test Loss = 0.253275, Learning Rate = 3.263354e-06\n",
      "Epoch 15066/20000: Train Loss = 0.435097, Test Loss = 0.252024, Learning Rate = 3.262114e-06\n",
      "Epoch 15067/20000: Train Loss = 0.435594, Test Loss = 0.252467, Learning Rate = 3.260875e-06\n",
      "Epoch 15068/20000: Train Loss = 0.435689, Test Loss = 0.251150, Learning Rate = 3.259636e-06\n",
      "Epoch 15069/20000: Train Loss = 0.435115, Test Loss = 0.250100, Learning Rate = 3.258397e-06\n",
      "Epoch 15070/20000: Train Loss = 0.435667, Test Loss = 0.249944, Learning Rate = 3.257159e-06\n",
      "Epoch 15071/20000: Train Loss = 0.435097, Test Loss = 0.251824, Learning Rate = 3.255921e-06\n",
      "Epoch 15072/20000: Train Loss = 0.435990, Test Loss = 0.253356, Learning Rate = 3.254684e-06\n",
      "Epoch 15073/20000: Train Loss = 0.435421, Test Loss = 0.253419, Learning Rate = 3.253447e-06\n",
      "Epoch 15074/20000: Train Loss = 0.435192, Test Loss = 0.251225, Learning Rate = 3.252211e-06\n",
      "Epoch 15075/20000: Train Loss = 0.435820, Test Loss = 0.252575, Learning Rate = 3.250975e-06\n",
      "Epoch 15076/20000: Train Loss = 0.435225, Test Loss = 0.253749, Learning Rate = 3.249740e-06\n",
      "Epoch 15077/20000: Train Loss = 0.435507, Test Loss = 0.255808, Learning Rate = 3.248505e-06\n",
      "Epoch 15078/20000: Train Loss = 0.435193, Test Loss = 0.254119, Learning Rate = 3.247271e-06\n",
      "Epoch 15079/20000: Train Loss = 0.435150, Test Loss = 0.253957, Learning Rate = 3.246037e-06\n",
      "Epoch 15080/20000: Train Loss = 0.435198, Test Loss = 0.252853, Learning Rate = 3.244804e-06\n",
      "Epoch 15081/20000: Train Loss = 0.435119, Test Loss = 0.253845, Learning Rate = 3.243571e-06\n",
      "Epoch 15082/20000: Train Loss = 0.434985, Test Loss = 0.253309, Learning Rate = 3.242338e-06\n",
      "Epoch 15083/20000: Train Loss = 0.435149, Test Loss = 0.250357, Learning Rate = 3.241106e-06\n",
      "Epoch 15084/20000: Train Loss = 0.434942, Test Loss = 0.251656, Learning Rate = 3.239875e-06\n",
      "Epoch 15085/20000: Train Loss = 0.435214, Test Loss = 0.251813, Learning Rate = 3.238644e-06\n",
      "Epoch 15086/20000: Train Loss = 0.435302, Test Loss = 0.254228, Learning Rate = 3.237413e-06\n",
      "Epoch 15087/20000: Train Loss = 0.435357, Test Loss = 0.251423, Learning Rate = 3.236183e-06\n",
      "Epoch 15088/20000: Train Loss = 0.435030, Test Loss = 0.250220, Learning Rate = 3.234953e-06\n",
      "Epoch 15089/20000: Train Loss = 0.435264, Test Loss = 0.251321, Learning Rate = 3.233724e-06\n",
      "Epoch 15090/20000: Train Loss = 0.435109, Test Loss = 0.252615, Learning Rate = 3.232495e-06\n",
      "Epoch 15091/20000: Train Loss = 0.435095, Test Loss = 0.253110, Learning Rate = 3.231267e-06\n",
      "Epoch 15092/20000: Train Loss = 0.435214, Test Loss = 0.253006, Learning Rate = 3.230039e-06\n",
      "Epoch 15093/20000: Train Loss = 0.435041, Test Loss = 0.253762, Learning Rate = 3.228812e-06\n",
      "Epoch 15094/20000: Train Loss = 0.435184, Test Loss = 0.255186, Learning Rate = 3.227585e-06\n",
      "Epoch 15095/20000: Train Loss = 0.435125, Test Loss = 0.253936, Learning Rate = 3.226359e-06\n",
      "Epoch 15096/20000: Train Loss = 0.435187, Test Loss = 0.252913, Learning Rate = 3.225133e-06\n",
      "Epoch 15097/20000: Train Loss = 0.435126, Test Loss = 0.254241, Learning Rate = 3.223907e-06\n",
      "Epoch 15098/20000: Train Loss = 0.435237, Test Loss = 0.253082, Learning Rate = 3.222682e-06\n",
      "Epoch 15099/20000: Train Loss = 0.435066, Test Loss = 0.254677, Learning Rate = 3.221458e-06\n",
      "Epoch 15100/20000: Train Loss = 0.434993, Test Loss = 0.254977, Learning Rate = 3.220234e-06\n",
      "Epoch 15101/20000: Train Loss = 0.435426, Test Loss = 0.253739, Learning Rate = 3.219010e-06\n",
      "Epoch 15102/20000: Train Loss = 0.435097, Test Loss = 0.252540, Learning Rate = 3.217787e-06\n",
      "Epoch 15103/20000: Train Loss = 0.435242, Test Loss = 0.252504, Learning Rate = 3.216564e-06\n",
      "Epoch 15104/20000: Train Loss = 0.435241, Test Loss = 0.253108, Learning Rate = 3.215342e-06\n",
      "Epoch 15105/20000: Train Loss = 0.435148, Test Loss = 0.251126, Learning Rate = 3.214120e-06\n",
      "Epoch 15106/20000: Train Loss = 0.435723, Test Loss = 0.252508, Learning Rate = 3.212899e-06\n",
      "Epoch 15107/20000: Train Loss = 0.435272, Test Loss = 0.252019, Learning Rate = 3.211678e-06\n",
      "Epoch 15108/20000: Train Loss = 0.435263, Test Loss = 0.251026, Learning Rate = 3.210458e-06\n",
      "Epoch 15109/20000: Train Loss = 0.435184, Test Loss = 0.251379, Learning Rate = 3.209238e-06\n",
      "Epoch 15110/20000: Train Loss = 0.435080, Test Loss = 0.250729, Learning Rate = 3.208019e-06\n",
      "Epoch 15111/20000: Train Loss = 0.435603, Test Loss = 0.254583, Learning Rate = 3.206800e-06\n",
      "Epoch 15112/20000: Train Loss = 0.435552, Test Loss = 0.252062, Learning Rate = 3.205581e-06\n",
      "Epoch 15113/20000: Train Loss = 0.435586, Test Loss = 0.255760, Learning Rate = 3.204363e-06\n",
      "Epoch 15114/20000: Train Loss = 0.435138, Test Loss = 0.253460, Learning Rate = 3.203146e-06\n",
      "Epoch 15115/20000: Train Loss = 0.435060, Test Loss = 0.255589, Learning Rate = 3.201929e-06\n",
      "Epoch 15116/20000: Train Loss = 0.435060, Test Loss = 0.253994, Learning Rate = 3.200712e-06\n",
      "Epoch 15117/20000: Train Loss = 0.435062, Test Loss = 0.252202, Learning Rate = 3.199496e-06\n",
      "Epoch 15118/20000: Train Loss = 0.435211, Test Loss = 0.253877, Learning Rate = 3.198280e-06\n",
      "Epoch 15119/20000: Train Loss = 0.435185, Test Loss = 0.252778, Learning Rate = 3.197065e-06\n",
      "Epoch 15120/20000: Train Loss = 0.434982, Test Loss = 0.251420, Learning Rate = 3.195850e-06\n",
      "Epoch 15121/20000: Train Loss = 0.435220, Test Loss = 0.251572, Learning Rate = 3.194636e-06\n",
      "Epoch 15122/20000: Train Loss = 0.435371, Test Loss = 0.253128, Learning Rate = 3.193422e-06\n",
      "Epoch 15123/20000: Train Loss = 0.435294, Test Loss = 0.252230, Learning Rate = 3.192208e-06\n",
      "Epoch 15124/20000: Train Loss = 0.435275, Test Loss = 0.252421, Learning Rate = 3.190995e-06\n",
      "Epoch 15125/20000: Train Loss = 0.435097, Test Loss = 0.251681, Learning Rate = 3.189783e-06\n",
      "Epoch 15126/20000: Train Loss = 0.435236, Test Loss = 0.251647, Learning Rate = 3.188571e-06\n",
      "Epoch 15127/20000: Train Loss = 0.435225, Test Loss = 0.252723, Learning Rate = 3.187359e-06\n",
      "Epoch 15128/20000: Train Loss = 0.435613, Test Loss = 0.252853, Learning Rate = 3.186148e-06\n",
      "Epoch 15129/20000: Train Loss = 0.435009, Test Loss = 0.250807, Learning Rate = 3.184937e-06\n",
      "Epoch 15130/20000: Train Loss = 0.434976, Test Loss = 0.250994, Learning Rate = 3.183727e-06\n",
      "Epoch 15131/20000: Train Loss = 0.435115, Test Loss = 0.250155, Learning Rate = 3.182518e-06\n",
      "Epoch 15132/20000: Train Loss = 0.435087, Test Loss = 0.250687, Learning Rate = 3.181308e-06\n",
      "Epoch 15133/20000: Train Loss = 0.436280, Test Loss = 0.248959, Learning Rate = 3.180099e-06\n",
      "Epoch 15134/20000: Train Loss = 0.435077, Test Loss = 0.248614, Learning Rate = 3.178891e-06\n",
      "Epoch 15135/20000: Train Loss = 0.435153, Test Loss = 0.250319, Learning Rate = 3.177683e-06\n",
      "Epoch 15136/20000: Train Loss = 0.435040, Test Loss = 0.250689, Learning Rate = 3.176476e-06\n",
      "Epoch 15137/20000: Train Loss = 0.435023, Test Loss = 0.250800, Learning Rate = 3.175269e-06\n",
      "Epoch 15138/20000: Train Loss = 0.434996, Test Loss = 0.249694, Learning Rate = 3.174062e-06\n",
      "Epoch 15139/20000: Train Loss = 0.435062, Test Loss = 0.250338, Learning Rate = 3.172856e-06\n",
      "Epoch 15140/20000: Train Loss = 0.435145, Test Loss = 0.249753, Learning Rate = 3.171651e-06\n",
      "Epoch 15141/20000: Train Loss = 0.435341, Test Loss = 0.251573, Learning Rate = 3.170446e-06\n",
      "Epoch 15142/20000: Train Loss = 0.435155, Test Loss = 0.252985, Learning Rate = 3.169241e-06\n",
      "Epoch 15143/20000: Train Loss = 0.435359, Test Loss = 0.252278, Learning Rate = 3.168037e-06\n",
      "Epoch 15144/20000: Train Loss = 0.435285, Test Loss = 0.252110, Learning Rate = 3.166833e-06\n",
      "Epoch 15145/20000: Train Loss = 0.435423, Test Loss = 0.249671, Learning Rate = 3.165630e-06\n",
      "Epoch 15146/20000: Train Loss = 0.435059, Test Loss = 0.253016, Learning Rate = 3.164427e-06\n",
      "Epoch 15147/20000: Train Loss = 0.435035, Test Loss = 0.250429, Learning Rate = 3.163224e-06\n",
      "Epoch 15148/20000: Train Loss = 0.435144, Test Loss = 0.249933, Learning Rate = 3.162022e-06\n",
      "Epoch 15149/20000: Train Loss = 0.435855, Test Loss = 0.250271, Learning Rate = 3.160821e-06\n",
      "Epoch 15150/20000: Train Loss = 0.435163, Test Loss = 0.254190, Learning Rate = 3.159620e-06\n",
      "Epoch 15151/20000: Train Loss = 0.435364, Test Loss = 0.251692, Learning Rate = 3.158419e-06\n",
      "Epoch 15152/20000: Train Loss = 0.435209, Test Loss = 0.251294, Learning Rate = 3.157219e-06\n",
      "Epoch 15153/20000: Train Loss = 0.435130, Test Loss = 0.248524, Learning Rate = 3.156019e-06\n",
      "Epoch 15154/20000: Train Loss = 0.435291, Test Loss = 0.249053, Learning Rate = 3.154820e-06\n",
      "Epoch 15155/20000: Train Loss = 0.435062, Test Loss = 0.249719, Learning Rate = 3.153622e-06\n",
      "Epoch 15156/20000: Train Loss = 0.435253, Test Loss = 0.249160, Learning Rate = 3.152423e-06\n",
      "Epoch 15157/20000: Train Loss = 0.435319, Test Loss = 0.251660, Learning Rate = 3.151225e-06\n",
      "Epoch 15158/20000: Train Loss = 0.435583, Test Loss = 0.251005, Learning Rate = 3.150028e-06\n",
      "Epoch 15159/20000: Train Loss = 0.435504, Test Loss = 0.252057, Learning Rate = 3.148831e-06\n",
      "Epoch 15160/20000: Train Loss = 0.435344, Test Loss = 0.251500, Learning Rate = 3.147635e-06\n",
      "Epoch 15161/20000: Train Loss = 0.434930, Test Loss = 0.250356, Learning Rate = 3.146439e-06\n",
      "Epoch 15162/20000: Train Loss = 0.435425, Test Loss = 0.250834, Learning Rate = 3.145243e-06\n",
      "Epoch 15163/20000: Train Loss = 0.435037, Test Loss = 0.249922, Learning Rate = 3.144048e-06\n",
      "Epoch 15164/20000: Train Loss = 0.435266, Test Loss = 0.249532, Learning Rate = 3.142853e-06\n",
      "Epoch 15165/20000: Train Loss = 0.435085, Test Loss = 0.252924, Learning Rate = 3.141659e-06\n",
      "Epoch 15166/20000: Train Loss = 0.436467, Test Loss = 0.253447, Learning Rate = 3.140465e-06\n",
      "Epoch 15167/20000: Train Loss = 0.435183, Test Loss = 0.248646, Learning Rate = 3.139272e-06\n",
      "Epoch 15168/20000: Train Loss = 0.435278, Test Loss = 0.251405, Learning Rate = 3.138079e-06\n",
      "Epoch 15169/20000: Train Loss = 0.435227, Test Loss = 0.251667, Learning Rate = 3.136887e-06\n",
      "Epoch 15170/20000: Train Loss = 0.435009, Test Loss = 0.252819, Learning Rate = 3.135695e-06\n",
      "Epoch 15171/20000: Train Loss = 0.435305, Test Loss = 0.254805, Learning Rate = 3.134503e-06\n",
      "Epoch 15172/20000: Train Loss = 0.435110, Test Loss = 0.252039, Learning Rate = 3.133312e-06\n",
      "Epoch 15173/20000: Train Loss = 0.435077, Test Loss = 0.252679, Learning Rate = 3.132122e-06\n",
      "Epoch 15174/20000: Train Loss = 0.435059, Test Loss = 0.254140, Learning Rate = 3.130932e-06\n",
      "Epoch 15175/20000: Train Loss = 0.435223, Test Loss = 0.254051, Learning Rate = 3.129742e-06\n",
      "Epoch 15176/20000: Train Loss = 0.435498, Test Loss = 0.252345, Learning Rate = 3.128553e-06\n",
      "Epoch 15177/20000: Train Loss = 0.435494, Test Loss = 0.253658, Learning Rate = 3.127364e-06\n",
      "Epoch 15178/20000: Train Loss = 0.435151, Test Loss = 0.252310, Learning Rate = 3.126176e-06\n",
      "Epoch 15179/20000: Train Loss = 0.435283, Test Loss = 0.252349, Learning Rate = 3.124988e-06\n",
      "Epoch 15180/20000: Train Loss = 0.435114, Test Loss = 0.253132, Learning Rate = 3.123800e-06\n",
      "Epoch 15181/20000: Train Loss = 0.435092, Test Loss = 0.251815, Learning Rate = 3.122614e-06\n",
      "Epoch 15182/20000: Train Loss = 0.435285, Test Loss = 0.251186, Learning Rate = 3.121427e-06\n",
      "Epoch 15183/20000: Train Loss = 0.435125, Test Loss = 0.252400, Learning Rate = 3.120241e-06\n",
      "Epoch 15184/20000: Train Loss = 0.435412, Test Loss = 0.251637, Learning Rate = 3.119055e-06\n",
      "Epoch 15185/20000: Train Loss = 0.435016, Test Loss = 0.251228, Learning Rate = 3.117870e-06\n",
      "Epoch 15186/20000: Train Loss = 0.434991, Test Loss = 0.252457, Learning Rate = 3.116685e-06\n",
      "Epoch 15187/20000: Train Loss = 0.434949, Test Loss = 0.251373, Learning Rate = 3.115501e-06\n",
      "Epoch 15188/20000: Train Loss = 0.434974, Test Loss = 0.253571, Learning Rate = 3.114317e-06\n",
      "Epoch 15189/20000: Train Loss = 0.435059, Test Loss = 0.253191, Learning Rate = 3.113134e-06\n",
      "Epoch 15190/20000: Train Loss = 0.436558, Test Loss = 0.255361, Learning Rate = 3.111951e-06\n",
      "Epoch 15191/20000: Train Loss = 0.435552, Test Loss = 0.254111, Learning Rate = 3.110769e-06\n",
      "Epoch 15192/20000: Train Loss = 0.435341, Test Loss = 0.252375, Learning Rate = 3.109587e-06\n",
      "Epoch 15193/20000: Train Loss = 0.435030, Test Loss = 0.253045, Learning Rate = 3.108405e-06\n",
      "Epoch 15194/20000: Train Loss = 0.435528, Test Loss = 0.250500, Learning Rate = 3.107224e-06\n",
      "Epoch 15195/20000: Train Loss = 0.435157, Test Loss = 0.252565, Learning Rate = 3.106043e-06\n",
      "Epoch 15196/20000: Train Loss = 0.435273, Test Loss = 0.253390, Learning Rate = 3.104863e-06\n",
      "Epoch 15197/20000: Train Loss = 0.435138, Test Loss = 0.255820, Learning Rate = 3.103683e-06\n",
      "Epoch 15198/20000: Train Loss = 0.435035, Test Loss = 0.254585, Learning Rate = 3.102504e-06\n",
      "Epoch 15199/20000: Train Loss = 0.435028, Test Loss = 0.253239, Learning Rate = 3.101325e-06\n",
      "Epoch 15200/20000: Train Loss = 0.435057, Test Loss = 0.255010, Learning Rate = 3.100147e-06\n",
      "Epoch 15201/20000: Train Loss = 0.435016, Test Loss = 0.252719, Learning Rate = 3.098969e-06\n",
      "Epoch 15202/20000: Train Loss = 0.435288, Test Loss = 0.255566, Learning Rate = 3.097791e-06\n",
      "Epoch 15203/20000: Train Loss = 0.435152, Test Loss = 0.254263, Learning Rate = 3.096614e-06\n",
      "Epoch 15204/20000: Train Loss = 0.435352, Test Loss = 0.252045, Learning Rate = 3.095438e-06\n",
      "Epoch 15205/20000: Train Loss = 0.435033, Test Loss = 0.252336, Learning Rate = 3.094261e-06\n",
      "Epoch 15206/20000: Train Loss = 0.435015, Test Loss = 0.250411, Learning Rate = 3.093086e-06\n",
      "Epoch 15207/20000: Train Loss = 0.434984, Test Loss = 0.251836, Learning Rate = 3.091910e-06\n",
      "Epoch 15208/20000: Train Loss = 0.436720, Test Loss = 0.253982, Learning Rate = 3.090736e-06\n",
      "Epoch 15209/20000: Train Loss = 0.434899, Test Loss = 0.250384, Learning Rate = 3.089561e-06\n",
      "Epoch 15210/20000: Train Loss = 0.435022, Test Loss = 0.251066, Learning Rate = 3.088387e-06\n",
      "Epoch 15211/20000: Train Loss = 0.435054, Test Loss = 0.251700, Learning Rate = 3.087214e-06\n",
      "Epoch 15212/20000: Train Loss = 0.435004, Test Loss = 0.249708, Learning Rate = 3.086041e-06\n",
      "Epoch 15213/20000: Train Loss = 0.435341, Test Loss = 0.251298, Learning Rate = 3.084868e-06\n",
      "Epoch 15214/20000: Train Loss = 0.435316, Test Loss = 0.250275, Learning Rate = 3.083696e-06\n",
      "Epoch 15215/20000: Train Loss = 0.435167, Test Loss = 0.252673, Learning Rate = 3.082524e-06\n",
      "Epoch 15216/20000: Train Loss = 0.435211, Test Loss = 0.253631, Learning Rate = 3.081353e-06\n",
      "Epoch 15217/20000: Train Loss = 0.435202, Test Loss = 0.251642, Learning Rate = 3.080182e-06\n",
      "Epoch 15218/20000: Train Loss = 0.435134, Test Loss = 0.253349, Learning Rate = 3.079012e-06\n",
      "Epoch 15219/20000: Train Loss = 0.435277, Test Loss = 0.252901, Learning Rate = 3.077842e-06\n",
      "Epoch 15220/20000: Train Loss = 0.435488, Test Loss = 0.254025, Learning Rate = 3.076672e-06\n",
      "Epoch 15221/20000: Train Loss = 0.435100, Test Loss = 0.251319, Learning Rate = 3.075503e-06\n",
      "Epoch 15222/20000: Train Loss = 0.434916, Test Loss = 0.252727, Learning Rate = 3.074335e-06\n",
      "Epoch 15223/20000: Train Loss = 0.434962, Test Loss = 0.252047, Learning Rate = 3.073166e-06\n",
      "Epoch 15224/20000: Train Loss = 0.435179, Test Loss = 0.254350, Learning Rate = 3.071999e-06\n",
      "Epoch 15225/20000: Train Loss = 0.435304, Test Loss = 0.253162, Learning Rate = 3.070831e-06\n",
      "Epoch 15226/20000: Train Loss = 0.435209, Test Loss = 0.254248, Learning Rate = 3.069665e-06\n",
      "Epoch 15227/20000: Train Loss = 0.434962, Test Loss = 0.253256, Learning Rate = 3.068498e-06\n",
      "Epoch 15228/20000: Train Loss = 0.435185, Test Loss = 0.255579, Learning Rate = 3.067332e-06\n",
      "Epoch 15229/20000: Train Loss = 0.435200, Test Loss = 0.253847, Learning Rate = 3.066167e-06\n",
      "Epoch 15230/20000: Train Loss = 0.435289, Test Loss = 0.251887, Learning Rate = 3.065002e-06\n",
      "Epoch 15231/20000: Train Loss = 0.435112, Test Loss = 0.252717, Learning Rate = 3.063837e-06\n",
      "Epoch 15232/20000: Train Loss = 0.435015, Test Loss = 0.252438, Learning Rate = 3.062673e-06\n",
      "Epoch 15233/20000: Train Loss = 0.434961, Test Loss = 0.253761, Learning Rate = 3.061509e-06\n",
      "Epoch 15234/20000: Train Loss = 0.435105, Test Loss = 0.253718, Learning Rate = 3.060346e-06\n",
      "Epoch 15235/20000: Train Loss = 0.434878, Test Loss = 0.253688, Learning Rate = 3.059183e-06\n",
      "Epoch 15236/20000: Train Loss = 0.434893, Test Loss = 0.251485, Learning Rate = 3.058021e-06\n",
      "Epoch 15237/20000: Train Loss = 0.435298, Test Loss = 0.252154, Learning Rate = 3.056859e-06\n",
      "Epoch 15238/20000: Train Loss = 0.435340, Test Loss = 0.251446, Learning Rate = 3.055697e-06\n",
      "Epoch 15239/20000: Train Loss = 0.434933, Test Loss = 0.253224, Learning Rate = 3.054536e-06\n",
      "Epoch 15240/20000: Train Loss = 0.435233, Test Loss = 0.253842, Learning Rate = 3.053375e-06\n",
      "Epoch 15241/20000: Train Loss = 0.435337, Test Loss = 0.251763, Learning Rate = 3.052215e-06\n",
      "Epoch 15242/20000: Train Loss = 0.435081, Test Loss = 0.250238, Learning Rate = 3.051055e-06\n",
      "Epoch 15243/20000: Train Loss = 0.434865, Test Loss = 0.252705, Learning Rate = 3.049896e-06\n",
      "Epoch 15244/20000: Train Loss = 0.435068, Test Loss = 0.252350, Learning Rate = 3.048737e-06\n",
      "Epoch 15245/20000: Train Loss = 0.435044, Test Loss = 0.251818, Learning Rate = 3.047579e-06\n",
      "Epoch 15246/20000: Train Loss = 0.434940, Test Loss = 0.251781, Learning Rate = 3.046421e-06\n",
      "Epoch 15247/20000: Train Loss = 0.434997, Test Loss = 0.252892, Learning Rate = 3.045263e-06\n",
      "Epoch 15248/20000: Train Loss = 0.434897, Test Loss = 0.251783, Learning Rate = 3.044106e-06\n",
      "Epoch 15249/20000: Train Loss = 0.435326, Test Loss = 0.253296, Learning Rate = 3.042949e-06\n",
      "Epoch 15250/20000: Train Loss = 0.435031, Test Loss = 0.253133, Learning Rate = 3.041793e-06\n",
      "Epoch 15251/20000: Train Loss = 0.435478, Test Loss = 0.251972, Learning Rate = 3.040637e-06\n",
      "Epoch 15252/20000: Train Loss = 0.436590, Test Loss = 0.257363, Learning Rate = 3.039482e-06\n",
      "Epoch 15253/20000: Train Loss = 0.435310, Test Loss = 0.252016, Learning Rate = 3.038327e-06\n",
      "Epoch 15254/20000: Train Loss = 0.435803, Test Loss = 0.252552, Learning Rate = 3.037173e-06\n",
      "Epoch 15255/20000: Train Loss = 0.435367, Test Loss = 0.249643, Learning Rate = 3.036019e-06\n",
      "Epoch 15256/20000: Train Loss = 0.435387, Test Loss = 0.251562, Learning Rate = 3.034865e-06\n",
      "Epoch 15257/20000: Train Loss = 0.434940, Test Loss = 0.250752, Learning Rate = 3.033712e-06\n",
      "Epoch 15258/20000: Train Loss = 0.435041, Test Loss = 0.248327, Learning Rate = 3.032559e-06\n",
      "Epoch 15259/20000: Train Loss = 0.434939, Test Loss = 0.248547, Learning Rate = 3.031407e-06\n",
      "Epoch 15260/20000: Train Loss = 0.435028, Test Loss = 0.249691, Learning Rate = 3.030255e-06\n",
      "Epoch 15261/20000: Train Loss = 0.435063, Test Loss = 0.248272, Learning Rate = 3.029104e-06\n",
      "Epoch 15262/20000: Train Loss = 0.435202, Test Loss = 0.248769, Learning Rate = 3.027953e-06\n",
      "Epoch 15263/20000: Train Loss = 0.435784, Test Loss = 0.247402, Learning Rate = 3.026802e-06\n",
      "Epoch 15264/20000: Train Loss = 0.436160, Test Loss = 0.247005, Learning Rate = 3.025652e-06\n",
      "Epoch 15265/20000: Train Loss = 0.434996, Test Loss = 0.250995, Learning Rate = 3.024502e-06\n",
      "Epoch 15266/20000: Train Loss = 0.435279, Test Loss = 0.246684, Learning Rate = 3.023353e-06\n",
      "Epoch 15267/20000: Train Loss = 0.435249, Test Loss = 0.249885, Learning Rate = 3.022204e-06\n",
      "Epoch 15268/20000: Train Loss = 0.435085, Test Loss = 0.249541, Learning Rate = 3.021056e-06\n",
      "Epoch 15269/20000: Train Loss = 0.434952, Test Loss = 0.248991, Learning Rate = 3.019908e-06\n",
      "Epoch 15270/20000: Train Loss = 0.435087, Test Loss = 0.249127, Learning Rate = 3.018760e-06\n",
      "Epoch 15271/20000: Train Loss = 0.434916, Test Loss = 0.249429, Learning Rate = 3.017613e-06\n",
      "Epoch 15272/20000: Train Loss = 0.435291, Test Loss = 0.250583, Learning Rate = 3.016467e-06\n",
      "Epoch 15273/20000: Train Loss = 0.435134, Test Loss = 0.249243, Learning Rate = 3.015321e-06\n",
      "Epoch 15274/20000: Train Loss = 0.435130, Test Loss = 0.250281, Learning Rate = 3.014175e-06\n",
      "Epoch 15275/20000: Train Loss = 0.434919, Test Loss = 0.252518, Learning Rate = 3.013030e-06\n",
      "Epoch 15276/20000: Train Loss = 0.434926, Test Loss = 0.251700, Learning Rate = 3.011885e-06\n",
      "Epoch 15277/20000: Train Loss = 0.435033, Test Loss = 0.251878, Learning Rate = 3.010740e-06\n",
      "Epoch 15278/20000: Train Loss = 0.435094, Test Loss = 0.251576, Learning Rate = 3.009596e-06\n",
      "Epoch 15279/20000: Train Loss = 0.435207, Test Loss = 0.251012, Learning Rate = 3.008453e-06\n",
      "Epoch 15280/20000: Train Loss = 0.435034, Test Loss = 0.251517, Learning Rate = 3.007310e-06\n",
      "Epoch 15281/20000: Train Loss = 0.435294, Test Loss = 0.251384, Learning Rate = 3.006167e-06\n",
      "Epoch 15282/20000: Train Loss = 0.434934, Test Loss = 0.252429, Learning Rate = 3.005025e-06\n",
      "Epoch 15283/20000: Train Loss = 0.435150, Test Loss = 0.250662, Learning Rate = 3.003883e-06\n",
      "Epoch 15284/20000: Train Loss = 0.434928, Test Loss = 0.253347, Learning Rate = 3.002741e-06\n",
      "Epoch 15285/20000: Train Loss = 0.435137, Test Loss = 0.252637, Learning Rate = 3.001600e-06\n",
      "Epoch 15286/20000: Train Loss = 0.435107, Test Loss = 0.251705, Learning Rate = 3.000460e-06\n",
      "Epoch 15287/20000: Train Loss = 0.435085, Test Loss = 0.253741, Learning Rate = 2.999320e-06\n",
      "Epoch 15288/20000: Train Loss = 0.435065, Test Loss = 0.252625, Learning Rate = 2.998180e-06\n",
      "Epoch 15289/20000: Train Loss = 0.435136, Test Loss = 0.252003, Learning Rate = 2.997041e-06\n",
      "Epoch 15290/20000: Train Loss = 0.434993, Test Loss = 0.253183, Learning Rate = 2.995902e-06\n",
      "Epoch 15291/20000: Train Loss = 0.434994, Test Loss = 0.254595, Learning Rate = 2.994764e-06\n",
      "Epoch 15292/20000: Train Loss = 0.435462, Test Loss = 0.253715, Learning Rate = 2.993626e-06\n",
      "Epoch 15293/20000: Train Loss = 0.435167, Test Loss = 0.255272, Learning Rate = 2.992488e-06\n",
      "Epoch 15294/20000: Train Loss = 0.435680, Test Loss = 0.255015, Learning Rate = 2.991351e-06\n",
      "Epoch 15295/20000: Train Loss = 0.435402, Test Loss = 0.254065, Learning Rate = 2.990215e-06\n",
      "Epoch 15296/20000: Train Loss = 0.435106, Test Loss = 0.252440, Learning Rate = 2.989078e-06\n",
      "Epoch 15297/20000: Train Loss = 0.435243, Test Loss = 0.253573, Learning Rate = 2.987943e-06\n",
      "Epoch 15298/20000: Train Loss = 0.434898, Test Loss = 0.253750, Learning Rate = 2.986807e-06\n",
      "Epoch 15299/20000: Train Loss = 0.434963, Test Loss = 0.254155, Learning Rate = 2.985672e-06\n",
      "Epoch 15300/20000: Train Loss = 0.434992, Test Loss = 0.253422, Learning Rate = 2.984538e-06\n",
      "Epoch 15301/20000: Train Loss = 0.435173, Test Loss = 0.252897, Learning Rate = 2.983404e-06\n",
      "Epoch 15302/20000: Train Loss = 0.435034, Test Loss = 0.253498, Learning Rate = 2.982270e-06\n",
      "Epoch 15303/20000: Train Loss = 0.434810, Test Loss = 0.254333, Learning Rate = 2.981137e-06\n",
      "Epoch 15304/20000: Train Loss = 0.434944, Test Loss = 0.253690, Learning Rate = 2.980004e-06\n",
      "Epoch 15305/20000: Train Loss = 0.435299, Test Loss = 0.254663, Learning Rate = 2.978872e-06\n",
      "Epoch 15306/20000: Train Loss = 0.435021, Test Loss = 0.256145, Learning Rate = 2.977740e-06\n",
      "Epoch 15307/20000: Train Loss = 0.435044, Test Loss = 0.253547, Learning Rate = 2.976609e-06\n",
      "Epoch 15308/20000: Train Loss = 0.435863, Test Loss = 0.253796, Learning Rate = 2.975478e-06\n",
      "Epoch 15309/20000: Train Loss = 0.435383, Test Loss = 0.251254, Learning Rate = 2.974347e-06\n",
      "Epoch 15310/20000: Train Loss = 0.435437, Test Loss = 0.254194, Learning Rate = 2.973217e-06\n",
      "Epoch 15311/20000: Train Loss = 0.434963, Test Loss = 0.253305, Learning Rate = 2.972087e-06\n",
      "Epoch 15312/20000: Train Loss = 0.434919, Test Loss = 0.254330, Learning Rate = 2.970958e-06\n",
      "Epoch 15313/20000: Train Loss = 0.435694, Test Loss = 0.252358, Learning Rate = 2.969829e-06\n",
      "Epoch 15314/20000: Train Loss = 0.435763, Test Loss = 0.251938, Learning Rate = 2.968700e-06\n",
      "Epoch 15315/20000: Train Loss = 0.435028, Test Loss = 0.252299, Learning Rate = 2.967572e-06\n",
      "Epoch 15316/20000: Train Loss = 0.435053, Test Loss = 0.250037, Learning Rate = 2.966445e-06\n",
      "Epoch 15317/20000: Train Loss = 0.435440, Test Loss = 0.249779, Learning Rate = 2.965318e-06\n",
      "Epoch 15318/20000: Train Loss = 0.435399, Test Loss = 0.252850, Learning Rate = 2.964191e-06\n",
      "Epoch 15319/20000: Train Loss = 0.434873, Test Loss = 0.251996, Learning Rate = 2.963065e-06\n",
      "Epoch 15320/20000: Train Loss = 0.435125, Test Loss = 0.252839, Learning Rate = 2.961939e-06\n",
      "Epoch 15321/20000: Train Loss = 0.435007, Test Loss = 0.251241, Learning Rate = 2.960813e-06\n",
      "Epoch 15322/20000: Train Loss = 0.434920, Test Loss = 0.251390, Learning Rate = 2.959688e-06\n",
      "Epoch 15323/20000: Train Loss = 0.435102, Test Loss = 0.252709, Learning Rate = 2.958564e-06\n",
      "Epoch 15324/20000: Train Loss = 0.434949, Test Loss = 0.252553, Learning Rate = 2.957439e-06\n",
      "Epoch 15325/20000: Train Loss = 0.435140, Test Loss = 0.250787, Learning Rate = 2.956316e-06\n",
      "Epoch 15326/20000: Train Loss = 0.434806, Test Loss = 0.254274, Learning Rate = 2.955192e-06\n",
      "Epoch 15327/20000: Train Loss = 0.435053, Test Loss = 0.254003, Learning Rate = 2.954070e-06\n",
      "Epoch 15328/20000: Train Loss = 0.435068, Test Loss = 0.253913, Learning Rate = 2.952947e-06\n",
      "Epoch 15329/20000: Train Loss = 0.434967, Test Loss = 0.252435, Learning Rate = 2.951825e-06\n",
      "Epoch 15330/20000: Train Loss = 0.435132, Test Loss = 0.253000, Learning Rate = 2.950703e-06\n",
      "Epoch 15331/20000: Train Loss = 0.435030, Test Loss = 0.252871, Learning Rate = 2.949582e-06\n",
      "Epoch 15332/20000: Train Loss = 0.434865, Test Loss = 0.253713, Learning Rate = 2.948461e-06\n",
      "Epoch 15333/20000: Train Loss = 0.434798, Test Loss = 0.250394, Learning Rate = 2.947341e-06\n",
      "Epoch 15334/20000: Train Loss = 0.435074, Test Loss = 0.250661, Learning Rate = 2.946221e-06\n",
      "Epoch 15335/20000: Train Loss = 0.435007, Test Loss = 0.250482, Learning Rate = 2.945102e-06\n",
      "Epoch 15336/20000: Train Loss = 0.434926, Test Loss = 0.250951, Learning Rate = 2.943983e-06\n",
      "Epoch 15337/20000: Train Loss = 0.435145, Test Loss = 0.252151, Learning Rate = 2.942864e-06\n",
      "Epoch 15338/20000: Train Loss = 0.435092, Test Loss = 0.251323, Learning Rate = 2.941746e-06\n",
      "Epoch 15339/20000: Train Loss = 0.434877, Test Loss = 0.251861, Learning Rate = 2.940628e-06\n",
      "Epoch 15340/20000: Train Loss = 0.435945, Test Loss = 0.251922, Learning Rate = 2.939511e-06\n",
      "Epoch 15341/20000: Train Loss = 0.435221, Test Loss = 0.252339, Learning Rate = 2.938394e-06\n",
      "Epoch 15342/20000: Train Loss = 0.434993, Test Loss = 0.252430, Learning Rate = 2.937277e-06\n",
      "Epoch 15343/20000: Train Loss = 0.434926, Test Loss = 0.253013, Learning Rate = 2.936161e-06\n",
      "Epoch 15344/20000: Train Loss = 0.435069, Test Loss = 0.252658, Learning Rate = 2.935046e-06\n",
      "Epoch 15345/20000: Train Loss = 0.435402, Test Loss = 0.251769, Learning Rate = 2.933930e-06\n",
      "Epoch 15346/20000: Train Loss = 0.434901, Test Loss = 0.252251, Learning Rate = 2.932815e-06\n",
      "Epoch 15347/20000: Train Loss = 0.435225, Test Loss = 0.254707, Learning Rate = 2.931701e-06\n",
      "Epoch 15348/20000: Train Loss = 0.434959, Test Loss = 0.255012, Learning Rate = 2.930587e-06\n",
      "Epoch 15349/20000: Train Loss = 0.435126, Test Loss = 0.253376, Learning Rate = 2.929474e-06\n",
      "Epoch 15350/20000: Train Loss = 0.435207, Test Loss = 0.253032, Learning Rate = 2.928360e-06\n",
      "Epoch 15351/20000: Train Loss = 0.435146, Test Loss = 0.253870, Learning Rate = 2.927248e-06\n",
      "Epoch 15352/20000: Train Loss = 0.434903, Test Loss = 0.254696, Learning Rate = 2.926135e-06\n",
      "Epoch 15353/20000: Train Loss = 0.434810, Test Loss = 0.252871, Learning Rate = 2.925024e-06\n",
      "Epoch 15354/20000: Train Loss = 0.435274, Test Loss = 0.253719, Learning Rate = 2.923912e-06\n",
      "Epoch 15355/20000: Train Loss = 0.434801, Test Loss = 0.252912, Learning Rate = 2.922801e-06\n",
      "Epoch 15356/20000: Train Loss = 0.434983, Test Loss = 0.253469, Learning Rate = 2.921691e-06\n",
      "Epoch 15357/20000: Train Loss = 0.434981, Test Loss = 0.252732, Learning Rate = 2.920580e-06\n",
      "Epoch 15358/20000: Train Loss = 0.435120, Test Loss = 0.252875, Learning Rate = 2.919471e-06\n",
      "Epoch 15359/20000: Train Loss = 0.435118, Test Loss = 0.249891, Learning Rate = 2.918361e-06\n",
      "Epoch 15360/20000: Train Loss = 0.434919, Test Loss = 0.249208, Learning Rate = 2.917252e-06\n",
      "Epoch 15361/20000: Train Loss = 0.435187, Test Loss = 0.251228, Learning Rate = 2.916144e-06\n",
      "Epoch 15362/20000: Train Loss = 0.435034, Test Loss = 0.249957, Learning Rate = 2.915036e-06\n",
      "Epoch 15363/20000: Train Loss = 0.434956, Test Loss = 0.251203, Learning Rate = 2.913928e-06\n",
      "Epoch 15364/20000: Train Loss = 0.435013, Test Loss = 0.251042, Learning Rate = 2.912821e-06\n",
      "Epoch 15365/20000: Train Loss = 0.434964, Test Loss = 0.250934, Learning Rate = 2.911714e-06\n",
      "Epoch 15366/20000: Train Loss = 0.435098, Test Loss = 0.250124, Learning Rate = 2.910608e-06\n",
      "Epoch 15367/20000: Train Loss = 0.435113, Test Loss = 0.252306, Learning Rate = 2.909502e-06\n",
      "Epoch 15368/20000: Train Loss = 0.434949, Test Loss = 0.251876, Learning Rate = 2.908396e-06\n",
      "Epoch 15369/20000: Train Loss = 0.435217, Test Loss = 0.251748, Learning Rate = 2.907291e-06\n",
      "Epoch 15370/20000: Train Loss = 0.435534, Test Loss = 0.252685, Learning Rate = 2.906187e-06\n",
      "Epoch 15371/20000: Train Loss = 0.434871, Test Loss = 0.250469, Learning Rate = 2.905082e-06\n",
      "Epoch 15372/20000: Train Loss = 0.434830, Test Loss = 0.251654, Learning Rate = 2.903979e-06\n",
      "Epoch 15373/20000: Train Loss = 0.435285, Test Loss = 0.251660, Learning Rate = 2.902875e-06\n",
      "Epoch 15374/20000: Train Loss = 0.435388, Test Loss = 0.249969, Learning Rate = 2.901772e-06\n",
      "Epoch 15375/20000: Train Loss = 0.434968, Test Loss = 0.252198, Learning Rate = 2.900669e-06\n",
      "Epoch 15376/20000: Train Loss = 0.435033, Test Loss = 0.252351, Learning Rate = 2.899567e-06\n",
      "Epoch 15377/20000: Train Loss = 0.434891, Test Loss = 0.253077, Learning Rate = 2.898466e-06\n",
      "Epoch 15378/20000: Train Loss = 0.434983, Test Loss = 0.254480, Learning Rate = 2.897364e-06\n",
      "Epoch 15379/20000: Train Loss = 0.434974, Test Loss = 0.253473, Learning Rate = 2.896263e-06\n",
      "Epoch 15380/20000: Train Loss = 0.435065, Test Loss = 0.252235, Learning Rate = 2.895163e-06\n",
      "Epoch 15381/20000: Train Loss = 0.435184, Test Loss = 0.253695, Learning Rate = 2.894063e-06\n",
      "Epoch 15382/20000: Train Loss = 0.435282, Test Loss = 0.255535, Learning Rate = 2.892963e-06\n",
      "Epoch 15383/20000: Train Loss = 0.435205, Test Loss = 0.254178, Learning Rate = 2.891864e-06\n",
      "Epoch 15384/20000: Train Loss = 0.435015, Test Loss = 0.254929, Learning Rate = 2.890765e-06\n",
      "Epoch 15385/20000: Train Loss = 0.435161, Test Loss = 0.253447, Learning Rate = 2.889667e-06\n",
      "Epoch 15386/20000: Train Loss = 0.435028, Test Loss = 0.253945, Learning Rate = 2.888569e-06\n",
      "Epoch 15387/20000: Train Loss = 0.435024, Test Loss = 0.253163, Learning Rate = 2.887471e-06\n",
      "Epoch 15388/20000: Train Loss = 0.435330, Test Loss = 0.252680, Learning Rate = 2.886374e-06\n",
      "Epoch 15389/20000: Train Loss = 0.434834, Test Loss = 0.255139, Learning Rate = 2.885277e-06\n",
      "Epoch 15390/20000: Train Loss = 0.434980, Test Loss = 0.252685, Learning Rate = 2.884181e-06\n",
      "Epoch 15391/20000: Train Loss = 0.435202, Test Loss = 0.254791, Learning Rate = 2.883085e-06\n",
      "Epoch 15392/20000: Train Loss = 0.435371, Test Loss = 0.253326, Learning Rate = 2.881989e-06\n",
      "Epoch 15393/20000: Train Loss = 0.434877, Test Loss = 0.254192, Learning Rate = 2.880894e-06\n",
      "Epoch 15394/20000: Train Loss = 0.436222, Test Loss = 0.252785, Learning Rate = 2.879800e-06\n",
      "Epoch 15395/20000: Train Loss = 0.434729, Test Loss = 0.252863, Learning Rate = 2.878705e-06\n",
      "Epoch 15396/20000: Train Loss = 0.435144, Test Loss = 0.252567, Learning Rate = 2.877612e-06\n",
      "Epoch 15397/20000: Train Loss = 0.435561, Test Loss = 0.255400, Learning Rate = 2.876518e-06\n",
      "Epoch 15398/20000: Train Loss = 0.435095, Test Loss = 0.253295, Learning Rate = 2.875425e-06\n",
      "Epoch 15399/20000: Train Loss = 0.435481, Test Loss = 0.253673, Learning Rate = 2.874333e-06\n",
      "Epoch 15400/20000: Train Loss = 0.434912, Test Loss = 0.253870, Learning Rate = 2.873240e-06\n",
      "Epoch 15401/20000: Train Loss = 0.435144, Test Loss = 0.252779, Learning Rate = 2.872149e-06\n",
      "Epoch 15402/20000: Train Loss = 0.435086, Test Loss = 0.251376, Learning Rate = 2.871057e-06\n",
      "Epoch 15403/20000: Train Loss = 0.435241, Test Loss = 0.248820, Learning Rate = 2.869966e-06\n",
      "Epoch 15404/20000: Train Loss = 0.436227, Test Loss = 0.251778, Learning Rate = 2.868876e-06\n",
      "Epoch 15405/20000: Train Loss = 0.434917, Test Loss = 0.249251, Learning Rate = 2.867786e-06\n",
      "Epoch 15406/20000: Train Loss = 0.434881, Test Loss = 0.249543, Learning Rate = 2.866696e-06\n",
      "Epoch 15407/20000: Train Loss = 0.434955, Test Loss = 0.251181, Learning Rate = 2.865607e-06\n",
      "Epoch 15408/20000: Train Loss = 0.435522, Test Loss = 0.251464, Learning Rate = 2.864518e-06\n",
      "Epoch 15409/20000: Train Loss = 0.434907, Test Loss = 0.248342, Learning Rate = 2.863429e-06\n",
      "Epoch 15410/20000: Train Loss = 0.435489, Test Loss = 0.249126, Learning Rate = 2.862341e-06\n",
      "Epoch 15411/20000: Train Loss = 0.435125, Test Loss = 0.250154, Learning Rate = 2.861254e-06\n",
      "Epoch 15412/20000: Train Loss = 0.435114, Test Loss = 0.249775, Learning Rate = 2.860167e-06\n",
      "Epoch 15413/20000: Train Loss = 0.435015, Test Loss = 0.249966, Learning Rate = 2.859080e-06\n",
      "Epoch 15414/20000: Train Loss = 0.435237, Test Loss = 0.248907, Learning Rate = 2.857993e-06\n",
      "Epoch 15415/20000: Train Loss = 0.434984, Test Loss = 0.250549, Learning Rate = 2.856908e-06\n",
      "Epoch 15416/20000: Train Loss = 0.435184, Test Loss = 0.250352, Learning Rate = 2.855822e-06\n",
      "Epoch 15417/20000: Train Loss = 0.435035, Test Loss = 0.251224, Learning Rate = 2.854737e-06\n",
      "Epoch 15418/20000: Train Loss = 0.434939, Test Loss = 0.249667, Learning Rate = 2.853652e-06\n",
      "Epoch 15419/20000: Train Loss = 0.435009, Test Loss = 0.250085, Learning Rate = 2.852568e-06\n",
      "Epoch 15420/20000: Train Loss = 0.434932, Test Loss = 0.251469, Learning Rate = 2.851484e-06\n",
      "Epoch 15421/20000: Train Loss = 0.435865, Test Loss = 0.251939, Learning Rate = 2.850400e-06\n",
      "Epoch 15422/20000: Train Loss = 0.435423, Test Loss = 0.256889, Learning Rate = 2.849317e-06\n",
      "Epoch 15423/20000: Train Loss = 0.435018, Test Loss = 0.254555, Learning Rate = 2.848235e-06\n",
      "Epoch 15424/20000: Train Loss = 0.435071, Test Loss = 0.253845, Learning Rate = 2.847152e-06\n",
      "Epoch 15425/20000: Train Loss = 0.434933, Test Loss = 0.253929, Learning Rate = 2.846071e-06\n",
      "Epoch 15426/20000: Train Loss = 0.434961, Test Loss = 0.251853, Learning Rate = 2.844989e-06\n",
      "Epoch 15427/20000: Train Loss = 0.434905, Test Loss = 0.252558, Learning Rate = 2.843908e-06\n",
      "Epoch 15428/20000: Train Loss = 0.434868, Test Loss = 0.253436, Learning Rate = 2.842828e-06\n",
      "Epoch 15429/20000: Train Loss = 0.435293, Test Loss = 0.255034, Learning Rate = 2.841747e-06\n",
      "Epoch 15430/20000: Train Loss = 0.435162, Test Loss = 0.252874, Learning Rate = 2.840668e-06\n",
      "Epoch 15431/20000: Train Loss = 0.435130, Test Loss = 0.253303, Learning Rate = 2.839588e-06\n",
      "Epoch 15432/20000: Train Loss = 0.435180, Test Loss = 0.252233, Learning Rate = 2.838509e-06\n",
      "Epoch 15433/20000: Train Loss = 0.435103, Test Loss = 0.253369, Learning Rate = 2.837431e-06\n",
      "Epoch 15434/20000: Train Loss = 0.434824, Test Loss = 0.253563, Learning Rate = 2.836353e-06\n",
      "Epoch 15435/20000: Train Loss = 0.434830, Test Loss = 0.253369, Learning Rate = 2.835275e-06\n",
      "Epoch 15436/20000: Train Loss = 0.434954, Test Loss = 0.252718, Learning Rate = 2.834197e-06\n",
      "Epoch 15437/20000: Train Loss = 0.435060, Test Loss = 0.249642, Learning Rate = 2.833121e-06\n",
      "Epoch 15438/20000: Train Loss = 0.435588, Test Loss = 0.252443, Learning Rate = 2.832044e-06\n",
      "Epoch 15439/20000: Train Loss = 0.435589, Test Loss = 0.251548, Learning Rate = 2.830968e-06\n",
      "Epoch 15440/20000: Train Loss = 0.434975, Test Loss = 0.254144, Learning Rate = 2.829892e-06\n",
      "Epoch 15441/20000: Train Loss = 0.434972, Test Loss = 0.254170, Learning Rate = 2.828817e-06\n",
      "Epoch 15442/20000: Train Loss = 0.434940, Test Loss = 0.252395, Learning Rate = 2.827742e-06\n",
      "Epoch 15443/20000: Train Loss = 0.434940, Test Loss = 0.251331, Learning Rate = 2.826668e-06\n",
      "Epoch 15444/20000: Train Loss = 0.435124, Test Loss = 0.253411, Learning Rate = 2.825594e-06\n",
      "Epoch 15445/20000: Train Loss = 0.434942, Test Loss = 0.252847, Learning Rate = 2.824520e-06\n",
      "Epoch 15446/20000: Train Loss = 0.435167, Test Loss = 0.253168, Learning Rate = 2.823447e-06\n",
      "Epoch 15447/20000: Train Loss = 0.435093, Test Loss = 0.252881, Learning Rate = 2.822374e-06\n",
      "Epoch 15448/20000: Train Loss = 0.435474, Test Loss = 0.250121, Learning Rate = 2.821301e-06\n",
      "Epoch 15449/20000: Train Loss = 0.434717, Test Loss = 0.251856, Learning Rate = 2.820229e-06\n",
      "Epoch 15450/20000: Train Loss = 0.434881, Test Loss = 0.250509, Learning Rate = 2.819158e-06\n",
      "Epoch 15451/20000: Train Loss = 0.434808, Test Loss = 0.251716, Learning Rate = 2.818087e-06\n",
      "Epoch 15452/20000: Train Loss = 0.435162, Test Loss = 0.250510, Learning Rate = 2.817016e-06\n",
      "Epoch 15453/20000: Train Loss = 0.434898, Test Loss = 0.252084, Learning Rate = 2.815945e-06\n",
      "Epoch 15454/20000: Train Loss = 0.435060, Test Loss = 0.250325, Learning Rate = 2.814875e-06\n",
      "Epoch 15455/20000: Train Loss = 0.434869, Test Loss = 0.253993, Learning Rate = 2.813806e-06\n",
      "Epoch 15456/20000: Train Loss = 0.434954, Test Loss = 0.253786, Learning Rate = 2.812737e-06\n",
      "Epoch 15457/20000: Train Loss = 0.435033, Test Loss = 0.252802, Learning Rate = 2.811668e-06\n",
      "Epoch 15458/20000: Train Loss = 0.435127, Test Loss = 0.253784, Learning Rate = 2.810600e-06\n",
      "Epoch 15459/20000: Train Loss = 0.434947, Test Loss = 0.254037, Learning Rate = 2.809532e-06\n",
      "Epoch 15460/20000: Train Loss = 0.434889, Test Loss = 0.253394, Learning Rate = 2.808464e-06\n",
      "Epoch 15461/20000: Train Loss = 0.434875, Test Loss = 0.252056, Learning Rate = 2.807397e-06\n",
      "Epoch 15462/20000: Train Loss = 0.435093, Test Loss = 0.253834, Learning Rate = 2.806330e-06\n",
      "Epoch 15463/20000: Train Loss = 0.435061, Test Loss = 0.253677, Learning Rate = 2.805264e-06\n",
      "Epoch 15464/20000: Train Loss = 0.435166, Test Loss = 0.253230, Learning Rate = 2.804198e-06\n",
      "Epoch 15465/20000: Train Loss = 0.435100, Test Loss = 0.253954, Learning Rate = 2.803132e-06\n",
      "Epoch 15466/20000: Train Loss = 0.434960, Test Loss = 0.254623, Learning Rate = 2.802067e-06\n",
      "Epoch 15467/20000: Train Loss = 0.435267, Test Loss = 0.254317, Learning Rate = 2.801003e-06\n",
      "Epoch 15468/20000: Train Loss = 0.434968, Test Loss = 0.253012, Learning Rate = 2.799938e-06\n",
      "Epoch 15469/20000: Train Loss = 0.435412, Test Loss = 0.254965, Learning Rate = 2.798874e-06\n",
      "Epoch 15470/20000: Train Loss = 0.435017, Test Loss = 0.255989, Learning Rate = 2.797811e-06\n",
      "Epoch 15471/20000: Train Loss = 0.434924, Test Loss = 0.255798, Learning Rate = 2.796748e-06\n",
      "Epoch 15472/20000: Train Loss = 0.435127, Test Loss = 0.257309, Learning Rate = 2.795685e-06\n",
      "Epoch 15473/20000: Train Loss = 0.434917, Test Loss = 0.256717, Learning Rate = 2.794623e-06\n",
      "Epoch 15474/20000: Train Loss = 0.435057, Test Loss = 0.256251, Learning Rate = 2.793561e-06\n",
      "Epoch 15475/20000: Train Loss = 0.435292, Test Loss = 0.255913, Learning Rate = 2.792499e-06\n",
      "Epoch 15476/20000: Train Loss = 0.435315, Test Loss = 0.253369, Learning Rate = 2.791438e-06\n",
      "Epoch 15477/20000: Train Loss = 0.434988, Test Loss = 0.254548, Learning Rate = 2.790378e-06\n",
      "Epoch 15478/20000: Train Loss = 0.435058, Test Loss = 0.254335, Learning Rate = 2.789317e-06\n",
      "Epoch 15479/20000: Train Loss = 0.434935, Test Loss = 0.255869, Learning Rate = 2.788258e-06\n",
      "Epoch 15480/20000: Train Loss = 0.435267, Test Loss = 0.255318, Learning Rate = 2.787198e-06\n",
      "Epoch 15481/20000: Train Loss = 0.435480, Test Loss = 0.255675, Learning Rate = 2.786139e-06\n",
      "Epoch 15482/20000: Train Loss = 0.435192, Test Loss = 0.255115, Learning Rate = 2.785080e-06\n",
      "Epoch 15483/20000: Train Loss = 0.435082, Test Loss = 0.253839, Learning Rate = 2.784022e-06\n",
      "Epoch 15484/20000: Train Loss = 0.435027, Test Loss = 0.253731, Learning Rate = 2.782964e-06\n",
      "Epoch 15485/20000: Train Loss = 0.435065, Test Loss = 0.253557, Learning Rate = 2.781907e-06\n",
      "Epoch 15486/20000: Train Loss = 0.434827, Test Loss = 0.252623, Learning Rate = 2.780850e-06\n",
      "Epoch 15487/20000: Train Loss = 0.434832, Test Loss = 0.254232, Learning Rate = 2.779793e-06\n",
      "Epoch 15488/20000: Train Loss = 0.434958, Test Loss = 0.253063, Learning Rate = 2.778737e-06\n",
      "Epoch 15489/20000: Train Loss = 0.435143, Test Loss = 0.254159, Learning Rate = 2.777681e-06\n",
      "Epoch 15490/20000: Train Loss = 0.434988, Test Loss = 0.254490, Learning Rate = 2.776626e-06\n",
      "Epoch 15491/20000: Train Loss = 0.435194, Test Loss = 0.256499, Learning Rate = 2.775571e-06\n",
      "Epoch 15492/20000: Train Loss = 0.434923, Test Loss = 0.256110, Learning Rate = 2.774516e-06\n",
      "Epoch 15493/20000: Train Loss = 0.435148, Test Loss = 0.257461, Learning Rate = 2.773462e-06\n",
      "Epoch 15494/20000: Train Loss = 0.435032, Test Loss = 0.256101, Learning Rate = 2.772408e-06\n",
      "Epoch 15495/20000: Train Loss = 0.434979, Test Loss = 0.256091, Learning Rate = 2.771354e-06\n",
      "Epoch 15496/20000: Train Loss = 0.434977, Test Loss = 0.255283, Learning Rate = 2.770301e-06\n",
      "Epoch 15497/20000: Train Loss = 0.434861, Test Loss = 0.255089, Learning Rate = 2.769249e-06\n",
      "Epoch 15498/20000: Train Loss = 0.435162, Test Loss = 0.254712, Learning Rate = 2.768196e-06\n",
      "Epoch 15499/20000: Train Loss = 0.435388, Test Loss = 0.255621, Learning Rate = 2.767145e-06\n",
      "Epoch 15500/20000: Train Loss = 0.434983, Test Loss = 0.253649, Learning Rate = 2.766093e-06\n",
      "Epoch 15501/20000: Train Loss = 0.435417, Test Loss = 0.254961, Learning Rate = 2.765042e-06\n",
      "Epoch 15502/20000: Train Loss = 0.435149, Test Loss = 0.256999, Learning Rate = 2.763991e-06\n",
      "Epoch 15503/20000: Train Loss = 0.434823, Test Loss = 0.255912, Learning Rate = 2.762941e-06\n",
      "Epoch 15504/20000: Train Loss = 0.434972, Test Loss = 0.256533, Learning Rate = 2.761891e-06\n",
      "Epoch 15505/20000: Train Loss = 0.435214, Test Loss = 0.256125, Learning Rate = 2.760842e-06\n",
      "Epoch 15506/20000: Train Loss = 0.435143, Test Loss = 0.255904, Learning Rate = 2.759793e-06\n",
      "Epoch 15507/20000: Train Loss = 0.435033, Test Loss = 0.253128, Learning Rate = 2.758744e-06\n",
      "Epoch 15508/20000: Train Loss = 0.435437, Test Loss = 0.253204, Learning Rate = 2.757696e-06\n",
      "Epoch 15509/20000: Train Loss = 0.434892, Test Loss = 0.254686, Learning Rate = 2.756648e-06\n",
      "Epoch 15510/20000: Train Loss = 0.435335, Test Loss = 0.252922, Learning Rate = 2.755601e-06\n",
      "Epoch 15511/20000: Train Loss = 0.435213, Test Loss = 0.256016, Learning Rate = 2.754554e-06\n",
      "Epoch 15512/20000: Train Loss = 0.435281, Test Loss = 0.254335, Learning Rate = 2.753507e-06\n",
      "Epoch 15513/20000: Train Loss = 0.435231, Test Loss = 0.253868, Learning Rate = 2.752461e-06\n",
      "Epoch 15514/20000: Train Loss = 0.435062, Test Loss = 0.255400, Learning Rate = 2.751415e-06\n",
      "Epoch 15515/20000: Train Loss = 0.434888, Test Loss = 0.254597, Learning Rate = 2.750369e-06\n",
      "Epoch 15516/20000: Train Loss = 0.435567, Test Loss = 0.254743, Learning Rate = 2.749324e-06\n",
      "Epoch 15517/20000: Train Loss = 0.435184, Test Loss = 0.255406, Learning Rate = 2.748280e-06\n",
      "Epoch 15518/20000: Train Loss = 0.434745, Test Loss = 0.254287, Learning Rate = 2.747235e-06\n",
      "Epoch 15519/20000: Train Loss = 0.434917, Test Loss = 0.252610, Learning Rate = 2.746192e-06\n",
      "Epoch 15520/20000: Train Loss = 0.434965, Test Loss = 0.251162, Learning Rate = 2.745148e-06\n",
      "Epoch 15521/20000: Train Loss = 0.435272, Test Loss = 0.252908, Learning Rate = 2.744105e-06\n",
      "Epoch 15522/20000: Train Loss = 0.434893, Test Loss = 0.252440, Learning Rate = 2.743062e-06\n",
      "Epoch 15523/20000: Train Loss = 0.434964, Test Loss = 0.253167, Learning Rate = 2.742020e-06\n",
      "Epoch 15524/20000: Train Loss = 0.434899, Test Loss = 0.254265, Learning Rate = 2.740978e-06\n",
      "Epoch 15525/20000: Train Loss = 0.434974, Test Loss = 0.250771, Learning Rate = 2.739937e-06\n",
      "Epoch 15526/20000: Train Loss = 0.435082, Test Loss = 0.251309, Learning Rate = 2.738896e-06\n",
      "Epoch 15527/20000: Train Loss = 0.434873, Test Loss = 0.251473, Learning Rate = 2.737855e-06\n",
      "Epoch 15528/20000: Train Loss = 0.434998, Test Loss = 0.252784, Learning Rate = 2.736815e-06\n",
      "Epoch 15529/20000: Train Loss = 0.435023, Test Loss = 0.251912, Learning Rate = 2.735775e-06\n",
      "Epoch 15530/20000: Train Loss = 0.435068, Test Loss = 0.252633, Learning Rate = 2.734735e-06\n",
      "Epoch 15531/20000: Train Loss = 0.434979, Test Loss = 0.251464, Learning Rate = 2.733696e-06\n",
      "Epoch 15532/20000: Train Loss = 0.435309, Test Loss = 0.251008, Learning Rate = 2.732657e-06\n",
      "Epoch 15533/20000: Train Loss = 0.434936, Test Loss = 0.250608, Learning Rate = 2.731619e-06\n",
      "Epoch 15534/20000: Train Loss = 0.434847, Test Loss = 0.251395, Learning Rate = 2.730581e-06\n",
      "Epoch 15535/20000: Train Loss = 0.434928, Test Loss = 0.250158, Learning Rate = 2.729543e-06\n",
      "Epoch 15536/20000: Train Loss = 0.435142, Test Loss = 0.251234, Learning Rate = 2.728506e-06\n",
      "Epoch 15537/20000: Train Loss = 0.434870, Test Loss = 0.249568, Learning Rate = 2.727469e-06\n",
      "Epoch 15538/20000: Train Loss = 0.434893, Test Loss = 0.248410, Learning Rate = 2.726433e-06\n",
      "Epoch 15539/20000: Train Loss = 0.434855, Test Loss = 0.249938, Learning Rate = 2.725397e-06\n",
      "Epoch 15540/20000: Train Loss = 0.435124, Test Loss = 0.249245, Learning Rate = 2.724362e-06\n",
      "Epoch 15541/20000: Train Loss = 0.434862, Test Loss = 0.248437, Learning Rate = 2.723326e-06\n",
      "Epoch 15542/20000: Train Loss = 0.434879, Test Loss = 0.251306, Learning Rate = 2.722292e-06\n",
      "Epoch 15543/20000: Train Loss = 0.434809, Test Loss = 0.251716, Learning Rate = 2.721257e-06\n",
      "Epoch 15544/20000: Train Loss = 0.435002, Test Loss = 0.249599, Learning Rate = 2.720223e-06\n",
      "Epoch 15545/20000: Train Loss = 0.434755, Test Loss = 0.250992, Learning Rate = 2.719190e-06\n",
      "Epoch 15546/20000: Train Loss = 0.435154, Test Loss = 0.252279, Learning Rate = 2.718156e-06\n",
      "Epoch 15547/20000: Train Loss = 0.434994, Test Loss = 0.250679, Learning Rate = 2.717124e-06\n",
      "Epoch 15548/20000: Train Loss = 0.434912, Test Loss = 0.249688, Learning Rate = 2.716091e-06\n",
      "Epoch 15549/20000: Train Loss = 0.435185, Test Loss = 0.253806, Learning Rate = 2.715059e-06\n",
      "Epoch 15550/20000: Train Loss = 0.435127, Test Loss = 0.254191, Learning Rate = 2.714027e-06\n",
      "Epoch 15551/20000: Train Loss = 0.434897, Test Loss = 0.255061, Learning Rate = 2.712996e-06\n",
      "Epoch 15552/20000: Train Loss = 0.435006, Test Loss = 0.254448, Learning Rate = 2.711965e-06\n",
      "Epoch 15553/20000: Train Loss = 0.434731, Test Loss = 0.255750, Learning Rate = 2.710935e-06\n",
      "Epoch 15554/20000: Train Loss = 0.434821, Test Loss = 0.254683, Learning Rate = 2.709905e-06\n",
      "Epoch 15555/20000: Train Loss = 0.435074, Test Loss = 0.255213, Learning Rate = 2.708875e-06\n",
      "Epoch 15556/20000: Train Loss = 0.435226, Test Loss = 0.254021, Learning Rate = 2.707846e-06\n",
      "Epoch 15557/20000: Train Loss = 0.435036, Test Loss = 0.256657, Learning Rate = 2.706817e-06\n",
      "Epoch 15558/20000: Train Loss = 0.435072, Test Loss = 0.254380, Learning Rate = 2.705788e-06\n",
      "Epoch 15559/20000: Train Loss = 0.434965, Test Loss = 0.254479, Learning Rate = 2.704760e-06\n",
      "Epoch 15560/20000: Train Loss = 0.435485, Test Loss = 0.257213, Learning Rate = 2.703732e-06\n",
      "Epoch 15561/20000: Train Loss = 0.434796, Test Loss = 0.256339, Learning Rate = 2.702705e-06\n",
      "Epoch 15562/20000: Train Loss = 0.434820, Test Loss = 0.254491, Learning Rate = 2.701678e-06\n",
      "Epoch 15563/20000: Train Loss = 0.435194, Test Loss = 0.256374, Learning Rate = 2.700652e-06\n",
      "Epoch 15564/20000: Train Loss = 0.434820, Test Loss = 0.254487, Learning Rate = 2.699625e-06\n",
      "Epoch 15565/20000: Train Loss = 0.434806, Test Loss = 0.254253, Learning Rate = 2.698600e-06\n",
      "Epoch 15566/20000: Train Loss = 0.434906, Test Loss = 0.253561, Learning Rate = 2.697574e-06\n",
      "Epoch 15567/20000: Train Loss = 0.436708, Test Loss = 0.253557, Learning Rate = 2.696549e-06\n",
      "Epoch 15568/20000: Train Loss = 0.435153, Test Loss = 0.253896, Learning Rate = 2.695525e-06\n",
      "Epoch 15569/20000: Train Loss = 0.434844, Test Loss = 0.252371, Learning Rate = 2.694500e-06\n",
      "Epoch 15570/20000: Train Loss = 0.434811, Test Loss = 0.253404, Learning Rate = 2.693477e-06\n",
      "Epoch 15571/20000: Train Loss = 0.434776, Test Loss = 0.252896, Learning Rate = 2.692453e-06\n",
      "Epoch 15572/20000: Train Loss = 0.435181, Test Loss = 0.253013, Learning Rate = 2.691430e-06\n",
      "Epoch 15573/20000: Train Loss = 0.435529, Test Loss = 0.252469, Learning Rate = 2.690407e-06\n",
      "Epoch 15574/20000: Train Loss = 0.435049, Test Loss = 0.252570, Learning Rate = 2.689385e-06\n",
      "Epoch 15575/20000: Train Loss = 0.434948, Test Loss = 0.253241, Learning Rate = 2.688363e-06\n",
      "Epoch 15576/20000: Train Loss = 0.434783, Test Loss = 0.255035, Learning Rate = 2.687342e-06\n",
      "Epoch 15577/20000: Train Loss = 0.434866, Test Loss = 0.253963, Learning Rate = 2.686321e-06\n",
      "Epoch 15578/20000: Train Loss = 0.435280, Test Loss = 0.251856, Learning Rate = 2.685300e-06\n",
      "Epoch 15579/20000: Train Loss = 0.435100, Test Loss = 0.252413, Learning Rate = 2.684280e-06\n",
      "Epoch 15580/20000: Train Loss = 0.435030, Test Loss = 0.250505, Learning Rate = 2.683260e-06\n",
      "Epoch 15581/20000: Train Loss = 0.434850, Test Loss = 0.250557, Learning Rate = 2.682240e-06\n",
      "Epoch 15582/20000: Train Loss = 0.434983, Test Loss = 0.249432, Learning Rate = 2.681221e-06\n",
      "Epoch 15583/20000: Train Loss = 0.435422, Test Loss = 0.251314, Learning Rate = 2.680202e-06\n",
      "Epoch 15584/20000: Train Loss = 0.434740, Test Loss = 0.250948, Learning Rate = 2.679184e-06\n",
      "Epoch 15585/20000: Train Loss = 0.434812, Test Loss = 0.252922, Learning Rate = 2.678166e-06\n",
      "Epoch 15586/20000: Train Loss = 0.434829, Test Loss = 0.253080, Learning Rate = 2.677148e-06\n",
      "Epoch 15587/20000: Train Loss = 0.435401, Test Loss = 0.254962, Learning Rate = 2.676131e-06\n",
      "Epoch 15588/20000: Train Loss = 0.435271, Test Loss = 0.255556, Learning Rate = 2.675114e-06\n",
      "Epoch 15589/20000: Train Loss = 0.434777, Test Loss = 0.253660, Learning Rate = 2.674097e-06\n",
      "Epoch 15590/20000: Train Loss = 0.435047, Test Loss = 0.256288, Learning Rate = 2.673081e-06\n",
      "Epoch 15591/20000: Train Loss = 0.434952, Test Loss = 0.252044, Learning Rate = 2.672066e-06\n",
      "Epoch 15592/20000: Train Loss = 0.434957, Test Loss = 0.254606, Learning Rate = 2.671050e-06\n",
      "Epoch 15593/20000: Train Loss = 0.435587, Test Loss = 0.254500, Learning Rate = 2.670035e-06\n",
      "Epoch 15594/20000: Train Loss = 0.435003, Test Loss = 0.255038, Learning Rate = 2.669021e-06\n",
      "Epoch 15595/20000: Train Loss = 0.435111, Test Loss = 0.254783, Learning Rate = 2.668007e-06\n",
      "Epoch 15596/20000: Train Loss = 0.434950, Test Loss = 0.252213, Learning Rate = 2.666993e-06\n",
      "Epoch 15597/20000: Train Loss = 0.434982, Test Loss = 0.256127, Learning Rate = 2.665980e-06\n",
      "Epoch 15598/20000: Train Loss = 0.435592, Test Loss = 0.256958, Learning Rate = 2.664967e-06\n",
      "Epoch 15599/20000: Train Loss = 0.435016, Test Loss = 0.253369, Learning Rate = 2.663954e-06\n",
      "Epoch 15600/20000: Train Loss = 0.434763, Test Loss = 0.252193, Learning Rate = 2.662942e-06\n",
      "Epoch 15601/20000: Train Loss = 0.434918, Test Loss = 0.252270, Learning Rate = 2.661930e-06\n",
      "Epoch 15602/20000: Train Loss = 0.434883, Test Loss = 0.252234, Learning Rate = 2.660918e-06\n",
      "Epoch 15603/20000: Train Loss = 0.435231, Test Loss = 0.251356, Learning Rate = 2.659907e-06\n",
      "Epoch 15604/20000: Train Loss = 0.434921, Test Loss = 0.250723, Learning Rate = 2.658897e-06\n",
      "Epoch 15605/20000: Train Loss = 0.434922, Test Loss = 0.250329, Learning Rate = 2.657886e-06\n",
      "Epoch 15606/20000: Train Loss = 0.434770, Test Loss = 0.251279, Learning Rate = 2.656876e-06\n",
      "Epoch 15607/20000: Train Loss = 0.434900, Test Loss = 0.250464, Learning Rate = 2.655867e-06\n",
      "Epoch 15608/20000: Train Loss = 0.434742, Test Loss = 0.250170, Learning Rate = 2.654858e-06\n",
      "Epoch 15609/20000: Train Loss = 0.434884, Test Loss = 0.249012, Learning Rate = 2.653849e-06\n",
      "Epoch 15610/20000: Train Loss = 0.434772, Test Loss = 0.248542, Learning Rate = 2.652841e-06\n",
      "Epoch 15611/20000: Train Loss = 0.434922, Test Loss = 0.250091, Learning Rate = 2.651832e-06\n",
      "Epoch 15612/20000: Train Loss = 0.434824, Test Loss = 0.251088, Learning Rate = 2.650825e-06\n",
      "Epoch 15613/20000: Train Loss = 0.434905, Test Loss = 0.249906, Learning Rate = 2.649818e-06\n",
      "Epoch 15614/20000: Train Loss = 0.434892, Test Loss = 0.250016, Learning Rate = 2.648811e-06\n",
      "Epoch 15615/20000: Train Loss = 0.434915, Test Loss = 0.250469, Learning Rate = 2.647804e-06\n",
      "Epoch 15616/20000: Train Loss = 0.435557, Test Loss = 0.250649, Learning Rate = 2.646798e-06\n",
      "Epoch 15617/20000: Train Loss = 0.434977, Test Loss = 0.251832, Learning Rate = 2.645792e-06\n",
      "Epoch 15618/20000: Train Loss = 0.435263, Test Loss = 0.252293, Learning Rate = 2.644787e-06\n",
      "Epoch 15619/20000: Train Loss = 0.435022, Test Loss = 0.252068, Learning Rate = 2.643782e-06\n",
      "Epoch 15620/20000: Train Loss = 0.434807, Test Loss = 0.252089, Learning Rate = 2.642778e-06\n",
      "Epoch 15621/20000: Train Loss = 0.435051, Test Loss = 0.254118, Learning Rate = 2.641773e-06\n",
      "Epoch 15622/20000: Train Loss = 0.435038, Test Loss = 0.252351, Learning Rate = 2.640770e-06\n",
      "Epoch 15623/20000: Train Loss = 0.434966, Test Loss = 0.251693, Learning Rate = 2.639766e-06\n",
      "Epoch 15624/20000: Train Loss = 0.434911, Test Loss = 0.254090, Learning Rate = 2.638763e-06\n",
      "Epoch 15625/20000: Train Loss = 0.434770, Test Loss = 0.252181, Learning Rate = 2.637761e-06\n",
      "Epoch 15626/20000: Train Loss = 0.435046, Test Loss = 0.252638, Learning Rate = 2.636758e-06\n",
      "Epoch 15627/20000: Train Loss = 0.434843, Test Loss = 0.252883, Learning Rate = 2.635756e-06\n",
      "Epoch 15628/20000: Train Loss = 0.434813, Test Loss = 0.251311, Learning Rate = 2.634755e-06\n",
      "Epoch 15629/20000: Train Loss = 0.434727, Test Loss = 0.251431, Learning Rate = 2.633754e-06\n",
      "Epoch 15630/20000: Train Loss = 0.434938, Test Loss = 0.253452, Learning Rate = 2.632753e-06\n",
      "Epoch 15631/20000: Train Loss = 0.434872, Test Loss = 0.251174, Learning Rate = 2.631753e-06\n",
      "Epoch 15632/20000: Train Loss = 0.434904, Test Loss = 0.254106, Learning Rate = 2.630753e-06\n",
      "Epoch 15633/20000: Train Loss = 0.434950, Test Loss = 0.254899, Learning Rate = 2.629753e-06\n",
      "Epoch 15634/20000: Train Loss = 0.434964, Test Loss = 0.252934, Learning Rate = 2.628754e-06\n",
      "Epoch 15635/20000: Train Loss = 0.434813, Test Loss = 0.253820, Learning Rate = 2.627755e-06\n",
      "Epoch 15636/20000: Train Loss = 0.435264, Test Loss = 0.254290, Learning Rate = 2.626756e-06\n",
      "Epoch 15637/20000: Train Loss = 0.434858, Test Loss = 0.253736, Learning Rate = 2.625758e-06\n",
      "Epoch 15638/20000: Train Loss = 0.434958, Test Loss = 0.252411, Learning Rate = 2.624761e-06\n",
      "Epoch 15639/20000: Train Loss = 0.434798, Test Loss = 0.253481, Learning Rate = 2.623763e-06\n",
      "Epoch 15640/20000: Train Loss = 0.435282, Test Loss = 0.254736, Learning Rate = 2.622766e-06\n",
      "Epoch 15641/20000: Train Loss = 0.435106, Test Loss = 0.254249, Learning Rate = 2.621770e-06\n",
      "Epoch 15642/20000: Train Loss = 0.434728, Test Loss = 0.253334, Learning Rate = 2.620774e-06\n",
      "Epoch 15643/20000: Train Loss = 0.435592, Test Loss = 0.256083, Learning Rate = 2.619778e-06\n",
      "Epoch 15644/20000: Train Loss = 0.434860, Test Loss = 0.255331, Learning Rate = 2.618782e-06\n",
      "Epoch 15645/20000: Train Loss = 0.434785, Test Loss = 0.254805, Learning Rate = 2.617787e-06\n",
      "Epoch 15646/20000: Train Loss = 0.435121, Test Loss = 0.252374, Learning Rate = 2.616792e-06\n",
      "Epoch 15647/20000: Train Loss = 0.434899, Test Loss = 0.254499, Learning Rate = 2.615798e-06\n",
      "Epoch 15648/20000: Train Loss = 0.434923, Test Loss = 0.252729, Learning Rate = 2.614804e-06\n",
      "Epoch 15649/20000: Train Loss = 0.434837, Test Loss = 0.253551, Learning Rate = 2.613811e-06\n",
      "Epoch 15650/20000: Train Loss = 0.435111, Test Loss = 0.255301, Learning Rate = 2.612818e-06\n",
      "Epoch 15651/20000: Train Loss = 0.435073, Test Loss = 0.252730, Learning Rate = 2.611825e-06\n",
      "Epoch 15652/20000: Train Loss = 0.434973, Test Loss = 0.253584, Learning Rate = 2.610832e-06\n",
      "Epoch 15653/20000: Train Loss = 0.434988, Test Loss = 0.252366, Learning Rate = 2.609840e-06\n",
      "Epoch 15654/20000: Train Loss = 0.434972, Test Loss = 0.253516, Learning Rate = 2.608849e-06\n",
      "Epoch 15655/20000: Train Loss = 0.434953, Test Loss = 0.253545, Learning Rate = 2.607857e-06\n",
      "Epoch 15656/20000: Train Loss = 0.434842, Test Loss = 0.250885, Learning Rate = 2.606866e-06\n",
      "Epoch 15657/20000: Train Loss = 0.434836, Test Loss = 0.251197, Learning Rate = 2.605876e-06\n",
      "Epoch 15658/20000: Train Loss = 0.434736, Test Loss = 0.252118, Learning Rate = 2.604886e-06\n",
      "Epoch 15659/20000: Train Loss = 0.434808, Test Loss = 0.251176, Learning Rate = 2.603896e-06\n",
      "Epoch 15660/20000: Train Loss = 0.435020, Test Loss = 0.250800, Learning Rate = 2.602906e-06\n",
      "Epoch 15661/20000: Train Loss = 0.434714, Test Loss = 0.251455, Learning Rate = 2.601917e-06\n",
      "Epoch 15662/20000: Train Loss = 0.434946, Test Loss = 0.252599, Learning Rate = 2.600929e-06\n",
      "Epoch 15663/20000: Train Loss = 0.434786, Test Loss = 0.252818, Learning Rate = 2.599941e-06\n",
      "Epoch 15664/20000: Train Loss = 0.434970, Test Loss = 0.252626, Learning Rate = 2.598953e-06\n",
      "Epoch 15665/20000: Train Loss = 0.434938, Test Loss = 0.253425, Learning Rate = 2.597965e-06\n",
      "Epoch 15666/20000: Train Loss = 0.434790, Test Loss = 0.254026, Learning Rate = 2.596978e-06\n",
      "Epoch 15667/20000: Train Loss = 0.435233, Test Loss = 0.252961, Learning Rate = 2.595991e-06\n",
      "Epoch 15668/20000: Train Loss = 0.434812, Test Loss = 0.252060, Learning Rate = 2.595005e-06\n",
      "Epoch 15669/20000: Train Loss = 0.434835, Test Loss = 0.253022, Learning Rate = 2.594019e-06\n",
      "Epoch 15670/20000: Train Loss = 0.434861, Test Loss = 0.251739, Learning Rate = 2.593033e-06\n",
      "Epoch 15671/20000: Train Loss = 0.434838, Test Loss = 0.252269, Learning Rate = 2.592048e-06\n",
      "Epoch 15672/20000: Train Loss = 0.434844, Test Loss = 0.253621, Learning Rate = 2.591063e-06\n",
      "Epoch 15673/20000: Train Loss = 0.434843, Test Loss = 0.254134, Learning Rate = 2.590078e-06\n",
      "Epoch 15674/20000: Train Loss = 0.434832, Test Loss = 0.253137, Learning Rate = 2.589094e-06\n",
      "Epoch 15675/20000: Train Loss = 0.435176, Test Loss = 0.252989, Learning Rate = 2.588110e-06\n",
      "Epoch 15676/20000: Train Loss = 0.434771, Test Loss = 0.251145, Learning Rate = 2.587127e-06\n",
      "Epoch 15677/20000: Train Loss = 0.435269, Test Loss = 0.252108, Learning Rate = 2.586144e-06\n",
      "Epoch 15678/20000: Train Loss = 0.434945, Test Loss = 0.251153, Learning Rate = 2.585161e-06\n",
      "Epoch 15679/20000: Train Loss = 0.435351, Test Loss = 0.251630, Learning Rate = 2.584179e-06\n",
      "Epoch 15680/20000: Train Loss = 0.435131, Test Loss = 0.250802, Learning Rate = 2.583197e-06\n",
      "Epoch 15681/20000: Train Loss = 0.434726, Test Loss = 0.250845, Learning Rate = 2.582215e-06\n",
      "Epoch 15682/20000: Train Loss = 0.434862, Test Loss = 0.252481, Learning Rate = 2.581234e-06\n",
      "Epoch 15683/20000: Train Loss = 0.435613, Test Loss = 0.254956, Learning Rate = 2.580254e-06\n",
      "Epoch 15684/20000: Train Loss = 0.434790, Test Loss = 0.251704, Learning Rate = 2.579273e-06\n",
      "Epoch 15685/20000: Train Loss = 0.434970, Test Loss = 0.250877, Learning Rate = 2.578293e-06\n",
      "Epoch 15686/20000: Train Loss = 0.434945, Test Loss = 0.253169, Learning Rate = 2.577313e-06\n",
      "Epoch 15687/20000: Train Loss = 0.434833, Test Loss = 0.253677, Learning Rate = 2.576334e-06\n",
      "Epoch 15688/20000: Train Loss = 0.434928, Test Loss = 0.252903, Learning Rate = 2.575355e-06\n",
      "Epoch 15689/20000: Train Loss = 0.434825, Test Loss = 0.253679, Learning Rate = 2.574377e-06\n",
      "Epoch 15690/20000: Train Loss = 0.434898, Test Loss = 0.252612, Learning Rate = 2.573398e-06\n",
      "Epoch 15691/20000: Train Loss = 0.434909, Test Loss = 0.252985, Learning Rate = 2.572421e-06\n",
      "Epoch 15692/20000: Train Loss = 0.434714, Test Loss = 0.252009, Learning Rate = 2.571443e-06\n",
      "Epoch 15693/20000: Train Loss = 0.435002, Test Loss = 0.251453, Learning Rate = 2.570466e-06\n",
      "Epoch 15694/20000: Train Loss = 0.434830, Test Loss = 0.251182, Learning Rate = 2.569489e-06\n",
      "Epoch 15695/20000: Train Loss = 0.434900, Test Loss = 0.250760, Learning Rate = 2.568513e-06\n",
      "Epoch 15696/20000: Train Loss = 0.434841, Test Loss = 0.252491, Learning Rate = 2.567537e-06\n",
      "Epoch 15697/20000: Train Loss = 0.434736, Test Loss = 0.250874, Learning Rate = 2.566561e-06\n",
      "Epoch 15698/20000: Train Loss = 0.434875, Test Loss = 0.251878, Learning Rate = 2.565586e-06\n",
      "Epoch 15699/20000: Train Loss = 0.434957, Test Loss = 0.252981, Learning Rate = 2.564611e-06\n",
      "Epoch 15700/20000: Train Loss = 0.434784, Test Loss = 0.253064, Learning Rate = 2.563637e-06\n",
      "Epoch 15701/20000: Train Loss = 0.434885, Test Loss = 0.253088, Learning Rate = 2.562663e-06\n",
      "Epoch 15702/20000: Train Loss = 0.435203, Test Loss = 0.254042, Learning Rate = 2.561689e-06\n",
      "Epoch 15703/20000: Train Loss = 0.435067, Test Loss = 0.252949, Learning Rate = 2.560716e-06\n",
      "Epoch 15704/20000: Train Loss = 0.434973, Test Loss = 0.253616, Learning Rate = 2.559743e-06\n",
      "Epoch 15705/20000: Train Loss = 0.435075, Test Loss = 0.254034, Learning Rate = 2.558770e-06\n",
      "Epoch 15706/20000: Train Loss = 0.434803, Test Loss = 0.253324, Learning Rate = 2.557798e-06\n",
      "Epoch 15707/20000: Train Loss = 0.434708, Test Loss = 0.253358, Learning Rate = 2.556826e-06\n",
      "Epoch 15708/20000: Train Loss = 0.434944, Test Loss = 0.252956, Learning Rate = 2.555854e-06\n",
      "Epoch 15709/20000: Train Loss = 0.434759, Test Loss = 0.252982, Learning Rate = 2.554883e-06\n",
      "Epoch 15710/20000: Train Loss = 0.435019, Test Loss = 0.253188, Learning Rate = 2.553912e-06\n",
      "Epoch 15711/20000: Train Loss = 0.435060, Test Loss = 0.253341, Learning Rate = 2.552942e-06\n",
      "Epoch 15712/20000: Train Loss = 0.434992, Test Loss = 0.252167, Learning Rate = 2.551972e-06\n",
      "Epoch 15713/20000: Train Loss = 0.435148, Test Loss = 0.252093, Learning Rate = 2.551002e-06\n",
      "Epoch 15714/20000: Train Loss = 0.435130, Test Loss = 0.253866, Learning Rate = 2.550033e-06\n",
      "Epoch 15715/20000: Train Loss = 0.434969, Test Loss = 0.250888, Learning Rate = 2.549064e-06\n",
      "Epoch 15716/20000: Train Loss = 0.435507, Test Loss = 0.250828, Learning Rate = 2.548095e-06\n",
      "Epoch 15717/20000: Train Loss = 0.435312, Test Loss = 0.250010, Learning Rate = 2.547127e-06\n",
      "Epoch 15718/20000: Train Loss = 0.435042, Test Loss = 0.250847, Learning Rate = 2.546159e-06\n",
      "Epoch 15719/20000: Train Loss = 0.434740, Test Loss = 0.251339, Learning Rate = 2.545192e-06\n",
      "Epoch 15720/20000: Train Loss = 0.435145, Test Loss = 0.251022, Learning Rate = 2.544225e-06\n",
      "Epoch 15721/20000: Train Loss = 0.435169, Test Loss = 0.248685, Learning Rate = 2.543258e-06\n",
      "Epoch 15722/20000: Train Loss = 0.434898, Test Loss = 0.249742, Learning Rate = 2.542292e-06\n",
      "Epoch 15723/20000: Train Loss = 0.434912, Test Loss = 0.249528, Learning Rate = 2.541326e-06\n",
      "Epoch 15724/20000: Train Loss = 0.434766, Test Loss = 0.250197, Learning Rate = 2.540360e-06\n",
      "Epoch 15725/20000: Train Loss = 0.434904, Test Loss = 0.249539, Learning Rate = 2.539395e-06\n",
      "Epoch 15726/20000: Train Loss = 0.434759, Test Loss = 0.248860, Learning Rate = 2.538430e-06\n",
      "Epoch 15727/20000: Train Loss = 0.434891, Test Loss = 0.249258, Learning Rate = 2.537465e-06\n",
      "Epoch 15728/20000: Train Loss = 0.434695, Test Loss = 0.250963, Learning Rate = 2.536501e-06\n",
      "Epoch 15729/20000: Train Loss = 0.434813, Test Loss = 0.251593, Learning Rate = 2.535537e-06\n",
      "Epoch 15730/20000: Train Loss = 0.434894, Test Loss = 0.248574, Learning Rate = 2.534574e-06\n",
      "Epoch 15731/20000: Train Loss = 0.434752, Test Loss = 0.251207, Learning Rate = 2.533611e-06\n",
      "Epoch 15732/20000: Train Loss = 0.435098, Test Loss = 0.249318, Learning Rate = 2.532648e-06\n",
      "Epoch 15733/20000: Train Loss = 0.434738, Test Loss = 0.250347, Learning Rate = 2.531686e-06\n",
      "Epoch 15734/20000: Train Loss = 0.435054, Test Loss = 0.249521, Learning Rate = 2.530724e-06\n",
      "Epoch 15735/20000: Train Loss = 0.434830, Test Loss = 0.250237, Learning Rate = 2.529762e-06\n",
      "Epoch 15736/20000: Train Loss = 0.435101, Test Loss = 0.250533, Learning Rate = 2.528801e-06\n",
      "Epoch 15737/20000: Train Loss = 0.435401, Test Loss = 0.252292, Learning Rate = 2.527840e-06\n",
      "Epoch 15738/20000: Train Loss = 0.434637, Test Loss = 0.250054, Learning Rate = 2.526880e-06\n",
      "Epoch 15739/20000: Train Loss = 0.435061, Test Loss = 0.251607, Learning Rate = 2.525919e-06\n",
      "Epoch 15740/20000: Train Loss = 0.434870, Test Loss = 0.250583, Learning Rate = 2.524960e-06\n",
      "Epoch 15741/20000: Train Loss = 0.435316, Test Loss = 0.250048, Learning Rate = 2.524000e-06\n",
      "Epoch 15742/20000: Train Loss = 0.435077, Test Loss = 0.251320, Learning Rate = 2.523041e-06\n",
      "Epoch 15743/20000: Train Loss = 0.434993, Test Loss = 0.252105, Learning Rate = 2.522083e-06\n",
      "Epoch 15744/20000: Train Loss = 0.435317, Test Loss = 0.251366, Learning Rate = 2.521124e-06\n",
      "Epoch 15745/20000: Train Loss = 0.434888, Test Loss = 0.251003, Learning Rate = 2.520166e-06\n",
      "Epoch 15746/20000: Train Loss = 0.435112, Test Loss = 0.253554, Learning Rate = 2.519209e-06\n",
      "Epoch 15747/20000: Train Loss = 0.435129, Test Loss = 0.251406, Learning Rate = 2.518251e-06\n",
      "Epoch 15748/20000: Train Loss = 0.434866, Test Loss = 0.252395, Learning Rate = 2.517295e-06\n",
      "Epoch 15749/20000: Train Loss = 0.434690, Test Loss = 0.253980, Learning Rate = 2.516338e-06\n",
      "Epoch 15750/20000: Train Loss = 0.434754, Test Loss = 0.252723, Learning Rate = 2.515382e-06\n",
      "Epoch 15751/20000: Train Loss = 0.434761, Test Loss = 0.251982, Learning Rate = 2.514426e-06\n",
      "Epoch 15752/20000: Train Loss = 0.434850, Test Loss = 0.252834, Learning Rate = 2.513471e-06\n",
      "Epoch 15753/20000: Train Loss = 0.434745, Test Loss = 0.253078, Learning Rate = 2.512516e-06\n",
      "Epoch 15754/20000: Train Loss = 0.434776, Test Loss = 0.252343, Learning Rate = 2.511561e-06\n",
      "Epoch 15755/20000: Train Loss = 0.434742, Test Loss = 0.251589, Learning Rate = 2.510607e-06\n",
      "Epoch 15756/20000: Train Loss = 0.435313, Test Loss = 0.253537, Learning Rate = 2.509653e-06\n",
      "Epoch 15757/20000: Train Loss = 0.434913, Test Loss = 0.253140, Learning Rate = 2.508699e-06\n",
      "Epoch 15758/20000: Train Loss = 0.434705, Test Loss = 0.253296, Learning Rate = 2.507746e-06\n",
      "Epoch 15759/20000: Train Loss = 0.434737, Test Loss = 0.253186, Learning Rate = 2.506793e-06\n",
      "Epoch 15760/20000: Train Loss = 0.434740, Test Loss = 0.254245, Learning Rate = 2.505840e-06\n",
      "Epoch 15761/20000: Train Loss = 0.434691, Test Loss = 0.253354, Learning Rate = 2.504888e-06\n",
      "Epoch 15762/20000: Train Loss = 0.435455, Test Loss = 0.251098, Learning Rate = 2.503937e-06\n",
      "Epoch 15763/20000: Train Loss = 0.434793, Test Loss = 0.252739, Learning Rate = 2.502985e-06\n",
      "Epoch 15764/20000: Train Loss = 0.434846, Test Loss = 0.254128, Learning Rate = 2.502034e-06\n",
      "Epoch 15765/20000: Train Loss = 0.434774, Test Loss = 0.253415, Learning Rate = 2.501083e-06\n",
      "Epoch 15766/20000: Train Loss = 0.435347, Test Loss = 0.253232, Learning Rate = 2.500133e-06\n",
      "Epoch 15767/20000: Train Loss = 0.434962, Test Loss = 0.253862, Learning Rate = 2.499183e-06\n",
      "Epoch 15768/20000: Train Loss = 0.434689, Test Loss = 0.254643, Learning Rate = 2.498233e-06\n",
      "Epoch 15769/20000: Train Loss = 0.434768, Test Loss = 0.254199, Learning Rate = 2.497284e-06\n",
      "Epoch 15770/20000: Train Loss = 0.434889, Test Loss = 0.253453, Learning Rate = 2.496335e-06\n",
      "Epoch 15771/20000: Train Loss = 0.434787, Test Loss = 0.254983, Learning Rate = 2.495387e-06\n",
      "Epoch 15772/20000: Train Loss = 0.434816, Test Loss = 0.255423, Learning Rate = 2.494438e-06\n",
      "Epoch 15773/20000: Train Loss = 0.435079, Test Loss = 0.254939, Learning Rate = 2.493491e-06\n",
      "Epoch 15774/20000: Train Loss = 0.434814, Test Loss = 0.254951, Learning Rate = 2.492543e-06\n",
      "Epoch 15775/20000: Train Loss = 0.434828, Test Loss = 0.255664, Learning Rate = 2.491596e-06\n",
      "Epoch 15776/20000: Train Loss = 0.434930, Test Loss = 0.255251, Learning Rate = 2.490649e-06\n",
      "Epoch 15777/20000: Train Loss = 0.434792, Test Loss = 0.252801, Learning Rate = 2.489703e-06\n",
      "Epoch 15778/20000: Train Loss = 0.434813, Test Loss = 0.253729, Learning Rate = 2.488757e-06\n",
      "Epoch 15779/20000: Train Loss = 0.434844, Test Loss = 0.253435, Learning Rate = 2.487811e-06\n",
      "Epoch 15780/20000: Train Loss = 0.434790, Test Loss = 0.253209, Learning Rate = 2.486866e-06\n",
      "Epoch 15781/20000: Train Loss = 0.434700, Test Loss = 0.253291, Learning Rate = 2.485921e-06\n",
      "Epoch 15782/20000: Train Loss = 0.434933, Test Loss = 0.253877, Learning Rate = 2.484976e-06\n",
      "Epoch 15783/20000: Train Loss = 0.435136, Test Loss = 0.252679, Learning Rate = 2.484032e-06\n",
      "Epoch 15784/20000: Train Loss = 0.434859, Test Loss = 0.252294, Learning Rate = 2.483088e-06\n",
      "Epoch 15785/20000: Train Loss = 0.434745, Test Loss = 0.252878, Learning Rate = 2.482145e-06\n",
      "Epoch 15786/20000: Train Loss = 0.435134, Test Loss = 0.251823, Learning Rate = 2.481202e-06\n",
      "Epoch 15787/20000: Train Loss = 0.434882, Test Loss = 0.252384, Learning Rate = 2.480259e-06\n",
      "Epoch 15788/20000: Train Loss = 0.435258, Test Loss = 0.252500, Learning Rate = 2.479317e-06\n",
      "Epoch 15789/20000: Train Loss = 0.435021, Test Loss = 0.254144, Learning Rate = 2.478374e-06\n",
      "Epoch 15790/20000: Train Loss = 0.434944, Test Loss = 0.254035, Learning Rate = 2.477433e-06\n",
      "Epoch 15791/20000: Train Loss = 0.435023, Test Loss = 0.253894, Learning Rate = 2.476491e-06\n",
      "Epoch 15792/20000: Train Loss = 0.434938, Test Loss = 0.253411, Learning Rate = 2.475550e-06\n",
      "Epoch 15793/20000: Train Loss = 0.434793, Test Loss = 0.253158, Learning Rate = 2.474610e-06\n",
      "Epoch 15794/20000: Train Loss = 0.434815, Test Loss = 0.255142, Learning Rate = 2.473669e-06\n",
      "Epoch 15795/20000: Train Loss = 0.434823, Test Loss = 0.253968, Learning Rate = 2.472730e-06\n",
      "Epoch 15796/20000: Train Loss = 0.434996, Test Loss = 0.255320, Learning Rate = 2.471790e-06\n",
      "Epoch 15797/20000: Train Loss = 0.434911, Test Loss = 0.252140, Learning Rate = 2.470851e-06\n",
      "Epoch 15798/20000: Train Loss = 0.434683, Test Loss = 0.253859, Learning Rate = 2.469912e-06\n",
      "Epoch 15799/20000: Train Loss = 0.434883, Test Loss = 0.253179, Learning Rate = 2.468973e-06\n",
      "Epoch 15800/20000: Train Loss = 0.434729, Test Loss = 0.253405, Learning Rate = 2.468035e-06\n",
      "Epoch 15801/20000: Train Loss = 0.434631, Test Loss = 0.253656, Learning Rate = 2.467097e-06\n",
      "Epoch 15802/20000: Train Loss = 0.434803, Test Loss = 0.254801, Learning Rate = 2.466160e-06\n",
      "Epoch 15803/20000: Train Loss = 0.434763, Test Loss = 0.253174, Learning Rate = 2.465223e-06\n",
      "Epoch 15804/20000: Train Loss = 0.434823, Test Loss = 0.253959, Learning Rate = 2.464286e-06\n",
      "Epoch 15805/20000: Train Loss = 0.434979, Test Loss = 0.254458, Learning Rate = 2.463350e-06\n",
      "Epoch 15806/20000: Train Loss = 0.435163, Test Loss = 0.255228, Learning Rate = 2.462414e-06\n",
      "Epoch 15807/20000: Train Loss = 0.435328, Test Loss = 0.253849, Learning Rate = 2.461478e-06\n",
      "Epoch 15808/20000: Train Loss = 0.435482, Test Loss = 0.255709, Learning Rate = 2.460543e-06\n",
      "Epoch 15809/20000: Train Loss = 0.434931, Test Loss = 0.256410, Learning Rate = 2.459608e-06\n",
      "Epoch 15810/20000: Train Loss = 0.434644, Test Loss = 0.254810, Learning Rate = 2.458673e-06\n",
      "Epoch 15811/20000: Train Loss = 0.434763, Test Loss = 0.253260, Learning Rate = 2.457739e-06\n",
      "Epoch 15812/20000: Train Loss = 0.434819, Test Loss = 0.256083, Learning Rate = 2.456805e-06\n",
      "Epoch 15813/20000: Train Loss = 0.434942, Test Loss = 0.252896, Learning Rate = 2.455872e-06\n",
      "Epoch 15814/20000: Train Loss = 0.434721, Test Loss = 0.254499, Learning Rate = 2.454939e-06\n",
      "Epoch 15815/20000: Train Loss = 0.434844, Test Loss = 0.255434, Learning Rate = 2.454006e-06\n",
      "Epoch 15816/20000: Train Loss = 0.434890, Test Loss = 0.252970, Learning Rate = 2.453073e-06\n",
      "Epoch 15817/20000: Train Loss = 0.434826, Test Loss = 0.255622, Learning Rate = 2.452141e-06\n",
      "Epoch 15818/20000: Train Loss = 0.434890, Test Loss = 0.255803, Learning Rate = 2.451209e-06\n",
      "Epoch 15819/20000: Train Loss = 0.434961, Test Loss = 0.253947, Learning Rate = 2.450278e-06\n",
      "Epoch 15820/20000: Train Loss = 0.434733, Test Loss = 0.255453, Learning Rate = 2.449347e-06\n",
      "Epoch 15821/20000: Train Loss = 0.435397, Test Loss = 0.255095, Learning Rate = 2.448416e-06\n",
      "Epoch 15822/20000: Train Loss = 0.435198, Test Loss = 0.254240, Learning Rate = 2.447486e-06\n",
      "Epoch 15823/20000: Train Loss = 0.434996, Test Loss = 0.254588, Learning Rate = 2.446556e-06\n",
      "Epoch 15824/20000: Train Loss = 0.434754, Test Loss = 0.252384, Learning Rate = 2.445626e-06\n",
      "Epoch 15825/20000: Train Loss = 0.435134, Test Loss = 0.253335, Learning Rate = 2.444697e-06\n",
      "Epoch 15826/20000: Train Loss = 0.434908, Test Loss = 0.250985, Learning Rate = 2.443768e-06\n",
      "Epoch 15827/20000: Train Loss = 0.434688, Test Loss = 0.252963, Learning Rate = 2.442840e-06\n",
      "Epoch 15828/20000: Train Loss = 0.435035, Test Loss = 0.253300, Learning Rate = 2.441911e-06\n",
      "Epoch 15829/20000: Train Loss = 0.434845, Test Loss = 0.253160, Learning Rate = 2.440984e-06\n",
      "Epoch 15830/20000: Train Loss = 0.434751, Test Loss = 0.251834, Learning Rate = 2.440056e-06\n",
      "Epoch 15831/20000: Train Loss = 0.435000, Test Loss = 0.252498, Learning Rate = 2.439129e-06\n",
      "Epoch 15832/20000: Train Loss = 0.434910, Test Loss = 0.253401, Learning Rate = 2.438202e-06\n",
      "Epoch 15833/20000: Train Loss = 0.434957, Test Loss = 0.249932, Learning Rate = 2.437276e-06\n",
      "Epoch 15834/20000: Train Loss = 0.435247, Test Loss = 0.250973, Learning Rate = 2.436350e-06\n",
      "Epoch 15835/20000: Train Loss = 0.434939, Test Loss = 0.251229, Learning Rate = 2.435424e-06\n",
      "Epoch 15836/20000: Train Loss = 0.435153, Test Loss = 0.253782, Learning Rate = 2.434498e-06\n",
      "Epoch 15837/20000: Train Loss = 0.434769, Test Loss = 0.251283, Learning Rate = 2.433573e-06\n",
      "Epoch 15838/20000: Train Loss = 0.434904, Test Loss = 0.251847, Learning Rate = 2.432649e-06\n",
      "Epoch 15839/20000: Train Loss = 0.434854, Test Loss = 0.252014, Learning Rate = 2.431724e-06\n",
      "Epoch 15840/20000: Train Loss = 0.434864, Test Loss = 0.251359, Learning Rate = 2.430800e-06\n",
      "Epoch 15841/20000: Train Loss = 0.435151, Test Loss = 0.253520, Learning Rate = 2.429877e-06\n",
      "Epoch 15842/20000: Train Loss = 0.434981, Test Loss = 0.250900, Learning Rate = 2.428953e-06\n",
      "Epoch 15843/20000: Train Loss = 0.434804, Test Loss = 0.251906, Learning Rate = 2.428031e-06\n",
      "Epoch 15844/20000: Train Loss = 0.434766, Test Loss = 0.252526, Learning Rate = 2.427108e-06\n",
      "Epoch 15845/20000: Train Loss = 0.434704, Test Loss = 0.250901, Learning Rate = 2.426186e-06\n",
      "Epoch 15846/20000: Train Loss = 0.434673, Test Loss = 0.251001, Learning Rate = 2.425264e-06\n",
      "Epoch 15847/20000: Train Loss = 0.435019, Test Loss = 0.252969, Learning Rate = 2.424342e-06\n",
      "Epoch 15848/20000: Train Loss = 0.434830, Test Loss = 0.253078, Learning Rate = 2.423421e-06\n",
      "Epoch 15849/20000: Train Loss = 0.434819, Test Loss = 0.251617, Learning Rate = 2.422500e-06\n",
      "Epoch 15850/20000: Train Loss = 0.435153, Test Loss = 0.253070, Learning Rate = 2.421580e-06\n",
      "Epoch 15851/20000: Train Loss = 0.434844, Test Loss = 0.252295, Learning Rate = 2.420660e-06\n",
      "Epoch 15852/20000: Train Loss = 0.434739, Test Loss = 0.253335, Learning Rate = 2.419740e-06\n",
      "Epoch 15853/20000: Train Loss = 0.434945, Test Loss = 0.254636, Learning Rate = 2.418820e-06\n",
      "Epoch 15854/20000: Train Loss = 0.434811, Test Loss = 0.253116, Learning Rate = 2.417901e-06\n",
      "Epoch 15855/20000: Train Loss = 0.434867, Test Loss = 0.253263, Learning Rate = 2.416983e-06\n",
      "Epoch 15856/20000: Train Loss = 0.434772, Test Loss = 0.253929, Learning Rate = 2.416064e-06\n",
      "Epoch 15857/20000: Train Loss = 0.434915, Test Loss = 0.254615, Learning Rate = 2.415146e-06\n",
      "Epoch 15858/20000: Train Loss = 0.434918, Test Loss = 0.254660, Learning Rate = 2.414228e-06\n",
      "Epoch 15859/20000: Train Loss = 0.435148, Test Loss = 0.251591, Learning Rate = 2.413311e-06\n",
      "Epoch 15860/20000: Train Loss = 0.435073, Test Loss = 0.255272, Learning Rate = 2.412394e-06\n",
      "Epoch 15861/20000: Train Loss = 0.434584, Test Loss = 0.253181, Learning Rate = 2.411477e-06\n",
      "Epoch 15862/20000: Train Loss = 0.434731, Test Loss = 0.252478, Learning Rate = 2.410561e-06\n",
      "Epoch 15863/20000: Train Loss = 0.434884, Test Loss = 0.252336, Learning Rate = 2.409645e-06\n",
      "Epoch 15864/20000: Train Loss = 0.434754, Test Loss = 0.251991, Learning Rate = 2.408730e-06\n",
      "Epoch 15865/20000: Train Loss = 0.434985, Test Loss = 0.252673, Learning Rate = 2.407814e-06\n",
      "Epoch 15866/20000: Train Loss = 0.434840, Test Loss = 0.252294, Learning Rate = 2.406899e-06\n",
      "Epoch 15867/20000: Train Loss = 0.434730, Test Loss = 0.250953, Learning Rate = 2.405985e-06\n",
      "Epoch 15868/20000: Train Loss = 0.434806, Test Loss = 0.251324, Learning Rate = 2.405071e-06\n",
      "Epoch 15869/20000: Train Loss = 0.434776, Test Loss = 0.251407, Learning Rate = 2.404157e-06\n",
      "Epoch 15870/20000: Train Loss = 0.435070, Test Loss = 0.250206, Learning Rate = 2.403243e-06\n",
      "Epoch 15871/20000: Train Loss = 0.435005, Test Loss = 0.251797, Learning Rate = 2.402330e-06\n",
      "Epoch 15872/20000: Train Loss = 0.434959, Test Loss = 0.249075, Learning Rate = 2.401417e-06\n",
      "Epoch 15873/20000: Train Loss = 0.435337, Test Loss = 0.252036, Learning Rate = 2.400505e-06\n",
      "Epoch 15874/20000: Train Loss = 0.434902, Test Loss = 0.251771, Learning Rate = 2.399593e-06\n",
      "Epoch 15875/20000: Train Loss = 0.434871, Test Loss = 0.252512, Learning Rate = 2.398681e-06\n",
      "Epoch 15876/20000: Train Loss = 0.434825, Test Loss = 0.252299, Learning Rate = 2.397770e-06\n",
      "Epoch 15877/20000: Train Loss = 0.435066, Test Loss = 0.251954, Learning Rate = 2.396858e-06\n",
      "Epoch 15878/20000: Train Loss = 0.434866, Test Loss = 0.252175, Learning Rate = 2.395948e-06\n",
      "Epoch 15879/20000: Train Loss = 0.434643, Test Loss = 0.250969, Learning Rate = 2.395037e-06\n",
      "Epoch 15880/20000: Train Loss = 0.434772, Test Loss = 0.254175, Learning Rate = 2.394127e-06\n",
      "Epoch 15881/20000: Train Loss = 0.434715, Test Loss = 0.253012, Learning Rate = 2.393218e-06\n",
      "Epoch 15882/20000: Train Loss = 0.434679, Test Loss = 0.252195, Learning Rate = 2.392308e-06\n",
      "Epoch 15883/20000: Train Loss = 0.434842, Test Loss = 0.253638, Learning Rate = 2.391399e-06\n",
      "Epoch 15884/20000: Train Loss = 0.434772, Test Loss = 0.253184, Learning Rate = 2.390491e-06\n",
      "Epoch 15885/20000: Train Loss = 0.435139, Test Loss = 0.251983, Learning Rate = 2.389582e-06\n",
      "Epoch 15886/20000: Train Loss = 0.434860, Test Loss = 0.252379, Learning Rate = 2.388674e-06\n",
      "Epoch 15887/20000: Train Loss = 0.434965, Test Loss = 0.254965, Learning Rate = 2.387767e-06\n",
      "Epoch 15888/20000: Train Loss = 0.435059, Test Loss = 0.251929, Learning Rate = 2.386859e-06\n",
      "Epoch 15889/20000: Train Loss = 0.434741, Test Loss = 0.252804, Learning Rate = 2.385952e-06\n",
      "Epoch 15890/20000: Train Loss = 0.435086, Test Loss = 0.254534, Learning Rate = 2.385046e-06\n",
      "Epoch 15891/20000: Train Loss = 0.435009, Test Loss = 0.252708, Learning Rate = 2.384140e-06\n",
      "Epoch 15892/20000: Train Loss = 0.434791, Test Loss = 0.252862, Learning Rate = 2.383234e-06\n",
      "Epoch 15893/20000: Train Loss = 0.434736, Test Loss = 0.253586, Learning Rate = 2.382328e-06\n",
      "Epoch 15894/20000: Train Loss = 0.434657, Test Loss = 0.251928, Learning Rate = 2.381423e-06\n",
      "Epoch 15895/20000: Train Loss = 0.435006, Test Loss = 0.253110, Learning Rate = 2.380518e-06\n",
      "Epoch 15896/20000: Train Loss = 0.434917, Test Loss = 0.253816, Learning Rate = 2.379613e-06\n",
      "Epoch 15897/20000: Train Loss = 0.435114, Test Loss = 0.254357, Learning Rate = 2.378709e-06\n",
      "Epoch 15898/20000: Train Loss = 0.434876, Test Loss = 0.254045, Learning Rate = 2.377805e-06\n",
      "Epoch 15899/20000: Train Loss = 0.434711, Test Loss = 0.252642, Learning Rate = 2.376902e-06\n",
      "Epoch 15900/20000: Train Loss = 0.434637, Test Loss = 0.253262, Learning Rate = 2.375999e-06\n",
      "Epoch 15901/20000: Train Loss = 0.434655, Test Loss = 0.253229, Learning Rate = 2.375096e-06\n",
      "Epoch 15902/20000: Train Loss = 0.434788, Test Loss = 0.253512, Learning Rate = 2.374193e-06\n",
      "Epoch 15903/20000: Train Loss = 0.434688, Test Loss = 0.252967, Learning Rate = 2.373291e-06\n",
      "Epoch 15904/20000: Train Loss = 0.434706, Test Loss = 0.254520, Learning Rate = 2.372390e-06\n",
      "Epoch 15905/20000: Train Loss = 0.434781, Test Loss = 0.252410, Learning Rate = 2.371488e-06\n",
      "Epoch 15906/20000: Train Loss = 0.434753, Test Loss = 0.252907, Learning Rate = 2.370587e-06\n",
      "Epoch 15907/20000: Train Loss = 0.434930, Test Loss = 0.253803, Learning Rate = 2.369686e-06\n",
      "Epoch 15908/20000: Train Loss = 0.434658, Test Loss = 0.252668, Learning Rate = 2.368786e-06\n",
      "Epoch 15909/20000: Train Loss = 0.434777, Test Loss = 0.252541, Learning Rate = 2.367886e-06\n",
      "Epoch 15910/20000: Train Loss = 0.434687, Test Loss = 0.254376, Learning Rate = 2.366986e-06\n",
      "Epoch 15911/20000: Train Loss = 0.435205, Test Loss = 0.252389, Learning Rate = 2.366087e-06\n",
      "Epoch 15912/20000: Train Loss = 0.434923, Test Loss = 0.251945, Learning Rate = 2.365188e-06\n",
      "Epoch 15913/20000: Train Loss = 0.434613, Test Loss = 0.253396, Learning Rate = 2.364289e-06\n",
      "Epoch 15914/20000: Train Loss = 0.435010, Test Loss = 0.253423, Learning Rate = 2.363390e-06\n",
      "Epoch 15915/20000: Train Loss = 0.434763, Test Loss = 0.253632, Learning Rate = 2.362492e-06\n",
      "Epoch 15916/20000: Train Loss = 0.434888, Test Loss = 0.253512, Learning Rate = 2.361595e-06\n",
      "Epoch 15917/20000: Train Loss = 0.434725, Test Loss = 0.254091, Learning Rate = 2.360697e-06\n",
      "Epoch 15918/20000: Train Loss = 0.434853, Test Loss = 0.251773, Learning Rate = 2.359800e-06\n",
      "Epoch 15919/20000: Train Loss = 0.434894, Test Loss = 0.252882, Learning Rate = 2.358904e-06\n",
      "Epoch 15920/20000: Train Loss = 0.434909, Test Loss = 0.251980, Learning Rate = 2.358007e-06\n",
      "Epoch 15921/20000: Train Loss = 0.434805, Test Loss = 0.252436, Learning Rate = 2.357111e-06\n",
      "Epoch 15922/20000: Train Loss = 0.434631, Test Loss = 0.251275, Learning Rate = 2.356216e-06\n",
      "Epoch 15923/20000: Train Loss = 0.435167, Test Loss = 0.252148, Learning Rate = 2.355321e-06\n",
      "Epoch 15924/20000: Train Loss = 0.435268, Test Loss = 0.250258, Learning Rate = 2.354426e-06\n",
      "Epoch 15925/20000: Train Loss = 0.434933, Test Loss = 0.251294, Learning Rate = 2.353531e-06\n",
      "Epoch 15926/20000: Train Loss = 0.434942, Test Loss = 0.252676, Learning Rate = 2.352637e-06\n",
      "Epoch 15927/20000: Train Loss = 0.434776, Test Loss = 0.250794, Learning Rate = 2.351743e-06\n",
      "Epoch 15928/20000: Train Loss = 0.434855, Test Loss = 0.250357, Learning Rate = 2.350849e-06\n",
      "Epoch 15929/20000: Train Loss = 0.435235, Test Loss = 0.254535, Learning Rate = 2.349956e-06\n",
      "Epoch 15930/20000: Train Loss = 0.434516, Test Loss = 0.252122, Learning Rate = 2.349063e-06\n",
      "Epoch 15931/20000: Train Loss = 0.434732, Test Loss = 0.252066, Learning Rate = 2.348170e-06\n",
      "Epoch 15932/20000: Train Loss = 0.434892, Test Loss = 0.252647, Learning Rate = 2.347278e-06\n",
      "Epoch 15933/20000: Train Loss = 0.434589, Test Loss = 0.251862, Learning Rate = 2.346386e-06\n",
      "Epoch 15934/20000: Train Loss = 0.434913, Test Loss = 0.251805, Learning Rate = 2.345495e-06\n",
      "Epoch 15935/20000: Train Loss = 0.434921, Test Loss = 0.252837, Learning Rate = 2.344603e-06\n",
      "Epoch 15936/20000: Train Loss = 0.434699, Test Loss = 0.253592, Learning Rate = 2.343713e-06\n",
      "Epoch 15937/20000: Train Loss = 0.435226, Test Loss = 0.253073, Learning Rate = 2.342822e-06\n",
      "Epoch 15938/20000: Train Loss = 0.434788, Test Loss = 0.251593, Learning Rate = 2.341932e-06\n",
      "Epoch 15939/20000: Train Loss = 0.434786, Test Loss = 0.253829, Learning Rate = 2.341042e-06\n",
      "Epoch 15940/20000: Train Loss = 0.434836, Test Loss = 0.252002, Learning Rate = 2.340152e-06\n",
      "Epoch 15941/20000: Train Loss = 0.434793, Test Loss = 0.254445, Learning Rate = 2.339263e-06\n",
      "Epoch 15942/20000: Train Loss = 0.434715, Test Loss = 0.253845, Learning Rate = 2.338374e-06\n",
      "Epoch 15943/20000: Train Loss = 0.434628, Test Loss = 0.254636, Learning Rate = 2.337486e-06\n",
      "Epoch 15944/20000: Train Loss = 0.434695, Test Loss = 0.253488, Learning Rate = 2.336598e-06\n",
      "Epoch 15945/20000: Train Loss = 0.435065, Test Loss = 0.253260, Learning Rate = 2.335710e-06\n",
      "Epoch 15946/20000: Train Loss = 0.434782, Test Loss = 0.254028, Learning Rate = 2.334822e-06\n",
      "Epoch 15947/20000: Train Loss = 0.435172, Test Loss = 0.252203, Learning Rate = 2.333935e-06\n",
      "Epoch 15948/20000: Train Loss = 0.434722, Test Loss = 0.252665, Learning Rate = 2.333048e-06\n",
      "Epoch 15949/20000: Train Loss = 0.434814, Test Loss = 0.252605, Learning Rate = 2.332162e-06\n",
      "Epoch 15950/20000: Train Loss = 0.434698, Test Loss = 0.253897, Learning Rate = 2.331276e-06\n",
      "Epoch 15951/20000: Train Loss = 0.434935, Test Loss = 0.252605, Learning Rate = 2.330390e-06\n",
      "Epoch 15952/20000: Train Loss = 0.434859, Test Loss = 0.253122, Learning Rate = 2.329504e-06\n",
      "Epoch 15953/20000: Train Loss = 0.434756, Test Loss = 0.252392, Learning Rate = 2.328619e-06\n",
      "Epoch 15954/20000: Train Loss = 0.434692, Test Loss = 0.252878, Learning Rate = 2.327734e-06\n",
      "Epoch 15955/20000: Train Loss = 0.435012, Test Loss = 0.253140, Learning Rate = 2.326850e-06\n",
      "Epoch 15956/20000: Train Loss = 0.434929, Test Loss = 0.252083, Learning Rate = 2.325966e-06\n",
      "Epoch 15957/20000: Train Loss = 0.434808, Test Loss = 0.251406, Learning Rate = 2.325082e-06\n",
      "Epoch 15958/20000: Train Loss = 0.435049, Test Loss = 0.251658, Learning Rate = 2.324199e-06\n",
      "Epoch 15959/20000: Train Loss = 0.434852, Test Loss = 0.251938, Learning Rate = 2.323315e-06\n",
      "Epoch 15960/20000: Train Loss = 0.434731, Test Loss = 0.253488, Learning Rate = 2.322433e-06\n",
      "Epoch 15961/20000: Train Loss = 0.434689, Test Loss = 0.253416, Learning Rate = 2.321550e-06\n",
      "Epoch 15962/20000: Train Loss = 0.434823, Test Loss = 0.254808, Learning Rate = 2.320668e-06\n",
      "Epoch 15963/20000: Train Loss = 0.434858, Test Loss = 0.253135, Learning Rate = 2.319786e-06\n",
      "Epoch 15964/20000: Train Loss = 0.434832, Test Loss = 0.252548, Learning Rate = 2.318905e-06\n",
      "Epoch 15965/20000: Train Loss = 0.435070, Test Loss = 0.253027, Learning Rate = 2.318024e-06\n",
      "Epoch 15966/20000: Train Loss = 0.434750, Test Loss = 0.253941, Learning Rate = 2.317143e-06\n",
      "Epoch 15967/20000: Train Loss = 0.434873, Test Loss = 0.254210, Learning Rate = 2.316262e-06\n",
      "Epoch 15968/20000: Train Loss = 0.434903, Test Loss = 0.253512, Learning Rate = 2.315382e-06\n",
      "Epoch 15969/20000: Train Loss = 0.434858, Test Loss = 0.251780, Learning Rate = 2.314502e-06\n",
      "Epoch 15970/20000: Train Loss = 0.434873, Test Loss = 0.252454, Learning Rate = 2.313623e-06\n",
      "Epoch 15971/20000: Train Loss = 0.434767, Test Loss = 0.251200, Learning Rate = 2.312744e-06\n",
      "Epoch 15972/20000: Train Loss = 0.434825, Test Loss = 0.253266, Learning Rate = 2.311865e-06\n",
      "Epoch 15973/20000: Train Loss = 0.434786, Test Loss = 0.252015, Learning Rate = 2.310987e-06\n",
      "Epoch 15974/20000: Train Loss = 0.434644, Test Loss = 0.251638, Learning Rate = 2.310109e-06\n",
      "Epoch 15975/20000: Train Loss = 0.435015, Test Loss = 0.251816, Learning Rate = 2.309231e-06\n",
      "Epoch 15976/20000: Train Loss = 0.434813, Test Loss = 0.250514, Learning Rate = 2.308353e-06\n",
      "Epoch 15977/20000: Train Loss = 0.434864, Test Loss = 0.252307, Learning Rate = 2.307476e-06\n",
      "Epoch 15978/20000: Train Loss = 0.435198, Test Loss = 0.250381, Learning Rate = 2.306599e-06\n",
      "Epoch 15979/20000: Train Loss = 0.434711, Test Loss = 0.251766, Learning Rate = 2.305723e-06\n",
      "Epoch 15980/20000: Train Loss = 0.434919, Test Loss = 0.250320, Learning Rate = 2.304847e-06\n",
      "Epoch 15981/20000: Train Loss = 0.435771, Test Loss = 0.253548, Learning Rate = 2.303971e-06\n",
      "Epoch 15982/20000: Train Loss = 0.434710, Test Loss = 0.251913, Learning Rate = 2.303096e-06\n",
      "Epoch 15983/20000: Train Loss = 0.435141, Test Loss = 0.253568, Learning Rate = 2.302221e-06\n",
      "Epoch 15984/20000: Train Loss = 0.434692, Test Loss = 0.252279, Learning Rate = 2.301346e-06\n",
      "Epoch 15985/20000: Train Loss = 0.434969, Test Loss = 0.252868, Learning Rate = 2.300471e-06\n",
      "Epoch 15986/20000: Train Loss = 0.434753, Test Loss = 0.252448, Learning Rate = 2.299597e-06\n",
      "Epoch 15987/20000: Train Loss = 0.434799, Test Loss = 0.252849, Learning Rate = 2.298723e-06\n",
      "Epoch 15988/20000: Train Loss = 0.434745, Test Loss = 0.251581, Learning Rate = 2.297850e-06\n",
      "Epoch 15989/20000: Train Loss = 0.435005, Test Loss = 0.251100, Learning Rate = 2.296977e-06\n",
      "Epoch 15990/20000: Train Loss = 0.435035, Test Loss = 0.252564, Learning Rate = 2.296104e-06\n",
      "Epoch 15991/20000: Train Loss = 0.434676, Test Loss = 0.252466, Learning Rate = 2.295232e-06\n",
      "Epoch 15992/20000: Train Loss = 0.434641, Test Loss = 0.250738, Learning Rate = 2.294359e-06\n",
      "Epoch 15993/20000: Train Loss = 0.434867, Test Loss = 0.250892, Learning Rate = 2.293488e-06\n",
      "Epoch 15994/20000: Train Loss = 0.434806, Test Loss = 0.252923, Learning Rate = 2.292616e-06\n",
      "Epoch 15995/20000: Train Loss = 0.434658, Test Loss = 0.252503, Learning Rate = 2.291745e-06\n",
      "Epoch 15996/20000: Train Loss = 0.434880, Test Loss = 0.251468, Learning Rate = 2.290874e-06\n",
      "Epoch 15997/20000: Train Loss = 0.434660, Test Loss = 0.251578, Learning Rate = 2.290004e-06\n",
      "Epoch 15998/20000: Train Loss = 0.434705, Test Loss = 0.252656, Learning Rate = 2.289134e-06\n",
      "Epoch 15999/20000: Train Loss = 0.434725, Test Loss = 0.251701, Learning Rate = 2.288264e-06\n",
      "Epoch 16000/20000: Train Loss = 0.434642, Test Loss = 0.252115, Learning Rate = 2.287394e-06\n",
      "Epoch 16001/20000: Train Loss = 0.435030, Test Loss = 0.251948, Learning Rate = 2.286525e-06\n",
      "Epoch 16002/20000: Train Loss = 0.434873, Test Loss = 0.250502, Learning Rate = 2.285656e-06\n",
      "Epoch 16003/20000: Train Loss = 0.434745, Test Loss = 0.251525, Learning Rate = 2.284788e-06\n",
      "Epoch 16004/20000: Train Loss = 0.435342, Test Loss = 0.250993, Learning Rate = 2.283920e-06\n",
      "Epoch 16005/20000: Train Loss = 0.434934, Test Loss = 0.250964, Learning Rate = 2.283052e-06\n",
      "Epoch 16006/20000: Train Loss = 0.434737, Test Loss = 0.250679, Learning Rate = 2.282184e-06\n",
      "Epoch 16007/20000: Train Loss = 0.435699, Test Loss = 0.252727, Learning Rate = 2.281317e-06\n",
      "Epoch 16008/20000: Train Loss = 0.434332, Test Loss = 0.251381, Learning Rate = 2.280450e-06\n",
      "Epoch 16009/20000: Train Loss = 0.434953, Test Loss = 0.251856, Learning Rate = 2.279584e-06\n",
      "Epoch 16010/20000: Train Loss = 0.434786, Test Loss = 0.253608, Learning Rate = 2.278718e-06\n",
      "Epoch 16011/20000: Train Loss = 0.434914, Test Loss = 0.254094, Learning Rate = 2.277852e-06\n",
      "Epoch 16012/20000: Train Loss = 0.434616, Test Loss = 0.255374, Learning Rate = 2.276986e-06\n",
      "Epoch 16013/20000: Train Loss = 0.434710, Test Loss = 0.253867, Learning Rate = 2.276121e-06\n",
      "Epoch 16014/20000: Train Loss = 0.435128, Test Loss = 0.254364, Learning Rate = 2.275256e-06\n",
      "Epoch 16015/20000: Train Loss = 0.434701, Test Loss = 0.256277, Learning Rate = 2.274392e-06\n",
      "Epoch 16016/20000: Train Loss = 0.434646, Test Loss = 0.255908, Learning Rate = 2.273528e-06\n",
      "Epoch 16017/20000: Train Loss = 0.434645, Test Loss = 0.253824, Learning Rate = 2.272664e-06\n",
      "Epoch 16018/20000: Train Loss = 0.434811, Test Loss = 0.253676, Learning Rate = 2.271800e-06\n",
      "Epoch 16019/20000: Train Loss = 0.435379, Test Loss = 0.255242, Learning Rate = 2.270937e-06\n",
      "Epoch 16020/20000: Train Loss = 0.434860, Test Loss = 0.254816, Learning Rate = 2.270074e-06\n",
      "Epoch 16021/20000: Train Loss = 0.435092, Test Loss = 0.253813, Learning Rate = 2.269211e-06\n",
      "Epoch 16022/20000: Train Loss = 0.434856, Test Loss = 0.253360, Learning Rate = 2.268349e-06\n",
      "Epoch 16023/20000: Train Loss = 0.435168, Test Loss = 0.251929, Learning Rate = 2.267487e-06\n",
      "Epoch 16024/20000: Train Loss = 0.434647, Test Loss = 0.252757, Learning Rate = 2.266626e-06\n",
      "Epoch 16025/20000: Train Loss = 0.434602, Test Loss = 0.254347, Learning Rate = 2.265764e-06\n",
      "Epoch 16026/20000: Train Loss = 0.434701, Test Loss = 0.253345, Learning Rate = 2.264904e-06\n",
      "Epoch 16027/20000: Train Loss = 0.434629, Test Loss = 0.253052, Learning Rate = 2.264043e-06\n",
      "Epoch 16028/20000: Train Loss = 0.434734, Test Loss = 0.253341, Learning Rate = 2.263183e-06\n",
      "Epoch 16029/20000: Train Loss = 0.434807, Test Loss = 0.252925, Learning Rate = 2.262323e-06\n",
      "Epoch 16030/20000: Train Loss = 0.435136, Test Loss = 0.254046, Learning Rate = 2.261463e-06\n",
      "Epoch 16031/20000: Train Loss = 0.434857, Test Loss = 0.253414, Learning Rate = 2.260604e-06\n",
      "Epoch 16032/20000: Train Loss = 0.434822, Test Loss = 0.253675, Learning Rate = 2.259745e-06\n",
      "Epoch 16033/20000: Train Loss = 0.434694, Test Loss = 0.253623, Learning Rate = 2.258886e-06\n",
      "Epoch 16034/20000: Train Loss = 0.434748, Test Loss = 0.255236, Learning Rate = 2.258028e-06\n",
      "Epoch 16035/20000: Train Loss = 0.434813, Test Loss = 0.254633, Learning Rate = 2.257170e-06\n",
      "Epoch 16036/20000: Train Loss = 0.434532, Test Loss = 0.252572, Learning Rate = 2.256312e-06\n",
      "Epoch 16037/20000: Train Loss = 0.434866, Test Loss = 0.251916, Learning Rate = 2.255455e-06\n",
      "Epoch 16038/20000: Train Loss = 0.434909, Test Loss = 0.254351, Learning Rate = 2.254598e-06\n",
      "Epoch 16039/20000: Train Loss = 0.434850, Test Loss = 0.253303, Learning Rate = 2.253741e-06\n",
      "Epoch 16040/20000: Train Loss = 0.435230, Test Loss = 0.253014, Learning Rate = 2.252885e-06\n",
      "Epoch 16041/20000: Train Loss = 0.434876, Test Loss = 0.254356, Learning Rate = 2.252029e-06\n",
      "Epoch 16042/20000: Train Loss = 0.434666, Test Loss = 0.254013, Learning Rate = 2.251173e-06\n",
      "Epoch 16043/20000: Train Loss = 0.434985, Test Loss = 0.252878, Learning Rate = 2.250318e-06\n",
      "Epoch 16044/20000: Train Loss = 0.434622, Test Loss = 0.253019, Learning Rate = 2.249463e-06\n",
      "Epoch 16045/20000: Train Loss = 0.434846, Test Loss = 0.253853, Learning Rate = 2.248608e-06\n",
      "Epoch 16046/20000: Train Loss = 0.434788, Test Loss = 0.252881, Learning Rate = 2.247754e-06\n",
      "Epoch 16047/20000: Train Loss = 0.434612, Test Loss = 0.255085, Learning Rate = 2.246899e-06\n",
      "Epoch 16048/20000: Train Loss = 0.434682, Test Loss = 0.255562, Learning Rate = 2.246046e-06\n",
      "Epoch 16049/20000: Train Loss = 0.435336, Test Loss = 0.254521, Learning Rate = 2.245192e-06\n",
      "Epoch 16050/20000: Train Loss = 0.434682, Test Loss = 0.254651, Learning Rate = 2.244339e-06\n",
      "Epoch 16051/20000: Train Loss = 0.434753, Test Loss = 0.254279, Learning Rate = 2.243486e-06\n",
      "Epoch 16052/20000: Train Loss = 0.434676, Test Loss = 0.253454, Learning Rate = 2.242634e-06\n",
      "Epoch 16053/20000: Train Loss = 0.434768, Test Loss = 0.253367, Learning Rate = 2.241782e-06\n",
      "Epoch 16054/20000: Train Loss = 0.434992, Test Loss = 0.252065, Learning Rate = 2.240930e-06\n",
      "Epoch 16055/20000: Train Loss = 0.434916, Test Loss = 0.251689, Learning Rate = 2.240078e-06\n",
      "Epoch 16056/20000: Train Loss = 0.434876, Test Loss = 0.253235, Learning Rate = 2.239227e-06\n",
      "Epoch 16057/20000: Train Loss = 0.434974, Test Loss = 0.255177, Learning Rate = 2.238376e-06\n",
      "Epoch 16058/20000: Train Loss = 0.435662, Test Loss = 0.253606, Learning Rate = 2.237526e-06\n",
      "Epoch 16059/20000: Train Loss = 0.436015, Test Loss = 0.255876, Learning Rate = 2.236676e-06\n",
      "Epoch 16060/20000: Train Loss = 0.434861, Test Loss = 0.253765, Learning Rate = 2.235826e-06\n",
      "Epoch 16061/20000: Train Loss = 0.435117, Test Loss = 0.253256, Learning Rate = 2.234976e-06\n",
      "Epoch 16062/20000: Train Loss = 0.434887, Test Loss = 0.253005, Learning Rate = 2.234127e-06\n",
      "Epoch 16063/20000: Train Loss = 0.434933, Test Loss = 0.253937, Learning Rate = 2.233278e-06\n",
      "Epoch 16064/20000: Train Loss = 0.434675, Test Loss = 0.252938, Learning Rate = 2.232430e-06\n",
      "Epoch 16065/20000: Train Loss = 0.434697, Test Loss = 0.254135, Learning Rate = 2.231581e-06\n",
      "Epoch 16066/20000: Train Loss = 0.434545, Test Loss = 0.254024, Learning Rate = 2.230733e-06\n",
      "Epoch 16067/20000: Train Loss = 0.434654, Test Loss = 0.254248, Learning Rate = 2.229886e-06\n",
      "Epoch 16068/20000: Train Loss = 0.434867, Test Loss = 0.253672, Learning Rate = 2.229038e-06\n",
      "Epoch 16069/20000: Train Loss = 0.434876, Test Loss = 0.253871, Learning Rate = 2.228191e-06\n",
      "Epoch 16070/20000: Train Loss = 0.434953, Test Loss = 0.253701, Learning Rate = 2.227345e-06\n",
      "Epoch 16071/20000: Train Loss = 0.434764, Test Loss = 0.254816, Learning Rate = 2.226498e-06\n",
      "Epoch 16072/20000: Train Loss = 0.434737, Test Loss = 0.254457, Learning Rate = 2.225652e-06\n",
      "Epoch 16073/20000: Train Loss = 0.435143, Test Loss = 0.254475, Learning Rate = 2.224807e-06\n",
      "Epoch 16074/20000: Train Loss = 0.434701, Test Loss = 0.253516, Learning Rate = 2.223961e-06\n",
      "Epoch 16075/20000: Train Loss = 0.434821, Test Loss = 0.254450, Learning Rate = 2.223116e-06\n",
      "Epoch 16076/20000: Train Loss = 0.434587, Test Loss = 0.253933, Learning Rate = 2.222272e-06\n",
      "Epoch 16077/20000: Train Loss = 0.434860, Test Loss = 0.253942, Learning Rate = 2.221427e-06\n",
      "Epoch 16078/20000: Train Loss = 0.435153, Test Loss = 0.253287, Learning Rate = 2.220583e-06\n",
      "Epoch 16079/20000: Train Loss = 0.434633, Test Loss = 0.254071, Learning Rate = 2.219739e-06\n",
      "Epoch 16080/20000: Train Loss = 0.434635, Test Loss = 0.252777, Learning Rate = 2.218896e-06\n",
      "Epoch 16081/20000: Train Loss = 0.434816, Test Loss = 0.252337, Learning Rate = 2.218053e-06\n",
      "Epoch 16082/20000: Train Loss = 0.434579, Test Loss = 0.252445, Learning Rate = 2.217210e-06\n",
      "Epoch 16083/20000: Train Loss = 0.434993, Test Loss = 0.253093, Learning Rate = 2.216368e-06\n",
      "Epoch 16084/20000: Train Loss = 0.434931, Test Loss = 0.251622, Learning Rate = 2.215525e-06\n",
      "Epoch 16085/20000: Train Loss = 0.434923, Test Loss = 0.252540, Learning Rate = 2.214684e-06\n",
      "Epoch 16086/20000: Train Loss = 0.434735, Test Loss = 0.252980, Learning Rate = 2.213842e-06\n",
      "Epoch 16087/20000: Train Loss = 0.434779, Test Loss = 0.252177, Learning Rate = 2.213001e-06\n",
      "Epoch 16088/20000: Train Loss = 0.434755, Test Loss = 0.252896, Learning Rate = 2.212160e-06\n",
      "Epoch 16089/20000: Train Loss = 0.434749, Test Loss = 0.250896, Learning Rate = 2.211319e-06\n",
      "Epoch 16090/20000: Train Loss = 0.435509, Test Loss = 0.251814, Learning Rate = 2.210479e-06\n",
      "Epoch 16091/20000: Train Loss = 0.434894, Test Loss = 0.252058, Learning Rate = 2.209639e-06\n",
      "Epoch 16092/20000: Train Loss = 0.434841, Test Loss = 0.250711, Learning Rate = 2.208800e-06\n",
      "Epoch 16093/20000: Train Loss = 0.434870, Test Loss = 0.251221, Learning Rate = 2.207960e-06\n",
      "Epoch 16094/20000: Train Loss = 0.434776, Test Loss = 0.252548, Learning Rate = 2.207121e-06\n",
      "Epoch 16095/20000: Train Loss = 0.434705, Test Loss = 0.254077, Learning Rate = 2.206283e-06\n",
      "Epoch 16096/20000: Train Loss = 0.434678, Test Loss = 0.253725, Learning Rate = 2.205444e-06\n",
      "Epoch 16097/20000: Train Loss = 0.434709, Test Loss = 0.251639, Learning Rate = 2.204606e-06\n",
      "Epoch 16098/20000: Train Loss = 0.434750, Test Loss = 0.252421, Learning Rate = 2.203769e-06\n",
      "Epoch 16099/20000: Train Loss = 0.434693, Test Loss = 0.252353, Learning Rate = 2.202931e-06\n",
      "Epoch 16100/20000: Train Loss = 0.434634, Test Loss = 0.250346, Learning Rate = 2.202094e-06\n",
      "Epoch 16101/20000: Train Loss = 0.434707, Test Loss = 0.249751, Learning Rate = 2.201258e-06\n",
      "Epoch 16102/20000: Train Loss = 0.434617, Test Loss = 0.249981, Learning Rate = 2.200421e-06\n",
      "Epoch 16103/20000: Train Loss = 0.434817, Test Loss = 0.250610, Learning Rate = 2.199585e-06\n",
      "Epoch 16104/20000: Train Loss = 0.434526, Test Loss = 0.249994, Learning Rate = 2.198749e-06\n",
      "Epoch 16105/20000: Train Loss = 0.435028, Test Loss = 0.249746, Learning Rate = 2.197914e-06\n",
      "Epoch 16106/20000: Train Loss = 0.434938, Test Loss = 0.249492, Learning Rate = 2.197079e-06\n",
      "Epoch 16107/20000: Train Loss = 0.434713, Test Loss = 0.249102, Learning Rate = 2.196244e-06\n",
      "Epoch 16108/20000: Train Loss = 0.434975, Test Loss = 0.248238, Learning Rate = 2.195409e-06\n",
      "Epoch 16109/20000: Train Loss = 0.434806, Test Loss = 0.249407, Learning Rate = 2.194575e-06\n",
      "Epoch 16110/20000: Train Loss = 0.434743, Test Loss = 0.248643, Learning Rate = 2.193741e-06\n",
      "Epoch 16111/20000: Train Loss = 0.435066, Test Loss = 0.251901, Learning Rate = 2.192908e-06\n",
      "Epoch 16112/20000: Train Loss = 0.434817, Test Loss = 0.251376, Learning Rate = 2.192074e-06\n",
      "Epoch 16113/20000: Train Loss = 0.434690, Test Loss = 0.251456, Learning Rate = 2.191241e-06\n",
      "Epoch 16114/20000: Train Loss = 0.434607, Test Loss = 0.250783, Learning Rate = 2.190409e-06\n",
      "Epoch 16115/20000: Train Loss = 0.434990, Test Loss = 0.251436, Learning Rate = 2.189577e-06\n",
      "Epoch 16116/20000: Train Loss = 0.434960, Test Loss = 0.251564, Learning Rate = 2.188745e-06\n",
      "Epoch 16117/20000: Train Loss = 0.435108, Test Loss = 0.252344, Learning Rate = 2.187913e-06\n",
      "Epoch 16118/20000: Train Loss = 0.434755, Test Loss = 0.252384, Learning Rate = 2.187082e-06\n",
      "Epoch 16119/20000: Train Loss = 0.434941, Test Loss = 0.252175, Learning Rate = 2.186251e-06\n",
      "Epoch 16120/20000: Train Loss = 0.434590, Test Loss = 0.250800, Learning Rate = 2.185420e-06\n",
      "Epoch 16121/20000: Train Loss = 0.434757, Test Loss = 0.251459, Learning Rate = 2.184589e-06\n",
      "Epoch 16122/20000: Train Loss = 0.434714, Test Loss = 0.250684, Learning Rate = 2.183759e-06\n",
      "Epoch 16123/20000: Train Loss = 0.434705, Test Loss = 0.252767, Learning Rate = 2.182930e-06\n",
      "Epoch 16124/20000: Train Loss = 0.434587, Test Loss = 0.251800, Learning Rate = 2.182100e-06\n",
      "Epoch 16125/20000: Train Loss = 0.434592, Test Loss = 0.251506, Learning Rate = 2.181271e-06\n",
      "Epoch 16126/20000: Train Loss = 0.434632, Test Loss = 0.252548, Learning Rate = 2.180442e-06\n",
      "Epoch 16127/20000: Train Loss = 0.435117, Test Loss = 0.251461, Learning Rate = 2.179614e-06\n",
      "Epoch 16128/20000: Train Loss = 0.434775, Test Loss = 0.252720, Learning Rate = 2.178785e-06\n",
      "Epoch 16129/20000: Train Loss = 0.434670, Test Loss = 0.252165, Learning Rate = 2.177958e-06\n",
      "Epoch 16130/20000: Train Loss = 0.434573, Test Loss = 0.252779, Learning Rate = 2.177130e-06\n",
      "Epoch 16131/20000: Train Loss = 0.434769, Test Loss = 0.252857, Learning Rate = 2.176303e-06\n",
      "Epoch 16132/20000: Train Loss = 0.434727, Test Loss = 0.252892, Learning Rate = 2.175476e-06\n",
      "Epoch 16133/20000: Train Loss = 0.434789, Test Loss = 0.253754, Learning Rate = 2.174649e-06\n",
      "Epoch 16134/20000: Train Loss = 0.434698, Test Loss = 0.252657, Learning Rate = 2.173823e-06\n",
      "Epoch 16135/20000: Train Loss = 0.434940, Test Loss = 0.252007, Learning Rate = 2.172997e-06\n",
      "Epoch 16136/20000: Train Loss = 0.434585, Test Loss = 0.252040, Learning Rate = 2.172171e-06\n",
      "Epoch 16137/20000: Train Loss = 0.434760, Test Loss = 0.251683, Learning Rate = 2.171346e-06\n",
      "Epoch 16138/20000: Train Loss = 0.434853, Test Loss = 0.252085, Learning Rate = 2.170521e-06\n",
      "Epoch 16139/20000: Train Loss = 0.434977, Test Loss = 0.252199, Learning Rate = 2.169696e-06\n",
      "Epoch 16140/20000: Train Loss = 0.434764, Test Loss = 0.252951, Learning Rate = 2.168872e-06\n",
      "Epoch 16141/20000: Train Loss = 0.434664, Test Loss = 0.253946, Learning Rate = 2.168048e-06\n",
      "Epoch 16142/20000: Train Loss = 0.434522, Test Loss = 0.253809, Learning Rate = 2.167224e-06\n",
      "Epoch 16143/20000: Train Loss = 0.434639, Test Loss = 0.252597, Learning Rate = 2.166400e-06\n",
      "Epoch 16144/20000: Train Loss = 0.434695, Test Loss = 0.252795, Learning Rate = 2.165577e-06\n",
      "Epoch 16145/20000: Train Loss = 0.434787, Test Loss = 0.250721, Learning Rate = 2.164754e-06\n",
      "Epoch 16146/20000: Train Loss = 0.434680, Test Loss = 0.252131, Learning Rate = 2.163932e-06\n",
      "Epoch 16147/20000: Train Loss = 0.434712, Test Loss = 0.252204, Learning Rate = 2.163109e-06\n",
      "Epoch 16148/20000: Train Loss = 0.434638, Test Loss = 0.254424, Learning Rate = 2.162287e-06\n",
      "Epoch 16149/20000: Train Loss = 0.435390, Test Loss = 0.251513, Learning Rate = 2.161466e-06\n",
      "Epoch 16150/20000: Train Loss = 0.434985, Test Loss = 0.252102, Learning Rate = 2.160645e-06\n",
      "Epoch 16151/20000: Train Loss = 0.434748, Test Loss = 0.250838, Learning Rate = 2.159824e-06\n",
      "Epoch 16152/20000: Train Loss = 0.434749, Test Loss = 0.249861, Learning Rate = 2.159003e-06\n",
      "Epoch 16153/20000: Train Loss = 0.434657, Test Loss = 0.250076, Learning Rate = 2.158183e-06\n",
      "Epoch 16154/20000: Train Loss = 0.434673, Test Loss = 0.251425, Learning Rate = 2.157362e-06\n",
      "Epoch 16155/20000: Train Loss = 0.434724, Test Loss = 0.249708, Learning Rate = 2.156543e-06\n",
      "Epoch 16156/20000: Train Loss = 0.434692, Test Loss = 0.250614, Learning Rate = 2.155723e-06\n",
      "Epoch 16157/20000: Train Loss = 0.434634, Test Loss = 0.251407, Learning Rate = 2.154904e-06\n",
      "Epoch 16158/20000: Train Loss = 0.434755, Test Loss = 0.250630, Learning Rate = 2.154085e-06\n",
      "Epoch 16159/20000: Train Loss = 0.434808, Test Loss = 0.250911, Learning Rate = 2.153267e-06\n",
      "Epoch 16160/20000: Train Loss = 0.434847, Test Loss = 0.252485, Learning Rate = 2.152449e-06\n",
      "Epoch 16161/20000: Train Loss = 0.434687, Test Loss = 0.252265, Learning Rate = 2.151631e-06\n",
      "Epoch 16162/20000: Train Loss = 0.434704, Test Loss = 0.253397, Learning Rate = 2.150813e-06\n",
      "Epoch 16163/20000: Train Loss = 0.434922, Test Loss = 0.253704, Learning Rate = 2.149996e-06\n",
      "Epoch 16164/20000: Train Loss = 0.434874, Test Loss = 0.254126, Learning Rate = 2.149179e-06\n",
      "Epoch 16165/20000: Train Loss = 0.434546, Test Loss = 0.253475, Learning Rate = 2.148362e-06\n",
      "Epoch 16166/20000: Train Loss = 0.434897, Test Loss = 0.252426, Learning Rate = 2.147546e-06\n",
      "Epoch 16167/20000: Train Loss = 0.434588, Test Loss = 0.252948, Learning Rate = 2.146730e-06\n",
      "Epoch 16168/20000: Train Loss = 0.434657, Test Loss = 0.253800, Learning Rate = 2.145914e-06\n",
      "Epoch 16169/20000: Train Loss = 0.434701, Test Loss = 0.252710, Learning Rate = 2.145099e-06\n",
      "Epoch 16170/20000: Train Loss = 0.434963, Test Loss = 0.252797, Learning Rate = 2.144284e-06\n",
      "Epoch 16171/20000: Train Loss = 0.434715, Test Loss = 0.252896, Learning Rate = 2.143469e-06\n",
      "Epoch 16172/20000: Train Loss = 0.434725, Test Loss = 0.252399, Learning Rate = 2.142655e-06\n",
      "Epoch 16173/20000: Train Loss = 0.434756, Test Loss = 0.252465, Learning Rate = 2.141841e-06\n",
      "Epoch 16174/20000: Train Loss = 0.434715, Test Loss = 0.251250, Learning Rate = 2.141027e-06\n",
      "Epoch 16175/20000: Train Loss = 0.434703, Test Loss = 0.250301, Learning Rate = 2.140213e-06\n",
      "Epoch 16176/20000: Train Loss = 0.435150, Test Loss = 0.250769, Learning Rate = 2.139400e-06\n",
      "Epoch 16177/20000: Train Loss = 0.434664, Test Loss = 0.250728, Learning Rate = 2.138587e-06\n",
      "Epoch 16178/20000: Train Loss = 0.434833, Test Loss = 0.249219, Learning Rate = 2.137774e-06\n",
      "Epoch 16179/20000: Train Loss = 0.434991, Test Loss = 0.250096, Learning Rate = 2.136962e-06\n",
      "Epoch 16180/20000: Train Loss = 0.434833, Test Loss = 0.249686, Learning Rate = 2.136150e-06\n",
      "Epoch 16181/20000: Train Loss = 0.434677, Test Loss = 0.252167, Learning Rate = 2.135339e-06\n",
      "Epoch 16182/20000: Train Loss = 0.434806, Test Loss = 0.251012, Learning Rate = 2.134527e-06\n",
      "Epoch 16183/20000: Train Loss = 0.434674, Test Loss = 0.251452, Learning Rate = 2.133716e-06\n",
      "Epoch 16184/20000: Train Loss = 0.434559, Test Loss = 0.252119, Learning Rate = 2.132905e-06\n",
      "Epoch 16185/20000: Train Loss = 0.434736, Test Loss = 0.251751, Learning Rate = 2.132095e-06\n",
      "Epoch 16186/20000: Train Loss = 0.434630, Test Loss = 0.252042, Learning Rate = 2.131285e-06\n",
      "Epoch 16187/20000: Train Loss = 0.434835, Test Loss = 0.252195, Learning Rate = 2.130475e-06\n",
      "Epoch 16188/20000: Train Loss = 0.434599, Test Loss = 0.250352, Learning Rate = 2.129665e-06\n",
      "Epoch 16189/20000: Train Loss = 0.434843, Test Loss = 0.251503, Learning Rate = 2.128856e-06\n",
      "Epoch 16190/20000: Train Loss = 0.434700, Test Loss = 0.252555, Learning Rate = 2.128047e-06\n",
      "Epoch 16191/20000: Train Loss = 0.434718, Test Loss = 0.252468, Learning Rate = 2.127239e-06\n",
      "Epoch 16192/20000: Train Loss = 0.434573, Test Loss = 0.251054, Learning Rate = 2.126430e-06\n",
      "Epoch 16193/20000: Train Loss = 0.434726, Test Loss = 0.251855, Learning Rate = 2.125622e-06\n",
      "Epoch 16194/20000: Train Loss = 0.434733, Test Loss = 0.252735, Learning Rate = 2.124815e-06\n",
      "Epoch 16195/20000: Train Loss = 0.434550, Test Loss = 0.253660, Learning Rate = 2.124007e-06\n",
      "Epoch 16196/20000: Train Loss = 0.434904, Test Loss = 0.254882, Learning Rate = 2.123200e-06\n",
      "Epoch 16197/20000: Train Loss = 0.434797, Test Loss = 0.253882, Learning Rate = 2.122394e-06\n",
      "Epoch 16198/20000: Train Loss = 0.434608, Test Loss = 0.254273, Learning Rate = 2.121587e-06\n",
      "Epoch 16199/20000: Train Loss = 0.434575, Test Loss = 0.255010, Learning Rate = 2.120781e-06\n",
      "Epoch 16200/20000: Train Loss = 0.434690, Test Loss = 0.253632, Learning Rate = 2.119975e-06\n",
      "Epoch 16201/20000: Train Loss = 0.434569, Test Loss = 0.252683, Learning Rate = 2.119170e-06\n",
      "Epoch 16202/20000: Train Loss = 0.434620, Test Loss = 0.253625, Learning Rate = 2.118364e-06\n",
      "Epoch 16203/20000: Train Loss = 0.434527, Test Loss = 0.253514, Learning Rate = 2.117559e-06\n",
      "Epoch 16204/20000: Train Loss = 0.434687, Test Loss = 0.253871, Learning Rate = 2.116755e-06\n",
      "Epoch 16205/20000: Train Loss = 0.434698, Test Loss = 0.252058, Learning Rate = 2.115950e-06\n",
      "Epoch 16206/20000: Train Loss = 0.434577, Test Loss = 0.251434, Learning Rate = 2.115146e-06\n",
      "Epoch 16207/20000: Train Loss = 0.434578, Test Loss = 0.253382, Learning Rate = 2.114343e-06\n",
      "Epoch 16208/20000: Train Loss = 0.434677, Test Loss = 0.254221, Learning Rate = 2.113539e-06\n",
      "Epoch 16209/20000: Train Loss = 0.434786, Test Loss = 0.253919, Learning Rate = 2.112736e-06\n",
      "Epoch 16210/20000: Train Loss = 0.434837, Test Loss = 0.253978, Learning Rate = 2.111934e-06\n",
      "Epoch 16211/20000: Train Loss = 0.434989, Test Loss = 0.253372, Learning Rate = 2.111131e-06\n",
      "Epoch 16212/20000: Train Loss = 0.434737, Test Loss = 0.252969, Learning Rate = 2.110329e-06\n",
      "Epoch 16213/20000: Train Loss = 0.434724, Test Loss = 0.253732, Learning Rate = 2.109527e-06\n",
      "Epoch 16214/20000: Train Loss = 0.434758, Test Loss = 0.254002, Learning Rate = 2.108725e-06\n",
      "Epoch 16215/20000: Train Loss = 0.434908, Test Loss = 0.255464, Learning Rate = 2.107924e-06\n",
      "Epoch 16216/20000: Train Loss = 0.434735, Test Loss = 0.255579, Learning Rate = 2.107123e-06\n",
      "Epoch 16217/20000: Train Loss = 0.434906, Test Loss = 0.255365, Learning Rate = 2.106323e-06\n",
      "Epoch 16218/20000: Train Loss = 0.434800, Test Loss = 0.254738, Learning Rate = 2.105522e-06\n",
      "Epoch 16219/20000: Train Loss = 0.434598, Test Loss = 0.254473, Learning Rate = 2.104722e-06\n",
      "Epoch 16220/20000: Train Loss = 0.434706, Test Loss = 0.252776, Learning Rate = 2.103922e-06\n",
      "Epoch 16221/20000: Train Loss = 0.434540, Test Loss = 0.255079, Learning Rate = 2.103123e-06\n",
      "Epoch 16222/20000: Train Loss = 0.434527, Test Loss = 0.252909, Learning Rate = 2.102324e-06\n",
      "Epoch 16223/20000: Train Loss = 0.434636, Test Loss = 0.252891, Learning Rate = 2.101525e-06\n",
      "Epoch 16224/20000: Train Loss = 0.434607, Test Loss = 0.251443, Learning Rate = 2.100727e-06\n",
      "Epoch 16225/20000: Train Loss = 0.434627, Test Loss = 0.253423, Learning Rate = 2.099928e-06\n",
      "Epoch 16226/20000: Train Loss = 0.434881, Test Loss = 0.250192, Learning Rate = 2.099130e-06\n",
      "Epoch 16227/20000: Train Loss = 0.434632, Test Loss = 0.254223, Learning Rate = 2.098333e-06\n",
      "Epoch 16228/20000: Train Loss = 0.434809, Test Loss = 0.254711, Learning Rate = 2.097535e-06\n",
      "Epoch 16229/20000: Train Loss = 0.434946, Test Loss = 0.253517, Learning Rate = 2.096738e-06\n",
      "Epoch 16230/20000: Train Loss = 0.434789, Test Loss = 0.254253, Learning Rate = 2.095942e-06\n",
      "Epoch 16231/20000: Train Loss = 0.434707, Test Loss = 0.253915, Learning Rate = 2.095145e-06\n",
      "Epoch 16232/20000: Train Loss = 0.434670, Test Loss = 0.253532, Learning Rate = 2.094349e-06\n",
      "Epoch 16233/20000: Train Loss = 0.434548, Test Loss = 0.252531, Learning Rate = 2.093553e-06\n",
      "Epoch 16234/20000: Train Loss = 0.434703, Test Loss = 0.254164, Learning Rate = 2.092758e-06\n",
      "Epoch 16235/20000: Train Loss = 0.434654, Test Loss = 0.253919, Learning Rate = 2.091963e-06\n",
      "Epoch 16236/20000: Train Loss = 0.434830, Test Loss = 0.252886, Learning Rate = 2.091168e-06\n",
      "Epoch 16237/20000: Train Loss = 0.434585, Test Loss = 0.254018, Learning Rate = 2.090373e-06\n",
      "Epoch 16238/20000: Train Loss = 0.434794, Test Loss = 0.254737, Learning Rate = 2.089579e-06\n",
      "Epoch 16239/20000: Train Loss = 0.434807, Test Loss = 0.252270, Learning Rate = 2.088785e-06\n",
      "Epoch 16240/20000: Train Loss = 0.434985, Test Loss = 0.253869, Learning Rate = 2.087991e-06\n",
      "Epoch 16241/20000: Train Loss = 0.434697, Test Loss = 0.252833, Learning Rate = 2.087198e-06\n",
      "Epoch 16242/20000: Train Loss = 0.434933, Test Loss = 0.252861, Learning Rate = 2.086405e-06\n",
      "Epoch 16243/20000: Train Loss = 0.434648, Test Loss = 0.252741, Learning Rate = 2.085612e-06\n",
      "Epoch 16244/20000: Train Loss = 0.434771, Test Loss = 0.252526, Learning Rate = 2.084820e-06\n",
      "Epoch 16245/20000: Train Loss = 0.434599, Test Loss = 0.252089, Learning Rate = 2.084027e-06\n",
      "Epoch 16246/20000: Train Loss = 0.434574, Test Loss = 0.252241, Learning Rate = 2.083236e-06\n",
      "Epoch 16247/20000: Train Loss = 0.434680, Test Loss = 0.253844, Learning Rate = 2.082444e-06\n",
      "Epoch 16248/20000: Train Loss = 0.434683, Test Loss = 0.252757, Learning Rate = 2.081653e-06\n",
      "Epoch 16249/20000: Train Loss = 0.434568, Test Loss = 0.251759, Learning Rate = 2.080862e-06\n",
      "Epoch 16250/20000: Train Loss = 0.434951, Test Loss = 0.251825, Learning Rate = 2.080071e-06\n",
      "Epoch 16251/20000: Train Loss = 0.434757, Test Loss = 0.250848, Learning Rate = 2.079281e-06\n",
      "Epoch 16252/20000: Train Loss = 0.434683, Test Loss = 0.251043, Learning Rate = 2.078491e-06\n",
      "Epoch 16253/20000: Train Loss = 0.434763, Test Loss = 0.251878, Learning Rate = 2.077701e-06\n",
      "Epoch 16254/20000: Train Loss = 0.434766, Test Loss = 0.251223, Learning Rate = 2.076911e-06\n",
      "Epoch 16255/20000: Train Loss = 0.435099, Test Loss = 0.252770, Learning Rate = 2.076122e-06\n",
      "Epoch 16256/20000: Train Loss = 0.434734, Test Loss = 0.253011, Learning Rate = 2.075333e-06\n",
      "Epoch 16257/20000: Train Loss = 0.434745, Test Loss = 0.251295, Learning Rate = 2.074545e-06\n",
      "Epoch 16258/20000: Train Loss = 0.434628, Test Loss = 0.252326, Learning Rate = 2.073757e-06\n",
      "Epoch 16259/20000: Train Loss = 0.434572, Test Loss = 0.251428, Learning Rate = 2.072969e-06\n",
      "Epoch 16260/20000: Train Loss = 0.434763, Test Loss = 0.250807, Learning Rate = 2.072181e-06\n",
      "Epoch 16261/20000: Train Loss = 0.434506, Test Loss = 0.251082, Learning Rate = 2.071394e-06\n",
      "Epoch 16262/20000: Train Loss = 0.434607, Test Loss = 0.251370, Learning Rate = 2.070606e-06\n",
      "Epoch 16263/20000: Train Loss = 0.434608, Test Loss = 0.251032, Learning Rate = 2.069820e-06\n",
      "Epoch 16264/20000: Train Loss = 0.434549, Test Loss = 0.252176, Learning Rate = 2.069033e-06\n",
      "Epoch 16265/20000: Train Loss = 0.434689, Test Loss = 0.252586, Learning Rate = 2.068247e-06\n",
      "Epoch 16266/20000: Train Loss = 0.434575, Test Loss = 0.253171, Learning Rate = 2.067461e-06\n",
      "Epoch 16267/20000: Train Loss = 0.434901, Test Loss = 0.252260, Learning Rate = 2.066676e-06\n",
      "Epoch 16268/20000: Train Loss = 0.434696, Test Loss = 0.253201, Learning Rate = 2.065890e-06\n",
      "Epoch 16269/20000: Train Loss = 0.434637, Test Loss = 0.252951, Learning Rate = 2.065105e-06\n",
      "Epoch 16270/20000: Train Loss = 0.434562, Test Loss = 0.252227, Learning Rate = 2.064321e-06\n",
      "Epoch 16271/20000: Train Loss = 0.434552, Test Loss = 0.253986, Learning Rate = 2.063536e-06\n",
      "Epoch 16272/20000: Train Loss = 0.434676, Test Loss = 0.253863, Learning Rate = 2.062752e-06\n",
      "Epoch 16273/20000: Train Loss = 0.434629, Test Loss = 0.252827, Learning Rate = 2.061968e-06\n",
      "Epoch 16274/20000: Train Loss = 0.434521, Test Loss = 0.252458, Learning Rate = 2.061185e-06\n",
      "Epoch 16275/20000: Train Loss = 0.434596, Test Loss = 0.253283, Learning Rate = 2.060402e-06\n",
      "Epoch 16276/20000: Train Loss = 0.435228, Test Loss = 0.253291, Learning Rate = 2.059619e-06\n",
      "Epoch 16277/20000: Train Loss = 0.434680, Test Loss = 0.252482, Learning Rate = 2.058836e-06\n",
      "Epoch 16278/20000: Train Loss = 0.434658, Test Loss = 0.252397, Learning Rate = 2.058054e-06\n",
      "Epoch 16279/20000: Train Loss = 0.434594, Test Loss = 0.252119, Learning Rate = 2.057272e-06\n",
      "Epoch 16280/20000: Train Loss = 0.434559, Test Loss = 0.251726, Learning Rate = 2.056490e-06\n",
      "Epoch 16281/20000: Train Loss = 0.434660, Test Loss = 0.253585, Learning Rate = 2.055709e-06\n",
      "Epoch 16282/20000: Train Loss = 0.434716, Test Loss = 0.252227, Learning Rate = 2.054928e-06\n",
      "Epoch 16283/20000: Train Loss = 0.434571, Test Loss = 0.251524, Learning Rate = 2.054147e-06\n",
      "Epoch 16284/20000: Train Loss = 0.434620, Test Loss = 0.251947, Learning Rate = 2.053366e-06\n",
      "Epoch 16285/20000: Train Loss = 0.434859, Test Loss = 0.250231, Learning Rate = 2.052586e-06\n",
      "Epoch 16286/20000: Train Loss = 0.434580, Test Loss = 0.250223, Learning Rate = 2.051806e-06\n",
      "Epoch 16287/20000: Train Loss = 0.434799, Test Loss = 0.249636, Learning Rate = 2.051027e-06\n",
      "Epoch 16288/20000: Train Loss = 0.434747, Test Loss = 0.250914, Learning Rate = 2.050247e-06\n",
      "Epoch 16289/20000: Train Loss = 0.434626, Test Loss = 0.250783, Learning Rate = 2.049468e-06\n",
      "Epoch 16290/20000: Train Loss = 0.434564, Test Loss = 0.253075, Learning Rate = 2.048689e-06\n",
      "Epoch 16291/20000: Train Loss = 0.434781, Test Loss = 0.254342, Learning Rate = 2.047911e-06\n",
      "Epoch 16292/20000: Train Loss = 0.434668, Test Loss = 0.252084, Learning Rate = 2.047133e-06\n",
      "Epoch 16293/20000: Train Loss = 0.434543, Test Loss = 0.253867, Learning Rate = 2.046355e-06\n",
      "Epoch 16294/20000: Train Loss = 0.434749, Test Loss = 0.253740, Learning Rate = 2.045577e-06\n",
      "Epoch 16295/20000: Train Loss = 0.434455, Test Loss = 0.252782, Learning Rate = 2.044800e-06\n",
      "Epoch 16296/20000: Train Loss = 0.434670, Test Loss = 0.252527, Learning Rate = 2.044023e-06\n",
      "Epoch 16297/20000: Train Loss = 0.434619, Test Loss = 0.252121, Learning Rate = 2.043246e-06\n",
      "Epoch 16298/20000: Train Loss = 0.434822, Test Loss = 0.252073, Learning Rate = 2.042470e-06\n",
      "Epoch 16299/20000: Train Loss = 0.434721, Test Loss = 0.251547, Learning Rate = 2.041694e-06\n",
      "Epoch 16300/20000: Train Loss = 0.434786, Test Loss = 0.253089, Learning Rate = 2.040918e-06\n",
      "Epoch 16301/20000: Train Loss = 0.434813, Test Loss = 0.252515, Learning Rate = 2.040143e-06\n",
      "Epoch 16302/20000: Train Loss = 0.434538, Test Loss = 0.252032, Learning Rate = 2.039368e-06\n",
      "Epoch 16303/20000: Train Loss = 0.434535, Test Loss = 0.252037, Learning Rate = 2.038593e-06\n",
      "Epoch 16304/20000: Train Loss = 0.434928, Test Loss = 0.250723, Learning Rate = 2.037818e-06\n",
      "Epoch 16305/20000: Train Loss = 0.434738, Test Loss = 0.251707, Learning Rate = 2.037044e-06\n",
      "Epoch 16306/20000: Train Loss = 0.434578, Test Loss = 0.251729, Learning Rate = 2.036270e-06\n",
      "Epoch 16307/20000: Train Loss = 0.434540, Test Loss = 0.251507, Learning Rate = 2.035496e-06\n",
      "Epoch 16308/20000: Train Loss = 0.434705, Test Loss = 0.251470, Learning Rate = 2.034723e-06\n",
      "Epoch 16309/20000: Train Loss = 0.434632, Test Loss = 0.251382, Learning Rate = 2.033949e-06\n",
      "Epoch 16310/20000: Train Loss = 0.434651, Test Loss = 0.251836, Learning Rate = 2.033177e-06\n",
      "Epoch 16311/20000: Train Loss = 0.434668, Test Loss = 0.251500, Learning Rate = 2.032404e-06\n",
      "Epoch 16312/20000: Train Loss = 0.434712, Test Loss = 0.252568, Learning Rate = 2.031632e-06\n",
      "Epoch 16313/20000: Train Loss = 0.434561, Test Loss = 0.252967, Learning Rate = 2.030860e-06\n",
      "Epoch 16314/20000: Train Loss = 0.434752, Test Loss = 0.252459, Learning Rate = 2.030088e-06\n",
      "Epoch 16315/20000: Train Loss = 0.434807, Test Loss = 0.252546, Learning Rate = 2.029317e-06\n",
      "Epoch 16316/20000: Train Loss = 0.434672, Test Loss = 0.251033, Learning Rate = 2.028546e-06\n",
      "Epoch 16317/20000: Train Loss = 0.434767, Test Loss = 0.250153, Learning Rate = 2.027775e-06\n",
      "Epoch 16318/20000: Train Loss = 0.434657, Test Loss = 0.250004, Learning Rate = 2.027004e-06\n",
      "Epoch 16319/20000: Train Loss = 0.434762, Test Loss = 0.250671, Learning Rate = 2.026234e-06\n",
      "Epoch 16320/20000: Train Loss = 0.434971, Test Loss = 0.250483, Learning Rate = 2.025464e-06\n",
      "Epoch 16321/20000: Train Loss = 0.434539, Test Loss = 0.251942, Learning Rate = 2.024695e-06\n",
      "Epoch 16322/20000: Train Loss = 0.434557, Test Loss = 0.252482, Learning Rate = 2.023925e-06\n",
      "Epoch 16323/20000: Train Loss = 0.434477, Test Loss = 0.252867, Learning Rate = 2.023156e-06\n",
      "Epoch 16324/20000: Train Loss = 0.434614, Test Loss = 0.252068, Learning Rate = 2.022387e-06\n",
      "Epoch 16325/20000: Train Loss = 0.434585, Test Loss = 0.252991, Learning Rate = 2.021619e-06\n",
      "Epoch 16326/20000: Train Loss = 0.434543, Test Loss = 0.250887, Learning Rate = 2.020851e-06\n",
      "Epoch 16327/20000: Train Loss = 0.434788, Test Loss = 0.252147, Learning Rate = 2.020083e-06\n",
      "Epoch 16328/20000: Train Loss = 0.434489, Test Loss = 0.252692, Learning Rate = 2.019315e-06\n",
      "Epoch 16329/20000: Train Loss = 0.434611, Test Loss = 0.253147, Learning Rate = 2.018548e-06\n",
      "Epoch 16330/20000: Train Loss = 0.434584, Test Loss = 0.252214, Learning Rate = 2.017781e-06\n",
      "Epoch 16331/20000: Train Loss = 0.434531, Test Loss = 0.251723, Learning Rate = 2.017014e-06\n",
      "Epoch 16332/20000: Train Loss = 0.434652, Test Loss = 0.252409, Learning Rate = 2.016248e-06\n",
      "Epoch 16333/20000: Train Loss = 0.434508, Test Loss = 0.252008, Learning Rate = 2.015482e-06\n",
      "Epoch 16334/20000: Train Loss = 0.434687, Test Loss = 0.254380, Learning Rate = 2.014716e-06\n",
      "Epoch 16335/20000: Train Loss = 0.434814, Test Loss = 0.253841, Learning Rate = 2.013951e-06\n",
      "Epoch 16336/20000: Train Loss = 0.434503, Test Loss = 0.253798, Learning Rate = 2.013185e-06\n",
      "Epoch 16337/20000: Train Loss = 0.434485, Test Loss = 0.252918, Learning Rate = 2.012420e-06\n",
      "Epoch 16338/20000: Train Loss = 0.434853, Test Loss = 0.253218, Learning Rate = 2.011656e-06\n",
      "Epoch 16339/20000: Train Loss = 0.434545, Test Loss = 0.254886, Learning Rate = 2.010891e-06\n",
      "Epoch 16340/20000: Train Loss = 0.434605, Test Loss = 0.255246, Learning Rate = 2.010127e-06\n",
      "Epoch 16341/20000: Train Loss = 0.434779, Test Loss = 0.254521, Learning Rate = 2.009363e-06\n",
      "Epoch 16342/20000: Train Loss = 0.434576, Test Loss = 0.252406, Learning Rate = 2.008600e-06\n",
      "Epoch 16343/20000: Train Loss = 0.434612, Test Loss = 0.253384, Learning Rate = 2.007837e-06\n",
      "Epoch 16344/20000: Train Loss = 0.434693, Test Loss = 0.253820, Learning Rate = 2.007074e-06\n",
      "Epoch 16345/20000: Train Loss = 0.434762, Test Loss = 0.254735, Learning Rate = 2.006311e-06\n",
      "Epoch 16346/20000: Train Loss = 0.434547, Test Loss = 0.253921, Learning Rate = 2.005549e-06\n",
      "Epoch 16347/20000: Train Loss = 0.434671, Test Loss = 0.252844, Learning Rate = 2.004787e-06\n",
      "Epoch 16348/20000: Train Loss = 0.435033, Test Loss = 0.253054, Learning Rate = 2.004025e-06\n",
      "Epoch 16349/20000: Train Loss = 0.434863, Test Loss = 0.253210, Learning Rate = 2.003264e-06\n",
      "Epoch 16350/20000: Train Loss = 0.434828, Test Loss = 0.254326, Learning Rate = 2.002502e-06\n",
      "Epoch 16351/20000: Train Loss = 0.434741, Test Loss = 0.253294, Learning Rate = 2.001741e-06\n",
      "Epoch 16352/20000: Train Loss = 0.434813, Test Loss = 0.254230, Learning Rate = 2.000981e-06\n",
      "Epoch 16353/20000: Train Loss = 0.434651, Test Loss = 0.255873, Learning Rate = 2.000221e-06\n",
      "Epoch 16354/20000: Train Loss = 0.434664, Test Loss = 0.254233, Learning Rate = 1.999460e-06\n",
      "Epoch 16355/20000: Train Loss = 0.434626, Test Loss = 0.253428, Learning Rate = 1.998701e-06\n",
      "Epoch 16356/20000: Train Loss = 0.434542, Test Loss = 0.254238, Learning Rate = 1.997941e-06\n",
      "Epoch 16357/20000: Train Loss = 0.434477, Test Loss = 0.254237, Learning Rate = 1.997182e-06\n",
      "Epoch 16358/20000: Train Loss = 0.434565, Test Loss = 0.254123, Learning Rate = 1.996423e-06\n",
      "Epoch 16359/20000: Train Loss = 0.434625, Test Loss = 0.253207, Learning Rate = 1.995665e-06\n",
      "Epoch 16360/20000: Train Loss = 0.434791, Test Loss = 0.253255, Learning Rate = 1.994906e-06\n",
      "Epoch 16361/20000: Train Loss = 0.434562, Test Loss = 0.253200, Learning Rate = 1.994148e-06\n",
      "Epoch 16362/20000: Train Loss = 0.434708, Test Loss = 0.252529, Learning Rate = 1.993391e-06\n",
      "Epoch 16363/20000: Train Loss = 0.434622, Test Loss = 0.251676, Learning Rate = 1.992633e-06\n",
      "Epoch 16364/20000: Train Loss = 0.434532, Test Loss = 0.252315, Learning Rate = 1.991876e-06\n",
      "Epoch 16365/20000: Train Loss = 0.434788, Test Loss = 0.250950, Learning Rate = 1.991119e-06\n",
      "Epoch 16366/20000: Train Loss = 0.434532, Test Loss = 0.250792, Learning Rate = 1.990363e-06\n",
      "Epoch 16367/20000: Train Loss = 0.435109, Test Loss = 0.251626, Learning Rate = 1.989606e-06\n",
      "Epoch 16368/20000: Train Loss = 0.434868, Test Loss = 0.253524, Learning Rate = 1.988850e-06\n",
      "Epoch 16369/20000: Train Loss = 0.434870, Test Loss = 0.251988, Learning Rate = 1.988095e-06\n",
      "Epoch 16370/20000: Train Loss = 0.434855, Test Loss = 0.252141, Learning Rate = 1.987339e-06\n",
      "Epoch 16371/20000: Train Loss = 0.434725, Test Loss = 0.252327, Learning Rate = 1.986584e-06\n",
      "Epoch 16372/20000: Train Loss = 0.434537, Test Loss = 0.252008, Learning Rate = 1.985829e-06\n",
      "Epoch 16373/20000: Train Loss = 0.434614, Test Loss = 0.252434, Learning Rate = 1.985075e-06\n",
      "Epoch 16374/20000: Train Loss = 0.434510, Test Loss = 0.252711, Learning Rate = 1.984320e-06\n",
      "Epoch 16375/20000: Train Loss = 0.434647, Test Loss = 0.254404, Learning Rate = 1.983566e-06\n",
      "Epoch 16376/20000: Train Loss = 0.434625, Test Loss = 0.253772, Learning Rate = 1.982813e-06\n",
      "Epoch 16377/20000: Train Loss = 0.434747, Test Loss = 0.253104, Learning Rate = 1.982059e-06\n",
      "Epoch 16378/20000: Train Loss = 0.434721, Test Loss = 0.255203, Learning Rate = 1.981306e-06\n",
      "Epoch 16379/20000: Train Loss = 0.434683, Test Loss = 0.255292, Learning Rate = 1.980553e-06\n",
      "Epoch 16380/20000: Train Loss = 0.434617, Test Loss = 0.254493, Learning Rate = 1.979801e-06\n",
      "Epoch 16381/20000: Train Loss = 0.434752, Test Loss = 0.253028, Learning Rate = 1.979048e-06\n",
      "Epoch 16382/20000: Train Loss = 0.434577, Test Loss = 0.253346, Learning Rate = 1.978296e-06\n",
      "Epoch 16383/20000: Train Loss = 0.434540, Test Loss = 0.253286, Learning Rate = 1.977545e-06\n",
      "Epoch 16384/20000: Train Loss = 0.434559, Test Loss = 0.254317, Learning Rate = 1.976793e-06\n",
      "Epoch 16385/20000: Train Loss = 0.434547, Test Loss = 0.254266, Learning Rate = 1.976042e-06\n",
      "Epoch 16386/20000: Train Loss = 0.434773, Test Loss = 0.253939, Learning Rate = 1.975291e-06\n",
      "Epoch 16387/20000: Train Loss = 0.434527, Test Loss = 0.253650, Learning Rate = 1.974541e-06\n",
      "Epoch 16388/20000: Train Loss = 0.434531, Test Loss = 0.251899, Learning Rate = 1.973791e-06\n",
      "Epoch 16389/20000: Train Loss = 0.434932, Test Loss = 0.252426, Learning Rate = 1.973041e-06\n",
      "Epoch 16390/20000: Train Loss = 0.434798, Test Loss = 0.254565, Learning Rate = 1.972291e-06\n",
      "Epoch 16391/20000: Train Loss = 0.434764, Test Loss = 0.253394, Learning Rate = 1.971541e-06\n",
      "Epoch 16392/20000: Train Loss = 0.434660, Test Loss = 0.253089, Learning Rate = 1.970792e-06\n",
      "Epoch 16393/20000: Train Loss = 0.434559, Test Loss = 0.252845, Learning Rate = 1.970043e-06\n",
      "Epoch 16394/20000: Train Loss = 0.434640, Test Loss = 0.252640, Learning Rate = 1.969295e-06\n",
      "Epoch 16395/20000: Train Loss = 0.434510, Test Loss = 0.252436, Learning Rate = 1.968547e-06\n",
      "Epoch 16396/20000: Train Loss = 0.434677, Test Loss = 0.252667, Learning Rate = 1.967799e-06\n",
      "Epoch 16397/20000: Train Loss = 0.434589, Test Loss = 0.254684, Learning Rate = 1.967051e-06\n",
      "Epoch 16398/20000: Train Loss = 0.434810, Test Loss = 0.253964, Learning Rate = 1.966304e-06\n",
      "Epoch 16399/20000: Train Loss = 0.434604, Test Loss = 0.251308, Learning Rate = 1.965556e-06\n",
      "Epoch 16400/20000: Train Loss = 0.434881, Test Loss = 0.252580, Learning Rate = 1.964810e-06\n",
      "Epoch 16401/20000: Train Loss = 0.434549, Test Loss = 0.252087, Learning Rate = 1.964063e-06\n",
      "Epoch 16402/20000: Train Loss = 0.434921, Test Loss = 0.250777, Learning Rate = 1.963317e-06\n",
      "Epoch 16403/20000: Train Loss = 0.434676, Test Loss = 0.252230, Learning Rate = 1.962571e-06\n",
      "Epoch 16404/20000: Train Loss = 0.434673, Test Loss = 0.252664, Learning Rate = 1.961825e-06\n",
      "Epoch 16405/20000: Train Loss = 0.434533, Test Loss = 0.252265, Learning Rate = 1.961079e-06\n",
      "Epoch 16406/20000: Train Loss = 0.434663, Test Loss = 0.251580, Learning Rate = 1.960334e-06\n",
      "Epoch 16407/20000: Train Loss = 0.434567, Test Loss = 0.253072, Learning Rate = 1.959589e-06\n",
      "Epoch 16408/20000: Train Loss = 0.434765, Test Loss = 0.254373, Learning Rate = 1.958845e-06\n",
      "Epoch 16409/20000: Train Loss = 0.434686, Test Loss = 0.254888, Learning Rate = 1.958101e-06\n",
      "Epoch 16410/20000: Train Loss = 0.434622, Test Loss = 0.256256, Learning Rate = 1.957357e-06\n",
      "Epoch 16411/20000: Train Loss = 0.434652, Test Loss = 0.254887, Learning Rate = 1.956613e-06\n",
      "Epoch 16412/20000: Train Loss = 0.434609, Test Loss = 0.254985, Learning Rate = 1.955869e-06\n",
      "Epoch 16413/20000: Train Loss = 0.435096, Test Loss = 0.254400, Learning Rate = 1.955126e-06\n",
      "Epoch 16414/20000: Train Loss = 0.434805, Test Loss = 0.252191, Learning Rate = 1.954383e-06\n",
      "Epoch 16415/20000: Train Loss = 0.434560, Test Loss = 0.253711, Learning Rate = 1.953641e-06\n",
      "Epoch 16416/20000: Train Loss = 0.434755, Test Loss = 0.254483, Learning Rate = 1.952898e-06\n",
      "Epoch 16417/20000: Train Loss = 0.434566, Test Loss = 0.253088, Learning Rate = 1.952156e-06\n",
      "Epoch 16418/20000: Train Loss = 0.434587, Test Loss = 0.253091, Learning Rate = 1.951415e-06\n",
      "Epoch 16419/20000: Train Loss = 0.434637, Test Loss = 0.253864, Learning Rate = 1.950673e-06\n",
      "Epoch 16420/20000: Train Loss = 0.434627, Test Loss = 0.253350, Learning Rate = 1.949932e-06\n",
      "Epoch 16421/20000: Train Loss = 0.434608, Test Loss = 0.253208, Learning Rate = 1.949191e-06\n",
      "Epoch 16422/20000: Train Loss = 0.434628, Test Loss = 0.252124, Learning Rate = 1.948450e-06\n",
      "Epoch 16423/20000: Train Loss = 0.434676, Test Loss = 0.253972, Learning Rate = 1.947710e-06\n",
      "Epoch 16424/20000: Train Loss = 0.434965, Test Loss = 0.253128, Learning Rate = 1.946970e-06\n",
      "Epoch 16425/20000: Train Loss = 0.434615, Test Loss = 0.253266, Learning Rate = 1.946230e-06\n",
      "Epoch 16426/20000: Train Loss = 0.434667, Test Loss = 0.253822, Learning Rate = 1.945491e-06\n",
      "Epoch 16427/20000: Train Loss = 0.435009, Test Loss = 0.254057, Learning Rate = 1.944751e-06\n",
      "Epoch 16428/20000: Train Loss = 0.434514, Test Loss = 0.252773, Learning Rate = 1.944012e-06\n",
      "Epoch 16429/20000: Train Loss = 0.434697, Test Loss = 0.254137, Learning Rate = 1.943274e-06\n",
      "Epoch 16430/20000: Train Loss = 0.434704, Test Loss = 0.254372, Learning Rate = 1.942535e-06\n",
      "Epoch 16431/20000: Train Loss = 0.434693, Test Loss = 0.253557, Learning Rate = 1.941797e-06\n",
      "Epoch 16432/20000: Train Loss = 0.434672, Test Loss = 0.253267, Learning Rate = 1.941059e-06\n",
      "Epoch 16433/20000: Train Loss = 0.434503, Test Loss = 0.253553, Learning Rate = 1.940322e-06\n",
      "Epoch 16434/20000: Train Loss = 0.434595, Test Loss = 0.251926, Learning Rate = 1.939584e-06\n",
      "Epoch 16435/20000: Train Loss = 0.434634, Test Loss = 0.252414, Learning Rate = 1.938848e-06\n",
      "Epoch 16436/20000: Train Loss = 0.434718, Test Loss = 0.253000, Learning Rate = 1.938111e-06\n",
      "Epoch 16437/20000: Train Loss = 0.434562, Test Loss = 0.254034, Learning Rate = 1.937374e-06\n",
      "Epoch 16438/20000: Train Loss = 0.434851, Test Loss = 0.253853, Learning Rate = 1.936638e-06\n",
      "Epoch 16439/20000: Train Loss = 0.434725, Test Loss = 0.255272, Learning Rate = 1.935902e-06\n",
      "Epoch 16440/20000: Train Loss = 0.434902, Test Loss = 0.255644, Learning Rate = 1.935167e-06\n",
      "Epoch 16441/20000: Train Loss = 0.434702, Test Loss = 0.255376, Learning Rate = 1.934431e-06\n",
      "Epoch 16442/20000: Train Loss = 0.434698, Test Loss = 0.256061, Learning Rate = 1.933696e-06\n",
      "Epoch 16443/20000: Train Loss = 0.434620, Test Loss = 0.254617, Learning Rate = 1.932962e-06\n",
      "Epoch 16444/20000: Train Loss = 0.434569, Test Loss = 0.255451, Learning Rate = 1.932227e-06\n",
      "Epoch 16445/20000: Train Loss = 0.434747, Test Loss = 0.253656, Learning Rate = 1.931493e-06\n",
      "Epoch 16446/20000: Train Loss = 0.435079, Test Loss = 0.251900, Learning Rate = 1.930759e-06\n",
      "Epoch 16447/20000: Train Loss = 0.434719, Test Loss = 0.254798, Learning Rate = 1.930025e-06\n",
      "Epoch 16448/20000: Train Loss = 0.434832, Test Loss = 0.254701, Learning Rate = 1.929292e-06\n",
      "Epoch 16449/20000: Train Loss = 0.434915, Test Loss = 0.254315, Learning Rate = 1.928559e-06\n",
      "Epoch 16450/20000: Train Loss = 0.434563, Test Loss = 0.253336, Learning Rate = 1.927826e-06\n",
      "Epoch 16451/20000: Train Loss = 0.434442, Test Loss = 0.253763, Learning Rate = 1.927094e-06\n",
      "Epoch 16452/20000: Train Loss = 0.434625, Test Loss = 0.252098, Learning Rate = 1.926361e-06\n",
      "Epoch 16453/20000: Train Loss = 0.434473, Test Loss = 0.251964, Learning Rate = 1.925629e-06\n",
      "Epoch 16454/20000: Train Loss = 0.434600, Test Loss = 0.251394, Learning Rate = 1.924898e-06\n",
      "Epoch 16455/20000: Train Loss = 0.434503, Test Loss = 0.251103, Learning Rate = 1.924166e-06\n",
      "Epoch 16456/20000: Train Loss = 0.434484, Test Loss = 0.252971, Learning Rate = 1.923435e-06\n",
      "Epoch 16457/20000: Train Loss = 0.434666, Test Loss = 0.251358, Learning Rate = 1.922704e-06\n",
      "Epoch 16458/20000: Train Loss = 0.434710, Test Loss = 0.251067, Learning Rate = 1.921974e-06\n",
      "Epoch 16459/20000: Train Loss = 0.434507, Test Loss = 0.251239, Learning Rate = 1.921244e-06\n",
      "Epoch 16460/20000: Train Loss = 0.434605, Test Loss = 0.251784, Learning Rate = 1.920514e-06\n",
      "Epoch 16461/20000: Train Loss = 0.434520, Test Loss = 0.251848, Learning Rate = 1.919784e-06\n",
      "Epoch 16462/20000: Train Loss = 0.434671, Test Loss = 0.253100, Learning Rate = 1.919054e-06\n",
      "Epoch 16463/20000: Train Loss = 0.434509, Test Loss = 0.252814, Learning Rate = 1.918325e-06\n",
      "Epoch 16464/20000: Train Loss = 0.434549, Test Loss = 0.251965, Learning Rate = 1.917596e-06\n",
      "Epoch 16465/20000: Train Loss = 0.434813, Test Loss = 0.253388, Learning Rate = 1.916868e-06\n",
      "Epoch 16466/20000: Train Loss = 0.434641, Test Loss = 0.252417, Learning Rate = 1.916139e-06\n",
      "Epoch 16467/20000: Train Loss = 0.435114, Test Loss = 0.251530, Learning Rate = 1.915411e-06\n",
      "Epoch 16468/20000: Train Loss = 0.434512, Test Loss = 0.252415, Learning Rate = 1.914683e-06\n",
      "Epoch 16469/20000: Train Loss = 0.434604, Test Loss = 0.252697, Learning Rate = 1.913956e-06\n",
      "Epoch 16470/20000: Train Loss = 0.434602, Test Loss = 0.253073, Learning Rate = 1.913229e-06\n",
      "Epoch 16471/20000: Train Loss = 0.434526, Test Loss = 0.252585, Learning Rate = 1.912502e-06\n",
      "Epoch 16472/20000: Train Loss = 0.434822, Test Loss = 0.251730, Learning Rate = 1.911775e-06\n",
      "Epoch 16473/20000: Train Loss = 0.434582, Test Loss = 0.253056, Learning Rate = 1.911048e-06\n",
      "Epoch 16474/20000: Train Loss = 0.434718, Test Loss = 0.253919, Learning Rate = 1.910322e-06\n",
      "Epoch 16475/20000: Train Loss = 0.434795, Test Loss = 0.253589, Learning Rate = 1.909596e-06\n",
      "Epoch 16476/20000: Train Loss = 0.434648, Test Loss = 0.253488, Learning Rate = 1.908871e-06\n",
      "Epoch 16477/20000: Train Loss = 0.434565, Test Loss = 0.252215, Learning Rate = 1.908146e-06\n",
      "Epoch 16478/20000: Train Loss = 0.434762, Test Loss = 0.252307, Learning Rate = 1.907420e-06\n",
      "Epoch 16479/20000: Train Loss = 0.434578, Test Loss = 0.253816, Learning Rate = 1.906696e-06\n",
      "Epoch 16480/20000: Train Loss = 0.434703, Test Loss = 0.251970, Learning Rate = 1.905971e-06\n",
      "Epoch 16481/20000: Train Loss = 0.434689, Test Loss = 0.251896, Learning Rate = 1.905247e-06\n",
      "Epoch 16482/20000: Train Loss = 0.434889, Test Loss = 0.251957, Learning Rate = 1.904523e-06\n",
      "Epoch 16483/20000: Train Loss = 0.434856, Test Loss = 0.254109, Learning Rate = 1.903799e-06\n",
      "Epoch 16484/20000: Train Loss = 0.434862, Test Loss = 0.255049, Learning Rate = 1.903076e-06\n",
      "Epoch 16485/20000: Train Loss = 0.434875, Test Loss = 0.255686, Learning Rate = 1.902353e-06\n",
      "Epoch 16486/20000: Train Loss = 0.435036, Test Loss = 0.254982, Learning Rate = 1.901630e-06\n",
      "Epoch 16487/20000: Train Loss = 0.434689, Test Loss = 0.254773, Learning Rate = 1.900907e-06\n",
      "Epoch 16488/20000: Train Loss = 0.435251, Test Loss = 0.257364, Learning Rate = 1.900185e-06\n",
      "Epoch 16489/20000: Train Loss = 0.434638, Test Loss = 0.255185, Learning Rate = 1.899463e-06\n",
      "Epoch 16490/20000: Train Loss = 0.434542, Test Loss = 0.253598, Learning Rate = 1.898741e-06\n",
      "Epoch 16491/20000: Train Loss = 0.434853, Test Loss = 0.251979, Learning Rate = 1.898020e-06\n",
      "Epoch 16492/20000: Train Loss = 0.434750, Test Loss = 0.253401, Learning Rate = 1.897299e-06\n",
      "Epoch 16493/20000: Train Loss = 0.434505, Test Loss = 0.253851, Learning Rate = 1.896578e-06\n",
      "Epoch 16494/20000: Train Loss = 0.434588, Test Loss = 0.252510, Learning Rate = 1.895857e-06\n",
      "Epoch 16495/20000: Train Loss = 0.434658, Test Loss = 0.252908, Learning Rate = 1.895137e-06\n",
      "Epoch 16496/20000: Train Loss = 0.434463, Test Loss = 0.251870, Learning Rate = 1.894417e-06\n",
      "Epoch 16497/20000: Train Loss = 0.434836, Test Loss = 0.251980, Learning Rate = 1.893697e-06\n",
      "Epoch 16498/20000: Train Loss = 0.434663, Test Loss = 0.250701, Learning Rate = 1.892977e-06\n",
      "Epoch 16499/20000: Train Loss = 0.434608, Test Loss = 0.252358, Learning Rate = 1.892258e-06\n",
      "Epoch 16500/20000: Train Loss = 0.434664, Test Loss = 0.252795, Learning Rate = 1.891539e-06\n",
      "Epoch 16501/20000: Train Loss = 0.434691, Test Loss = 0.252566, Learning Rate = 1.890820e-06\n",
      "Epoch 16502/20000: Train Loss = 0.434608, Test Loss = 0.252922, Learning Rate = 1.890102e-06\n",
      "Epoch 16503/20000: Train Loss = 0.434510, Test Loss = 0.252958, Learning Rate = 1.889384e-06\n",
      "Epoch 16504/20000: Train Loss = 0.434527, Test Loss = 0.252341, Learning Rate = 1.888666e-06\n",
      "Epoch 16505/20000: Train Loss = 0.434597, Test Loss = 0.252032, Learning Rate = 1.887948e-06\n",
      "Epoch 16506/20000: Train Loss = 0.434565, Test Loss = 0.251570, Learning Rate = 1.887231e-06\n",
      "Epoch 16507/20000: Train Loss = 0.434537, Test Loss = 0.252645, Learning Rate = 1.886514e-06\n",
      "Epoch 16508/20000: Train Loss = 0.434615, Test Loss = 0.252365, Learning Rate = 1.885797e-06\n",
      "Epoch 16509/20000: Train Loss = 0.434562, Test Loss = 0.252932, Learning Rate = 1.885080e-06\n",
      "Epoch 16510/20000: Train Loss = 0.434492, Test Loss = 0.252051, Learning Rate = 1.884364e-06\n",
      "Epoch 16511/20000: Train Loss = 0.434707, Test Loss = 0.253845, Learning Rate = 1.883648e-06\n",
      "Epoch 16512/20000: Train Loss = 0.434478, Test Loss = 0.253712, Learning Rate = 1.882932e-06\n",
      "Epoch 16513/20000: Train Loss = 0.434966, Test Loss = 0.253506, Learning Rate = 1.882217e-06\n",
      "Epoch 16514/20000: Train Loss = 0.434629, Test Loss = 0.253918, Learning Rate = 1.881502e-06\n",
      "Epoch 16515/20000: Train Loss = 0.434488, Test Loss = 0.253838, Learning Rate = 1.880787e-06\n",
      "Epoch 16516/20000: Train Loss = 0.434834, Test Loss = 0.255087, Learning Rate = 1.880072e-06\n",
      "Epoch 16517/20000: Train Loss = 0.434614, Test Loss = 0.253698, Learning Rate = 1.879358e-06\n",
      "Epoch 16518/20000: Train Loss = 0.434685, Test Loss = 0.254172, Learning Rate = 1.878644e-06\n",
      "Epoch 16519/20000: Train Loss = 0.434485, Test Loss = 0.254615, Learning Rate = 1.877930e-06\n",
      "Epoch 16520/20000: Train Loss = 0.434514, Test Loss = 0.255255, Learning Rate = 1.877216e-06\n",
      "Epoch 16521/20000: Train Loss = 0.434606, Test Loss = 0.255172, Learning Rate = 1.876503e-06\n",
      "Epoch 16522/20000: Train Loss = 0.434522, Test Loss = 0.254458, Learning Rate = 1.875790e-06\n",
      "Epoch 16523/20000: Train Loss = 0.434576, Test Loss = 0.253479, Learning Rate = 1.875077e-06\n",
      "Epoch 16524/20000: Train Loss = 0.434445, Test Loss = 0.253564, Learning Rate = 1.874365e-06\n",
      "Epoch 16525/20000: Train Loss = 0.434485, Test Loss = 0.254622, Learning Rate = 1.873652e-06\n",
      "Epoch 16526/20000: Train Loss = 0.434487, Test Loss = 0.254829, Learning Rate = 1.872940e-06\n",
      "Epoch 16527/20000: Train Loss = 0.434446, Test Loss = 0.254753, Learning Rate = 1.872229e-06\n",
      "Epoch 16528/20000: Train Loss = 0.434752, Test Loss = 0.252869, Learning Rate = 1.871517e-06\n",
      "Epoch 16529/20000: Train Loss = 0.434475, Test Loss = 0.252997, Learning Rate = 1.870806e-06\n",
      "Epoch 16530/20000: Train Loss = 0.434548, Test Loss = 0.254431, Learning Rate = 1.870095e-06\n",
      "Epoch 16531/20000: Train Loss = 0.434704, Test Loss = 0.252040, Learning Rate = 1.869385e-06\n",
      "Epoch 16532/20000: Train Loss = 0.434765, Test Loss = 0.253363, Learning Rate = 1.868674e-06\n",
      "Epoch 16533/20000: Train Loss = 0.434563, Test Loss = 0.254623, Learning Rate = 1.867964e-06\n",
      "Epoch 16534/20000: Train Loss = 0.434515, Test Loss = 0.254380, Learning Rate = 1.867255e-06\n",
      "Epoch 16535/20000: Train Loss = 0.434507, Test Loss = 0.253598, Learning Rate = 1.866545e-06\n",
      "Epoch 16536/20000: Train Loss = 0.434537, Test Loss = 0.252599, Learning Rate = 1.865836e-06\n",
      "Epoch 16537/20000: Train Loss = 0.434620, Test Loss = 0.253741, Learning Rate = 1.865127e-06\n",
      "Epoch 16538/20000: Train Loss = 0.434590, Test Loss = 0.254803, Learning Rate = 1.864418e-06\n",
      "Epoch 16539/20000: Train Loss = 0.434576, Test Loss = 0.254135, Learning Rate = 1.863710e-06\n",
      "Epoch 16540/20000: Train Loss = 0.434483, Test Loss = 0.253902, Learning Rate = 1.863002e-06\n",
      "Epoch 16541/20000: Train Loss = 0.434769, Test Loss = 0.254237, Learning Rate = 1.862294e-06\n",
      "Epoch 16542/20000: Train Loss = 0.434801, Test Loss = 0.253577, Learning Rate = 1.861586e-06\n",
      "Epoch 16543/20000: Train Loss = 0.435451, Test Loss = 0.255192, Learning Rate = 1.860879e-06\n",
      "Epoch 16544/20000: Train Loss = 0.434815, Test Loss = 0.252070, Learning Rate = 1.860172e-06\n",
      "Epoch 16545/20000: Train Loss = 0.434838, Test Loss = 0.253940, Learning Rate = 1.859465e-06\n",
      "Epoch 16546/20000: Train Loss = 0.434732, Test Loss = 0.253562, Learning Rate = 1.858758e-06\n",
      "Epoch 16547/20000: Train Loss = 0.434699, Test Loss = 0.253990, Learning Rate = 1.858052e-06\n",
      "Epoch 16548/20000: Train Loss = 0.434532, Test Loss = 0.254578, Learning Rate = 1.857346e-06\n",
      "Epoch 16549/20000: Train Loss = 0.434555, Test Loss = 0.252868, Learning Rate = 1.856640e-06\n",
      "Epoch 16550/20000: Train Loss = 0.434742, Test Loss = 0.254026, Learning Rate = 1.855935e-06\n",
      "Epoch 16551/20000: Train Loss = 0.434676, Test Loss = 0.252659, Learning Rate = 1.855230e-06\n",
      "Epoch 16552/20000: Train Loss = 0.434732, Test Loss = 0.254412, Learning Rate = 1.854525e-06\n",
      "Epoch 16553/20000: Train Loss = 0.434657, Test Loss = 0.254029, Learning Rate = 1.853820e-06\n",
      "Epoch 16554/20000: Train Loss = 0.434546, Test Loss = 0.253489, Learning Rate = 1.853116e-06\n",
      "Epoch 16555/20000: Train Loss = 0.434472, Test Loss = 0.254059, Learning Rate = 1.852412e-06\n",
      "Epoch 16556/20000: Train Loss = 0.434639, Test Loss = 0.253832, Learning Rate = 1.851708e-06\n",
      "Epoch 16557/20000: Train Loss = 0.434504, Test Loss = 0.253426, Learning Rate = 1.851004e-06\n",
      "Epoch 16558/20000: Train Loss = 0.434613, Test Loss = 0.253214, Learning Rate = 1.850301e-06\n",
      "Epoch 16559/20000: Train Loss = 0.434770, Test Loss = 0.254009, Learning Rate = 1.849598e-06\n",
      "Epoch 16560/20000: Train Loss = 0.434535, Test Loss = 0.253126, Learning Rate = 1.848895e-06\n",
      "Epoch 16561/20000: Train Loss = 0.434509, Test Loss = 0.252153, Learning Rate = 1.848192e-06\n",
      "Epoch 16562/20000: Train Loss = 0.434700, Test Loss = 0.253387, Learning Rate = 1.847490e-06\n",
      "Epoch 16563/20000: Train Loss = 0.434570, Test Loss = 0.252642, Learning Rate = 1.846788e-06\n",
      "Epoch 16564/20000: Train Loss = 0.434569, Test Loss = 0.253421, Learning Rate = 1.846086e-06\n",
      "Epoch 16565/20000: Train Loss = 0.434566, Test Loss = 0.253499, Learning Rate = 1.845385e-06\n",
      "Epoch 16566/20000: Train Loss = 0.434632, Test Loss = 0.253455, Learning Rate = 1.844684e-06\n",
      "Epoch 16567/20000: Train Loss = 0.434443, Test Loss = 0.254630, Learning Rate = 1.843983e-06\n",
      "Epoch 16568/20000: Train Loss = 0.434465, Test Loss = 0.254337, Learning Rate = 1.843282e-06\n",
      "Epoch 16569/20000: Train Loss = 0.434553, Test Loss = 0.255938, Learning Rate = 1.842582e-06\n",
      "Epoch 16570/20000: Train Loss = 0.434543, Test Loss = 0.255569, Learning Rate = 1.841882e-06\n",
      "Epoch 16571/20000: Train Loss = 0.434613, Test Loss = 0.254334, Learning Rate = 1.841182e-06\n",
      "Epoch 16572/20000: Train Loss = 0.434389, Test Loss = 0.254622, Learning Rate = 1.840482e-06\n",
      "Epoch 16573/20000: Train Loss = 0.434627, Test Loss = 0.253935, Learning Rate = 1.839783e-06\n",
      "Epoch 16574/20000: Train Loss = 0.434473, Test Loss = 0.254766, Learning Rate = 1.839084e-06\n",
      "Epoch 16575/20000: Train Loss = 0.434776, Test Loss = 0.253793, Learning Rate = 1.838385e-06\n",
      "Epoch 16576/20000: Train Loss = 0.434812, Test Loss = 0.254341, Learning Rate = 1.837686e-06\n",
      "Epoch 16577/20000: Train Loss = 0.434611, Test Loss = 0.252274, Learning Rate = 1.836988e-06\n",
      "Epoch 16578/20000: Train Loss = 0.434511, Test Loss = 0.253712, Learning Rate = 1.836290e-06\n",
      "Epoch 16579/20000: Train Loss = 0.434448, Test Loss = 0.252961, Learning Rate = 1.835592e-06\n",
      "Epoch 16580/20000: Train Loss = 0.434578, Test Loss = 0.253715, Learning Rate = 1.834895e-06\n",
      "Epoch 16581/20000: Train Loss = 0.434587, Test Loss = 0.252959, Learning Rate = 1.834198e-06\n",
      "Epoch 16582/20000: Train Loss = 0.434621, Test Loss = 0.252869, Learning Rate = 1.833501e-06\n",
      "Epoch 16583/20000: Train Loss = 0.434500, Test Loss = 0.252396, Learning Rate = 1.832804e-06\n",
      "Epoch 16584/20000: Train Loss = 0.434703, Test Loss = 0.253899, Learning Rate = 1.832108e-06\n",
      "Epoch 16585/20000: Train Loss = 0.434560, Test Loss = 0.255739, Learning Rate = 1.831411e-06\n",
      "Epoch 16586/20000: Train Loss = 0.434748, Test Loss = 0.254204, Learning Rate = 1.830716e-06\n",
      "Epoch 16587/20000: Train Loss = 0.434531, Test Loss = 0.253487, Learning Rate = 1.830020e-06\n",
      "Epoch 16588/20000: Train Loss = 0.434539, Test Loss = 0.254320, Learning Rate = 1.829325e-06\n",
      "Epoch 16589/20000: Train Loss = 0.434447, Test Loss = 0.254692, Learning Rate = 1.828629e-06\n",
      "Epoch 16590/20000: Train Loss = 0.434457, Test Loss = 0.255543, Learning Rate = 1.827935e-06\n",
      "Epoch 16591/20000: Train Loss = 0.434629, Test Loss = 0.256801, Learning Rate = 1.827240e-06\n",
      "Epoch 16592/20000: Train Loss = 0.434486, Test Loss = 0.256553, Learning Rate = 1.826546e-06\n",
      "Epoch 16593/20000: Train Loss = 0.434404, Test Loss = 0.255884, Learning Rate = 1.825852e-06\n",
      "Epoch 16594/20000: Train Loss = 0.434625, Test Loss = 0.255874, Learning Rate = 1.825158e-06\n",
      "Epoch 16595/20000: Train Loss = 0.434627, Test Loss = 0.256680, Learning Rate = 1.824464e-06\n",
      "Epoch 16596/20000: Train Loss = 0.434564, Test Loss = 0.256995, Learning Rate = 1.823771e-06\n",
      "Epoch 16597/20000: Train Loss = 0.434560, Test Loss = 0.255385, Learning Rate = 1.823078e-06\n",
      "Epoch 16598/20000: Train Loss = 0.434692, Test Loss = 0.256194, Learning Rate = 1.822386e-06\n",
      "Epoch 16599/20000: Train Loss = 0.434399, Test Loss = 0.254468, Learning Rate = 1.821693e-06\n",
      "Epoch 16600/20000: Train Loss = 0.434530, Test Loss = 0.253919, Learning Rate = 1.821001e-06\n",
      "Epoch 16601/20000: Train Loss = 0.434647, Test Loss = 0.253388, Learning Rate = 1.820309e-06\n",
      "Epoch 16602/20000: Train Loss = 0.434455, Test Loss = 0.254007, Learning Rate = 1.819617e-06\n",
      "Epoch 16603/20000: Train Loss = 0.434517, Test Loss = 0.253666, Learning Rate = 1.818926e-06\n",
      "Epoch 16604/20000: Train Loss = 0.434505, Test Loss = 0.254271, Learning Rate = 1.818235e-06\n",
      "Epoch 16605/20000: Train Loss = 0.434516, Test Loss = 0.252893, Learning Rate = 1.817544e-06\n",
      "Epoch 16606/20000: Train Loss = 0.434441, Test Loss = 0.254574, Learning Rate = 1.816853e-06\n",
      "Epoch 16607/20000: Train Loss = 0.434456, Test Loss = 0.254921, Learning Rate = 1.816163e-06\n",
      "Epoch 16608/20000: Train Loss = 0.434601, Test Loss = 0.253825, Learning Rate = 1.815473e-06\n",
      "Epoch 16609/20000: Train Loss = 0.434782, Test Loss = 0.254855, Learning Rate = 1.814783e-06\n",
      "Epoch 16610/20000: Train Loss = 0.434513, Test Loss = 0.254339, Learning Rate = 1.814093e-06\n",
      "Epoch 16611/20000: Train Loss = 0.434523, Test Loss = 0.254436, Learning Rate = 1.813404e-06\n",
      "Epoch 16612/20000: Train Loss = 0.434454, Test Loss = 0.253811, Learning Rate = 1.812715e-06\n",
      "Epoch 16613/20000: Train Loss = 0.434513, Test Loss = 0.254229, Learning Rate = 1.812026e-06\n",
      "Epoch 16614/20000: Train Loss = 0.434630, Test Loss = 0.252897, Learning Rate = 1.811338e-06\n",
      "Epoch 16615/20000: Train Loss = 0.434633, Test Loss = 0.254642, Learning Rate = 1.810649e-06\n",
      "Epoch 16616/20000: Train Loss = 0.435069, Test Loss = 0.253201, Learning Rate = 1.809961e-06\n",
      "Epoch 16617/20000: Train Loss = 0.434692, Test Loss = 0.254237, Learning Rate = 1.809274e-06\n",
      "Epoch 16618/20000: Train Loss = 0.434612, Test Loss = 0.253439, Learning Rate = 1.808586e-06\n",
      "Epoch 16619/20000: Train Loss = 0.434503, Test Loss = 0.253452, Learning Rate = 1.807899e-06\n",
      "Epoch 16620/20000: Train Loss = 0.434609, Test Loss = 0.253136, Learning Rate = 1.807212e-06\n",
      "Epoch 16621/20000: Train Loss = 0.434875, Test Loss = 0.253413, Learning Rate = 1.806525e-06\n",
      "Epoch 16622/20000: Train Loss = 0.434478, Test Loss = 0.252445, Learning Rate = 1.805839e-06\n",
      "Epoch 16623/20000: Train Loss = 0.434701, Test Loss = 0.252945, Learning Rate = 1.805153e-06\n",
      "Epoch 16624/20000: Train Loss = 0.434687, Test Loss = 0.252935, Learning Rate = 1.804467e-06\n",
      "Epoch 16625/20000: Train Loss = 0.435230, Test Loss = 0.253548, Learning Rate = 1.803781e-06\n",
      "Epoch 16626/20000: Train Loss = 0.434696, Test Loss = 0.253743, Learning Rate = 1.803096e-06\n",
      "Epoch 16627/20000: Train Loss = 0.434663, Test Loss = 0.252034, Learning Rate = 1.802411e-06\n",
      "Epoch 16628/20000: Train Loss = 0.434472, Test Loss = 0.252608, Learning Rate = 1.801726e-06\n",
      "Epoch 16629/20000: Train Loss = 0.434600, Test Loss = 0.253313, Learning Rate = 1.801041e-06\n",
      "Epoch 16630/20000: Train Loss = 0.434553, Test Loss = 0.253305, Learning Rate = 1.800357e-06\n",
      "Epoch 16631/20000: Train Loss = 0.434568, Test Loss = 0.253326, Learning Rate = 1.799673e-06\n",
      "Epoch 16632/20000: Train Loss = 0.434522, Test Loss = 0.252948, Learning Rate = 1.798989e-06\n",
      "Epoch 16633/20000: Train Loss = 0.434564, Test Loss = 0.252038, Learning Rate = 1.798305e-06\n",
      "Epoch 16634/20000: Train Loss = 0.434436, Test Loss = 0.253239, Learning Rate = 1.797622e-06\n",
      "Epoch 16635/20000: Train Loss = 0.434560, Test Loss = 0.253045, Learning Rate = 1.796939e-06\n",
      "Epoch 16636/20000: Train Loss = 0.434509, Test Loss = 0.251357, Learning Rate = 1.796256e-06\n",
      "Epoch 16637/20000: Train Loss = 0.434485, Test Loss = 0.251202, Learning Rate = 1.795574e-06\n",
      "Epoch 16638/20000: Train Loss = 0.434453, Test Loss = 0.250871, Learning Rate = 1.794891e-06\n",
      "Epoch 16639/20000: Train Loss = 0.434699, Test Loss = 0.250947, Learning Rate = 1.794209e-06\n",
      "Epoch 16640/20000: Train Loss = 0.434601, Test Loss = 0.251796, Learning Rate = 1.793528e-06\n",
      "Epoch 16641/20000: Train Loss = 0.434665, Test Loss = 0.250496, Learning Rate = 1.792846e-06\n",
      "Epoch 16642/20000: Train Loss = 0.434734, Test Loss = 0.251885, Learning Rate = 1.792165e-06\n",
      "Epoch 16643/20000: Train Loss = 0.434799, Test Loss = 0.251001, Learning Rate = 1.791484e-06\n",
      "Epoch 16644/20000: Train Loss = 0.434981, Test Loss = 0.250415, Learning Rate = 1.790803e-06\n",
      "Epoch 16645/20000: Train Loss = 0.434591, Test Loss = 0.253016, Learning Rate = 1.790123e-06\n",
      "Epoch 16646/20000: Train Loss = 0.434414, Test Loss = 0.251884, Learning Rate = 1.789443e-06\n",
      "Epoch 16647/20000: Train Loss = 0.434592, Test Loss = 0.250839, Learning Rate = 1.788763e-06\n",
      "Epoch 16648/20000: Train Loss = 0.434452, Test Loss = 0.251275, Learning Rate = 1.788083e-06\n",
      "Epoch 16649/20000: Train Loss = 0.434469, Test Loss = 0.252279, Learning Rate = 1.787404e-06\n",
      "Epoch 16650/20000: Train Loss = 0.434556, Test Loss = 0.253047, Learning Rate = 1.786724e-06\n",
      "Epoch 16651/20000: Train Loss = 0.434514, Test Loss = 0.253343, Learning Rate = 1.786046e-06\n",
      "Epoch 16652/20000: Train Loss = 0.434840, Test Loss = 0.253203, Learning Rate = 1.785367e-06\n",
      "Epoch 16653/20000: Train Loss = 0.434439, Test Loss = 0.252810, Learning Rate = 1.784689e-06\n",
      "Epoch 16654/20000: Train Loss = 0.434417, Test Loss = 0.253920, Learning Rate = 1.784010e-06\n",
      "Epoch 16655/20000: Train Loss = 0.434613, Test Loss = 0.253550, Learning Rate = 1.783332e-06\n",
      "Epoch 16656/20000: Train Loss = 0.434608, Test Loss = 0.254803, Learning Rate = 1.782655e-06\n",
      "Epoch 16657/20000: Train Loss = 0.434801, Test Loss = 0.255169, Learning Rate = 1.781978e-06\n",
      "Epoch 16658/20000: Train Loss = 0.434729, Test Loss = 0.255563, Learning Rate = 1.781300e-06\n",
      "Epoch 16659/20000: Train Loss = 0.434469, Test Loss = 0.253468, Learning Rate = 1.780624e-06\n",
      "Epoch 16660/20000: Train Loss = 0.434485, Test Loss = 0.254591, Learning Rate = 1.779947e-06\n",
      "Epoch 16661/20000: Train Loss = 0.434581, Test Loss = 0.252921, Learning Rate = 1.779271e-06\n",
      "Epoch 16662/20000: Train Loss = 0.434463, Test Loss = 0.252946, Learning Rate = 1.778595e-06\n",
      "Epoch 16663/20000: Train Loss = 0.434577, Test Loss = 0.252570, Learning Rate = 1.777919e-06\n",
      "Epoch 16664/20000: Train Loss = 0.434446, Test Loss = 0.251960, Learning Rate = 1.777243e-06\n",
      "Epoch 16665/20000: Train Loss = 0.434542, Test Loss = 0.252734, Learning Rate = 1.776568e-06\n",
      "Epoch 16666/20000: Train Loss = 0.434643, Test Loss = 0.252271, Learning Rate = 1.775893e-06\n",
      "Epoch 16667/20000: Train Loss = 0.434583, Test Loss = 0.254661, Learning Rate = 1.775218e-06\n",
      "Epoch 16668/20000: Train Loss = 0.434753, Test Loss = 0.254666, Learning Rate = 1.774544e-06\n",
      "Epoch 16669/20000: Train Loss = 0.434454, Test Loss = 0.251993, Learning Rate = 1.773869e-06\n",
      "Epoch 16670/20000: Train Loss = 0.434510, Test Loss = 0.252360, Learning Rate = 1.773195e-06\n",
      "Epoch 16671/20000: Train Loss = 0.434396, Test Loss = 0.251519, Learning Rate = 1.772521e-06\n",
      "Epoch 16672/20000: Train Loss = 0.434613, Test Loss = 0.252264, Learning Rate = 1.771848e-06\n",
      "Epoch 16673/20000: Train Loss = 0.435769, Test Loss = 0.252661, Learning Rate = 1.771175e-06\n",
      "Epoch 16674/20000: Train Loss = 0.434599, Test Loss = 0.253010, Learning Rate = 1.770502e-06\n",
      "Epoch 16675/20000: Train Loss = 0.434842, Test Loss = 0.251388, Learning Rate = 1.769829e-06\n",
      "Epoch 16676/20000: Train Loss = 0.434519, Test Loss = 0.252875, Learning Rate = 1.769156e-06\n",
      "Epoch 16677/20000: Train Loss = 0.435143, Test Loss = 0.252546, Learning Rate = 1.768484e-06\n",
      "Epoch 16678/20000: Train Loss = 0.434895, Test Loss = 0.252280, Learning Rate = 1.767812e-06\n",
      "Epoch 16679/20000: Train Loss = 0.434670, Test Loss = 0.252427, Learning Rate = 1.767141e-06\n",
      "Epoch 16680/20000: Train Loss = 0.434633, Test Loss = 0.252625, Learning Rate = 1.766469e-06\n",
      "Epoch 16681/20000: Train Loss = 0.434524, Test Loss = 0.250687, Learning Rate = 1.765798e-06\n",
      "Epoch 16682/20000: Train Loss = 0.435397, Test Loss = 0.252728, Learning Rate = 1.765127e-06\n",
      "Epoch 16683/20000: Train Loss = 0.434611, Test Loss = 0.252142, Learning Rate = 1.764456e-06\n",
      "Epoch 16684/20000: Train Loss = 0.434511, Test Loss = 0.252051, Learning Rate = 1.763786e-06\n",
      "Epoch 16685/20000: Train Loss = 0.434457, Test Loss = 0.251131, Learning Rate = 1.763116e-06\n",
      "Epoch 16686/20000: Train Loss = 0.434497, Test Loss = 0.251782, Learning Rate = 1.762446e-06\n",
      "Epoch 16687/20000: Train Loss = 0.434405, Test Loss = 0.252088, Learning Rate = 1.761776e-06\n",
      "Epoch 16688/20000: Train Loss = 0.434717, Test Loss = 0.252457, Learning Rate = 1.761107e-06\n",
      "Epoch 16689/20000: Train Loss = 0.434589, Test Loss = 0.253619, Learning Rate = 1.760437e-06\n",
      "Epoch 16690/20000: Train Loss = 0.434594, Test Loss = 0.251952, Learning Rate = 1.759768e-06\n",
      "Epoch 16691/20000: Train Loss = 0.434547, Test Loss = 0.252694, Learning Rate = 1.759100e-06\n",
      "Epoch 16692/20000: Train Loss = 0.434650, Test Loss = 0.252773, Learning Rate = 1.758431e-06\n",
      "Epoch 16693/20000: Train Loss = 0.434514, Test Loss = 0.253225, Learning Rate = 1.757763e-06\n",
      "Epoch 16694/20000: Train Loss = 0.434440, Test Loss = 0.252092, Learning Rate = 1.757095e-06\n",
      "Epoch 16695/20000: Train Loss = 0.434580, Test Loss = 0.252080, Learning Rate = 1.756428e-06\n",
      "Epoch 16696/20000: Train Loss = 0.435422, Test Loss = 0.253259, Learning Rate = 1.755760e-06\n",
      "Epoch 16697/20000: Train Loss = 0.434542, Test Loss = 0.252536, Learning Rate = 1.755093e-06\n",
      "Epoch 16698/20000: Train Loss = 0.434442, Test Loss = 0.252492, Learning Rate = 1.754426e-06\n",
      "Epoch 16699/20000: Train Loss = 0.434519, Test Loss = 0.253816, Learning Rate = 1.753760e-06\n",
      "Epoch 16700/20000: Train Loss = 0.434401, Test Loss = 0.252835, Learning Rate = 1.753093e-06\n",
      "Epoch 16701/20000: Train Loss = 0.434659, Test Loss = 0.252531, Learning Rate = 1.752427e-06\n",
      "Epoch 16702/20000: Train Loss = 0.434649, Test Loss = 0.251470, Learning Rate = 1.751761e-06\n",
      "Epoch 16703/20000: Train Loss = 0.434446, Test Loss = 0.252770, Learning Rate = 1.751096e-06\n",
      "Epoch 16704/20000: Train Loss = 0.434453, Test Loss = 0.252371, Learning Rate = 1.750430e-06\n",
      "Epoch 16705/20000: Train Loss = 0.434524, Test Loss = 0.252330, Learning Rate = 1.749765e-06\n",
      "Epoch 16706/20000: Train Loss = 0.434486, Test Loss = 0.251271, Learning Rate = 1.749100e-06\n",
      "Epoch 16707/20000: Train Loss = 0.434555, Test Loss = 0.252422, Learning Rate = 1.748436e-06\n",
      "Epoch 16708/20000: Train Loss = 0.434604, Test Loss = 0.251565, Learning Rate = 1.747771e-06\n",
      "Epoch 16709/20000: Train Loss = 0.434411, Test Loss = 0.251375, Learning Rate = 1.747107e-06\n",
      "Epoch 16710/20000: Train Loss = 0.434702, Test Loss = 0.251716, Learning Rate = 1.746443e-06\n",
      "Epoch 16711/20000: Train Loss = 0.434536, Test Loss = 0.250852, Learning Rate = 1.745780e-06\n",
      "Epoch 16712/20000: Train Loss = 0.434527, Test Loss = 0.250390, Learning Rate = 1.745116e-06\n",
      "Epoch 16713/20000: Train Loss = 0.434739, Test Loss = 0.250432, Learning Rate = 1.744453e-06\n",
      "Epoch 16714/20000: Train Loss = 0.434713, Test Loss = 0.250308, Learning Rate = 1.743790e-06\n",
      "Epoch 16715/20000: Train Loss = 0.434435, Test Loss = 0.250934, Learning Rate = 1.743128e-06\n",
      "Epoch 16716/20000: Train Loss = 0.434472, Test Loss = 0.251271, Learning Rate = 1.742465e-06\n",
      "Epoch 16717/20000: Train Loss = 0.434427, Test Loss = 0.251759, Learning Rate = 1.741803e-06\n",
      "Epoch 16718/20000: Train Loss = 0.434507, Test Loss = 0.250168, Learning Rate = 1.741142e-06\n",
      "Epoch 16719/20000: Train Loss = 0.434512, Test Loss = 0.249837, Learning Rate = 1.740480e-06\n",
      "Epoch 16720/20000: Train Loss = 0.434556, Test Loss = 0.251072, Learning Rate = 1.739819e-06\n",
      "Epoch 16721/20000: Train Loss = 0.434588, Test Loss = 0.250742, Learning Rate = 1.739158e-06\n",
      "Epoch 16722/20000: Train Loss = 0.434428, Test Loss = 0.250651, Learning Rate = 1.738497e-06\n",
      "Epoch 16723/20000: Train Loss = 0.434515, Test Loss = 0.250580, Learning Rate = 1.737836e-06\n",
      "Epoch 16724/20000: Train Loss = 0.434694, Test Loss = 0.251890, Learning Rate = 1.737176e-06\n",
      "Epoch 16725/20000: Train Loss = 0.434493, Test Loss = 0.252150, Learning Rate = 1.736516e-06\n",
      "Epoch 16726/20000: Train Loss = 0.434576, Test Loss = 0.250812, Learning Rate = 1.735856e-06\n",
      "Epoch 16727/20000: Train Loss = 0.434643, Test Loss = 0.251193, Learning Rate = 1.735196e-06\n",
      "Epoch 16728/20000: Train Loss = 0.434447, Test Loss = 0.252545, Learning Rate = 1.734537e-06\n",
      "Epoch 16729/20000: Train Loss = 0.434735, Test Loss = 0.251667, Learning Rate = 1.733878e-06\n",
      "Epoch 16730/20000: Train Loss = 0.434778, Test Loss = 0.252349, Learning Rate = 1.733219e-06\n",
      "Epoch 16731/20000: Train Loss = 0.434834, Test Loss = 0.252955, Learning Rate = 1.732560e-06\n",
      "Epoch 16732/20000: Train Loss = 0.434558, Test Loss = 0.250813, Learning Rate = 1.731902e-06\n",
      "Epoch 16733/20000: Train Loss = 0.434683, Test Loss = 0.252225, Learning Rate = 1.731244e-06\n",
      "Epoch 16734/20000: Train Loss = 0.434439, Test Loss = 0.252027, Learning Rate = 1.730586e-06\n",
      "Epoch 16735/20000: Train Loss = 0.434600, Test Loss = 0.252805, Learning Rate = 1.729929e-06\n",
      "Epoch 16736/20000: Train Loss = 0.434544, Test Loss = 0.251840, Learning Rate = 1.729271e-06\n",
      "Epoch 16737/20000: Train Loss = 0.434465, Test Loss = 0.252503, Learning Rate = 1.728614e-06\n",
      "Epoch 16738/20000: Train Loss = 0.434527, Test Loss = 0.251605, Learning Rate = 1.727957e-06\n",
      "Epoch 16739/20000: Train Loss = 0.434428, Test Loss = 0.252671, Learning Rate = 1.727301e-06\n",
      "Epoch 16740/20000: Train Loss = 0.434502, Test Loss = 0.253311, Learning Rate = 1.726645e-06\n",
      "Epoch 16741/20000: Train Loss = 0.434386, Test Loss = 0.253541, Learning Rate = 1.725988e-06\n",
      "Epoch 16742/20000: Train Loss = 0.435037, Test Loss = 0.254286, Learning Rate = 1.725333e-06\n",
      "Epoch 16743/20000: Train Loss = 0.434785, Test Loss = 0.252668, Learning Rate = 1.724677e-06\n",
      "Epoch 16744/20000: Train Loss = 0.434402, Test Loss = 0.253207, Learning Rate = 1.724022e-06\n",
      "Epoch 16745/20000: Train Loss = 0.434417, Test Loss = 0.252338, Learning Rate = 1.723367e-06\n",
      "Epoch 16746/20000: Train Loss = 0.434497, Test Loss = 0.252823, Learning Rate = 1.722712e-06\n",
      "Epoch 16747/20000: Train Loss = 0.434851, Test Loss = 0.254271, Learning Rate = 1.722057e-06\n",
      "Epoch 16748/20000: Train Loss = 0.434769, Test Loss = 0.252362, Learning Rate = 1.721403e-06\n",
      "Epoch 16749/20000: Train Loss = 0.434485, Test Loss = 0.253190, Learning Rate = 1.720749e-06\n",
      "Epoch 16750/20000: Train Loss = 0.434407, Test Loss = 0.253376, Learning Rate = 1.720095e-06\n",
      "Epoch 16751/20000: Train Loss = 0.434478, Test Loss = 0.253306, Learning Rate = 1.719441e-06\n",
      "Epoch 16752/20000: Train Loss = 0.434573, Test Loss = 0.254051, Learning Rate = 1.718788e-06\n",
      "Epoch 16753/20000: Train Loss = 0.434501, Test Loss = 0.253391, Learning Rate = 1.718135e-06\n",
      "Epoch 16754/20000: Train Loss = 0.434587, Test Loss = 0.253131, Learning Rate = 1.717482e-06\n",
      "Epoch 16755/20000: Train Loss = 0.434397, Test Loss = 0.253222, Learning Rate = 1.716830e-06\n",
      "Epoch 16756/20000: Train Loss = 0.434419, Test Loss = 0.253315, Learning Rate = 1.716177e-06\n",
      "Epoch 16757/20000: Train Loss = 0.434404, Test Loss = 0.252757, Learning Rate = 1.715525e-06\n",
      "Epoch 16758/20000: Train Loss = 0.434823, Test Loss = 0.253510, Learning Rate = 1.714873e-06\n",
      "Epoch 16759/20000: Train Loss = 0.434916, Test Loss = 0.254064, Learning Rate = 1.714222e-06\n",
      "Epoch 16760/20000: Train Loss = 0.434243, Test Loss = 0.252417, Learning Rate = 1.713570e-06\n",
      "Epoch 16761/20000: Train Loss = 0.434371, Test Loss = 0.253522, Learning Rate = 1.712919e-06\n",
      "Epoch 16762/20000: Train Loss = 0.434495, Test Loss = 0.253635, Learning Rate = 1.712268e-06\n",
      "Epoch 16763/20000: Train Loss = 0.434615, Test Loss = 0.252958, Learning Rate = 1.711618e-06\n",
      "Epoch 16764/20000: Train Loss = 0.434646, Test Loss = 0.252240, Learning Rate = 1.710967e-06\n",
      "Epoch 16765/20000: Train Loss = 0.434883, Test Loss = 0.252851, Learning Rate = 1.710317e-06\n",
      "Epoch 16766/20000: Train Loss = 0.434711, Test Loss = 0.252227, Learning Rate = 1.709667e-06\n",
      "Epoch 16767/20000: Train Loss = 0.434649, Test Loss = 0.252201, Learning Rate = 1.709018e-06\n",
      "Epoch 16768/20000: Train Loss = 0.434424, Test Loss = 0.251520, Learning Rate = 1.708368e-06\n",
      "Epoch 16769/20000: Train Loss = 0.434415, Test Loss = 0.251951, Learning Rate = 1.707719e-06\n",
      "Epoch 16770/20000: Train Loss = 0.434575, Test Loss = 0.252838, Learning Rate = 1.707070e-06\n",
      "Epoch 16771/20000: Train Loss = 0.434553, Test Loss = 0.250920, Learning Rate = 1.706422e-06\n",
      "Epoch 16772/20000: Train Loss = 0.434404, Test Loss = 0.253348, Learning Rate = 1.705773e-06\n",
      "Epoch 16773/20000: Train Loss = 0.434486, Test Loss = 0.253118, Learning Rate = 1.705125e-06\n",
      "Epoch 16774/20000: Train Loss = 0.434653, Test Loss = 0.254109, Learning Rate = 1.704477e-06\n",
      "Epoch 16775/20000: Train Loss = 0.434478, Test Loss = 0.253088, Learning Rate = 1.703830e-06\n",
      "Epoch 16776/20000: Train Loss = 0.434466, Test Loss = 0.252473, Learning Rate = 1.703182e-06\n",
      "Epoch 16777/20000: Train Loss = 0.434437, Test Loss = 0.253213, Learning Rate = 1.702535e-06\n",
      "Epoch 16778/20000: Train Loss = 0.434437, Test Loss = 0.252819, Learning Rate = 1.701888e-06\n",
      "Epoch 16779/20000: Train Loss = 0.434505, Test Loss = 0.252071, Learning Rate = 1.701241e-06\n",
      "Epoch 16780/20000: Train Loss = 0.434385, Test Loss = 0.252593, Learning Rate = 1.700595e-06\n",
      "Epoch 16781/20000: Train Loss = 0.434868, Test Loss = 0.253916, Learning Rate = 1.699949e-06\n",
      "Epoch 16782/20000: Train Loss = 0.434458, Test Loss = 0.252042, Learning Rate = 1.699303e-06\n",
      "Epoch 16783/20000: Train Loss = 0.434407, Test Loss = 0.252650, Learning Rate = 1.698657e-06\n",
      "Epoch 16784/20000: Train Loss = 0.434618, Test Loss = 0.252463, Learning Rate = 1.698012e-06\n",
      "Epoch 16785/20000: Train Loss = 0.434455, Test Loss = 0.252531, Learning Rate = 1.697367e-06\n",
      "Epoch 16786/20000: Train Loss = 0.435231, Test Loss = 0.252226, Learning Rate = 1.696722e-06\n",
      "Epoch 16787/20000: Train Loss = 0.434597, Test Loss = 0.252927, Learning Rate = 1.696077e-06\n",
      "Epoch 16788/20000: Train Loss = 0.434553, Test Loss = 0.252701, Learning Rate = 1.695432e-06\n",
      "Epoch 16789/20000: Train Loss = 0.434511, Test Loss = 0.253310, Learning Rate = 1.694788e-06\n",
      "Epoch 16790/20000: Train Loss = 0.434524, Test Loss = 0.253347, Learning Rate = 1.694144e-06\n",
      "Epoch 16791/20000: Train Loss = 0.434437, Test Loss = 0.252891, Learning Rate = 1.693500e-06\n",
      "Epoch 16792/20000: Train Loss = 0.434607, Test Loss = 0.252792, Learning Rate = 1.692857e-06\n",
      "Epoch 16793/20000: Train Loss = 0.434550, Test Loss = 0.253141, Learning Rate = 1.692214e-06\n",
      "Epoch 16794/20000: Train Loss = 0.434510, Test Loss = 0.253942, Learning Rate = 1.691571e-06\n",
      "Epoch 16795/20000: Train Loss = 0.434399, Test Loss = 0.253083, Learning Rate = 1.690928e-06\n",
      "Epoch 16796/20000: Train Loss = 0.434492, Test Loss = 0.252979, Learning Rate = 1.690285e-06\n",
      "Epoch 16797/20000: Train Loss = 0.434598, Test Loss = 0.252541, Learning Rate = 1.689643e-06\n",
      "Epoch 16798/20000: Train Loss = 0.434400, Test Loss = 0.252501, Learning Rate = 1.689001e-06\n",
      "Epoch 16799/20000: Train Loss = 0.434412, Test Loss = 0.251867, Learning Rate = 1.688359e-06\n",
      "Epoch 16800/20000: Train Loss = 0.434814, Test Loss = 0.250911, Learning Rate = 1.687718e-06\n",
      "Epoch 16801/20000: Train Loss = 0.434655, Test Loss = 0.251177, Learning Rate = 1.687077e-06\n",
      "Epoch 16802/20000: Train Loss = 0.434595, Test Loss = 0.251356, Learning Rate = 1.686436e-06\n",
      "Epoch 16803/20000: Train Loss = 0.434372, Test Loss = 0.252839, Learning Rate = 1.685795e-06\n",
      "Epoch 16804/20000: Train Loss = 0.434628, Test Loss = 0.252152, Learning Rate = 1.685154e-06\n",
      "Epoch 16805/20000: Train Loss = 0.434505, Test Loss = 0.251507, Learning Rate = 1.684514e-06\n",
      "Epoch 16806/20000: Train Loss = 0.434665, Test Loss = 0.250724, Learning Rate = 1.683874e-06\n",
      "Epoch 16807/20000: Train Loss = 0.434558, Test Loss = 0.251586, Learning Rate = 1.683234e-06\n",
      "Epoch 16808/20000: Train Loss = 0.434436, Test Loss = 0.251337, Learning Rate = 1.682594e-06\n",
      "Epoch 16809/20000: Train Loss = 0.434692, Test Loss = 0.250451, Learning Rate = 1.681955e-06\n",
      "Epoch 16810/20000: Train Loss = 0.434681, Test Loss = 0.250441, Learning Rate = 1.681316e-06\n",
      "Epoch 16811/20000: Train Loss = 0.434659, Test Loss = 0.250676, Learning Rate = 1.680677e-06\n",
      "Epoch 16812/20000: Train Loss = 0.434691, Test Loss = 0.252007, Learning Rate = 1.680039e-06\n",
      "Epoch 16813/20000: Train Loss = 0.434384, Test Loss = 0.251016, Learning Rate = 1.679400e-06\n",
      "Epoch 16814/20000: Train Loss = 0.434519, Test Loss = 0.250426, Learning Rate = 1.678762e-06\n",
      "Epoch 16815/20000: Train Loss = 0.434431, Test Loss = 0.250853, Learning Rate = 1.678124e-06\n",
      "Epoch 16816/20000: Train Loss = 0.434471, Test Loss = 0.250596, Learning Rate = 1.677486e-06\n",
      "Epoch 16817/20000: Train Loss = 0.434530, Test Loss = 0.250222, Learning Rate = 1.676849e-06\n",
      "Epoch 16818/20000: Train Loss = 0.434492, Test Loss = 0.249671, Learning Rate = 1.676212e-06\n",
      "Epoch 16819/20000: Train Loss = 0.434740, Test Loss = 0.251864, Learning Rate = 1.675575e-06\n",
      "Epoch 16820/20000: Train Loss = 0.434506, Test Loss = 0.249831, Learning Rate = 1.674938e-06\n",
      "Epoch 16821/20000: Train Loss = 0.434445, Test Loss = 0.251297, Learning Rate = 1.674302e-06\n",
      "Epoch 16822/20000: Train Loss = 0.434467, Test Loss = 0.252462, Learning Rate = 1.673666e-06\n",
      "Epoch 16823/20000: Train Loss = 0.434473, Test Loss = 0.251864, Learning Rate = 1.673030e-06\n",
      "Epoch 16824/20000: Train Loss = 0.434550, Test Loss = 0.252280, Learning Rate = 1.672394e-06\n",
      "Epoch 16825/20000: Train Loss = 0.434399, Test Loss = 0.251090, Learning Rate = 1.671759e-06\n",
      "Epoch 16826/20000: Train Loss = 0.434527, Test Loss = 0.251284, Learning Rate = 1.671123e-06\n",
      "Epoch 16827/20000: Train Loss = 0.434622, Test Loss = 0.253141, Learning Rate = 1.670488e-06\n",
      "Epoch 16828/20000: Train Loss = 0.434563, Test Loss = 0.251863, Learning Rate = 1.669854e-06\n",
      "Epoch 16829/20000: Train Loss = 0.434674, Test Loss = 0.252032, Learning Rate = 1.669219e-06\n",
      "Epoch 16830/20000: Train Loss = 0.434377, Test Loss = 0.251912, Learning Rate = 1.668585e-06\n",
      "Epoch 16831/20000: Train Loss = 0.434586, Test Loss = 0.251999, Learning Rate = 1.667951e-06\n",
      "Epoch 16832/20000: Train Loss = 0.434389, Test Loss = 0.252884, Learning Rate = 1.667317e-06\n",
      "Epoch 16833/20000: Train Loss = 0.434561, Test Loss = 0.252344, Learning Rate = 1.666684e-06\n",
      "Epoch 16834/20000: Train Loss = 0.434375, Test Loss = 0.252563, Learning Rate = 1.666050e-06\n",
      "Epoch 16835/20000: Train Loss = 0.434537, Test Loss = 0.250969, Learning Rate = 1.665417e-06\n",
      "Epoch 16836/20000: Train Loss = 0.434361, Test Loss = 0.251381, Learning Rate = 1.664784e-06\n",
      "Epoch 16837/20000: Train Loss = 0.434541, Test Loss = 0.252496, Learning Rate = 1.664152e-06\n",
      "Epoch 16838/20000: Train Loss = 0.434601, Test Loss = 0.253513, Learning Rate = 1.663520e-06\n",
      "Epoch 16839/20000: Train Loss = 0.434444, Test Loss = 0.253368, Learning Rate = 1.662887e-06\n",
      "Epoch 16840/20000: Train Loss = 0.434580, Test Loss = 0.251156, Learning Rate = 1.662256e-06\n",
      "Epoch 16841/20000: Train Loss = 0.434569, Test Loss = 0.250572, Learning Rate = 1.661624e-06\n",
      "Epoch 16842/20000: Train Loss = 0.434424, Test Loss = 0.251568, Learning Rate = 1.660993e-06\n",
      "Epoch 16843/20000: Train Loss = 0.434483, Test Loss = 0.252722, Learning Rate = 1.660361e-06\n",
      "Epoch 16844/20000: Train Loss = 0.434735, Test Loss = 0.253688, Learning Rate = 1.659731e-06\n",
      "Epoch 16845/20000: Train Loss = 0.434565, Test Loss = 0.251982, Learning Rate = 1.659100e-06\n",
      "Epoch 16846/20000: Train Loss = 0.434485, Test Loss = 0.253245, Learning Rate = 1.658469e-06\n",
      "Epoch 16847/20000: Train Loss = 0.434662, Test Loss = 0.251430, Learning Rate = 1.657839e-06\n",
      "Epoch 16848/20000: Train Loss = 0.434564, Test Loss = 0.252648, Learning Rate = 1.657209e-06\n",
      "Epoch 16849/20000: Train Loss = 0.434586, Test Loss = 0.252532, Learning Rate = 1.656580e-06\n",
      "Epoch 16850/20000: Train Loss = 0.434418, Test Loss = 0.252556, Learning Rate = 1.655950e-06\n",
      "Epoch 16851/20000: Train Loss = 0.434393, Test Loss = 0.252268, Learning Rate = 1.655321e-06\n",
      "Epoch 16852/20000: Train Loss = 0.434638, Test Loss = 0.251781, Learning Rate = 1.654692e-06\n",
      "Epoch 16853/20000: Train Loss = 0.434403, Test Loss = 0.250894, Learning Rate = 1.654063e-06\n",
      "Epoch 16854/20000: Train Loss = 0.434513, Test Loss = 0.252885, Learning Rate = 1.653435e-06\n",
      "Epoch 16855/20000: Train Loss = 0.434425, Test Loss = 0.251619, Learning Rate = 1.652807e-06\n",
      "Epoch 16856/20000: Train Loss = 0.434623, Test Loss = 0.251638, Learning Rate = 1.652179e-06\n",
      "Epoch 16857/20000: Train Loss = 0.434493, Test Loss = 0.252513, Learning Rate = 1.651551e-06\n",
      "Epoch 16858/20000: Train Loss = 0.434465, Test Loss = 0.251288, Learning Rate = 1.650923e-06\n",
      "Epoch 16859/20000: Train Loss = 0.434395, Test Loss = 0.251274, Learning Rate = 1.650296e-06\n",
      "Epoch 16860/20000: Train Loss = 0.434811, Test Loss = 0.251800, Learning Rate = 1.649669e-06\n",
      "Epoch 16861/20000: Train Loss = 0.435597, Test Loss = 0.253956, Learning Rate = 1.649042e-06\n",
      "Epoch 16862/20000: Train Loss = 0.434770, Test Loss = 0.251173, Learning Rate = 1.648415e-06\n",
      "Epoch 16863/20000: Train Loss = 0.434430, Test Loss = 0.251706, Learning Rate = 1.647789e-06\n",
      "Epoch 16864/20000: Train Loss = 0.434392, Test Loss = 0.251453, Learning Rate = 1.647163e-06\n",
      "Epoch 16865/20000: Train Loss = 0.434467, Test Loss = 0.251008, Learning Rate = 1.646537e-06\n",
      "Epoch 16866/20000: Train Loss = 0.434664, Test Loss = 0.251541, Learning Rate = 1.645911e-06\n",
      "Epoch 16867/20000: Train Loss = 0.434541, Test Loss = 0.253155, Learning Rate = 1.645286e-06\n",
      "Epoch 16868/20000: Train Loss = 0.434435, Test Loss = 0.251871, Learning Rate = 1.644661e-06\n",
      "Epoch 16869/20000: Train Loss = 0.434419, Test Loss = 0.252683, Learning Rate = 1.644036e-06\n",
      "Epoch 16870/20000: Train Loss = 0.434478, Test Loss = 0.252083, Learning Rate = 1.643411e-06\n",
      "Epoch 16871/20000: Train Loss = 0.434779, Test Loss = 0.251967, Learning Rate = 1.642787e-06\n",
      "Epoch 16872/20000: Train Loss = 0.434373, Test Loss = 0.252504, Learning Rate = 1.642163e-06\n",
      "Epoch 16873/20000: Train Loss = 0.434616, Test Loss = 0.252652, Learning Rate = 1.641539e-06\n",
      "Epoch 16874/20000: Train Loss = 0.434569, Test Loss = 0.253854, Learning Rate = 1.640915e-06\n",
      "Epoch 16875/20000: Train Loss = 0.434412, Test Loss = 0.253439, Learning Rate = 1.640291e-06\n",
      "Epoch 16876/20000: Train Loss = 0.434390, Test Loss = 0.252645, Learning Rate = 1.639668e-06\n",
      "Epoch 16877/20000: Train Loss = 0.434492, Test Loss = 0.252261, Learning Rate = 1.639045e-06\n",
      "Epoch 16878/20000: Train Loss = 0.434371, Test Loss = 0.253302, Learning Rate = 1.638422e-06\n",
      "Epoch 16879/20000: Train Loss = 0.434430, Test Loss = 0.254070, Learning Rate = 1.637800e-06\n",
      "Epoch 16880/20000: Train Loss = 0.434377, Test Loss = 0.253707, Learning Rate = 1.637177e-06\n",
      "Epoch 16881/20000: Train Loss = 0.434422, Test Loss = 0.253702, Learning Rate = 1.636555e-06\n",
      "Epoch 16882/20000: Train Loss = 0.434534, Test Loss = 0.254355, Learning Rate = 1.635933e-06\n",
      "Epoch 16883/20000: Train Loss = 0.434443, Test Loss = 0.254682, Learning Rate = 1.635312e-06\n",
      "Epoch 16884/20000: Train Loss = 0.434436, Test Loss = 0.254394, Learning Rate = 1.634690e-06\n",
      "Epoch 16885/20000: Train Loss = 0.434582, Test Loss = 0.254751, Learning Rate = 1.634069e-06\n",
      "Epoch 16886/20000: Train Loss = 0.434376, Test Loss = 0.254997, Learning Rate = 1.633448e-06\n",
      "Epoch 16887/20000: Train Loss = 0.434431, Test Loss = 0.254791, Learning Rate = 1.632828e-06\n",
      "Epoch 16888/20000: Train Loss = 0.434540, Test Loss = 0.254603, Learning Rate = 1.632207e-06\n",
      "Epoch 16889/20000: Train Loss = 0.434771, Test Loss = 0.253520, Learning Rate = 1.631587e-06\n",
      "Epoch 16890/20000: Train Loss = 0.434424, Test Loss = 0.254163, Learning Rate = 1.630967e-06\n",
      "Epoch 16891/20000: Train Loss = 0.434541, Test Loss = 0.253626, Learning Rate = 1.630347e-06\n",
      "Epoch 16892/20000: Train Loss = 0.434405, Test Loss = 0.253113, Learning Rate = 1.629728e-06\n",
      "Epoch 16893/20000: Train Loss = 0.434760, Test Loss = 0.254457, Learning Rate = 1.629109e-06\n",
      "Epoch 16894/20000: Train Loss = 0.434466, Test Loss = 0.252439, Learning Rate = 1.628490e-06\n",
      "Epoch 16895/20000: Train Loss = 0.434833, Test Loss = 0.252609, Learning Rate = 1.627871e-06\n",
      "Epoch 16896/20000: Train Loss = 0.434547, Test Loss = 0.253212, Learning Rate = 1.627252e-06\n",
      "Epoch 16897/20000: Train Loss = 0.434661, Test Loss = 0.253122, Learning Rate = 1.626634e-06\n",
      "Epoch 16898/20000: Train Loss = 0.434506, Test Loss = 0.251948, Learning Rate = 1.626016e-06\n",
      "Epoch 16899/20000: Train Loss = 0.434442, Test Loss = 0.252338, Learning Rate = 1.625398e-06\n",
      "Epoch 16900/20000: Train Loss = 0.434422, Test Loss = 0.251306, Learning Rate = 1.624781e-06\n",
      "Epoch 16901/20000: Train Loss = 0.434481, Test Loss = 0.251715, Learning Rate = 1.624163e-06\n",
      "Epoch 16902/20000: Train Loss = 0.434493, Test Loss = 0.252753, Learning Rate = 1.623546e-06\n",
      "Epoch 16903/20000: Train Loss = 0.434651, Test Loss = 0.250899, Learning Rate = 1.622929e-06\n",
      "Epoch 16904/20000: Train Loss = 0.434531, Test Loss = 0.252268, Learning Rate = 1.622312e-06\n",
      "Epoch 16905/20000: Train Loss = 0.434419, Test Loss = 0.252411, Learning Rate = 1.621696e-06\n",
      "Epoch 16906/20000: Train Loss = 0.434572, Test Loss = 0.252846, Learning Rate = 1.621080e-06\n",
      "Epoch 16907/20000: Train Loss = 0.434435, Test Loss = 0.252146, Learning Rate = 1.620464e-06\n",
      "Epoch 16908/20000: Train Loss = 0.434395, Test Loss = 0.251177, Learning Rate = 1.619848e-06\n",
      "Epoch 16909/20000: Train Loss = 0.434384, Test Loss = 0.251418, Learning Rate = 1.619233e-06\n",
      "Epoch 16910/20000: Train Loss = 0.434417, Test Loss = 0.250710, Learning Rate = 1.618617e-06\n",
      "Epoch 16911/20000: Train Loss = 0.434692, Test Loss = 0.250010, Learning Rate = 1.618002e-06\n",
      "Epoch 16912/20000: Train Loss = 0.434866, Test Loss = 0.251995, Learning Rate = 1.617388e-06\n",
      "Epoch 16913/20000: Train Loss = 0.434521, Test Loss = 0.252453, Learning Rate = 1.616773e-06\n",
      "Epoch 16914/20000: Train Loss = 0.434444, Test Loss = 0.251786, Learning Rate = 1.616159e-06\n",
      "Epoch 16915/20000: Train Loss = 0.434438, Test Loss = 0.252252, Learning Rate = 1.615545e-06\n",
      "Epoch 16916/20000: Train Loss = 0.434555, Test Loss = 0.252079, Learning Rate = 1.614931e-06\n",
      "Epoch 16917/20000: Train Loss = 0.434797, Test Loss = 0.251293, Learning Rate = 1.614317e-06\n",
      "Epoch 16918/20000: Train Loss = 0.434445, Test Loss = 0.252070, Learning Rate = 1.613704e-06\n",
      "Epoch 16919/20000: Train Loss = 0.434332, Test Loss = 0.252170, Learning Rate = 1.613090e-06\n",
      "Epoch 16920/20000: Train Loss = 0.434479, Test Loss = 0.252395, Learning Rate = 1.612478e-06\n",
      "Epoch 16921/20000: Train Loss = 0.434482, Test Loss = 0.252288, Learning Rate = 1.611865e-06\n",
      "Epoch 16922/20000: Train Loss = 0.434326, Test Loss = 0.252456, Learning Rate = 1.611252e-06\n",
      "Epoch 16923/20000: Train Loss = 0.434328, Test Loss = 0.252876, Learning Rate = 1.610640e-06\n",
      "Epoch 16924/20000: Train Loss = 0.434573, Test Loss = 0.253366, Learning Rate = 1.610028e-06\n",
      "Epoch 16925/20000: Train Loss = 0.434969, Test Loss = 0.252055, Learning Rate = 1.609416e-06\n",
      "Epoch 16926/20000: Train Loss = 0.434542, Test Loss = 0.252146, Learning Rate = 1.608805e-06\n",
      "Epoch 16927/20000: Train Loss = 0.434417, Test Loss = 0.252661, Learning Rate = 1.608194e-06\n",
      "Epoch 16928/20000: Train Loss = 0.434411, Test Loss = 0.253069, Learning Rate = 1.607582e-06\n",
      "Epoch 16929/20000: Train Loss = 0.434484, Test Loss = 0.253996, Learning Rate = 1.606972e-06\n",
      "Epoch 16930/20000: Train Loss = 0.434558, Test Loss = 0.252560, Learning Rate = 1.606361e-06\n",
      "Epoch 16931/20000: Train Loss = 0.434522, Test Loss = 0.251882, Learning Rate = 1.605751e-06\n",
      "Epoch 16932/20000: Train Loss = 0.434438, Test Loss = 0.251507, Learning Rate = 1.605141e-06\n",
      "Epoch 16933/20000: Train Loss = 0.434359, Test Loss = 0.252051, Learning Rate = 1.604531e-06\n",
      "Epoch 16934/20000: Train Loss = 0.434632, Test Loss = 0.252677, Learning Rate = 1.603921e-06\n",
      "Epoch 16935/20000: Train Loss = 0.434529, Test Loss = 0.251748, Learning Rate = 1.603311e-06\n",
      "Epoch 16936/20000: Train Loss = 0.434697, Test Loss = 0.253950, Learning Rate = 1.602702e-06\n",
      "Epoch 16937/20000: Train Loss = 0.434415, Test Loss = 0.253037, Learning Rate = 1.602093e-06\n",
      "Epoch 16938/20000: Train Loss = 0.434491, Test Loss = 0.253582, Learning Rate = 1.601485e-06\n",
      "Epoch 16939/20000: Train Loss = 0.434714, Test Loss = 0.252085, Learning Rate = 1.600876e-06\n",
      "Epoch 16940/20000: Train Loss = 0.434445, Test Loss = 0.252836, Learning Rate = 1.600268e-06\n",
      "Epoch 16941/20000: Train Loss = 0.435431, Test Loss = 0.253479, Learning Rate = 1.599660e-06\n",
      "Epoch 16942/20000: Train Loss = 0.434454, Test Loss = 0.252712, Learning Rate = 1.599052e-06\n",
      "Epoch 16943/20000: Train Loss = 0.435134, Test Loss = 0.250871, Learning Rate = 1.598444e-06\n",
      "Epoch 16944/20000: Train Loss = 0.434865, Test Loss = 0.253257, Learning Rate = 1.597837e-06\n",
      "Epoch 16945/20000: Train Loss = 0.434427, Test Loss = 0.253475, Learning Rate = 1.597230e-06\n",
      "Epoch 16946/20000: Train Loss = 0.434396, Test Loss = 0.253418, Learning Rate = 1.596623e-06\n",
      "Epoch 16947/20000: Train Loss = 0.434586, Test Loss = 0.252869, Learning Rate = 1.596016e-06\n",
      "Epoch 16948/20000: Train Loss = 0.434518, Test Loss = 0.252611, Learning Rate = 1.595410e-06\n",
      "Epoch 16949/20000: Train Loss = 0.434423, Test Loss = 0.253007, Learning Rate = 1.594804e-06\n",
      "Epoch 16950/20000: Train Loss = 0.434369, Test Loss = 0.253020, Learning Rate = 1.594198e-06\n",
      "Epoch 16951/20000: Train Loss = 0.434492, Test Loss = 0.253201, Learning Rate = 1.593592e-06\n",
      "Epoch 16952/20000: Train Loss = 0.434746, Test Loss = 0.253287, Learning Rate = 1.592986e-06\n",
      "Epoch 16953/20000: Train Loss = 0.434667, Test Loss = 0.254936, Learning Rate = 1.592381e-06\n",
      "Epoch 16954/20000: Train Loss = 0.434724, Test Loss = 0.253571, Learning Rate = 1.591776e-06\n",
      "Epoch 16955/20000: Train Loss = 0.434415, Test Loss = 0.253498, Learning Rate = 1.591171e-06\n",
      "Epoch 16956/20000: Train Loss = 0.434351, Test Loss = 0.252837, Learning Rate = 1.590566e-06\n",
      "Epoch 16957/20000: Train Loss = 0.434467, Test Loss = 0.254998, Learning Rate = 1.589962e-06\n",
      "Epoch 16958/20000: Train Loss = 0.434478, Test Loss = 0.253700, Learning Rate = 1.589358e-06\n",
      "Epoch 16959/20000: Train Loss = 0.434306, Test Loss = 0.253642, Learning Rate = 1.588754e-06\n",
      "Epoch 16960/20000: Train Loss = 0.434424, Test Loss = 0.253976, Learning Rate = 1.588150e-06\n",
      "Epoch 16961/20000: Train Loss = 0.434306, Test Loss = 0.252538, Learning Rate = 1.587547e-06\n",
      "Epoch 16962/20000: Train Loss = 0.434610, Test Loss = 0.253714, Learning Rate = 1.586944e-06\n",
      "Epoch 16963/20000: Train Loss = 0.435036, Test Loss = 0.252402, Learning Rate = 1.586341e-06\n",
      "Epoch 16964/20000: Train Loss = 0.434479, Test Loss = 0.253320, Learning Rate = 1.585738e-06\n",
      "Epoch 16965/20000: Train Loss = 0.435073, Test Loss = 0.254527, Learning Rate = 1.585135e-06\n",
      "Epoch 16966/20000: Train Loss = 0.434680, Test Loss = 0.254063, Learning Rate = 1.584533e-06\n",
      "Epoch 16967/20000: Train Loss = 0.434443, Test Loss = 0.254921, Learning Rate = 1.583931e-06\n",
      "Epoch 16968/20000: Train Loss = 0.434403, Test Loss = 0.253987, Learning Rate = 1.583329e-06\n",
      "Epoch 16969/20000: Train Loss = 0.434522, Test Loss = 0.254059, Learning Rate = 1.582728e-06\n",
      "Epoch 16970/20000: Train Loss = 0.434481, Test Loss = 0.252921, Learning Rate = 1.582126e-06\n",
      "Epoch 16971/20000: Train Loss = 0.434698, Test Loss = 0.253251, Learning Rate = 1.581525e-06\n",
      "Epoch 16972/20000: Train Loss = 0.434396, Test Loss = 0.253729, Learning Rate = 1.580924e-06\n",
      "Epoch 16973/20000: Train Loss = 0.434376, Test Loss = 0.251856, Learning Rate = 1.580323e-06\n",
      "Epoch 16974/20000: Train Loss = 0.434448, Test Loss = 0.251612, Learning Rate = 1.579723e-06\n",
      "Epoch 16975/20000: Train Loss = 0.434389, Test Loss = 0.252474, Learning Rate = 1.579123e-06\n",
      "Epoch 16976/20000: Train Loss = 0.434493, Test Loss = 0.253441, Learning Rate = 1.578523e-06\n",
      "Epoch 16977/20000: Train Loss = 0.434653, Test Loss = 0.252766, Learning Rate = 1.577923e-06\n",
      "Epoch 16978/20000: Train Loss = 0.434566, Test Loss = 0.252019, Learning Rate = 1.577323e-06\n",
      "Epoch 16979/20000: Train Loss = 0.434493, Test Loss = 0.253002, Learning Rate = 1.576724e-06\n",
      "Epoch 16980/20000: Train Loss = 0.434559, Test Loss = 0.251917, Learning Rate = 1.576125e-06\n",
      "Epoch 16981/20000: Train Loss = 0.434390, Test Loss = 0.252938, Learning Rate = 1.575526e-06\n",
      "Epoch 16982/20000: Train Loss = 0.434405, Test Loss = 0.252003, Learning Rate = 1.574927e-06\n",
      "Epoch 16983/20000: Train Loss = 0.434688, Test Loss = 0.253009, Learning Rate = 1.574329e-06\n",
      "Epoch 16984/20000: Train Loss = 0.434408, Test Loss = 0.253198, Learning Rate = 1.573731e-06\n",
      "Epoch 16985/20000: Train Loss = 0.434440, Test Loss = 0.252670, Learning Rate = 1.573133e-06\n",
      "Epoch 16986/20000: Train Loss = 0.434377, Test Loss = 0.253481, Learning Rate = 1.572535e-06\n",
      "Epoch 16987/20000: Train Loss = 0.434513, Test Loss = 0.254125, Learning Rate = 1.571937e-06\n",
      "Epoch 16988/20000: Train Loss = 0.434443, Test Loss = 0.254001, Learning Rate = 1.571340e-06\n",
      "Epoch 16989/20000: Train Loss = 0.434492, Test Loss = 0.253646, Learning Rate = 1.570743e-06\n",
      "Epoch 16990/20000: Train Loss = 0.434856, Test Loss = 0.254992, Learning Rate = 1.570146e-06\n",
      "Epoch 16991/20000: Train Loss = 0.434230, Test Loss = 0.253748, Learning Rate = 1.569550e-06\n",
      "Epoch 16992/20000: Train Loss = 0.434529, Test Loss = 0.253457, Learning Rate = 1.568953e-06\n",
      "Epoch 16993/20000: Train Loss = 0.434509, Test Loss = 0.253439, Learning Rate = 1.568357e-06\n",
      "Epoch 16994/20000: Train Loss = 0.434431, Test Loss = 0.252644, Learning Rate = 1.567761e-06\n",
      "Epoch 16995/20000: Train Loss = 0.434578, Test Loss = 0.253433, Learning Rate = 1.567165e-06\n",
      "Epoch 16996/20000: Train Loss = 0.434418, Test Loss = 0.253389, Learning Rate = 1.566570e-06\n",
      "Epoch 16997/20000: Train Loss = 0.434449, Test Loss = 0.252205, Learning Rate = 1.565975e-06\n",
      "Epoch 16998/20000: Train Loss = 0.434506, Test Loss = 0.251308, Learning Rate = 1.565380e-06\n",
      "Epoch 16999/20000: Train Loss = 0.434491, Test Loss = 0.252416, Learning Rate = 1.564785e-06\n",
      "Epoch 17000/20000: Train Loss = 0.434472, Test Loss = 0.252697, Learning Rate = 1.564190e-06\n",
      "Epoch 17001/20000: Train Loss = 0.434324, Test Loss = 0.252551, Learning Rate = 1.563596e-06\n",
      "Epoch 17002/20000: Train Loss = 0.434426, Test Loss = 0.252286, Learning Rate = 1.563002e-06\n",
      "Epoch 17003/20000: Train Loss = 0.434468, Test Loss = 0.252044, Learning Rate = 1.562408e-06\n",
      "Epoch 17004/20000: Train Loss = 0.434556, Test Loss = 0.252227, Learning Rate = 1.561814e-06\n",
      "Epoch 17005/20000: Train Loss = 0.434411, Test Loss = 0.251577, Learning Rate = 1.561221e-06\n",
      "Epoch 17006/20000: Train Loss = 0.434495, Test Loss = 0.253254, Learning Rate = 1.560627e-06\n",
      "Epoch 17007/20000: Train Loss = 0.434486, Test Loss = 0.253651, Learning Rate = 1.560034e-06\n",
      "Epoch 17008/20000: Train Loss = 0.434380, Test Loss = 0.252912, Learning Rate = 1.559442e-06\n",
      "Epoch 17009/20000: Train Loss = 0.434337, Test Loss = 0.252563, Learning Rate = 1.558849e-06\n",
      "Epoch 17010/20000: Train Loss = 0.434569, Test Loss = 0.253650, Learning Rate = 1.558257e-06\n",
      "Epoch 17011/20000: Train Loss = 0.434471, Test Loss = 0.252458, Learning Rate = 1.557665e-06\n",
      "Epoch 17012/20000: Train Loss = 0.434657, Test Loss = 0.254422, Learning Rate = 1.557073e-06\n",
      "Epoch 17013/20000: Train Loss = 0.434901, Test Loss = 0.253237, Learning Rate = 1.556481e-06\n",
      "Epoch 17014/20000: Train Loss = 0.434403, Test Loss = 0.253350, Learning Rate = 1.555890e-06\n",
      "Epoch 17015/20000: Train Loss = 0.434761, Test Loss = 0.252904, Learning Rate = 1.555299e-06\n",
      "Epoch 17016/20000: Train Loss = 0.434361, Test Loss = 0.253908, Learning Rate = 1.554708e-06\n",
      "Epoch 17017/20000: Train Loss = 0.434372, Test Loss = 0.252782, Learning Rate = 1.554117e-06\n",
      "Epoch 17018/20000: Train Loss = 0.434422, Test Loss = 0.253040, Learning Rate = 1.553526e-06\n",
      "Epoch 17019/20000: Train Loss = 0.434409, Test Loss = 0.253843, Learning Rate = 1.552936e-06\n",
      "Epoch 17020/20000: Train Loss = 0.434428, Test Loss = 0.251715, Learning Rate = 1.552346e-06\n",
      "Epoch 17021/20000: Train Loss = 0.434362, Test Loss = 0.253661, Learning Rate = 1.551756e-06\n",
      "Epoch 17022/20000: Train Loss = 0.434345, Test Loss = 0.253181, Learning Rate = 1.551167e-06\n",
      "Epoch 17023/20000: Train Loss = 0.434609, Test Loss = 0.253360, Learning Rate = 1.550577e-06\n",
      "Epoch 17024/20000: Train Loss = 0.434612, Test Loss = 0.252574, Learning Rate = 1.549988e-06\n",
      "Epoch 17025/20000: Train Loss = 0.434432, Test Loss = 0.253308, Learning Rate = 1.549399e-06\n",
      "Epoch 17026/20000: Train Loss = 0.434420, Test Loss = 0.253370, Learning Rate = 1.548810e-06\n",
      "Epoch 17027/20000: Train Loss = 0.434642, Test Loss = 0.252665, Learning Rate = 1.548222e-06\n",
      "Epoch 17028/20000: Train Loss = 0.434350, Test Loss = 0.253027, Learning Rate = 1.547633e-06\n",
      "Epoch 17029/20000: Train Loss = 0.434395, Test Loss = 0.253070, Learning Rate = 1.547045e-06\n",
      "Epoch 17030/20000: Train Loss = 0.434695, Test Loss = 0.251483, Learning Rate = 1.546458e-06\n",
      "Epoch 17031/20000: Train Loss = 0.434479, Test Loss = 0.252097, Learning Rate = 1.545870e-06\n",
      "Epoch 17032/20000: Train Loss = 0.434415, Test Loss = 0.252473, Learning Rate = 1.545283e-06\n",
      "Epoch 17033/20000: Train Loss = 0.434492, Test Loss = 0.253102, Learning Rate = 1.544695e-06\n",
      "Epoch 17034/20000: Train Loss = 0.434424, Test Loss = 0.252620, Learning Rate = 1.544108e-06\n",
      "Epoch 17035/20000: Train Loss = 0.434543, Test Loss = 0.253741, Learning Rate = 1.543522e-06\n",
      "Epoch 17036/20000: Train Loss = 0.434368, Test Loss = 0.253915, Learning Rate = 1.542935e-06\n",
      "Epoch 17037/20000: Train Loss = 0.434397, Test Loss = 0.253807, Learning Rate = 1.542349e-06\n",
      "Epoch 17038/20000: Train Loss = 0.434369, Test Loss = 0.253939, Learning Rate = 1.541763e-06\n",
      "Epoch 17039/20000: Train Loss = 0.434515, Test Loss = 0.253796, Learning Rate = 1.541177e-06\n",
      "Epoch 17040/20000: Train Loss = 0.434380, Test Loss = 0.252961, Learning Rate = 1.540591e-06\n",
      "Epoch 17041/20000: Train Loss = 0.434363, Test Loss = 0.252778, Learning Rate = 1.540006e-06\n",
      "Epoch 17042/20000: Train Loss = 0.434392, Test Loss = 0.253307, Learning Rate = 1.539421e-06\n",
      "Epoch 17043/20000: Train Loss = 0.434442, Test Loss = 0.252834, Learning Rate = 1.538836e-06\n",
      "Epoch 17044/20000: Train Loss = 0.434463, Test Loss = 0.252292, Learning Rate = 1.538251e-06\n",
      "Epoch 17045/20000: Train Loss = 0.434376, Test Loss = 0.252534, Learning Rate = 1.537667e-06\n",
      "Epoch 17046/20000: Train Loss = 0.434425, Test Loss = 0.254106, Learning Rate = 1.537083e-06\n",
      "Epoch 17047/20000: Train Loss = 0.434423, Test Loss = 0.252940, Learning Rate = 1.536498e-06\n",
      "Epoch 17048/20000: Train Loss = 0.434319, Test Loss = 0.252990, Learning Rate = 1.535915e-06\n",
      "Epoch 17049/20000: Train Loss = 0.434419, Test Loss = 0.253255, Learning Rate = 1.535331e-06\n",
      "Epoch 17050/20000: Train Loss = 0.434386, Test Loss = 0.254262, Learning Rate = 1.534748e-06\n",
      "Epoch 17051/20000: Train Loss = 0.434283, Test Loss = 0.253960, Learning Rate = 1.534164e-06\n",
      "Epoch 17052/20000: Train Loss = 0.434463, Test Loss = 0.253072, Learning Rate = 1.533582e-06\n",
      "Epoch 17053/20000: Train Loss = 0.434408, Test Loss = 0.254093, Learning Rate = 1.532999e-06\n",
      "Epoch 17054/20000: Train Loss = 0.434801, Test Loss = 0.254685, Learning Rate = 1.532416e-06\n",
      "Epoch 17055/20000: Train Loss = 0.434472, Test Loss = 0.253177, Learning Rate = 1.531834e-06\n",
      "Epoch 17056/20000: Train Loss = 0.434589, Test Loss = 0.252570, Learning Rate = 1.531252e-06\n",
      "Epoch 17057/20000: Train Loss = 0.434343, Test Loss = 0.252770, Learning Rate = 1.530670e-06\n",
      "Epoch 17058/20000: Train Loss = 0.434527, Test Loss = 0.251834, Learning Rate = 1.530089e-06\n",
      "Epoch 17059/20000: Train Loss = 0.434397, Test Loss = 0.253301, Learning Rate = 1.529507e-06\n",
      "Epoch 17060/20000: Train Loss = 0.434387, Test Loss = 0.253041, Learning Rate = 1.528926e-06\n",
      "Epoch 17061/20000: Train Loss = 0.434388, Test Loss = 0.251598, Learning Rate = 1.528345e-06\n",
      "Epoch 17062/20000: Train Loss = 0.434451, Test Loss = 0.252888, Learning Rate = 1.527764e-06\n",
      "Epoch 17063/20000: Train Loss = 0.434329, Test Loss = 0.252684, Learning Rate = 1.527184e-06\n",
      "Epoch 17064/20000: Train Loss = 0.434425, Test Loss = 0.253295, Learning Rate = 1.526604e-06\n",
      "Epoch 17065/20000: Train Loss = 0.434427, Test Loss = 0.251974, Learning Rate = 1.526023e-06\n",
      "Epoch 17066/20000: Train Loss = 0.434445, Test Loss = 0.252138, Learning Rate = 1.525444e-06\n",
      "Epoch 17067/20000: Train Loss = 0.434394, Test Loss = 0.252980, Learning Rate = 1.524864e-06\n",
      "Epoch 17068/20000: Train Loss = 0.434463, Test Loss = 0.252846, Learning Rate = 1.524285e-06\n",
      "Epoch 17069/20000: Train Loss = 0.434334, Test Loss = 0.252941, Learning Rate = 1.523705e-06\n",
      "Epoch 17070/20000: Train Loss = 0.434524, Test Loss = 0.253205, Learning Rate = 1.523126e-06\n",
      "Epoch 17071/20000: Train Loss = 0.434479, Test Loss = 0.254280, Learning Rate = 1.522548e-06\n",
      "Epoch 17072/20000: Train Loss = 0.434484, Test Loss = 0.252879, Learning Rate = 1.521969e-06\n",
      "Epoch 17073/20000: Train Loss = 0.434399, Test Loss = 0.252714, Learning Rate = 1.521391e-06\n",
      "Epoch 17074/20000: Train Loss = 0.434329, Test Loss = 0.252654, Learning Rate = 1.520813e-06\n",
      "Epoch 17075/20000: Train Loss = 0.434371, Test Loss = 0.251322, Learning Rate = 1.520235e-06\n",
      "Epoch 17076/20000: Train Loss = 0.434364, Test Loss = 0.251601, Learning Rate = 1.519657e-06\n",
      "Epoch 17077/20000: Train Loss = 0.434654, Test Loss = 0.251249, Learning Rate = 1.519080e-06\n",
      "Epoch 17078/20000: Train Loss = 0.434477, Test Loss = 0.251718, Learning Rate = 1.518503e-06\n",
      "Epoch 17079/20000: Train Loss = 0.434387, Test Loss = 0.249791, Learning Rate = 1.517926e-06\n",
      "Epoch 17080/20000: Train Loss = 0.434392, Test Loss = 0.250314, Learning Rate = 1.517349e-06\n",
      "Epoch 17081/20000: Train Loss = 0.434392, Test Loss = 0.251503, Learning Rate = 1.516772e-06\n",
      "Epoch 17082/20000: Train Loss = 0.434440, Test Loss = 0.251630, Learning Rate = 1.516196e-06\n",
      "Epoch 17083/20000: Train Loss = 0.434554, Test Loss = 0.251506, Learning Rate = 1.515620e-06\n",
      "Epoch 17084/20000: Train Loss = 0.434604, Test Loss = 0.252180, Learning Rate = 1.515044e-06\n",
      "Epoch 17085/20000: Train Loss = 0.434490, Test Loss = 0.251508, Learning Rate = 1.514468e-06\n",
      "Epoch 17086/20000: Train Loss = 0.434329, Test Loss = 0.250935, Learning Rate = 1.513893e-06\n",
      "Epoch 17087/20000: Train Loss = 0.434315, Test Loss = 0.251457, Learning Rate = 1.513318e-06\n",
      "Epoch 17088/20000: Train Loss = 0.434344, Test Loss = 0.250999, Learning Rate = 1.512743e-06\n",
      "Epoch 17089/20000: Train Loss = 0.434428, Test Loss = 0.250565, Learning Rate = 1.512168e-06\n",
      "Epoch 17090/20000: Train Loss = 0.434548, Test Loss = 0.250561, Learning Rate = 1.511593e-06\n",
      "Epoch 17091/20000: Train Loss = 0.434354, Test Loss = 0.251383, Learning Rate = 1.511019e-06\n",
      "Epoch 17092/20000: Train Loss = 0.434418, Test Loss = 0.249798, Learning Rate = 1.510445e-06\n",
      "Epoch 17093/20000: Train Loss = 0.434493, Test Loss = 0.249084, Learning Rate = 1.509871e-06\n",
      "Epoch 17094/20000: Train Loss = 0.434319, Test Loss = 0.251068, Learning Rate = 1.509297e-06\n",
      "Epoch 17095/20000: Train Loss = 0.434457, Test Loss = 0.249732, Learning Rate = 1.508724e-06\n",
      "Epoch 17096/20000: Train Loss = 0.434301, Test Loss = 0.250037, Learning Rate = 1.508150e-06\n",
      "Epoch 17097/20000: Train Loss = 0.434334, Test Loss = 0.250203, Learning Rate = 1.507577e-06\n",
      "Epoch 17098/20000: Train Loss = 0.434555, Test Loss = 0.250326, Learning Rate = 1.507004e-06\n",
      "Epoch 17099/20000: Train Loss = 0.434950, Test Loss = 0.251247, Learning Rate = 1.506432e-06\n",
      "Epoch 17100/20000: Train Loss = 0.434518, Test Loss = 0.249295, Learning Rate = 1.505859e-06\n",
      "Epoch 17101/20000: Train Loss = 0.434418, Test Loss = 0.250638, Learning Rate = 1.505287e-06\n",
      "Epoch 17102/20000: Train Loss = 0.434436, Test Loss = 0.249448, Learning Rate = 1.504715e-06\n",
      "Epoch 17103/20000: Train Loss = 0.434347, Test Loss = 0.249552, Learning Rate = 1.504143e-06\n",
      "Epoch 17104/20000: Train Loss = 0.434411, Test Loss = 0.250043, Learning Rate = 1.503572e-06\n",
      "Epoch 17105/20000: Train Loss = 0.434517, Test Loss = 0.250107, Learning Rate = 1.503001e-06\n",
      "Epoch 17106/20000: Train Loss = 0.434704, Test Loss = 0.250219, Learning Rate = 1.502429e-06\n",
      "Epoch 17107/20000: Train Loss = 0.434360, Test Loss = 0.249683, Learning Rate = 1.501859e-06\n",
      "Epoch 17108/20000: Train Loss = 0.434626, Test Loss = 0.250955, Learning Rate = 1.501288e-06\n",
      "Epoch 17109/20000: Train Loss = 0.434934, Test Loss = 0.250771, Learning Rate = 1.500717e-06\n",
      "Epoch 17110/20000: Train Loss = 0.434536, Test Loss = 0.250935, Learning Rate = 1.500147e-06\n",
      "Epoch 17111/20000: Train Loss = 0.434409, Test Loss = 0.251547, Learning Rate = 1.499577e-06\n",
      "Epoch 17112/20000: Train Loss = 0.434363, Test Loss = 0.250760, Learning Rate = 1.499007e-06\n",
      "Epoch 17113/20000: Train Loss = 0.434354, Test Loss = 0.250838, Learning Rate = 1.498438e-06\n",
      "Epoch 17114/20000: Train Loss = 0.434541, Test Loss = 0.249886, Learning Rate = 1.497868e-06\n",
      "Epoch 17115/20000: Train Loss = 0.434347, Test Loss = 0.251119, Learning Rate = 1.497299e-06\n",
      "Epoch 17116/20000: Train Loss = 0.434351, Test Loss = 0.251334, Learning Rate = 1.496730e-06\n",
      "Epoch 17117/20000: Train Loss = 0.434299, Test Loss = 0.250886, Learning Rate = 1.496162e-06\n",
      "Epoch 17118/20000: Train Loss = 0.434343, Test Loss = 0.251242, Learning Rate = 1.495593e-06\n",
      "Epoch 17119/20000: Train Loss = 0.434470, Test Loss = 0.251597, Learning Rate = 1.495025e-06\n",
      "Epoch 17120/20000: Train Loss = 0.434499, Test Loss = 0.252355, Learning Rate = 1.494457e-06\n",
      "Epoch 17121/20000: Train Loss = 0.434409, Test Loss = 0.251365, Learning Rate = 1.493889e-06\n",
      "Epoch 17122/20000: Train Loss = 0.434534, Test Loss = 0.251184, Learning Rate = 1.493321e-06\n",
      "Epoch 17123/20000: Train Loss = 0.434398, Test Loss = 0.252350, Learning Rate = 1.492754e-06\n",
      "Epoch 17124/20000: Train Loss = 0.434386, Test Loss = 0.251364, Learning Rate = 1.492187e-06\n",
      "Epoch 17125/20000: Train Loss = 0.434315, Test Loss = 0.250838, Learning Rate = 1.491620e-06\n",
      "Epoch 17126/20000: Train Loss = 0.434608, Test Loss = 0.250832, Learning Rate = 1.491053e-06\n",
      "Epoch 17127/20000: Train Loss = 0.434389, Test Loss = 0.251408, Learning Rate = 1.490486e-06\n",
      "Epoch 17128/20000: Train Loss = 0.434422, Test Loss = 0.251926, Learning Rate = 1.489920e-06\n",
      "Epoch 17129/20000: Train Loss = 0.434336, Test Loss = 0.251385, Learning Rate = 1.489354e-06\n",
      "Epoch 17130/20000: Train Loss = 0.434540, Test Loss = 0.251629, Learning Rate = 1.488788e-06\n",
      "Epoch 17131/20000: Train Loss = 0.434686, Test Loss = 0.251913, Learning Rate = 1.488222e-06\n",
      "Epoch 17132/20000: Train Loss = 0.434817, Test Loss = 0.252421, Learning Rate = 1.487657e-06\n",
      "Epoch 17133/20000: Train Loss = 0.434381, Test Loss = 0.253055, Learning Rate = 1.487092e-06\n",
      "Epoch 17134/20000: Train Loss = 0.434405, Test Loss = 0.251742, Learning Rate = 1.486526e-06\n",
      "Epoch 17135/20000: Train Loss = 0.434363, Test Loss = 0.250567, Learning Rate = 1.485962e-06\n",
      "Epoch 17136/20000: Train Loss = 0.434460, Test Loss = 0.250841, Learning Rate = 1.485397e-06\n",
      "Epoch 17137/20000: Train Loss = 0.434520, Test Loss = 0.250626, Learning Rate = 1.484833e-06\n",
      "Epoch 17138/20000: Train Loss = 0.434427, Test Loss = 0.251110, Learning Rate = 1.484268e-06\n",
      "Epoch 17139/20000: Train Loss = 0.434442, Test Loss = 0.250150, Learning Rate = 1.483704e-06\n",
      "Epoch 17140/20000: Train Loss = 0.434359, Test Loss = 0.250026, Learning Rate = 1.483141e-06\n",
      "Epoch 17141/20000: Train Loss = 0.434336, Test Loss = 0.249407, Learning Rate = 1.482577e-06\n",
      "Epoch 17142/20000: Train Loss = 0.434468, Test Loss = 0.250378, Learning Rate = 1.482014e-06\n",
      "Epoch 17143/20000: Train Loss = 0.435093, Test Loss = 0.251435, Learning Rate = 1.481451e-06\n",
      "Epoch 17144/20000: Train Loss = 0.434643, Test Loss = 0.250297, Learning Rate = 1.480888e-06\n",
      "Epoch 17145/20000: Train Loss = 0.434672, Test Loss = 0.251444, Learning Rate = 1.480325e-06\n",
      "Epoch 17146/20000: Train Loss = 0.434400, Test Loss = 0.250060, Learning Rate = 1.479763e-06\n",
      "Epoch 17147/20000: Train Loss = 0.434635, Test Loss = 0.250783, Learning Rate = 1.479200e-06\n",
      "Epoch 17148/20000: Train Loss = 0.434426, Test Loss = 0.251458, Learning Rate = 1.478638e-06\n",
      "Epoch 17149/20000: Train Loss = 0.434367, Test Loss = 0.251464, Learning Rate = 1.478076e-06\n",
      "Epoch 17150/20000: Train Loss = 0.434431, Test Loss = 0.251891, Learning Rate = 1.477515e-06\n",
      "Epoch 17151/20000: Train Loss = 0.434637, Test Loss = 0.251894, Learning Rate = 1.476953e-06\n",
      "Epoch 17152/20000: Train Loss = 0.434349, Test Loss = 0.251410, Learning Rate = 1.476392e-06\n",
      "Epoch 17153/20000: Train Loss = 0.434449, Test Loss = 0.251501, Learning Rate = 1.475831e-06\n",
      "Epoch 17154/20000: Train Loss = 0.434324, Test Loss = 0.250806, Learning Rate = 1.475270e-06\n",
      "Epoch 17155/20000: Train Loss = 0.434290, Test Loss = 0.251724, Learning Rate = 1.474710e-06\n",
      "Epoch 17156/20000: Train Loss = 0.434373, Test Loss = 0.251093, Learning Rate = 1.474149e-06\n",
      "Epoch 17157/20000: Train Loss = 0.434490, Test Loss = 0.251443, Learning Rate = 1.473589e-06\n",
      "Epoch 17158/20000: Train Loss = 0.434748, Test Loss = 0.251680, Learning Rate = 1.473029e-06\n",
      "Epoch 17159/20000: Train Loss = 0.434636, Test Loss = 0.252133, Learning Rate = 1.472470e-06\n",
      "Epoch 17160/20000: Train Loss = 0.434331, Test Loss = 0.250762, Learning Rate = 1.471910e-06\n",
      "Epoch 17161/20000: Train Loss = 0.434430, Test Loss = 0.251221, Learning Rate = 1.471351e-06\n",
      "Epoch 17162/20000: Train Loss = 0.434348, Test Loss = 0.253126, Learning Rate = 1.470792e-06\n",
      "Epoch 17163/20000: Train Loss = 0.434363, Test Loss = 0.252669, Learning Rate = 1.470233e-06\n",
      "Epoch 17164/20000: Train Loss = 0.434450, Test Loss = 0.251626, Learning Rate = 1.469674e-06\n",
      "Epoch 17165/20000: Train Loss = 0.434521, Test Loss = 0.253032, Learning Rate = 1.469116e-06\n",
      "Epoch 17166/20000: Train Loss = 0.434404, Test Loss = 0.252391, Learning Rate = 1.468558e-06\n",
      "Epoch 17167/20000: Train Loss = 0.434332, Test Loss = 0.252719, Learning Rate = 1.468000e-06\n",
      "Epoch 17168/20000: Train Loss = 0.434355, Test Loss = 0.252751, Learning Rate = 1.467442e-06\n",
      "Epoch 17169/20000: Train Loss = 0.434431, Test Loss = 0.252725, Learning Rate = 1.466884e-06\n",
      "Epoch 17170/20000: Train Loss = 0.434778, Test Loss = 0.252502, Learning Rate = 1.466327e-06\n",
      "Epoch 17171/20000: Train Loss = 0.434871, Test Loss = 0.253904, Learning Rate = 1.465770e-06\n",
      "Epoch 17172/20000: Train Loss = 0.434366, Test Loss = 0.252547, Learning Rate = 1.465213e-06\n",
      "Epoch 17173/20000: Train Loss = 0.434388, Test Loss = 0.252700, Learning Rate = 1.464656e-06\n",
      "Epoch 17174/20000: Train Loss = 0.434388, Test Loss = 0.252372, Learning Rate = 1.464100e-06\n",
      "Epoch 17175/20000: Train Loss = 0.434441, Test Loss = 0.251741, Learning Rate = 1.463543e-06\n",
      "Epoch 17176/20000: Train Loss = 0.435407, Test Loss = 0.254660, Learning Rate = 1.462987e-06\n",
      "Epoch 17177/20000: Train Loss = 0.434578, Test Loss = 0.254516, Learning Rate = 1.462431e-06\n",
      "Epoch 17178/20000: Train Loss = 0.434571, Test Loss = 0.254996, Learning Rate = 1.461876e-06\n",
      "Epoch 17179/20000: Train Loss = 0.434472, Test Loss = 0.253744, Learning Rate = 1.461320e-06\n",
      "Epoch 17180/20000: Train Loss = 0.434356, Test Loss = 0.252986, Learning Rate = 1.460765e-06\n",
      "Epoch 17181/20000: Train Loss = 0.434496, Test Loss = 0.254032, Learning Rate = 1.460210e-06\n",
      "Epoch 17182/20000: Train Loss = 0.434257, Test Loss = 0.253505, Learning Rate = 1.459655e-06\n",
      "Epoch 17183/20000: Train Loss = 0.434388, Test Loss = 0.254421, Learning Rate = 1.459100e-06\n",
      "Epoch 17184/20000: Train Loss = 0.434547, Test Loss = 0.252595, Learning Rate = 1.458546e-06\n",
      "Epoch 17185/20000: Train Loss = 0.434286, Test Loss = 0.252358, Learning Rate = 1.457992e-06\n",
      "Epoch 17186/20000: Train Loss = 0.434428, Test Loss = 0.252681, Learning Rate = 1.457438e-06\n",
      "Epoch 17187/20000: Train Loss = 0.434327, Test Loss = 0.253226, Learning Rate = 1.456884e-06\n",
      "Epoch 17188/20000: Train Loss = 0.434330, Test Loss = 0.253119, Learning Rate = 1.456330e-06\n",
      "Epoch 17189/20000: Train Loss = 0.434539, Test Loss = 0.254611, Learning Rate = 1.455777e-06\n",
      "Epoch 17190/20000: Train Loss = 0.434394, Test Loss = 0.254373, Learning Rate = 1.455224e-06\n",
      "Epoch 17191/20000: Train Loss = 0.434458, Test Loss = 0.252308, Learning Rate = 1.454671e-06\n",
      "Epoch 17192/20000: Train Loss = 0.434281, Test Loss = 0.253478, Learning Rate = 1.454118e-06\n",
      "Epoch 17193/20000: Train Loss = 0.434321, Test Loss = 0.253943, Learning Rate = 1.453566e-06\n",
      "Epoch 17194/20000: Train Loss = 0.434411, Test Loss = 0.253394, Learning Rate = 1.453013e-06\n",
      "Epoch 17195/20000: Train Loss = 0.434308, Test Loss = 0.252765, Learning Rate = 1.452461e-06\n",
      "Epoch 17196/20000: Train Loss = 0.434377, Test Loss = 0.253976, Learning Rate = 1.451909e-06\n",
      "Epoch 17197/20000: Train Loss = 0.434352, Test Loss = 0.253697, Learning Rate = 1.451358e-06\n",
      "Epoch 17198/20000: Train Loss = 0.434460, Test Loss = 0.254081, Learning Rate = 1.450806e-06\n",
      "Epoch 17199/20000: Train Loss = 0.434398, Test Loss = 0.253454, Learning Rate = 1.450255e-06\n",
      "Epoch 17200/20000: Train Loss = 0.434380, Test Loss = 0.253656, Learning Rate = 1.449704e-06\n",
      "Epoch 17201/20000: Train Loss = 0.434446, Test Loss = 0.254665, Learning Rate = 1.449153e-06\n",
      "Epoch 17202/20000: Train Loss = 0.434324, Test Loss = 0.253183, Learning Rate = 1.448602e-06\n",
      "Epoch 17203/20000: Train Loss = 0.434346, Test Loss = 0.252129, Learning Rate = 1.448052e-06\n",
      "Epoch 17204/20000: Train Loss = 0.434384, Test Loss = 0.252427, Learning Rate = 1.447502e-06\n",
      "Epoch 17205/20000: Train Loss = 0.434339, Test Loss = 0.251909, Learning Rate = 1.446952e-06\n",
      "Epoch 17206/20000: Train Loss = 0.434370, Test Loss = 0.252453, Learning Rate = 1.446402e-06\n",
      "Epoch 17207/20000: Train Loss = 0.434516, Test Loss = 0.253069, Learning Rate = 1.445852e-06\n",
      "Epoch 17208/20000: Train Loss = 0.434300, Test Loss = 0.253141, Learning Rate = 1.445303e-06\n",
      "Epoch 17209/20000: Train Loss = 0.434401, Test Loss = 0.254058, Learning Rate = 1.444754e-06\n",
      "Epoch 17210/20000: Train Loss = 0.434426, Test Loss = 0.254050, Learning Rate = 1.444205e-06\n",
      "Epoch 17211/20000: Train Loss = 0.434413, Test Loss = 0.253207, Learning Rate = 1.443656e-06\n",
      "Epoch 17212/20000: Train Loss = 0.434319, Test Loss = 0.253454, Learning Rate = 1.443107e-06\n",
      "Epoch 17213/20000: Train Loss = 0.434569, Test Loss = 0.253555, Learning Rate = 1.442559e-06\n",
      "Epoch 17214/20000: Train Loss = 0.434474, Test Loss = 0.253799, Learning Rate = 1.442011e-06\n",
      "Epoch 17215/20000: Train Loss = 0.434386, Test Loss = 0.253333, Learning Rate = 1.441463e-06\n",
      "Epoch 17216/20000: Train Loss = 0.434679, Test Loss = 0.252309, Learning Rate = 1.440915e-06\n",
      "Epoch 17217/20000: Train Loss = 0.434464, Test Loss = 0.252064, Learning Rate = 1.440368e-06\n",
      "Epoch 17218/20000: Train Loss = 0.434379, Test Loss = 0.252093, Learning Rate = 1.439820e-06\n",
      "Epoch 17219/20000: Train Loss = 0.434294, Test Loss = 0.252491, Learning Rate = 1.439273e-06\n",
      "Epoch 17220/20000: Train Loss = 0.434389, Test Loss = 0.252671, Learning Rate = 1.438726e-06\n",
      "Epoch 17221/20000: Train Loss = 0.434381, Test Loss = 0.252150, Learning Rate = 1.438180e-06\n",
      "Epoch 17222/20000: Train Loss = 0.434304, Test Loss = 0.252391, Learning Rate = 1.437633e-06\n",
      "Epoch 17223/20000: Train Loss = 0.434384, Test Loss = 0.252088, Learning Rate = 1.437087e-06\n",
      "Epoch 17224/20000: Train Loss = 0.434314, Test Loss = 0.252616, Learning Rate = 1.436541e-06\n",
      "Epoch 17225/20000: Train Loss = 0.434468, Test Loss = 0.252764, Learning Rate = 1.435995e-06\n",
      "Epoch 17226/20000: Train Loss = 0.434388, Test Loss = 0.252600, Learning Rate = 1.435449e-06\n",
      "Epoch 17227/20000: Train Loss = 0.434251, Test Loss = 0.251214, Learning Rate = 1.434904e-06\n",
      "Epoch 17228/20000: Train Loss = 0.434304, Test Loss = 0.251801, Learning Rate = 1.434359e-06\n",
      "Epoch 17229/20000: Train Loss = 0.434271, Test Loss = 0.251773, Learning Rate = 1.433814e-06\n",
      "Epoch 17230/20000: Train Loss = 0.434583, Test Loss = 0.251179, Learning Rate = 1.433269e-06\n",
      "Epoch 17231/20000: Train Loss = 0.434340, Test Loss = 0.252613, Learning Rate = 1.432724e-06\n",
      "Epoch 17232/20000: Train Loss = 0.434310, Test Loss = 0.251446, Learning Rate = 1.432180e-06\n",
      "Epoch 17233/20000: Train Loss = 0.434364, Test Loss = 0.251290, Learning Rate = 1.431636e-06\n",
      "Epoch 17234/20000: Train Loss = 0.434303, Test Loss = 0.251311, Learning Rate = 1.431092e-06\n",
      "Epoch 17235/20000: Train Loss = 0.434403, Test Loss = 0.251951, Learning Rate = 1.430548e-06\n",
      "Epoch 17236/20000: Train Loss = 0.434404, Test Loss = 0.251805, Learning Rate = 1.430004e-06\n",
      "Epoch 17237/20000: Train Loss = 0.434450, Test Loss = 0.252222, Learning Rate = 1.429461e-06\n",
      "Epoch 17238/20000: Train Loss = 0.434226, Test Loss = 0.251709, Learning Rate = 1.428918e-06\n",
      "Epoch 17239/20000: Train Loss = 0.434552, Test Loss = 0.250713, Learning Rate = 1.428375e-06\n",
      "Epoch 17240/20000: Train Loss = 0.434413, Test Loss = 0.251212, Learning Rate = 1.427832e-06\n",
      "Epoch 17241/20000: Train Loss = 0.434527, Test Loss = 0.251581, Learning Rate = 1.427290e-06\n",
      "Epoch 17242/20000: Train Loss = 0.434557, Test Loss = 0.250593, Learning Rate = 1.426747e-06\n",
      "Epoch 17243/20000: Train Loss = 0.434509, Test Loss = 0.251254, Learning Rate = 1.426205e-06\n",
      "Epoch 17244/20000: Train Loss = 0.434655, Test Loss = 0.251679, Learning Rate = 1.425663e-06\n",
      "Epoch 17245/20000: Train Loss = 0.434343, Test Loss = 0.250240, Learning Rate = 1.425122e-06\n",
      "Epoch 17246/20000: Train Loss = 0.434700, Test Loss = 0.250370, Learning Rate = 1.424580e-06\n",
      "Epoch 17247/20000: Train Loss = 0.434767, Test Loss = 0.251109, Learning Rate = 1.424039e-06\n",
      "Epoch 17248/20000: Train Loss = 0.434362, Test Loss = 0.251182, Learning Rate = 1.423498e-06\n",
      "Epoch 17249/20000: Train Loss = 0.434378, Test Loss = 0.250968, Learning Rate = 1.422957e-06\n",
      "Epoch 17250/20000: Train Loss = 0.434408, Test Loss = 0.250173, Learning Rate = 1.422416e-06\n",
      "Epoch 17251/20000: Train Loss = 0.434357, Test Loss = 0.251202, Learning Rate = 1.421876e-06\n",
      "Epoch 17252/20000: Train Loss = 0.434364, Test Loss = 0.250903, Learning Rate = 1.421335e-06\n",
      "Epoch 17253/20000: Train Loss = 0.434327, Test Loss = 0.251805, Learning Rate = 1.420795e-06\n",
      "Epoch 17254/20000: Train Loss = 0.434345, Test Loss = 0.251971, Learning Rate = 1.420255e-06\n",
      "Epoch 17255/20000: Train Loss = 0.434427, Test Loss = 0.252206, Learning Rate = 1.419716e-06\n",
      "Epoch 17256/20000: Train Loss = 0.434327, Test Loss = 0.252334, Learning Rate = 1.419176e-06\n",
      "Epoch 17257/20000: Train Loss = 0.434364, Test Loss = 0.253147, Learning Rate = 1.418637e-06\n",
      "Epoch 17258/20000: Train Loss = 0.434273, Test Loss = 0.251792, Learning Rate = 1.418098e-06\n",
      "Epoch 17259/20000: Train Loss = 0.434524, Test Loss = 0.251884, Learning Rate = 1.417559e-06\n",
      "Epoch 17260/20000: Train Loss = 0.434504, Test Loss = 0.252179, Learning Rate = 1.417021e-06\n",
      "Epoch 17261/20000: Train Loss = 0.434492, Test Loss = 0.252027, Learning Rate = 1.416482e-06\n",
      "Epoch 17262/20000: Train Loss = 0.434383, Test Loss = 0.252598, Learning Rate = 1.415944e-06\n",
      "Epoch 17263/20000: Train Loss = 0.434413, Test Loss = 0.251886, Learning Rate = 1.415406e-06\n",
      "Epoch 17264/20000: Train Loss = 0.434340, Test Loss = 0.251065, Learning Rate = 1.414868e-06\n",
      "Epoch 17265/20000: Train Loss = 0.434482, Test Loss = 0.251849, Learning Rate = 1.414331e-06\n",
      "Epoch 17266/20000: Train Loss = 0.434420, Test Loss = 0.250954, Learning Rate = 1.413793e-06\n",
      "Epoch 17267/20000: Train Loss = 0.434468, Test Loss = 0.251134, Learning Rate = 1.413256e-06\n",
      "Epoch 17268/20000: Train Loss = 0.434341, Test Loss = 0.251021, Learning Rate = 1.412719e-06\n",
      "Epoch 17269/20000: Train Loss = 0.434357, Test Loss = 0.251177, Learning Rate = 1.412182e-06\n",
      "Epoch 17270/20000: Train Loss = 0.434358, Test Loss = 0.251748, Learning Rate = 1.411646e-06\n",
      "Epoch 17271/20000: Train Loss = 0.434277, Test Loss = 0.251825, Learning Rate = 1.411109e-06\n",
      "Epoch 17272/20000: Train Loss = 0.434354, Test Loss = 0.251075, Learning Rate = 1.410573e-06\n",
      "Epoch 17273/20000: Train Loss = 0.434451, Test Loss = 0.251608, Learning Rate = 1.410037e-06\n",
      "Epoch 17274/20000: Train Loss = 0.434309, Test Loss = 0.251916, Learning Rate = 1.409501e-06\n",
      "Epoch 17275/20000: Train Loss = 0.434329, Test Loss = 0.251731, Learning Rate = 1.408966e-06\n",
      "Epoch 17276/20000: Train Loss = 0.434513, Test Loss = 0.251243, Learning Rate = 1.408430e-06\n",
      "Epoch 17277/20000: Train Loss = 0.434310, Test Loss = 0.251068, Learning Rate = 1.407895e-06\n",
      "Epoch 17278/20000: Train Loss = 0.434458, Test Loss = 0.251215, Learning Rate = 1.407360e-06\n",
      "Epoch 17279/20000: Train Loss = 0.434300, Test Loss = 0.251194, Learning Rate = 1.406825e-06\n",
      "Epoch 17280/20000: Train Loss = 0.434384, Test Loss = 0.250292, Learning Rate = 1.406291e-06\n",
      "Epoch 17281/20000: Train Loss = 0.434548, Test Loss = 0.251497, Learning Rate = 1.405756e-06\n",
      "Epoch 17282/20000: Train Loss = 0.434888, Test Loss = 0.251010, Learning Rate = 1.405222e-06\n",
      "Epoch 17283/20000: Train Loss = 0.434368, Test Loss = 0.250305, Learning Rate = 1.404688e-06\n",
      "Epoch 17284/20000: Train Loss = 0.434333, Test Loss = 0.250441, Learning Rate = 1.404155e-06\n",
      "Epoch 17285/20000: Train Loss = 0.434304, Test Loss = 0.251957, Learning Rate = 1.403621e-06\n",
      "Epoch 17286/20000: Train Loss = 0.434357, Test Loss = 0.252013, Learning Rate = 1.403088e-06\n",
      "Epoch 17287/20000: Train Loss = 0.434389, Test Loss = 0.252146, Learning Rate = 1.402555e-06\n",
      "Epoch 17288/20000: Train Loss = 0.434426, Test Loss = 0.252268, Learning Rate = 1.402022e-06\n",
      "Epoch 17289/20000: Train Loss = 0.434290, Test Loss = 0.251603, Learning Rate = 1.401489e-06\n",
      "Epoch 17290/20000: Train Loss = 0.434402, Test Loss = 0.251937, Learning Rate = 1.400956e-06\n",
      "Epoch 17291/20000: Train Loss = 0.434315, Test Loss = 0.251477, Learning Rate = 1.400424e-06\n",
      "Epoch 17292/20000: Train Loss = 0.434292, Test Loss = 0.252186, Learning Rate = 1.399892e-06\n",
      "Epoch 17293/20000: Train Loss = 0.434659, Test Loss = 0.251384, Learning Rate = 1.399360e-06\n",
      "Epoch 17294/20000: Train Loss = 0.434330, Test Loss = 0.251493, Learning Rate = 1.398828e-06\n",
      "Epoch 17295/20000: Train Loss = 0.434335, Test Loss = 0.250807, Learning Rate = 1.398297e-06\n",
      "Epoch 17296/20000: Train Loss = 0.434355, Test Loss = 0.251003, Learning Rate = 1.397765e-06\n",
      "Epoch 17297/20000: Train Loss = 0.434275, Test Loss = 0.251001, Learning Rate = 1.397234e-06\n",
      "Epoch 17298/20000: Train Loss = 0.434441, Test Loss = 0.251810, Learning Rate = 1.396703e-06\n",
      "Epoch 17299/20000: Train Loss = 0.434406, Test Loss = 0.250807, Learning Rate = 1.396173e-06\n",
      "Epoch 17300/20000: Train Loss = 0.434314, Test Loss = 0.250959, Learning Rate = 1.395642e-06\n",
      "Epoch 17301/20000: Train Loss = 0.434298, Test Loss = 0.250710, Learning Rate = 1.395112e-06\n",
      "Epoch 17302/20000: Train Loss = 0.434365, Test Loss = 0.250923, Learning Rate = 1.394582e-06\n",
      "Epoch 17303/20000: Train Loss = 0.434388, Test Loss = 0.250012, Learning Rate = 1.394052e-06\n",
      "Epoch 17304/20000: Train Loss = 0.434423, Test Loss = 0.252431, Learning Rate = 1.393522e-06\n",
      "Epoch 17305/20000: Train Loss = 0.434451, Test Loss = 0.250773, Learning Rate = 1.392993e-06\n",
      "Epoch 17306/20000: Train Loss = 0.434512, Test Loss = 0.250723, Learning Rate = 1.392463e-06\n",
      "Epoch 17307/20000: Train Loss = 0.434574, Test Loss = 0.251157, Learning Rate = 1.391934e-06\n",
      "Epoch 17308/20000: Train Loss = 0.434355, Test Loss = 0.250226, Learning Rate = 1.391405e-06\n",
      "Epoch 17309/20000: Train Loss = 0.434447, Test Loss = 0.250454, Learning Rate = 1.390877e-06\n",
      "Epoch 17310/20000: Train Loss = 0.434404, Test Loss = 0.251014, Learning Rate = 1.390348e-06\n",
      "Epoch 17311/20000: Train Loss = 0.434246, Test Loss = 0.250719, Learning Rate = 1.389820e-06\n",
      "Epoch 17312/20000: Train Loss = 0.434440, Test Loss = 0.250762, Learning Rate = 1.389292e-06\n",
      "Epoch 17313/20000: Train Loss = 0.434332, Test Loss = 0.251184, Learning Rate = 1.388764e-06\n",
      "Epoch 17314/20000: Train Loss = 0.434705, Test Loss = 0.250326, Learning Rate = 1.388236e-06\n",
      "Epoch 17315/20000: Train Loss = 0.434442, Test Loss = 0.252138, Learning Rate = 1.387709e-06\n",
      "Epoch 17316/20000: Train Loss = 0.434131, Test Loss = 0.251085, Learning Rate = 1.387181e-06\n",
      "Epoch 17317/20000: Train Loss = 0.434506, Test Loss = 0.251444, Learning Rate = 1.386654e-06\n",
      "Epoch 17318/20000: Train Loss = 0.434513, Test Loss = 0.251115, Learning Rate = 1.386127e-06\n",
      "Epoch 17319/20000: Train Loss = 0.434601, Test Loss = 0.250607, Learning Rate = 1.385601e-06\n",
      "Epoch 17320/20000: Train Loss = 0.434668, Test Loss = 0.250301, Learning Rate = 1.385074e-06\n",
      "Epoch 17321/20000: Train Loss = 0.434414, Test Loss = 0.249963, Learning Rate = 1.384548e-06\n",
      "Epoch 17322/20000: Train Loss = 0.434234, Test Loss = 0.250165, Learning Rate = 1.384022e-06\n",
      "Epoch 17323/20000: Train Loss = 0.434378, Test Loss = 0.251206, Learning Rate = 1.383496e-06\n",
      "Epoch 17324/20000: Train Loss = 0.434438, Test Loss = 0.250389, Learning Rate = 1.382970e-06\n",
      "Epoch 17325/20000: Train Loss = 0.434362, Test Loss = 0.250226, Learning Rate = 1.382445e-06\n",
      "Epoch 17326/20000: Train Loss = 0.434328, Test Loss = 0.250345, Learning Rate = 1.381920e-06\n",
      "Epoch 17327/20000: Train Loss = 0.434446, Test Loss = 0.250569, Learning Rate = 1.381394e-06\n",
      "Epoch 17328/20000: Train Loss = 0.434452, Test Loss = 0.251440, Learning Rate = 1.380870e-06\n",
      "Epoch 17329/20000: Train Loss = 0.434309, Test Loss = 0.251674, Learning Rate = 1.380345e-06\n",
      "Epoch 17330/20000: Train Loss = 0.434468, Test Loss = 0.251563, Learning Rate = 1.379820e-06\n",
      "Epoch 17331/20000: Train Loss = 0.434358, Test Loss = 0.250766, Learning Rate = 1.379296e-06\n",
      "Epoch 17332/20000: Train Loss = 0.434254, Test Loss = 0.251504, Learning Rate = 1.378772e-06\n",
      "Epoch 17333/20000: Train Loss = 0.434332, Test Loss = 0.250961, Learning Rate = 1.378248e-06\n",
      "Epoch 17334/20000: Train Loss = 0.434244, Test Loss = 0.251707, Learning Rate = 1.377724e-06\n",
      "Epoch 17335/20000: Train Loss = 0.434306, Test Loss = 0.251933, Learning Rate = 1.377201e-06\n",
      "Epoch 17336/20000: Train Loss = 0.434496, Test Loss = 0.252151, Learning Rate = 1.376678e-06\n",
      "Epoch 17337/20000: Train Loss = 0.434427, Test Loss = 0.252294, Learning Rate = 1.376155e-06\n",
      "Epoch 17338/20000: Train Loss = 0.434500, Test Loss = 0.252545, Learning Rate = 1.375632e-06\n",
      "Epoch 17339/20000: Train Loss = 0.434545, Test Loss = 0.253535, Learning Rate = 1.375109e-06\n",
      "Epoch 17340/20000: Train Loss = 0.434822, Test Loss = 0.252223, Learning Rate = 1.374586e-06\n",
      "Epoch 17341/20000: Train Loss = 0.434247, Test Loss = 0.250971, Learning Rate = 1.374064e-06\n",
      "Epoch 17342/20000: Train Loss = 0.434302, Test Loss = 0.252425, Learning Rate = 1.373542e-06\n",
      "Epoch 17343/20000: Train Loss = 0.434393, Test Loss = 0.252223, Learning Rate = 1.373020e-06\n",
      "Epoch 17344/20000: Train Loss = 0.434273, Test Loss = 0.252724, Learning Rate = 1.372498e-06\n",
      "Epoch 17345/20000: Train Loss = 0.434489, Test Loss = 0.252948, Learning Rate = 1.371977e-06\n",
      "Epoch 17346/20000: Train Loss = 0.434254, Test Loss = 0.252688, Learning Rate = 1.371456e-06\n",
      "Epoch 17347/20000: Train Loss = 0.434349, Test Loss = 0.253110, Learning Rate = 1.370934e-06\n",
      "Epoch 17348/20000: Train Loss = 0.434320, Test Loss = 0.252838, Learning Rate = 1.370414e-06\n",
      "Epoch 17349/20000: Train Loss = 0.434590, Test Loss = 0.252805, Learning Rate = 1.369893e-06\n",
      "Epoch 17350/20000: Train Loss = 0.434333, Test Loss = 0.252380, Learning Rate = 1.369372e-06\n",
      "Epoch 17351/20000: Train Loss = 0.434495, Test Loss = 0.251985, Learning Rate = 1.368852e-06\n",
      "Epoch 17352/20000: Train Loss = 0.434449, Test Loss = 0.253215, Learning Rate = 1.368332e-06\n",
      "Epoch 17353/20000: Train Loss = 0.434352, Test Loss = 0.251672, Learning Rate = 1.367812e-06\n",
      "Epoch 17354/20000: Train Loss = 0.434486, Test Loss = 0.251871, Learning Rate = 1.367292e-06\n",
      "Epoch 17355/20000: Train Loss = 0.434322, Test Loss = 0.252241, Learning Rate = 1.366773e-06\n",
      "Epoch 17356/20000: Train Loss = 0.434333, Test Loss = 0.251779, Learning Rate = 1.366253e-06\n",
      "Epoch 17357/20000: Train Loss = 0.434306, Test Loss = 0.251317, Learning Rate = 1.365734e-06\n",
      "Epoch 17358/20000: Train Loss = 0.434352, Test Loss = 0.252202, Learning Rate = 1.365215e-06\n",
      "Epoch 17359/20000: Train Loss = 0.434329, Test Loss = 0.251953, Learning Rate = 1.364696e-06\n",
      "Epoch 17360/20000: Train Loss = 0.434330, Test Loss = 0.251973, Learning Rate = 1.364178e-06\n",
      "Epoch 17361/20000: Train Loss = 0.434429, Test Loss = 0.252245, Learning Rate = 1.363660e-06\n",
      "Epoch 17362/20000: Train Loss = 0.434342, Test Loss = 0.251479, Learning Rate = 1.363141e-06\n",
      "Epoch 17363/20000: Train Loss = 0.434289, Test Loss = 0.252181, Learning Rate = 1.362623e-06\n",
      "Epoch 17364/20000: Train Loss = 0.434300, Test Loss = 0.252357, Learning Rate = 1.362106e-06\n",
      "Epoch 17365/20000: Train Loss = 0.434305, Test Loss = 0.252575, Learning Rate = 1.361588e-06\n",
      "Epoch 17366/20000: Train Loss = 0.434330, Test Loss = 0.252497, Learning Rate = 1.361071e-06\n",
      "Epoch 17367/20000: Train Loss = 0.434305, Test Loss = 0.252106, Learning Rate = 1.360554e-06\n",
      "Epoch 17368/20000: Train Loss = 0.434597, Test Loss = 0.252785, Learning Rate = 1.360037e-06\n",
      "Epoch 17369/20000: Train Loss = 0.434477, Test Loss = 0.253025, Learning Rate = 1.359520e-06\n",
      "Epoch 17370/20000: Train Loss = 0.434342, Test Loss = 0.253163, Learning Rate = 1.359003e-06\n",
      "Epoch 17371/20000: Train Loss = 0.434478, Test Loss = 0.252557, Learning Rate = 1.358487e-06\n",
      "Epoch 17372/20000: Train Loss = 0.434428, Test Loss = 0.252697, Learning Rate = 1.357971e-06\n",
      "Epoch 17373/20000: Train Loss = 0.434450, Test Loss = 0.253186, Learning Rate = 1.357455e-06\n",
      "Epoch 17374/20000: Train Loss = 0.434447, Test Loss = 0.253690, Learning Rate = 1.356939e-06\n",
      "Epoch 17375/20000: Train Loss = 0.434298, Test Loss = 0.253337, Learning Rate = 1.356423e-06\n",
      "Epoch 17376/20000: Train Loss = 0.434475, Test Loss = 0.252503, Learning Rate = 1.355908e-06\n",
      "Epoch 17377/20000: Train Loss = 0.434445, Test Loss = 0.252721, Learning Rate = 1.355393e-06\n",
      "Epoch 17378/20000: Train Loss = 0.434611, Test Loss = 0.252339, Learning Rate = 1.354878e-06\n",
      "Epoch 17379/20000: Train Loss = 0.434332, Test Loss = 0.252301, Learning Rate = 1.354363e-06\n",
      "Epoch 17380/20000: Train Loss = 0.434470, Test Loss = 0.250996, Learning Rate = 1.353848e-06\n",
      "Epoch 17381/20000: Train Loss = 0.434247, Test Loss = 0.251596, Learning Rate = 1.353334e-06\n",
      "Epoch 17382/20000: Train Loss = 0.434320, Test Loss = 0.251239, Learning Rate = 1.352820e-06\n",
      "Epoch 17383/20000: Train Loss = 0.434363, Test Loss = 0.251894, Learning Rate = 1.352306e-06\n",
      "Epoch 17384/20000: Train Loss = 0.434612, Test Loss = 0.251279, Learning Rate = 1.351792e-06\n",
      "Epoch 17385/20000: Train Loss = 0.434465, Test Loss = 0.252004, Learning Rate = 1.351278e-06\n",
      "Epoch 17386/20000: Train Loss = 0.434272, Test Loss = 0.252141, Learning Rate = 1.350765e-06\n",
      "Epoch 17387/20000: Train Loss = 0.434328, Test Loss = 0.252108, Learning Rate = 1.350251e-06\n",
      "Epoch 17388/20000: Train Loss = 0.434501, Test Loss = 0.252330, Learning Rate = 1.349738e-06\n",
      "Epoch 17389/20000: Train Loss = 0.434240, Test Loss = 0.251130, Learning Rate = 1.349225e-06\n",
      "Epoch 17390/20000: Train Loss = 0.434536, Test Loss = 0.252193, Learning Rate = 1.348713e-06\n",
      "Epoch 17391/20000: Train Loss = 0.434442, Test Loss = 0.252063, Learning Rate = 1.348200e-06\n",
      "Epoch 17392/20000: Train Loss = 0.434385, Test Loss = 0.251527, Learning Rate = 1.347688e-06\n",
      "Epoch 17393/20000: Train Loss = 0.434378, Test Loss = 0.253771, Learning Rate = 1.347176e-06\n",
      "Epoch 17394/20000: Train Loss = 0.434435, Test Loss = 0.252829, Learning Rate = 1.346664e-06\n",
      "Epoch 17395/20000: Train Loss = 0.434436, Test Loss = 0.251741, Learning Rate = 1.346152e-06\n",
      "Epoch 17396/20000: Train Loss = 0.434326, Test Loss = 0.251354, Learning Rate = 1.345641e-06\n",
      "Epoch 17397/20000: Train Loss = 0.434409, Test Loss = 0.252785, Learning Rate = 1.345130e-06\n",
      "Epoch 17398/20000: Train Loss = 0.434553, Test Loss = 0.251865, Learning Rate = 1.344618e-06\n",
      "Epoch 17399/20000: Train Loss = 0.434382, Test Loss = 0.252327, Learning Rate = 1.344108e-06\n",
      "Epoch 17400/20000: Train Loss = 0.434498, Test Loss = 0.252928, Learning Rate = 1.343597e-06\n",
      "Epoch 17401/20000: Train Loss = 0.434736, Test Loss = 0.253197, Learning Rate = 1.343086e-06\n",
      "Epoch 17402/20000: Train Loss = 0.434298, Test Loss = 0.252773, Learning Rate = 1.342576e-06\n",
      "Epoch 17403/20000: Train Loss = 0.434362, Test Loss = 0.253300, Learning Rate = 1.342066e-06\n",
      "Epoch 17404/20000: Train Loss = 0.434410, Test Loss = 0.253459, Learning Rate = 1.341556e-06\n",
      "Epoch 17405/20000: Train Loss = 0.434453, Test Loss = 0.252591, Learning Rate = 1.341046e-06\n",
      "Epoch 17406/20000: Train Loss = 0.434479, Test Loss = 0.253051, Learning Rate = 1.340537e-06\n",
      "Epoch 17407/20000: Train Loss = 0.434187, Test Loss = 0.252202, Learning Rate = 1.340027e-06\n",
      "Epoch 17408/20000: Train Loss = 0.434622, Test Loss = 0.251258, Learning Rate = 1.339518e-06\n",
      "Epoch 17409/20000: Train Loss = 0.434388, Test Loss = 0.253182, Learning Rate = 1.339009e-06\n",
      "Epoch 17410/20000: Train Loss = 0.434370, Test Loss = 0.253552, Learning Rate = 1.338500e-06\n",
      "Epoch 17411/20000: Train Loss = 0.434343, Test Loss = 0.252963, Learning Rate = 1.337992e-06\n",
      "Epoch 17412/20000: Train Loss = 0.434485, Test Loss = 0.252258, Learning Rate = 1.337483e-06\n",
      "Epoch 17413/20000: Train Loss = 0.434275, Test Loss = 0.253591, Learning Rate = 1.336975e-06\n",
      "Epoch 17414/20000: Train Loss = 0.434293, Test Loss = 0.253168, Learning Rate = 1.336467e-06\n",
      "Epoch 17415/20000: Train Loss = 0.434287, Test Loss = 0.252047, Learning Rate = 1.335959e-06\n",
      "Epoch 17416/20000: Train Loss = 0.434472, Test Loss = 0.251827, Learning Rate = 1.335452e-06\n",
      "Epoch 17417/20000: Train Loss = 0.434361, Test Loss = 0.251486, Learning Rate = 1.334944e-06\n",
      "Epoch 17418/20000: Train Loss = 0.434520, Test Loss = 0.252317, Learning Rate = 1.334437e-06\n",
      "Epoch 17419/20000: Train Loss = 0.434206, Test Loss = 0.252133, Learning Rate = 1.333930e-06\n",
      "Epoch 17420/20000: Train Loss = 0.434306, Test Loss = 0.252375, Learning Rate = 1.333423e-06\n",
      "Epoch 17421/20000: Train Loss = 0.434294, Test Loss = 0.252668, Learning Rate = 1.332916e-06\n",
      "Epoch 17422/20000: Train Loss = 0.434466, Test Loss = 0.253007, Learning Rate = 1.332410e-06\n",
      "Epoch 17423/20000: Train Loss = 0.434328, Test Loss = 0.252411, Learning Rate = 1.331904e-06\n",
      "Epoch 17424/20000: Train Loss = 0.434232, Test Loss = 0.252456, Learning Rate = 1.331397e-06\n",
      "Epoch 17425/20000: Train Loss = 0.434243, Test Loss = 0.253424, Learning Rate = 1.330892e-06\n",
      "Epoch 17426/20000: Train Loss = 0.434366, Test Loss = 0.253405, Learning Rate = 1.330386e-06\n",
      "Epoch 17427/20000: Train Loss = 0.434255, Test Loss = 0.252879, Learning Rate = 1.329880e-06\n",
      "Epoch 17428/20000: Train Loss = 0.434632, Test Loss = 0.252053, Learning Rate = 1.329375e-06\n",
      "Epoch 17429/20000: Train Loss = 0.434294, Test Loss = 0.252222, Learning Rate = 1.328870e-06\n",
      "Epoch 17430/20000: Train Loss = 0.434397, Test Loss = 0.252061, Learning Rate = 1.328365e-06\n",
      "Epoch 17431/20000: Train Loss = 0.434322, Test Loss = 0.250886, Learning Rate = 1.327860e-06\n",
      "Epoch 17432/20000: Train Loss = 0.434361, Test Loss = 0.252768, Learning Rate = 1.327356e-06\n",
      "Epoch 17433/20000: Train Loss = 0.434442, Test Loss = 0.252092, Learning Rate = 1.326851e-06\n",
      "Epoch 17434/20000: Train Loss = 0.434567, Test Loss = 0.253353, Learning Rate = 1.326347e-06\n",
      "Epoch 17435/20000: Train Loss = 0.434230, Test Loss = 0.252424, Learning Rate = 1.325843e-06\n",
      "Epoch 17436/20000: Train Loss = 0.434414, Test Loss = 0.252908, Learning Rate = 1.325339e-06\n",
      "Epoch 17437/20000: Train Loss = 0.434387, Test Loss = 0.251726, Learning Rate = 1.324836e-06\n",
      "Epoch 17438/20000: Train Loss = 0.434236, Test Loss = 0.253222, Learning Rate = 1.324332e-06\n",
      "Epoch 17439/20000: Train Loss = 0.434690, Test Loss = 0.252290, Learning Rate = 1.323829e-06\n",
      "Epoch 17440/20000: Train Loss = 0.434440, Test Loss = 0.252611, Learning Rate = 1.323326e-06\n",
      "Epoch 17441/20000: Train Loss = 0.434388, Test Loss = 0.251702, Learning Rate = 1.322823e-06\n",
      "Epoch 17442/20000: Train Loss = 0.434262, Test Loss = 0.252117, Learning Rate = 1.322321e-06\n",
      "Epoch 17443/20000: Train Loss = 0.434275, Test Loss = 0.252304, Learning Rate = 1.321818e-06\n",
      "Epoch 17444/20000: Train Loss = 0.434482, Test Loss = 0.251042, Learning Rate = 1.321316e-06\n",
      "Epoch 17445/20000: Train Loss = 0.434311, Test Loss = 0.251096, Learning Rate = 1.320814e-06\n",
      "Epoch 17446/20000: Train Loss = 0.434282, Test Loss = 0.251718, Learning Rate = 1.320312e-06\n",
      "Epoch 17447/20000: Train Loss = 0.434428, Test Loss = 0.252934, Learning Rate = 1.319810e-06\n",
      "Epoch 17448/20000: Train Loss = 0.434233, Test Loss = 0.252302, Learning Rate = 1.319309e-06\n",
      "Epoch 17449/20000: Train Loss = 0.434534, Test Loss = 0.252945, Learning Rate = 1.318808e-06\n",
      "Epoch 17450/20000: Train Loss = 0.434713, Test Loss = 0.252413, Learning Rate = 1.318306e-06\n",
      "Epoch 17451/20000: Train Loss = 0.434382, Test Loss = 0.252217, Learning Rate = 1.317806e-06\n",
      "Epoch 17452/20000: Train Loss = 0.434321, Test Loss = 0.252702, Learning Rate = 1.317305e-06\n",
      "Epoch 17453/20000: Train Loss = 0.434361, Test Loss = 0.251917, Learning Rate = 1.316804e-06\n",
      "Epoch 17454/20000: Train Loss = 0.434299, Test Loss = 0.252588, Learning Rate = 1.316304e-06\n",
      "Epoch 17455/20000: Train Loss = 0.434433, Test Loss = 0.252904, Learning Rate = 1.315804e-06\n",
      "Epoch 17456/20000: Train Loss = 0.434352, Test Loss = 0.252090, Learning Rate = 1.315304e-06\n",
      "Epoch 17457/20000: Train Loss = 0.434358, Test Loss = 0.251295, Learning Rate = 1.314804e-06\n",
      "Epoch 17458/20000: Train Loss = 0.434560, Test Loss = 0.251297, Learning Rate = 1.314304e-06\n",
      "Epoch 17459/20000: Train Loss = 0.434339, Test Loss = 0.251958, Learning Rate = 1.313805e-06\n",
      "Epoch 17460/20000: Train Loss = 0.434238, Test Loss = 0.251000, Learning Rate = 1.313306e-06\n",
      "Epoch 17461/20000: Train Loss = 0.434211, Test Loss = 0.250868, Learning Rate = 1.312807e-06\n",
      "Epoch 17462/20000: Train Loss = 0.434579, Test Loss = 0.251207, Learning Rate = 1.312308e-06\n",
      "Epoch 17463/20000: Train Loss = 0.434570, Test Loss = 0.250686, Learning Rate = 1.311809e-06\n",
      "Epoch 17464/20000: Train Loss = 0.434301, Test Loss = 0.250437, Learning Rate = 1.311311e-06\n",
      "Epoch 17465/20000: Train Loss = 0.434394, Test Loss = 0.250186, Learning Rate = 1.310813e-06\n",
      "Epoch 17466/20000: Train Loss = 0.434291, Test Loss = 0.251468, Learning Rate = 1.310315e-06\n",
      "Epoch 17467/20000: Train Loss = 0.434410, Test Loss = 0.251210, Learning Rate = 1.309817e-06\n",
      "Epoch 17468/20000: Train Loss = 0.434296, Test Loss = 0.250665, Learning Rate = 1.309319e-06\n",
      "Epoch 17469/20000: Train Loss = 0.434714, Test Loss = 0.251842, Learning Rate = 1.308821e-06\n",
      "Epoch 17470/20000: Train Loss = 0.434343, Test Loss = 0.252157, Learning Rate = 1.308324e-06\n",
      "Epoch 17471/20000: Train Loss = 0.434365, Test Loss = 0.252348, Learning Rate = 1.307827e-06\n",
      "Epoch 17472/20000: Train Loss = 0.434256, Test Loss = 0.252536, Learning Rate = 1.307330e-06\n",
      "Epoch 17473/20000: Train Loss = 0.434331, Test Loss = 0.251672, Learning Rate = 1.306833e-06\n",
      "Epoch 17474/20000: Train Loss = 0.434707, Test Loss = 0.253295, Learning Rate = 1.306337e-06\n",
      "Epoch 17475/20000: Train Loss = 0.434314, Test Loss = 0.252411, Learning Rate = 1.305840e-06\n",
      "Epoch 17476/20000: Train Loss = 0.434292, Test Loss = 0.252163, Learning Rate = 1.305344e-06\n",
      "Epoch 17477/20000: Train Loss = 0.434451, Test Loss = 0.251855, Learning Rate = 1.304848e-06\n",
      "Epoch 17478/20000: Train Loss = 0.434210, Test Loss = 0.252172, Learning Rate = 1.304352e-06\n",
      "Epoch 17479/20000: Train Loss = 0.434636, Test Loss = 0.252650, Learning Rate = 1.303857e-06\n",
      "Epoch 17480/20000: Train Loss = 0.434569, Test Loss = 0.252108, Learning Rate = 1.303361e-06\n",
      "Epoch 17481/20000: Train Loss = 0.434367, Test Loss = 0.252788, Learning Rate = 1.302866e-06\n",
      "Epoch 17482/20000: Train Loss = 0.434236, Test Loss = 0.251500, Learning Rate = 1.302371e-06\n",
      "Epoch 17483/20000: Train Loss = 0.434545, Test Loss = 0.252259, Learning Rate = 1.301876e-06\n",
      "Epoch 17484/20000: Train Loss = 0.434518, Test Loss = 0.252662, Learning Rate = 1.301382e-06\n",
      "Epoch 17485/20000: Train Loss = 0.434413, Test Loss = 0.252931, Learning Rate = 1.300887e-06\n",
      "Epoch 17486/20000: Train Loss = 0.434251, Test Loss = 0.252556, Learning Rate = 1.300393e-06\n",
      "Epoch 17487/20000: Train Loss = 0.434365, Test Loss = 0.252388, Learning Rate = 1.299899e-06\n",
      "Epoch 17488/20000: Train Loss = 0.434284, Test Loss = 0.252490, Learning Rate = 1.299405e-06\n",
      "Epoch 17489/20000: Train Loss = 0.434388, Test Loss = 0.252623, Learning Rate = 1.298911e-06\n",
      "Epoch 17490/20000: Train Loss = 0.434256, Test Loss = 0.253468, Learning Rate = 1.298417e-06\n",
      "Epoch 17491/20000: Train Loss = 0.434310, Test Loss = 0.251948, Learning Rate = 1.297924e-06\n",
      "Epoch 17492/20000: Train Loss = 0.434327, Test Loss = 0.253394, Learning Rate = 1.297431e-06\n",
      "Epoch 17493/20000: Train Loss = 0.434228, Test Loss = 0.252551, Learning Rate = 1.296938e-06\n",
      "Epoch 17494/20000: Train Loss = 0.434340, Test Loss = 0.252117, Learning Rate = 1.296445e-06\n",
      "Epoch 17495/20000: Train Loss = 0.434599, Test Loss = 0.253391, Learning Rate = 1.295952e-06\n",
      "Epoch 17496/20000: Train Loss = 0.434825, Test Loss = 0.251884, Learning Rate = 1.295460e-06\n",
      "Epoch 17497/20000: Train Loss = 0.434366, Test Loss = 0.251521, Learning Rate = 1.294968e-06\n",
      "Epoch 17498/20000: Train Loss = 0.434289, Test Loss = 0.251973, Learning Rate = 1.294476e-06\n",
      "Epoch 17499/20000: Train Loss = 0.434345, Test Loss = 0.252329, Learning Rate = 1.293984e-06\n",
      "Epoch 17500/20000: Train Loss = 0.434247, Test Loss = 0.253269, Learning Rate = 1.293492e-06\n",
      "Epoch 17501/20000: Train Loss = 0.434211, Test Loss = 0.253028, Learning Rate = 1.293001e-06\n",
      "Epoch 17502/20000: Train Loss = 0.434416, Test Loss = 0.252141, Learning Rate = 1.292509e-06\n",
      "Epoch 17503/20000: Train Loss = 0.434700, Test Loss = 0.251920, Learning Rate = 1.292018e-06\n",
      "Epoch 17504/20000: Train Loss = 0.434211, Test Loss = 0.251077, Learning Rate = 1.291527e-06\n",
      "Epoch 17505/20000: Train Loss = 0.434270, Test Loss = 0.252130, Learning Rate = 1.291037e-06\n",
      "Epoch 17506/20000: Train Loss = 0.434270, Test Loss = 0.252049, Learning Rate = 1.290546e-06\n",
      "Epoch 17507/20000: Train Loss = 0.434315, Test Loss = 0.252439, Learning Rate = 1.290056e-06\n",
      "Epoch 17508/20000: Train Loss = 0.434437, Test Loss = 0.253338, Learning Rate = 1.289565e-06\n",
      "Epoch 17509/20000: Train Loss = 0.434513, Test Loss = 0.253582, Learning Rate = 1.289075e-06\n",
      "Epoch 17510/20000: Train Loss = 0.434407, Test Loss = 0.253620, Learning Rate = 1.288586e-06\n",
      "Epoch 17511/20000: Train Loss = 0.434353, Test Loss = 0.253152, Learning Rate = 1.288096e-06\n",
      "Epoch 17512/20000: Train Loss = 0.434417, Test Loss = 0.253770, Learning Rate = 1.287607e-06\n",
      "Epoch 17513/20000: Train Loss = 0.434538, Test Loss = 0.252753, Learning Rate = 1.287117e-06\n",
      "Epoch 17514/20000: Train Loss = 0.434278, Test Loss = 0.254229, Learning Rate = 1.286628e-06\n",
      "Epoch 17515/20000: Train Loss = 0.434449, Test Loss = 0.253873, Learning Rate = 1.286139e-06\n",
      "Epoch 17516/20000: Train Loss = 0.434601, Test Loss = 0.254144, Learning Rate = 1.285651e-06\n",
      "Epoch 17517/20000: Train Loss = 0.434417, Test Loss = 0.254094, Learning Rate = 1.285162e-06\n",
      "Epoch 17518/20000: Train Loss = 0.434234, Test Loss = 0.253871, Learning Rate = 1.284674e-06\n",
      "Epoch 17519/20000: Train Loss = 0.434264, Test Loss = 0.254263, Learning Rate = 1.284186e-06\n",
      "Epoch 17520/20000: Train Loss = 0.434443, Test Loss = 0.254619, Learning Rate = 1.283698e-06\n",
      "Epoch 17521/20000: Train Loss = 0.434281, Test Loss = 0.253069, Learning Rate = 1.283210e-06\n",
      "Epoch 17522/20000: Train Loss = 0.434439, Test Loss = 0.254265, Learning Rate = 1.282722e-06\n",
      "Epoch 17523/20000: Train Loss = 0.434282, Test Loss = 0.254839, Learning Rate = 1.282235e-06\n",
      "Epoch 17524/20000: Train Loss = 0.434316, Test Loss = 0.254910, Learning Rate = 1.281748e-06\n",
      "Epoch 17525/20000: Train Loss = 0.434312, Test Loss = 0.253727, Learning Rate = 1.281261e-06\n",
      "Epoch 17526/20000: Train Loss = 0.434388, Test Loss = 0.253676, Learning Rate = 1.280774e-06\n",
      "Epoch 17527/20000: Train Loss = 0.434233, Test Loss = 0.253810, Learning Rate = 1.280287e-06\n",
      "Epoch 17528/20000: Train Loss = 0.434322, Test Loss = 0.254613, Learning Rate = 1.279801e-06\n",
      "Epoch 17529/20000: Train Loss = 0.434202, Test Loss = 0.254264, Learning Rate = 1.279314e-06\n",
      "Epoch 17530/20000: Train Loss = 0.434369, Test Loss = 0.254433, Learning Rate = 1.278828e-06\n",
      "Epoch 17531/20000: Train Loss = 0.434479, Test Loss = 0.253748, Learning Rate = 1.278342e-06\n",
      "Epoch 17532/20000: Train Loss = 0.434290, Test Loss = 0.253795, Learning Rate = 1.277857e-06\n",
      "Epoch 17533/20000: Train Loss = 0.434272, Test Loss = 0.253949, Learning Rate = 1.277371e-06\n",
      "Epoch 17534/20000: Train Loss = 0.434856, Test Loss = 0.252848, Learning Rate = 1.276886e-06\n",
      "Epoch 17535/20000: Train Loss = 0.434191, Test Loss = 0.254681, Learning Rate = 1.276401e-06\n",
      "Epoch 17536/20000: Train Loss = 0.434602, Test Loss = 0.253080, Learning Rate = 1.275916e-06\n",
      "Epoch 17537/20000: Train Loss = 0.434367, Test Loss = 0.252682, Learning Rate = 1.275431e-06\n",
      "Epoch 17538/20000: Train Loss = 0.434268, Test Loss = 0.252455, Learning Rate = 1.274946e-06\n",
      "Epoch 17539/20000: Train Loss = 0.434221, Test Loss = 0.252821, Learning Rate = 1.274462e-06\n",
      "Epoch 17540/20000: Train Loss = 0.434297, Test Loss = 0.253281, Learning Rate = 1.273977e-06\n",
      "Epoch 17541/20000: Train Loss = 0.434374, Test Loss = 0.253632, Learning Rate = 1.273493e-06\n",
      "Epoch 17542/20000: Train Loss = 0.434513, Test Loss = 0.253479, Learning Rate = 1.273009e-06\n",
      "Epoch 17543/20000: Train Loss = 0.434422, Test Loss = 0.252872, Learning Rate = 1.272526e-06\n",
      "Epoch 17544/20000: Train Loss = 0.434454, Test Loss = 0.253278, Learning Rate = 1.272042e-06\n",
      "Epoch 17545/20000: Train Loss = 0.434507, Test Loss = 0.252852, Learning Rate = 1.271559e-06\n",
      "Epoch 17546/20000: Train Loss = 0.434227, Test Loss = 0.254100, Learning Rate = 1.271076e-06\n",
      "Epoch 17547/20000: Train Loss = 0.434282, Test Loss = 0.253609, Learning Rate = 1.270593e-06\n",
      "Epoch 17548/20000: Train Loss = 0.434350, Test Loss = 0.253929, Learning Rate = 1.270110e-06\n",
      "Epoch 17549/20000: Train Loss = 0.434537, Test Loss = 0.253193, Learning Rate = 1.269627e-06\n",
      "Epoch 17550/20000: Train Loss = 0.434366, Test Loss = 0.253247, Learning Rate = 1.269145e-06\n",
      "Epoch 17551/20000: Train Loss = 0.434376, Test Loss = 0.253148, Learning Rate = 1.268663e-06\n",
      "Epoch 17552/20000: Train Loss = 0.434253, Test Loss = 0.253826, Learning Rate = 1.268181e-06\n",
      "Epoch 17553/20000: Train Loss = 0.434458, Test Loss = 0.253536, Learning Rate = 1.267699e-06\n",
      "Epoch 17554/20000: Train Loss = 0.434192, Test Loss = 0.252541, Learning Rate = 1.267217e-06\n",
      "Epoch 17555/20000: Train Loss = 0.434342, Test Loss = 0.252513, Learning Rate = 1.266736e-06\n",
      "Epoch 17556/20000: Train Loss = 0.434374, Test Loss = 0.252798, Learning Rate = 1.266254e-06\n",
      "Epoch 17557/20000: Train Loss = 0.434206, Test Loss = 0.253710, Learning Rate = 1.265773e-06\n",
      "Epoch 17558/20000: Train Loss = 0.434343, Test Loss = 0.254707, Learning Rate = 1.265292e-06\n",
      "Epoch 17559/20000: Train Loss = 0.434378, Test Loss = 0.252999, Learning Rate = 1.264811e-06\n",
      "Epoch 17560/20000: Train Loss = 0.434300, Test Loss = 0.253763, Learning Rate = 1.264331e-06\n",
      "Epoch 17561/20000: Train Loss = 0.434248, Test Loss = 0.254083, Learning Rate = 1.263850e-06\n",
      "Epoch 17562/20000: Train Loss = 0.434252, Test Loss = 0.253384, Learning Rate = 1.263370e-06\n",
      "Epoch 17563/20000: Train Loss = 0.434364, Test Loss = 0.253131, Learning Rate = 1.262890e-06\n",
      "Epoch 17564/20000: Train Loss = 0.434326, Test Loss = 0.253892, Learning Rate = 1.262410e-06\n",
      "Epoch 17565/20000: Train Loss = 0.434265, Test Loss = 0.253200, Learning Rate = 1.261931e-06\n",
      "Epoch 17566/20000: Train Loss = 0.434343, Test Loss = 0.254424, Learning Rate = 1.261451e-06\n",
      "Epoch 17567/20000: Train Loss = 0.434768, Test Loss = 0.253763, Learning Rate = 1.260972e-06\n",
      "Epoch 17568/20000: Train Loss = 0.434473, Test Loss = 0.253328, Learning Rate = 1.260493e-06\n",
      "Epoch 17569/20000: Train Loss = 0.434289, Test Loss = 0.253201, Learning Rate = 1.260014e-06\n",
      "Epoch 17570/20000: Train Loss = 0.434405, Test Loss = 0.252741, Learning Rate = 1.259535e-06\n",
      "Epoch 17571/20000: Train Loss = 0.434331, Test Loss = 0.253209, Learning Rate = 1.259056e-06\n",
      "Epoch 17572/20000: Train Loss = 0.434274, Test Loss = 0.253439, Learning Rate = 1.258578e-06\n",
      "Epoch 17573/20000: Train Loss = 0.434325, Test Loss = 0.253240, Learning Rate = 1.258100e-06\n",
      "Epoch 17574/20000: Train Loss = 0.434377, Test Loss = 0.253505, Learning Rate = 1.257622e-06\n",
      "Epoch 17575/20000: Train Loss = 0.434280, Test Loss = 0.253684, Learning Rate = 1.257144e-06\n",
      "Epoch 17576/20000: Train Loss = 0.434367, Test Loss = 0.253844, Learning Rate = 1.256666e-06\n",
      "Epoch 17577/20000: Train Loss = 0.434336, Test Loss = 0.254426, Learning Rate = 1.256189e-06\n",
      "Epoch 17578/20000: Train Loss = 0.434317, Test Loss = 0.255102, Learning Rate = 1.255711e-06\n",
      "Epoch 17579/20000: Train Loss = 0.434318, Test Loss = 0.254172, Learning Rate = 1.255234e-06\n",
      "Epoch 17580/20000: Train Loss = 0.434216, Test Loss = 0.254659, Learning Rate = 1.254757e-06\n",
      "Epoch 17581/20000: Train Loss = 0.434325, Test Loss = 0.254418, Learning Rate = 1.254280e-06\n",
      "Epoch 17582/20000: Train Loss = 0.434442, Test Loss = 0.254894, Learning Rate = 1.253804e-06\n",
      "Epoch 17583/20000: Train Loss = 0.434299, Test Loss = 0.254871, Learning Rate = 1.253327e-06\n",
      "Epoch 17584/20000: Train Loss = 0.434400, Test Loss = 0.254351, Learning Rate = 1.252851e-06\n",
      "Epoch 17585/20000: Train Loss = 0.434360, Test Loss = 0.253919, Learning Rate = 1.252375e-06\n",
      "Epoch 17586/20000: Train Loss = 0.434253, Test Loss = 0.254250, Learning Rate = 1.251899e-06\n",
      "Epoch 17587/20000: Train Loss = 0.434287, Test Loss = 0.253718, Learning Rate = 1.251424e-06\n",
      "Epoch 17588/20000: Train Loss = 0.434248, Test Loss = 0.253193, Learning Rate = 1.250948e-06\n",
      "Epoch 17589/20000: Train Loss = 0.434283, Test Loss = 0.252992, Learning Rate = 1.250473e-06\n",
      "Epoch 17590/20000: Train Loss = 0.434391, Test Loss = 0.252971, Learning Rate = 1.249998e-06\n",
      "Epoch 17591/20000: Train Loss = 0.434581, Test Loss = 0.252599, Learning Rate = 1.249523e-06\n",
      "Epoch 17592/20000: Train Loss = 0.434288, Test Loss = 0.252668, Learning Rate = 1.249048e-06\n",
      "Epoch 17593/20000: Train Loss = 0.434522, Test Loss = 0.252979, Learning Rate = 1.248573e-06\n",
      "Epoch 17594/20000: Train Loss = 0.434257, Test Loss = 0.252624, Learning Rate = 1.248099e-06\n",
      "Epoch 17595/20000: Train Loss = 0.434273, Test Loss = 0.252788, Learning Rate = 1.247625e-06\n",
      "Epoch 17596/20000: Train Loss = 0.434215, Test Loss = 0.252601, Learning Rate = 1.247150e-06\n",
      "Epoch 17597/20000: Train Loss = 0.434290, Test Loss = 0.252001, Learning Rate = 1.246677e-06\n",
      "Epoch 17598/20000: Train Loss = 0.434196, Test Loss = 0.253109, Learning Rate = 1.246203e-06\n",
      "Epoch 17599/20000: Train Loss = 0.434298, Test Loss = 0.253798, Learning Rate = 1.245729e-06\n",
      "Epoch 17600/20000: Train Loss = 0.434666, Test Loss = 0.253468, Learning Rate = 1.245256e-06\n",
      "Epoch 17601/20000: Train Loss = 0.434211, Test Loss = 0.255238, Learning Rate = 1.244783e-06\n",
      "Epoch 17602/20000: Train Loss = 0.434348, Test Loss = 0.253435, Learning Rate = 1.244310e-06\n",
      "Epoch 17603/20000: Train Loss = 0.434639, Test Loss = 0.254191, Learning Rate = 1.243837e-06\n",
      "Epoch 17604/20000: Train Loss = 0.434338, Test Loss = 0.253290, Learning Rate = 1.243364e-06\n",
      "Epoch 17605/20000: Train Loss = 0.434281, Test Loss = 0.253353, Learning Rate = 1.242892e-06\n",
      "Epoch 17606/20000: Train Loss = 0.434305, Test Loss = 0.253473, Learning Rate = 1.242420e-06\n",
      "Epoch 17607/20000: Train Loss = 0.434379, Test Loss = 0.254545, Learning Rate = 1.241948e-06\n",
      "Epoch 17608/20000: Train Loss = 0.434285, Test Loss = 0.254157, Learning Rate = 1.241476e-06\n",
      "Epoch 17609/20000: Train Loss = 0.434382, Test Loss = 0.252883, Learning Rate = 1.241004e-06\n",
      "Epoch 17610/20000: Train Loss = 0.434398, Test Loss = 0.253807, Learning Rate = 1.240532e-06\n",
      "Epoch 17611/20000: Train Loss = 0.434243, Test Loss = 0.253904, Learning Rate = 1.240061e-06\n",
      "Epoch 17612/20000: Train Loss = 0.434316, Test Loss = 0.254318, Learning Rate = 1.239590e-06\n",
      "Epoch 17613/20000: Train Loss = 0.434261, Test Loss = 0.254502, Learning Rate = 1.239119e-06\n",
      "Epoch 17614/20000: Train Loss = 0.434267, Test Loss = 0.253895, Learning Rate = 1.238648e-06\n",
      "Epoch 17615/20000: Train Loss = 0.434249, Test Loss = 0.253962, Learning Rate = 1.238177e-06\n",
      "Epoch 17616/20000: Train Loss = 0.434226, Test Loss = 0.255064, Learning Rate = 1.237707e-06\n",
      "Epoch 17617/20000: Train Loss = 0.434268, Test Loss = 0.254552, Learning Rate = 1.237237e-06\n",
      "Epoch 17618/20000: Train Loss = 0.434182, Test Loss = 0.253687, Learning Rate = 1.236767e-06\n",
      "Epoch 17619/20000: Train Loss = 0.434354, Test Loss = 0.253210, Learning Rate = 1.236297e-06\n",
      "Epoch 17620/20000: Train Loss = 0.434258, Test Loss = 0.253648, Learning Rate = 1.235827e-06\n",
      "Epoch 17621/20000: Train Loss = 0.434286, Test Loss = 0.252627, Learning Rate = 1.235357e-06\n",
      "Epoch 17622/20000: Train Loss = 0.434289, Test Loss = 0.253091, Learning Rate = 1.234888e-06\n",
      "Epoch 17623/20000: Train Loss = 0.434321, Test Loss = 0.252276, Learning Rate = 1.234419e-06\n",
      "Epoch 17624/20000: Train Loss = 0.434251, Test Loss = 0.253158, Learning Rate = 1.233950e-06\n",
      "Epoch 17625/20000: Train Loss = 0.434342, Test Loss = 0.252124, Learning Rate = 1.233481e-06\n",
      "Epoch 17626/20000: Train Loss = 0.434224, Test Loss = 0.252651, Learning Rate = 1.233012e-06\n",
      "Epoch 17627/20000: Train Loss = 0.434244, Test Loss = 0.253288, Learning Rate = 1.232544e-06\n",
      "Epoch 17628/20000: Train Loss = 0.434344, Test Loss = 0.253436, Learning Rate = 1.232075e-06\n",
      "Epoch 17629/20000: Train Loss = 0.434652, Test Loss = 0.252838, Learning Rate = 1.231607e-06\n",
      "Epoch 17630/20000: Train Loss = 0.434274, Test Loss = 0.253480, Learning Rate = 1.231139e-06\n",
      "Epoch 17631/20000: Train Loss = 0.434436, Test Loss = 0.253950, Learning Rate = 1.230671e-06\n",
      "Epoch 17632/20000: Train Loss = 0.434233, Test Loss = 0.253371, Learning Rate = 1.230204e-06\n",
      "Epoch 17633/20000: Train Loss = 0.434223, Test Loss = 0.253791, Learning Rate = 1.229736e-06\n",
      "Epoch 17634/20000: Train Loss = 0.434280, Test Loss = 0.253327, Learning Rate = 1.229269e-06\n",
      "Epoch 17635/20000: Train Loss = 0.434240, Test Loss = 0.252935, Learning Rate = 1.228802e-06\n",
      "Epoch 17636/20000: Train Loss = 0.434268, Test Loss = 0.253633, Learning Rate = 1.228335e-06\n",
      "Epoch 17637/20000: Train Loss = 0.434299, Test Loss = 0.253167, Learning Rate = 1.227868e-06\n",
      "Epoch 17638/20000: Train Loss = 0.434297, Test Loss = 0.254005, Learning Rate = 1.227402e-06\n",
      "Epoch 17639/20000: Train Loss = 0.434492, Test Loss = 0.253309, Learning Rate = 1.226935e-06\n",
      "Epoch 17640/20000: Train Loss = 0.434230, Test Loss = 0.254317, Learning Rate = 1.226469e-06\n",
      "Epoch 17641/20000: Train Loss = 0.434538, Test Loss = 0.254044, Learning Rate = 1.226003e-06\n",
      "Epoch 17642/20000: Train Loss = 0.434510, Test Loss = 0.253176, Learning Rate = 1.225537e-06\n",
      "Epoch 17643/20000: Train Loss = 0.434394, Test Loss = 0.253287, Learning Rate = 1.225072e-06\n",
      "Epoch 17644/20000: Train Loss = 0.434319, Test Loss = 0.253064, Learning Rate = 1.224606e-06\n",
      "Epoch 17645/20000: Train Loss = 0.434248, Test Loss = 0.253429, Learning Rate = 1.224141e-06\n",
      "Epoch 17646/20000: Train Loss = 0.434409, Test Loss = 0.252931, Learning Rate = 1.223676e-06\n",
      "Epoch 17647/20000: Train Loss = 0.434391, Test Loss = 0.252457, Learning Rate = 1.223211e-06\n",
      "Epoch 17648/20000: Train Loss = 0.434328, Test Loss = 0.251924, Learning Rate = 1.222746e-06\n",
      "Epoch 17649/20000: Train Loss = 0.434293, Test Loss = 0.252458, Learning Rate = 1.222281e-06\n",
      "Epoch 17650/20000: Train Loss = 0.434320, Test Loss = 0.251383, Learning Rate = 1.221817e-06\n",
      "Epoch 17651/20000: Train Loss = 0.434336, Test Loss = 0.252216, Learning Rate = 1.221353e-06\n",
      "Epoch 17652/20000: Train Loss = 0.434296, Test Loss = 0.252689, Learning Rate = 1.220888e-06\n",
      "Epoch 17653/20000: Train Loss = 0.434226, Test Loss = 0.252696, Learning Rate = 1.220425e-06\n",
      "Epoch 17654/20000: Train Loss = 0.434222, Test Loss = 0.251954, Learning Rate = 1.219961e-06\n",
      "Epoch 17655/20000: Train Loss = 0.434574, Test Loss = 0.252553, Learning Rate = 1.219497e-06\n",
      "Epoch 17656/20000: Train Loss = 0.434361, Test Loss = 0.251630, Learning Rate = 1.219034e-06\n",
      "Epoch 17657/20000: Train Loss = 0.434247, Test Loss = 0.252352, Learning Rate = 1.218571e-06\n",
      "Epoch 17658/20000: Train Loss = 0.434460, Test Loss = 0.251815, Learning Rate = 1.218108e-06\n",
      "Epoch 17659/20000: Train Loss = 0.434284, Test Loss = 0.253236, Learning Rate = 1.217645e-06\n",
      "Epoch 17660/20000: Train Loss = 0.434555, Test Loss = 0.252812, Learning Rate = 1.217182e-06\n",
      "Epoch 17661/20000: Train Loss = 0.434245, Test Loss = 0.253266, Learning Rate = 1.216720e-06\n",
      "Epoch 17662/20000: Train Loss = 0.434206, Test Loss = 0.252494, Learning Rate = 1.216257e-06\n",
      "Epoch 17663/20000: Train Loss = 0.434318, Test Loss = 0.252949, Learning Rate = 1.215795e-06\n",
      "Epoch 17664/20000: Train Loss = 0.434484, Test Loss = 0.253768, Learning Rate = 1.215333e-06\n",
      "Epoch 17665/20000: Train Loss = 0.434322, Test Loss = 0.254172, Learning Rate = 1.214871e-06\n",
      "Epoch 17666/20000: Train Loss = 0.434233, Test Loss = 0.254013, Learning Rate = 1.214410e-06\n",
      "Epoch 17667/20000: Train Loss = 0.434268, Test Loss = 0.253614, Learning Rate = 1.213948e-06\n",
      "Epoch 17668/20000: Train Loss = 0.434322, Test Loss = 0.254197, Learning Rate = 1.213487e-06\n",
      "Epoch 17669/20000: Train Loss = 0.434480, Test Loss = 0.254889, Learning Rate = 1.213026e-06\n",
      "Epoch 17670/20000: Train Loss = 0.434357, Test Loss = 0.253624, Learning Rate = 1.212565e-06\n",
      "Epoch 17671/20000: Train Loss = 0.434216, Test Loss = 0.253677, Learning Rate = 1.212104e-06\n",
      "Epoch 17672/20000: Train Loss = 0.434504, Test Loss = 0.254323, Learning Rate = 1.211644e-06\n",
      "Epoch 17673/20000: Train Loss = 0.434460, Test Loss = 0.254022, Learning Rate = 1.211183e-06\n",
      "Epoch 17674/20000: Train Loss = 0.434279, Test Loss = 0.254308, Learning Rate = 1.210723e-06\n",
      "Epoch 17675/20000: Train Loss = 0.434265, Test Loss = 0.253791, Learning Rate = 1.210263e-06\n",
      "Epoch 17676/20000: Train Loss = 0.434263, Test Loss = 0.253506, Learning Rate = 1.209803e-06\n",
      "Epoch 17677/20000: Train Loss = 0.434264, Test Loss = 0.251934, Learning Rate = 1.209344e-06\n",
      "Epoch 17678/20000: Train Loss = 0.434554, Test Loss = 0.252729, Learning Rate = 1.208884e-06\n",
      "Epoch 17679/20000: Train Loss = 0.434267, Test Loss = 0.251978, Learning Rate = 1.208425e-06\n",
      "Epoch 17680/20000: Train Loss = 0.434280, Test Loss = 0.252651, Learning Rate = 1.207966e-06\n",
      "Epoch 17681/20000: Train Loss = 0.434372, Test Loss = 0.252139, Learning Rate = 1.207507e-06\n",
      "Epoch 17682/20000: Train Loss = 0.434200, Test Loss = 0.253128, Learning Rate = 1.207048e-06\n",
      "Epoch 17683/20000: Train Loss = 0.434235, Test Loss = 0.253363, Learning Rate = 1.206589e-06\n",
      "Epoch 17684/20000: Train Loss = 0.434215, Test Loss = 0.254083, Learning Rate = 1.206131e-06\n",
      "Epoch 17685/20000: Train Loss = 0.434299, Test Loss = 0.253968, Learning Rate = 1.205672e-06\n",
      "Epoch 17686/20000: Train Loss = 0.434272, Test Loss = 0.252548, Learning Rate = 1.205214e-06\n",
      "Epoch 17687/20000: Train Loss = 0.434749, Test Loss = 0.254241, Learning Rate = 1.204756e-06\n",
      "Epoch 17688/20000: Train Loss = 0.434131, Test Loss = 0.253478, Learning Rate = 1.204298e-06\n",
      "Epoch 17689/20000: Train Loss = 0.434549, Test Loss = 0.252358, Learning Rate = 1.203841e-06\n",
      "Epoch 17690/20000: Train Loss = 0.434236, Test Loss = 0.253694, Learning Rate = 1.203383e-06\n",
      "Epoch 17691/20000: Train Loss = 0.434185, Test Loss = 0.254061, Learning Rate = 1.202926e-06\n",
      "Epoch 17692/20000: Train Loss = 0.434404, Test Loss = 0.254258, Learning Rate = 1.202469e-06\n",
      "Epoch 17693/20000: Train Loss = 0.434219, Test Loss = 0.253820, Learning Rate = 1.202012e-06\n",
      "Epoch 17694/20000: Train Loss = 0.434340, Test Loss = 0.252772, Learning Rate = 1.201555e-06\n",
      "Epoch 17695/20000: Train Loss = 0.434388, Test Loss = 0.253250, Learning Rate = 1.201099e-06\n",
      "Epoch 17696/20000: Train Loss = 0.434187, Test Loss = 0.252587, Learning Rate = 1.200642e-06\n",
      "Epoch 17697/20000: Train Loss = 0.434295, Test Loss = 0.252339, Learning Rate = 1.200186e-06\n",
      "Epoch 17698/20000: Train Loss = 0.434290, Test Loss = 0.252246, Learning Rate = 1.199730e-06\n",
      "Epoch 17699/20000: Train Loss = 0.434193, Test Loss = 0.252204, Learning Rate = 1.199274e-06\n",
      "Epoch 17700/20000: Train Loss = 0.434229, Test Loss = 0.251687, Learning Rate = 1.198819e-06\n",
      "Epoch 17701/20000: Train Loss = 0.434335, Test Loss = 0.252571, Learning Rate = 1.198363e-06\n",
      "Epoch 17702/20000: Train Loss = 0.434223, Test Loss = 0.252977, Learning Rate = 1.197908e-06\n",
      "Epoch 17703/20000: Train Loss = 0.434368, Test Loss = 0.253116, Learning Rate = 1.197453e-06\n",
      "Epoch 17704/20000: Train Loss = 0.434274, Test Loss = 0.252701, Learning Rate = 1.196998e-06\n",
      "Epoch 17705/20000: Train Loss = 0.434308, Test Loss = 0.252144, Learning Rate = 1.196543e-06\n",
      "Epoch 17706/20000: Train Loss = 0.434197, Test Loss = 0.252541, Learning Rate = 1.196088e-06\n",
      "Epoch 17707/20000: Train Loss = 0.434420, Test Loss = 0.252148, Learning Rate = 1.195634e-06\n",
      "Epoch 17708/20000: Train Loss = 0.434221, Test Loss = 0.252544, Learning Rate = 1.195179e-06\n",
      "Epoch 17709/20000: Train Loss = 0.434332, Test Loss = 0.252422, Learning Rate = 1.194725e-06\n",
      "Epoch 17710/20000: Train Loss = 0.434261, Test Loss = 0.253328, Learning Rate = 1.194271e-06\n",
      "Epoch 17711/20000: Train Loss = 0.434214, Test Loss = 0.252648, Learning Rate = 1.193817e-06\n",
      "Epoch 17712/20000: Train Loss = 0.434277, Test Loss = 0.252932, Learning Rate = 1.193364e-06\n",
      "Epoch 17713/20000: Train Loss = 0.434455, Test Loss = 0.252433, Learning Rate = 1.192910e-06\n",
      "Epoch 17714/20000: Train Loss = 0.434212, Test Loss = 0.252843, Learning Rate = 1.192457e-06\n",
      "Epoch 17715/20000: Train Loss = 0.434245, Test Loss = 0.253393, Learning Rate = 1.192004e-06\n",
      "Epoch 17716/20000: Train Loss = 0.434200, Test Loss = 0.253637, Learning Rate = 1.191551e-06\n",
      "Epoch 17717/20000: Train Loss = 0.434242, Test Loss = 0.253472, Learning Rate = 1.191098e-06\n",
      "Epoch 17718/20000: Train Loss = 0.434301, Test Loss = 0.253514, Learning Rate = 1.190646e-06\n",
      "Epoch 17719/20000: Train Loss = 0.434208, Test Loss = 0.253118, Learning Rate = 1.190193e-06\n",
      "Epoch 17720/20000: Train Loss = 0.434454, Test Loss = 0.253085, Learning Rate = 1.189741e-06\n",
      "Epoch 17721/20000: Train Loss = 0.434262, Test Loss = 0.253452, Learning Rate = 1.189289e-06\n",
      "Epoch 17722/20000: Train Loss = 0.434435, Test Loss = 0.252893, Learning Rate = 1.188837e-06\n",
      "Epoch 17723/20000: Train Loss = 0.434259, Test Loss = 0.252905, Learning Rate = 1.188385e-06\n",
      "Epoch 17724/20000: Train Loss = 0.434445, Test Loss = 0.252651, Learning Rate = 1.187934e-06\n",
      "Epoch 17725/20000: Train Loss = 0.434409, Test Loss = 0.253222, Learning Rate = 1.187482e-06\n",
      "Epoch 17726/20000: Train Loss = 0.434188, Test Loss = 0.252789, Learning Rate = 1.187031e-06\n",
      "Epoch 17727/20000: Train Loss = 0.434229, Test Loss = 0.252619, Learning Rate = 1.186580e-06\n",
      "Epoch 17728/20000: Train Loss = 0.434301, Test Loss = 0.253254, Learning Rate = 1.186129e-06\n",
      "Epoch 17729/20000: Train Loss = 0.434238, Test Loss = 0.252470, Learning Rate = 1.185679e-06\n",
      "Epoch 17730/20000: Train Loss = 0.434253, Test Loss = 0.253494, Learning Rate = 1.185228e-06\n",
      "Epoch 17731/20000: Train Loss = 0.434299, Test Loss = 0.253145, Learning Rate = 1.184778e-06\n",
      "Epoch 17732/20000: Train Loss = 0.434221, Test Loss = 0.252825, Learning Rate = 1.184328e-06\n",
      "Epoch 17733/20000: Train Loss = 0.434278, Test Loss = 0.252498, Learning Rate = 1.183878e-06\n",
      "Epoch 17734/20000: Train Loss = 0.434235, Test Loss = 0.251264, Learning Rate = 1.183428e-06\n",
      "Epoch 17735/20000: Train Loss = 0.434309, Test Loss = 0.251104, Learning Rate = 1.182978e-06\n",
      "Epoch 17736/20000: Train Loss = 0.434258, Test Loss = 0.251267, Learning Rate = 1.182529e-06\n",
      "Epoch 17737/20000: Train Loss = 0.434338, Test Loss = 0.252040, Learning Rate = 1.182079e-06\n",
      "Epoch 17738/20000: Train Loss = 0.434312, Test Loss = 0.251717, Learning Rate = 1.181630e-06\n",
      "Epoch 17739/20000: Train Loss = 0.434311, Test Loss = 0.251891, Learning Rate = 1.181181e-06\n",
      "Epoch 17740/20000: Train Loss = 0.434210, Test Loss = 0.252140, Learning Rate = 1.180732e-06\n",
      "Epoch 17741/20000: Train Loss = 0.434215, Test Loss = 0.252182, Learning Rate = 1.180284e-06\n",
      "Epoch 17742/20000: Train Loss = 0.434248, Test Loss = 0.252152, Learning Rate = 1.179835e-06\n",
      "Epoch 17743/20000: Train Loss = 0.434362, Test Loss = 0.253423, Learning Rate = 1.179387e-06\n",
      "Epoch 17744/20000: Train Loss = 0.434489, Test Loss = 0.254002, Learning Rate = 1.178939e-06\n",
      "Epoch 17745/20000: Train Loss = 0.434467, Test Loss = 0.254265, Learning Rate = 1.178491e-06\n",
      "Epoch 17746/20000: Train Loss = 0.434482, Test Loss = 0.253310, Learning Rate = 1.178043e-06\n",
      "Epoch 17747/20000: Train Loss = 0.434231, Test Loss = 0.253080, Learning Rate = 1.177595e-06\n",
      "Epoch 17748/20000: Train Loss = 0.434346, Test Loss = 0.253134, Learning Rate = 1.177148e-06\n",
      "Epoch 17749/20000: Train Loss = 0.434208, Test Loss = 0.252595, Learning Rate = 1.176701e-06\n",
      "Epoch 17750/20000: Train Loss = 0.434260, Test Loss = 0.253510, Learning Rate = 1.176254e-06\n",
      "Epoch 17751/20000: Train Loss = 0.434483, Test Loss = 0.252483, Learning Rate = 1.175807e-06\n",
      "Epoch 17752/20000: Train Loss = 0.434446, Test Loss = 0.254297, Learning Rate = 1.175360e-06\n",
      "Epoch 17753/20000: Train Loss = 0.434524, Test Loss = 0.253723, Learning Rate = 1.174913e-06\n",
      "Epoch 17754/20000: Train Loss = 0.434262, Test Loss = 0.253575, Learning Rate = 1.174467e-06\n",
      "Epoch 17755/20000: Train Loss = 0.434255, Test Loss = 0.254600, Learning Rate = 1.174020e-06\n",
      "Epoch 17756/20000: Train Loss = 0.434264, Test Loss = 0.254203, Learning Rate = 1.173574e-06\n",
      "Epoch 17757/20000: Train Loss = 0.434293, Test Loss = 0.254311, Learning Rate = 1.173128e-06\n",
      "Epoch 17758/20000: Train Loss = 0.434382, Test Loss = 0.253991, Learning Rate = 1.172683e-06\n",
      "Epoch 17759/20000: Train Loss = 0.434313, Test Loss = 0.254279, Learning Rate = 1.172237e-06\n",
      "Epoch 17760/20000: Train Loss = 0.434386, Test Loss = 0.255297, Learning Rate = 1.171792e-06\n",
      "Epoch 17761/20000: Train Loss = 0.434170, Test Loss = 0.254326, Learning Rate = 1.171346e-06\n",
      "Epoch 17762/20000: Train Loss = 0.434348, Test Loss = 0.254073, Learning Rate = 1.170901e-06\n",
      "Epoch 17763/20000: Train Loss = 0.434235, Test Loss = 0.254204, Learning Rate = 1.170456e-06\n",
      "Epoch 17764/20000: Train Loss = 0.434264, Test Loss = 0.253706, Learning Rate = 1.170012e-06\n",
      "Epoch 17765/20000: Train Loss = 0.434343, Test Loss = 0.253094, Learning Rate = 1.169567e-06\n",
      "Epoch 17766/20000: Train Loss = 0.434641, Test Loss = 0.253603, Learning Rate = 1.169123e-06\n",
      "Epoch 17767/20000: Train Loss = 0.434184, Test Loss = 0.252855, Learning Rate = 1.168679e-06\n",
      "Epoch 17768/20000: Train Loss = 0.434211, Test Loss = 0.253708, Learning Rate = 1.168234e-06\n",
      "Epoch 17769/20000: Train Loss = 0.434383, Test Loss = 0.253711, Learning Rate = 1.167791e-06\n",
      "Epoch 17770/20000: Train Loss = 0.434319, Test Loss = 0.253628, Learning Rate = 1.167347e-06\n",
      "Epoch 17771/20000: Train Loss = 0.434178, Test Loss = 0.253900, Learning Rate = 1.166903e-06\n",
      "Epoch 17772/20000: Train Loss = 0.434320, Test Loss = 0.253467, Learning Rate = 1.166460e-06\n",
      "Epoch 17773/20000: Train Loss = 0.434237, Test Loss = 0.253221, Learning Rate = 1.166017e-06\n",
      "Epoch 17774/20000: Train Loss = 0.434225, Test Loss = 0.253013, Learning Rate = 1.165574e-06\n",
      "Epoch 17775/20000: Train Loss = 0.434309, Test Loss = 0.252896, Learning Rate = 1.165131e-06\n",
      "Epoch 17776/20000: Train Loss = 0.434231, Test Loss = 0.252514, Learning Rate = 1.164688e-06\n",
      "Epoch 17777/20000: Train Loss = 0.434173, Test Loss = 0.253686, Learning Rate = 1.164245e-06\n",
      "Epoch 17778/20000: Train Loss = 0.434268, Test Loss = 0.253188, Learning Rate = 1.163803e-06\n",
      "Epoch 17779/20000: Train Loss = 0.434258, Test Loss = 0.253031, Learning Rate = 1.163361e-06\n",
      "Epoch 17780/20000: Train Loss = 0.434247, Test Loss = 0.253711, Learning Rate = 1.162919e-06\n",
      "Epoch 17781/20000: Train Loss = 0.434185, Test Loss = 0.252289, Learning Rate = 1.162477e-06\n",
      "Epoch 17782/20000: Train Loss = 0.434374, Test Loss = 0.252665, Learning Rate = 1.162035e-06\n",
      "Epoch 17783/20000: Train Loss = 0.434209, Test Loss = 0.253095, Learning Rate = 1.161594e-06\n",
      "Epoch 17784/20000: Train Loss = 0.434210, Test Loss = 0.252399, Learning Rate = 1.161152e-06\n",
      "Epoch 17785/20000: Train Loss = 0.434303, Test Loss = 0.252895, Learning Rate = 1.160711e-06\n",
      "Epoch 17786/20000: Train Loss = 0.434201, Test Loss = 0.253466, Learning Rate = 1.160270e-06\n",
      "Epoch 17787/20000: Train Loss = 0.434289, Test Loss = 0.252415, Learning Rate = 1.159829e-06\n",
      "Epoch 17788/20000: Train Loss = 0.434489, Test Loss = 0.252738, Learning Rate = 1.159388e-06\n",
      "Epoch 17789/20000: Train Loss = 0.434278, Test Loss = 0.253119, Learning Rate = 1.158948e-06\n",
      "Epoch 17790/20000: Train Loss = 0.434237, Test Loss = 0.252352, Learning Rate = 1.158508e-06\n",
      "Epoch 17791/20000: Train Loss = 0.434211, Test Loss = 0.252339, Learning Rate = 1.158067e-06\n",
      "Epoch 17792/20000: Train Loss = 0.434283, Test Loss = 0.253101, Learning Rate = 1.157627e-06\n",
      "Epoch 17793/20000: Train Loss = 0.434418, Test Loss = 0.252465, Learning Rate = 1.157187e-06\n",
      "Epoch 17794/20000: Train Loss = 0.434530, Test Loss = 0.252988, Learning Rate = 1.156748e-06\n",
      "Epoch 17795/20000: Train Loss = 0.434217, Test Loss = 0.252422, Learning Rate = 1.156308e-06\n",
      "Epoch 17796/20000: Train Loss = 0.434325, Test Loss = 0.252197, Learning Rate = 1.155869e-06\n",
      "Epoch 17797/20000: Train Loss = 0.434205, Test Loss = 0.252723, Learning Rate = 1.155430e-06\n",
      "Epoch 17798/20000: Train Loss = 0.434291, Test Loss = 0.253259, Learning Rate = 1.154991e-06\n",
      "Epoch 17799/20000: Train Loss = 0.434280, Test Loss = 0.252439, Learning Rate = 1.154552e-06\n",
      "Epoch 17800/20000: Train Loss = 0.434116, Test Loss = 0.253167, Learning Rate = 1.154113e-06\n",
      "Epoch 17801/20000: Train Loss = 0.434284, Test Loss = 0.252293, Learning Rate = 1.153675e-06\n",
      "Epoch 17802/20000: Train Loss = 0.434319, Test Loss = 0.253096, Learning Rate = 1.153236e-06\n",
      "Epoch 17803/20000: Train Loss = 0.434227, Test Loss = 0.252019, Learning Rate = 1.152798e-06\n",
      "Epoch 17804/20000: Train Loss = 0.434398, Test Loss = 0.252206, Learning Rate = 1.152360e-06\n",
      "Epoch 17805/20000: Train Loss = 0.434457, Test Loss = 0.251633, Learning Rate = 1.151922e-06\n",
      "Epoch 17806/20000: Train Loss = 0.434308, Test Loss = 0.250878, Learning Rate = 1.151484e-06\n",
      "Epoch 17807/20000: Train Loss = 0.434165, Test Loss = 0.251064, Learning Rate = 1.151047e-06\n",
      "Epoch 17808/20000: Train Loss = 0.434233, Test Loss = 0.252195, Learning Rate = 1.150609e-06\n",
      "Epoch 17809/20000: Train Loss = 0.434303, Test Loss = 0.252696, Learning Rate = 1.150172e-06\n",
      "Epoch 17810/20000: Train Loss = 0.434313, Test Loss = 0.251949, Learning Rate = 1.149735e-06\n",
      "Epoch 17811/20000: Train Loss = 0.434220, Test Loss = 0.252363, Learning Rate = 1.149298e-06\n",
      "Epoch 17812/20000: Train Loss = 0.434555, Test Loss = 0.251811, Learning Rate = 1.148862e-06\n",
      "Epoch 17813/20000: Train Loss = 0.434494, Test Loss = 0.253155, Learning Rate = 1.148425e-06\n",
      "Epoch 17814/20000: Train Loss = 0.434344, Test Loss = 0.251996, Learning Rate = 1.147989e-06\n",
      "Epoch 17815/20000: Train Loss = 0.434222, Test Loss = 0.251795, Learning Rate = 1.147553e-06\n",
      "Epoch 17816/20000: Train Loss = 0.434270, Test Loss = 0.251833, Learning Rate = 1.147117e-06\n",
      "Epoch 17817/20000: Train Loss = 0.434443, Test Loss = 0.252931, Learning Rate = 1.146681e-06\n",
      "Epoch 17818/20000: Train Loss = 0.434595, Test Loss = 0.253017, Learning Rate = 1.146245e-06\n",
      "Epoch 17819/20000: Train Loss = 0.434292, Test Loss = 0.253014, Learning Rate = 1.145809e-06\n",
      "Epoch 17820/20000: Train Loss = 0.434220, Test Loss = 0.252781, Learning Rate = 1.145374e-06\n",
      "Epoch 17821/20000: Train Loss = 0.434472, Test Loss = 0.252262, Learning Rate = 1.144939e-06\n",
      "Epoch 17822/20000: Train Loss = 0.434444, Test Loss = 0.253413, Learning Rate = 1.144504e-06\n",
      "Epoch 17823/20000: Train Loss = 0.434264, Test Loss = 0.252409, Learning Rate = 1.144069e-06\n",
      "Epoch 17824/20000: Train Loss = 0.434373, Test Loss = 0.253054, Learning Rate = 1.143634e-06\n",
      "Epoch 17825/20000: Train Loss = 0.434293, Test Loss = 0.253263, Learning Rate = 1.143200e-06\n",
      "Epoch 17826/20000: Train Loss = 0.434151, Test Loss = 0.252721, Learning Rate = 1.142765e-06\n",
      "Epoch 17827/20000: Train Loss = 0.434257, Test Loss = 0.253526, Learning Rate = 1.142331e-06\n",
      "Epoch 17828/20000: Train Loss = 0.434256, Test Loss = 0.252796, Learning Rate = 1.141897e-06\n",
      "Epoch 17829/20000: Train Loss = 0.434269, Test Loss = 0.252957, Learning Rate = 1.141463e-06\n",
      "Epoch 17830/20000: Train Loss = 0.434280, Test Loss = 0.253433, Learning Rate = 1.141029e-06\n",
      "Epoch 17831/20000: Train Loss = 0.434243, Test Loss = 0.252810, Learning Rate = 1.140596e-06\n",
      "Epoch 17832/20000: Train Loss = 0.434291, Test Loss = 0.252694, Learning Rate = 1.140162e-06\n",
      "Epoch 17833/20000: Train Loss = 0.434280, Test Loss = 0.251969, Learning Rate = 1.139729e-06\n",
      "Epoch 17834/20000: Train Loss = 0.434223, Test Loss = 0.252333, Learning Rate = 1.139296e-06\n",
      "Epoch 17835/20000: Train Loss = 0.434362, Test Loss = 0.252116, Learning Rate = 1.138863e-06\n",
      "Epoch 17836/20000: Train Loss = 0.434334, Test Loss = 0.253031, Learning Rate = 1.138430e-06\n",
      "Epoch 17837/20000: Train Loss = 0.434299, Test Loss = 0.253135, Learning Rate = 1.137998e-06\n",
      "Epoch 17838/20000: Train Loss = 0.434228, Test Loss = 0.253012, Learning Rate = 1.137565e-06\n",
      "Epoch 17839/20000: Train Loss = 0.434384, Test Loss = 0.253500, Learning Rate = 1.137133e-06\n",
      "Epoch 17840/20000: Train Loss = 0.434197, Test Loss = 0.253912, Learning Rate = 1.136701e-06\n",
      "Epoch 17841/20000: Train Loss = 0.434267, Test Loss = 0.253991, Learning Rate = 1.136269e-06\n",
      "Epoch 17842/20000: Train Loss = 0.434165, Test Loss = 0.254672, Learning Rate = 1.135837e-06\n",
      "Epoch 17843/20000: Train Loss = 0.434210, Test Loss = 0.254045, Learning Rate = 1.135406e-06\n",
      "Epoch 17844/20000: Train Loss = 0.434262, Test Loss = 0.253441, Learning Rate = 1.134974e-06\n",
      "Epoch 17845/20000: Train Loss = 0.434186, Test Loss = 0.253527, Learning Rate = 1.134543e-06\n",
      "Epoch 17846/20000: Train Loss = 0.434314, Test Loss = 0.253990, Learning Rate = 1.134112e-06\n",
      "Epoch 17847/20000: Train Loss = 0.434243, Test Loss = 0.254888, Learning Rate = 1.133681e-06\n",
      "Epoch 17848/20000: Train Loss = 0.434288, Test Loss = 0.253370, Learning Rate = 1.133250e-06\n",
      "Epoch 17849/20000: Train Loss = 0.434348, Test Loss = 0.254333, Learning Rate = 1.132820e-06\n",
      "Epoch 17850/20000: Train Loss = 0.434273, Test Loss = 0.254803, Learning Rate = 1.132389e-06\n",
      "Epoch 17851/20000: Train Loss = 0.434326, Test Loss = 0.254875, Learning Rate = 1.131959e-06\n",
      "Epoch 17852/20000: Train Loss = 0.434473, Test Loss = 0.255501, Learning Rate = 1.131529e-06\n",
      "Epoch 17853/20000: Train Loss = 0.434178, Test Loss = 0.254751, Learning Rate = 1.131099e-06\n",
      "Epoch 17854/20000: Train Loss = 0.434283, Test Loss = 0.255675, Learning Rate = 1.130669e-06\n",
      "Epoch 17855/20000: Train Loss = 0.434241, Test Loss = 0.254491, Learning Rate = 1.130240e-06\n",
      "Epoch 17856/20000: Train Loss = 0.434252, Test Loss = 0.254534, Learning Rate = 1.129810e-06\n",
      "Epoch 17857/20000: Train Loss = 0.434272, Test Loss = 0.254857, Learning Rate = 1.129381e-06\n",
      "Epoch 17858/20000: Train Loss = 0.434209, Test Loss = 0.254653, Learning Rate = 1.128952e-06\n",
      "Epoch 17859/20000: Train Loss = 0.434171, Test Loss = 0.253829, Learning Rate = 1.128523e-06\n",
      "Epoch 17860/20000: Train Loss = 0.434202, Test Loss = 0.253723, Learning Rate = 1.128094e-06\n",
      "Epoch 17861/20000: Train Loss = 0.434246, Test Loss = 0.253465, Learning Rate = 1.127665e-06\n",
      "Epoch 17862/20000: Train Loss = 0.434182, Test Loss = 0.253445, Learning Rate = 1.127237e-06\n",
      "Epoch 17863/20000: Train Loss = 0.434160, Test Loss = 0.253686, Learning Rate = 1.126808e-06\n",
      "Epoch 17864/20000: Train Loss = 0.434162, Test Loss = 0.253336, Learning Rate = 1.126380e-06\n",
      "Epoch 17865/20000: Train Loss = 0.434506, Test Loss = 0.254086, Learning Rate = 1.125952e-06\n",
      "Epoch 17866/20000: Train Loss = 0.434367, Test Loss = 0.253869, Learning Rate = 1.125525e-06\n",
      "Epoch 17867/20000: Train Loss = 0.434329, Test Loss = 0.254900, Learning Rate = 1.125097e-06\n",
      "Epoch 17868/20000: Train Loss = 0.434116, Test Loss = 0.254136, Learning Rate = 1.124669e-06\n",
      "Epoch 17869/20000: Train Loss = 0.434296, Test Loss = 0.254272, Learning Rate = 1.124242e-06\n",
      "Epoch 17870/20000: Train Loss = 0.434412, Test Loss = 0.253871, Learning Rate = 1.123815e-06\n",
      "Epoch 17871/20000: Train Loss = 0.434399, Test Loss = 0.254846, Learning Rate = 1.123388e-06\n",
      "Epoch 17872/20000: Train Loss = 0.434236, Test Loss = 0.253605, Learning Rate = 1.122961e-06\n",
      "Epoch 17873/20000: Train Loss = 0.434156, Test Loss = 0.254532, Learning Rate = 1.122534e-06\n",
      "Epoch 17874/20000: Train Loss = 0.434411, Test Loss = 0.254424, Learning Rate = 1.122108e-06\n",
      "Epoch 17875/20000: Train Loss = 0.434695, Test Loss = 0.254961, Learning Rate = 1.121681e-06\n",
      "Epoch 17876/20000: Train Loss = 0.434166, Test Loss = 0.254564, Learning Rate = 1.121255e-06\n",
      "Epoch 17877/20000: Train Loss = 0.434271, Test Loss = 0.255786, Learning Rate = 1.120829e-06\n",
      "Epoch 17878/20000: Train Loss = 0.434282, Test Loss = 0.254784, Learning Rate = 1.120403e-06\n",
      "Epoch 17879/20000: Train Loss = 0.434510, Test Loss = 0.254638, Learning Rate = 1.119977e-06\n",
      "Epoch 17880/20000: Train Loss = 0.434172, Test Loss = 0.253958, Learning Rate = 1.119552e-06\n",
      "Epoch 17881/20000: Train Loss = 0.434173, Test Loss = 0.253482, Learning Rate = 1.119127e-06\n",
      "Epoch 17882/20000: Train Loss = 0.434243, Test Loss = 0.253423, Learning Rate = 1.118701e-06\n",
      "Epoch 17883/20000: Train Loss = 0.434320, Test Loss = 0.253445, Learning Rate = 1.118276e-06\n",
      "Epoch 17884/20000: Train Loss = 0.434433, Test Loss = 0.253240, Learning Rate = 1.117851e-06\n",
      "Epoch 17885/20000: Train Loss = 0.434331, Test Loss = 0.252994, Learning Rate = 1.117427e-06\n",
      "Epoch 17886/20000: Train Loss = 0.434121, Test Loss = 0.253621, Learning Rate = 1.117002e-06\n",
      "Epoch 17887/20000: Train Loss = 0.434270, Test Loss = 0.253980, Learning Rate = 1.116578e-06\n",
      "Epoch 17888/20000: Train Loss = 0.434305, Test Loss = 0.253226, Learning Rate = 1.116153e-06\n",
      "Epoch 17889/20000: Train Loss = 0.434183, Test Loss = 0.253041, Learning Rate = 1.115729e-06\n",
      "Epoch 17890/20000: Train Loss = 0.434238, Test Loss = 0.252693, Learning Rate = 1.115305e-06\n",
      "Epoch 17891/20000: Train Loss = 0.434328, Test Loss = 0.252296, Learning Rate = 1.114881e-06\n",
      "Epoch 17892/20000: Train Loss = 0.434256, Test Loss = 0.252258, Learning Rate = 1.114458e-06\n",
      "Epoch 17893/20000: Train Loss = 0.434276, Test Loss = 0.252580, Learning Rate = 1.114034e-06\n",
      "Epoch 17894/20000: Train Loss = 0.434230, Test Loss = 0.252155, Learning Rate = 1.113611e-06\n",
      "Epoch 17895/20000: Train Loss = 0.434277, Test Loss = 0.252740, Learning Rate = 1.113188e-06\n",
      "Epoch 17896/20000: Train Loss = 0.434198, Test Loss = 0.251835, Learning Rate = 1.112765e-06\n",
      "Epoch 17897/20000: Train Loss = 0.434305, Test Loss = 0.252810, Learning Rate = 1.112342e-06\n",
      "Epoch 17898/20000: Train Loss = 0.434387, Test Loss = 0.252833, Learning Rate = 1.111919e-06\n",
      "Epoch 17899/20000: Train Loss = 0.434287, Test Loss = 0.253291, Learning Rate = 1.111497e-06\n",
      "Epoch 17900/20000: Train Loss = 0.434164, Test Loss = 0.253399, Learning Rate = 1.111075e-06\n",
      "Epoch 17901/20000: Train Loss = 0.434281, Test Loss = 0.252594, Learning Rate = 1.110652e-06\n",
      "Epoch 17902/20000: Train Loss = 0.434153, Test Loss = 0.252337, Learning Rate = 1.110230e-06\n",
      "Epoch 17903/20000: Train Loss = 0.434192, Test Loss = 0.252753, Learning Rate = 1.109809e-06\n",
      "Epoch 17904/20000: Train Loss = 0.434286, Test Loss = 0.252853, Learning Rate = 1.109387e-06\n",
      "Epoch 17905/20000: Train Loss = 0.434262, Test Loss = 0.252266, Learning Rate = 1.108965e-06\n",
      "Epoch 17906/20000: Train Loss = 0.434323, Test Loss = 0.251858, Learning Rate = 1.108544e-06\n",
      "Epoch 17907/20000: Train Loss = 0.434289, Test Loss = 0.251874, Learning Rate = 1.108123e-06\n",
      "Epoch 17908/20000: Train Loss = 0.434383, Test Loss = 0.252824, Learning Rate = 1.107702e-06\n",
      "Epoch 17909/20000: Train Loss = 0.434165, Test Loss = 0.253437, Learning Rate = 1.107281e-06\n",
      "Epoch 17910/20000: Train Loss = 0.434406, Test Loss = 0.253508, Learning Rate = 1.106860e-06\n",
      "Epoch 17911/20000: Train Loss = 0.434199, Test Loss = 0.253529, Learning Rate = 1.106439e-06\n",
      "Epoch 17912/20000: Train Loss = 0.434206, Test Loss = 0.253703, Learning Rate = 1.106019e-06\n",
      "Epoch 17913/20000: Train Loss = 0.434272, Test Loss = 0.252933, Learning Rate = 1.105599e-06\n",
      "Epoch 17914/20000: Train Loss = 0.434280, Test Loss = 0.252326, Learning Rate = 1.105179e-06\n",
      "Epoch 17915/20000: Train Loss = 0.434175, Test Loss = 0.252347, Learning Rate = 1.104759e-06\n",
      "Epoch 17916/20000: Train Loss = 0.434168, Test Loss = 0.252208, Learning Rate = 1.104339e-06\n",
      "Epoch 17917/20000: Train Loss = 0.434194, Test Loss = 0.252598, Learning Rate = 1.103919e-06\n",
      "Epoch 17918/20000: Train Loss = 0.434277, Test Loss = 0.252503, Learning Rate = 1.103500e-06\n",
      "Epoch 17919/20000: Train Loss = 0.434164, Test Loss = 0.252871, Learning Rate = 1.103081e-06\n",
      "Epoch 17920/20000: Train Loss = 0.434176, Test Loss = 0.253393, Learning Rate = 1.102661e-06\n",
      "Epoch 17921/20000: Train Loss = 0.434267, Test Loss = 0.253312, Learning Rate = 1.102242e-06\n",
      "Epoch 17922/20000: Train Loss = 0.434139, Test Loss = 0.252776, Learning Rate = 1.101824e-06\n",
      "Epoch 17923/20000: Train Loss = 0.434225, Test Loss = 0.253183, Learning Rate = 1.101405e-06\n",
      "Epoch 17924/20000: Train Loss = 0.434263, Test Loss = 0.252863, Learning Rate = 1.100986e-06\n",
      "Epoch 17925/20000: Train Loss = 0.434206, Test Loss = 0.252676, Learning Rate = 1.100568e-06\n",
      "Epoch 17926/20000: Train Loss = 0.434306, Test Loss = 0.252877, Learning Rate = 1.100150e-06\n",
      "Epoch 17927/20000: Train Loss = 0.434280, Test Loss = 0.253076, Learning Rate = 1.099732e-06\n",
      "Epoch 17928/20000: Train Loss = 0.434203, Test Loss = 0.252557, Learning Rate = 1.099314e-06\n",
      "Epoch 17929/20000: Train Loss = 0.434144, Test Loss = 0.252942, Learning Rate = 1.098896e-06\n",
      "Epoch 17930/20000: Train Loss = 0.434172, Test Loss = 0.252963, Learning Rate = 1.098479e-06\n",
      "Epoch 17931/20000: Train Loss = 0.434351, Test Loss = 0.253607, Learning Rate = 1.098061e-06\n",
      "Epoch 17932/20000: Train Loss = 0.434252, Test Loss = 0.254072, Learning Rate = 1.097644e-06\n",
      "Epoch 17933/20000: Train Loss = 0.434396, Test Loss = 0.253737, Learning Rate = 1.097227e-06\n",
      "Epoch 17934/20000: Train Loss = 0.434286, Test Loss = 0.252884, Learning Rate = 1.096810e-06\n",
      "Epoch 17935/20000: Train Loss = 0.434165, Test Loss = 0.252489, Learning Rate = 1.096393e-06\n",
      "Epoch 17936/20000: Train Loss = 0.434595, Test Loss = 0.253006, Learning Rate = 1.095977e-06\n",
      "Epoch 17937/20000: Train Loss = 0.434268, Test Loss = 0.252662, Learning Rate = 1.095560e-06\n",
      "Epoch 17938/20000: Train Loss = 0.434362, Test Loss = 0.252588, Learning Rate = 1.095144e-06\n",
      "Epoch 17939/20000: Train Loss = 0.434239, Test Loss = 0.252518, Learning Rate = 1.094728e-06\n",
      "Epoch 17940/20000: Train Loss = 0.434244, Test Loss = 0.252717, Learning Rate = 1.094312e-06\n",
      "Epoch 17941/20000: Train Loss = 0.434123, Test Loss = 0.252206, Learning Rate = 1.093896e-06\n",
      "Epoch 17942/20000: Train Loss = 0.434479, Test Loss = 0.252486, Learning Rate = 1.093480e-06\n",
      "Epoch 17943/20000: Train Loss = 0.434188, Test Loss = 0.252211, Learning Rate = 1.093065e-06\n",
      "Epoch 17944/20000: Train Loss = 0.434251, Test Loss = 0.252565, Learning Rate = 1.092650e-06\n",
      "Epoch 17945/20000: Train Loss = 0.434177, Test Loss = 0.253352, Learning Rate = 1.092234e-06\n",
      "Epoch 17946/20000: Train Loss = 0.434225, Test Loss = 0.253303, Learning Rate = 1.091819e-06\n",
      "Epoch 17947/20000: Train Loss = 0.434213, Test Loss = 0.253957, Learning Rate = 1.091405e-06\n",
      "Epoch 17948/20000: Train Loss = 0.434209, Test Loss = 0.253373, Learning Rate = 1.090990e-06\n",
      "Epoch 17949/20000: Train Loss = 0.434200, Test Loss = 0.253845, Learning Rate = 1.090575e-06\n",
      "Epoch 17950/20000: Train Loss = 0.434252, Test Loss = 0.254244, Learning Rate = 1.090161e-06\n",
      "Epoch 17951/20000: Train Loss = 0.434225, Test Loss = 0.254515, Learning Rate = 1.089747e-06\n",
      "Epoch 17952/20000: Train Loss = 0.434223, Test Loss = 0.254142, Learning Rate = 1.089333e-06\n",
      "Epoch 17953/20000: Train Loss = 0.434332, Test Loss = 0.254456, Learning Rate = 1.088919e-06\n",
      "Epoch 17954/20000: Train Loss = 0.434252, Test Loss = 0.254716, Learning Rate = 1.088505e-06\n",
      "Epoch 17955/20000: Train Loss = 0.434157, Test Loss = 0.254287, Learning Rate = 1.088091e-06\n",
      "Epoch 17956/20000: Train Loss = 0.434206, Test Loss = 0.254483, Learning Rate = 1.087678e-06\n",
      "Epoch 17957/20000: Train Loss = 0.434162, Test Loss = 0.253980, Learning Rate = 1.087265e-06\n",
      "Epoch 17958/20000: Train Loss = 0.434228, Test Loss = 0.253467, Learning Rate = 1.086852e-06\n",
      "Epoch 17959/20000: Train Loss = 0.434292, Test Loss = 0.254345, Learning Rate = 1.086439e-06\n",
      "Epoch 17960/20000: Train Loss = 0.434352, Test Loss = 0.254357, Learning Rate = 1.086026e-06\n",
      "Epoch 17961/20000: Train Loss = 0.434632, Test Loss = 0.252939, Learning Rate = 1.085613e-06\n",
      "Epoch 17962/20000: Train Loss = 0.434253, Test Loss = 0.253367, Learning Rate = 1.085201e-06\n",
      "Epoch 17963/20000: Train Loss = 0.434162, Test Loss = 0.252704, Learning Rate = 1.084788e-06\n",
      "Epoch 17964/20000: Train Loss = 0.434268, Test Loss = 0.252749, Learning Rate = 1.084376e-06\n",
      "Epoch 17965/20000: Train Loss = 0.434235, Test Loss = 0.253350, Learning Rate = 1.083964e-06\n",
      "Epoch 17966/20000: Train Loss = 0.434123, Test Loss = 0.252902, Learning Rate = 1.083552e-06\n",
      "Epoch 17967/20000: Train Loss = 0.434146, Test Loss = 0.252570, Learning Rate = 1.083140e-06\n",
      "Epoch 17968/20000: Train Loss = 0.434292, Test Loss = 0.252768, Learning Rate = 1.082729e-06\n",
      "Epoch 17969/20000: Train Loss = 0.434704, Test Loss = 0.253821, Learning Rate = 1.082317e-06\n",
      "Epoch 17970/20000: Train Loss = 0.434208, Test Loss = 0.252776, Learning Rate = 1.081906e-06\n",
      "Epoch 17971/20000: Train Loss = 0.434227, Test Loss = 0.252325, Learning Rate = 1.081495e-06\n",
      "Epoch 17972/20000: Train Loss = 0.434164, Test Loss = 0.253007, Learning Rate = 1.081084e-06\n",
      "Epoch 17973/20000: Train Loss = 0.434283, Test Loss = 0.254075, Learning Rate = 1.080673e-06\n",
      "Epoch 17974/20000: Train Loss = 0.434234, Test Loss = 0.253844, Learning Rate = 1.080263e-06\n",
      "Epoch 17975/20000: Train Loss = 0.434186, Test Loss = 0.253374, Learning Rate = 1.079852e-06\n",
      "Epoch 17976/20000: Train Loss = 0.434247, Test Loss = 0.253679, Learning Rate = 1.079442e-06\n",
      "Epoch 17977/20000: Train Loss = 0.434185, Test Loss = 0.253718, Learning Rate = 1.079032e-06\n",
      "Epoch 17978/20000: Train Loss = 0.434194, Test Loss = 0.254184, Learning Rate = 1.078622e-06\n",
      "Epoch 17979/20000: Train Loss = 0.434430, Test Loss = 0.253837, Learning Rate = 1.078212e-06\n",
      "Epoch 17980/20000: Train Loss = 0.434942, Test Loss = 0.253041, Learning Rate = 1.077802e-06\n",
      "Epoch 17981/20000: Train Loss = 0.434233, Test Loss = 0.253395, Learning Rate = 1.077393e-06\n",
      "Epoch 17982/20000: Train Loss = 0.434193, Test Loss = 0.253693, Learning Rate = 1.076983e-06\n",
      "Epoch 17983/20000: Train Loss = 0.434215, Test Loss = 0.253623, Learning Rate = 1.076574e-06\n",
      "Epoch 17984/20000: Train Loss = 0.434160, Test Loss = 0.253878, Learning Rate = 1.076165e-06\n",
      "Epoch 17985/20000: Train Loss = 0.434215, Test Loss = 0.253972, Learning Rate = 1.075756e-06\n",
      "Epoch 17986/20000: Train Loss = 0.434286, Test Loss = 0.254189, Learning Rate = 1.075347e-06\n",
      "Epoch 17987/20000: Train Loss = 0.434198, Test Loss = 0.254250, Learning Rate = 1.074939e-06\n",
      "Epoch 17988/20000: Train Loss = 0.434277, Test Loss = 0.254367, Learning Rate = 1.074530e-06\n",
      "Epoch 17989/20000: Train Loss = 0.434311, Test Loss = 0.254249, Learning Rate = 1.074122e-06\n",
      "Epoch 17990/20000: Train Loss = 0.434443, Test Loss = 0.254090, Learning Rate = 1.073714e-06\n",
      "Epoch 17991/20000: Train Loss = 0.434197, Test Loss = 0.253684, Learning Rate = 1.073306e-06\n",
      "Epoch 17992/20000: Train Loss = 0.434313, Test Loss = 0.253861, Learning Rate = 1.072898e-06\n",
      "Epoch 17993/20000: Train Loss = 0.434282, Test Loss = 0.254258, Learning Rate = 1.072490e-06\n",
      "Epoch 17994/20000: Train Loss = 0.434147, Test Loss = 0.254058, Learning Rate = 1.072083e-06\n",
      "Epoch 17995/20000: Train Loss = 0.434190, Test Loss = 0.254151, Learning Rate = 1.071676e-06\n",
      "Epoch 17996/20000: Train Loss = 0.434272, Test Loss = 0.254277, Learning Rate = 1.071268e-06\n",
      "Epoch 17997/20000: Train Loss = 0.434215, Test Loss = 0.253606, Learning Rate = 1.070861e-06\n",
      "Epoch 17998/20000: Train Loss = 0.434191, Test Loss = 0.254242, Learning Rate = 1.070454e-06\n",
      "Epoch 17999/20000: Train Loss = 0.434215, Test Loss = 0.253696, Learning Rate = 1.070048e-06\n",
      "Epoch 18000/20000: Train Loss = 0.434369, Test Loss = 0.253607, Learning Rate = 1.069641e-06\n",
      "Epoch 18001/20000: Train Loss = 0.434309, Test Loss = 0.252475, Learning Rate = 1.069235e-06\n",
      "Epoch 18002/20000: Train Loss = 0.434249, Test Loss = 0.252578, Learning Rate = 1.068828e-06\n",
      "Epoch 18003/20000: Train Loss = 0.434324, Test Loss = 0.252939, Learning Rate = 1.068422e-06\n",
      "Epoch 18004/20000: Train Loss = 0.434202, Test Loss = 0.254070, Learning Rate = 1.068016e-06\n",
      "Epoch 18005/20000: Train Loss = 0.434309, Test Loss = 0.253975, Learning Rate = 1.067610e-06\n",
      "Epoch 18006/20000: Train Loss = 0.434327, Test Loss = 0.253823, Learning Rate = 1.067205e-06\n",
      "Epoch 18007/20000: Train Loss = 0.434462, Test Loss = 0.253347, Learning Rate = 1.066799e-06\n",
      "Epoch 18008/20000: Train Loss = 0.434205, Test Loss = 0.253971, Learning Rate = 1.066394e-06\n",
      "Epoch 18009/20000: Train Loss = 0.434168, Test Loss = 0.252964, Learning Rate = 1.065989e-06\n",
      "Epoch 18010/20000: Train Loss = 0.434267, Test Loss = 0.253783, Learning Rate = 1.065584e-06\n",
      "Epoch 18011/20000: Train Loss = 0.434143, Test Loss = 0.252376, Learning Rate = 1.065179e-06\n",
      "Epoch 18012/20000: Train Loss = 0.434168, Test Loss = 0.252600, Learning Rate = 1.064774e-06\n",
      "Epoch 18013/20000: Train Loss = 0.434187, Test Loss = 0.253032, Learning Rate = 1.064369e-06\n",
      "Epoch 18014/20000: Train Loss = 0.434270, Test Loss = 0.253392, Learning Rate = 1.063965e-06\n",
      "Epoch 18015/20000: Train Loss = 0.434334, Test Loss = 0.253519, Learning Rate = 1.063561e-06\n",
      "Epoch 18016/20000: Train Loss = 0.434259, Test Loss = 0.253164, Learning Rate = 1.063157e-06\n",
      "Epoch 18017/20000: Train Loss = 0.434317, Test Loss = 0.253146, Learning Rate = 1.062753e-06\n",
      "Epoch 18018/20000: Train Loss = 0.434222, Test Loss = 0.252534, Learning Rate = 1.062349e-06\n",
      "Epoch 18019/20000: Train Loss = 0.434229, Test Loss = 0.253223, Learning Rate = 1.061945e-06\n",
      "Epoch 18020/20000: Train Loss = 0.434299, Test Loss = 0.253477, Learning Rate = 1.061542e-06\n",
      "Epoch 18021/20000: Train Loss = 0.434156, Test Loss = 0.252894, Learning Rate = 1.061138e-06\n",
      "Epoch 18022/20000: Train Loss = 0.434386, Test Loss = 0.253386, Learning Rate = 1.060735e-06\n",
      "Epoch 18023/20000: Train Loss = 0.434275, Test Loss = 0.252327, Learning Rate = 1.060332e-06\n",
      "Epoch 18024/20000: Train Loss = 0.434223, Test Loss = 0.252746, Learning Rate = 1.059929e-06\n",
      "Epoch 18025/20000: Train Loss = 0.434205, Test Loss = 0.253065, Learning Rate = 1.059526e-06\n",
      "Epoch 18026/20000: Train Loss = 0.434146, Test Loss = 0.252237, Learning Rate = 1.059124e-06\n",
      "Epoch 18027/20000: Train Loss = 0.434220, Test Loss = 0.252003, Learning Rate = 1.058721e-06\n",
      "Epoch 18028/20000: Train Loss = 0.434117, Test Loss = 0.251968, Learning Rate = 1.058319e-06\n",
      "Epoch 18029/20000: Train Loss = 0.434215, Test Loss = 0.252588, Learning Rate = 1.057917e-06\n",
      "Epoch 18030/20000: Train Loss = 0.434281, Test Loss = 0.252049, Learning Rate = 1.057515e-06\n",
      "Epoch 18031/20000: Train Loss = 0.434243, Test Loss = 0.252367, Learning Rate = 1.057113e-06\n",
      "Epoch 18032/20000: Train Loss = 0.434111, Test Loss = 0.252782, Learning Rate = 1.056711e-06\n",
      "Epoch 18033/20000: Train Loss = 0.434212, Test Loss = 0.252745, Learning Rate = 1.056310e-06\n",
      "Epoch 18034/20000: Train Loss = 0.434287, Test Loss = 0.252051, Learning Rate = 1.055909e-06\n",
      "Epoch 18035/20000: Train Loss = 0.434170, Test Loss = 0.252421, Learning Rate = 1.055507e-06\n",
      "Epoch 18036/20000: Train Loss = 0.434252, Test Loss = 0.252317, Learning Rate = 1.055106e-06\n",
      "Epoch 18037/20000: Train Loss = 0.434166, Test Loss = 0.252336, Learning Rate = 1.054705e-06\n",
      "Epoch 18038/20000: Train Loss = 0.434214, Test Loss = 0.252085, Learning Rate = 1.054305e-06\n",
      "Epoch 18039/20000: Train Loss = 0.434203, Test Loss = 0.252480, Learning Rate = 1.053904e-06\n",
      "Epoch 18040/20000: Train Loss = 0.434267, Test Loss = 0.252055, Learning Rate = 1.053504e-06\n",
      "Epoch 18041/20000: Train Loss = 0.434203, Test Loss = 0.251984, Learning Rate = 1.053103e-06\n",
      "Epoch 18042/20000: Train Loss = 0.434140, Test Loss = 0.251328, Learning Rate = 1.052703e-06\n",
      "Epoch 18043/20000: Train Loss = 0.434167, Test Loss = 0.250598, Learning Rate = 1.052303e-06\n",
      "Epoch 18044/20000: Train Loss = 0.434162, Test Loss = 0.251071, Learning Rate = 1.051903e-06\n",
      "Epoch 18045/20000: Train Loss = 0.434188, Test Loss = 0.252007, Learning Rate = 1.051504e-06\n",
      "Epoch 18046/20000: Train Loss = 0.434222, Test Loss = 0.251487, Learning Rate = 1.051104e-06\n",
      "Epoch 18047/20000: Train Loss = 0.434218, Test Loss = 0.251497, Learning Rate = 1.050705e-06\n",
      "Epoch 18048/20000: Train Loss = 0.434294, Test Loss = 0.251413, Learning Rate = 1.050305e-06\n",
      "Epoch 18049/20000: Train Loss = 0.434247, Test Loss = 0.250901, Learning Rate = 1.049906e-06\n",
      "Epoch 18050/20000: Train Loss = 0.434129, Test Loss = 0.251933, Learning Rate = 1.049507e-06\n",
      "Epoch 18051/20000: Train Loss = 0.434148, Test Loss = 0.251931, Learning Rate = 1.049109e-06\n",
      "Epoch 18052/20000: Train Loss = 0.434271, Test Loss = 0.251343, Learning Rate = 1.048710e-06\n",
      "Epoch 18053/20000: Train Loss = 0.434239, Test Loss = 0.252828, Learning Rate = 1.048311e-06\n",
      "Epoch 18054/20000: Train Loss = 0.434302, Test Loss = 0.251720, Learning Rate = 1.047913e-06\n",
      "Epoch 18055/20000: Train Loss = 0.434158, Test Loss = 0.251273, Learning Rate = 1.047515e-06\n",
      "Epoch 18056/20000: Train Loss = 0.434259, Test Loss = 0.251398, Learning Rate = 1.047117e-06\n",
      "Epoch 18057/20000: Train Loss = 0.434215, Test Loss = 0.251685, Learning Rate = 1.046719e-06\n",
      "Epoch 18058/20000: Train Loss = 0.434328, Test Loss = 0.251022, Learning Rate = 1.046321e-06\n",
      "Epoch 18059/20000: Train Loss = 0.434120, Test Loss = 0.251795, Learning Rate = 1.045924e-06\n",
      "Epoch 18060/20000: Train Loss = 0.434230, Test Loss = 0.251834, Learning Rate = 1.045526e-06\n",
      "Epoch 18061/20000: Train Loss = 0.434288, Test Loss = 0.250670, Learning Rate = 1.045129e-06\n",
      "Epoch 18062/20000: Train Loss = 0.434333, Test Loss = 0.251079, Learning Rate = 1.044732e-06\n",
      "Epoch 18063/20000: Train Loss = 0.434191, Test Loss = 0.251228, Learning Rate = 1.044335e-06\n",
      "Epoch 18064/20000: Train Loss = 0.434204, Test Loss = 0.252157, Learning Rate = 1.043938e-06\n",
      "Epoch 18065/20000: Train Loss = 0.434199, Test Loss = 0.251318, Learning Rate = 1.043541e-06\n",
      "Epoch 18066/20000: Train Loss = 0.434203, Test Loss = 0.252537, Learning Rate = 1.043145e-06\n",
      "Epoch 18067/20000: Train Loss = 0.434197, Test Loss = 0.251818, Learning Rate = 1.042749e-06\n",
      "Epoch 18068/20000: Train Loss = 0.434252, Test Loss = 0.251404, Learning Rate = 1.042352e-06\n",
      "Epoch 18069/20000: Train Loss = 0.434558, Test Loss = 0.251016, Learning Rate = 1.041956e-06\n",
      "Epoch 18070/20000: Train Loss = 0.434139, Test Loss = 0.250963, Learning Rate = 1.041560e-06\n",
      "Epoch 18071/20000: Train Loss = 0.434215, Test Loss = 0.251614, Learning Rate = 1.041165e-06\n",
      "Epoch 18072/20000: Train Loss = 0.434507, Test Loss = 0.252605, Learning Rate = 1.040769e-06\n",
      "Epoch 18073/20000: Train Loss = 0.434316, Test Loss = 0.252658, Learning Rate = 1.040374e-06\n",
      "Epoch 18074/20000: Train Loss = 0.434233, Test Loss = 0.251643, Learning Rate = 1.039978e-06\n",
      "Epoch 18075/20000: Train Loss = 0.434168, Test Loss = 0.252027, Learning Rate = 1.039583e-06\n",
      "Epoch 18076/20000: Train Loss = 0.434249, Test Loss = 0.252614, Learning Rate = 1.039188e-06\n",
      "Epoch 18077/20000: Train Loss = 0.434292, Test Loss = 0.252942, Learning Rate = 1.038793e-06\n",
      "Epoch 18078/20000: Train Loss = 0.434320, Test Loss = 0.252239, Learning Rate = 1.038398e-06\n",
      "Epoch 18079/20000: Train Loss = 0.434405, Test Loss = 0.253402, Learning Rate = 1.038004e-06\n",
      "Epoch 18080/20000: Train Loss = 0.434312, Test Loss = 0.252422, Learning Rate = 1.037609e-06\n",
      "Epoch 18081/20000: Train Loss = 0.434219, Test Loss = 0.253640, Learning Rate = 1.037215e-06\n",
      "Epoch 18082/20000: Train Loss = 0.434139, Test Loss = 0.253138, Learning Rate = 1.036821e-06\n",
      "Epoch 18083/20000: Train Loss = 0.434207, Test Loss = 0.253596, Learning Rate = 1.036427e-06\n",
      "Epoch 18084/20000: Train Loss = 0.434261, Test Loss = 0.253359, Learning Rate = 1.036033e-06\n",
      "Epoch 18085/20000: Train Loss = 0.434178, Test Loss = 0.253426, Learning Rate = 1.035640e-06\n",
      "Epoch 18086/20000: Train Loss = 0.434193, Test Loss = 0.252588, Learning Rate = 1.035246e-06\n",
      "Epoch 18087/20000: Train Loss = 0.434133, Test Loss = 0.253109, Learning Rate = 1.034853e-06\n",
      "Epoch 18088/20000: Train Loss = 0.434227, Test Loss = 0.252706, Learning Rate = 1.034460e-06\n",
      "Epoch 18089/20000: Train Loss = 0.434229, Test Loss = 0.252956, Learning Rate = 1.034067e-06\n",
      "Epoch 18090/20000: Train Loss = 0.434276, Test Loss = 0.253236, Learning Rate = 1.033674e-06\n",
      "Epoch 18091/20000: Train Loss = 0.434206, Test Loss = 0.252831, Learning Rate = 1.033281e-06\n",
      "Epoch 18092/20000: Train Loss = 0.434330, Test Loss = 0.252904, Learning Rate = 1.032888e-06\n",
      "Epoch 18093/20000: Train Loss = 0.434211, Test Loss = 0.252172, Learning Rate = 1.032496e-06\n",
      "Epoch 18094/20000: Train Loss = 0.434376, Test Loss = 0.252498, Learning Rate = 1.032103e-06\n",
      "Epoch 18095/20000: Train Loss = 0.434237, Test Loss = 0.253839, Learning Rate = 1.031711e-06\n",
      "Epoch 18096/20000: Train Loss = 0.434386, Test Loss = 0.253593, Learning Rate = 1.031319e-06\n",
      "Epoch 18097/20000: Train Loss = 0.434158, Test Loss = 0.253571, Learning Rate = 1.030927e-06\n",
      "Epoch 18098/20000: Train Loss = 0.434294, Test Loss = 0.252799, Learning Rate = 1.030536e-06\n",
      "Epoch 18099/20000: Train Loss = 0.434202, Test Loss = 0.252200, Learning Rate = 1.030144e-06\n",
      "Epoch 18100/20000: Train Loss = 0.434183, Test Loss = 0.251552, Learning Rate = 1.029753e-06\n",
      "Epoch 18101/20000: Train Loss = 0.434263, Test Loss = 0.251451, Learning Rate = 1.029361e-06\n",
      "Epoch 18102/20000: Train Loss = 0.434150, Test Loss = 0.251420, Learning Rate = 1.028970e-06\n",
      "Epoch 18103/20000: Train Loss = 0.434202, Test Loss = 0.251751, Learning Rate = 1.028579e-06\n",
      "Epoch 18104/20000: Train Loss = 0.434234, Test Loss = 0.251359, Learning Rate = 1.028188e-06\n",
      "Epoch 18105/20000: Train Loss = 0.434278, Test Loss = 0.251954, Learning Rate = 1.027798e-06\n",
      "Epoch 18106/20000: Train Loss = 0.434129, Test Loss = 0.251491, Learning Rate = 1.027407e-06\n",
      "Epoch 18107/20000: Train Loss = 0.434285, Test Loss = 0.251565, Learning Rate = 1.027017e-06\n",
      "Epoch 18108/20000: Train Loss = 0.434209, Test Loss = 0.250863, Learning Rate = 1.026627e-06\n",
      "Epoch 18109/20000: Train Loss = 0.434148, Test Loss = 0.251291, Learning Rate = 1.026236e-06\n",
      "Epoch 18110/20000: Train Loss = 0.434225, Test Loss = 0.251867, Learning Rate = 1.025847e-06\n",
      "Epoch 18111/20000: Train Loss = 0.434181, Test Loss = 0.251905, Learning Rate = 1.025457e-06\n",
      "Epoch 18112/20000: Train Loss = 0.434404, Test Loss = 0.251482, Learning Rate = 1.025067e-06\n",
      "Epoch 18113/20000: Train Loss = 0.434293, Test Loss = 0.252483, Learning Rate = 1.024678e-06\n",
      "Epoch 18114/20000: Train Loss = 0.434259, Test Loss = 0.252160, Learning Rate = 1.024288e-06\n",
      "Epoch 18115/20000: Train Loss = 0.434206, Test Loss = 0.252671, Learning Rate = 1.023899e-06\n",
      "Epoch 18116/20000: Train Loss = 0.434158, Test Loss = 0.252389, Learning Rate = 1.023510e-06\n",
      "Epoch 18117/20000: Train Loss = 0.434153, Test Loss = 0.252543, Learning Rate = 1.023121e-06\n",
      "Epoch 18118/20000: Train Loss = 0.434421, Test Loss = 0.251677, Learning Rate = 1.022732e-06\n",
      "Epoch 18119/20000: Train Loss = 0.434233, Test Loss = 0.251788, Learning Rate = 1.022344e-06\n",
      "Epoch 18120/20000: Train Loss = 0.434152, Test Loss = 0.252489, Learning Rate = 1.021955e-06\n",
      "Epoch 18121/20000: Train Loss = 0.434434, Test Loss = 0.251024, Learning Rate = 1.021567e-06\n",
      "Epoch 18122/20000: Train Loss = 0.434109, Test Loss = 0.251915, Learning Rate = 1.021179e-06\n",
      "Epoch 18123/20000: Train Loss = 0.434203, Test Loss = 0.252665, Learning Rate = 1.020791e-06\n",
      "Epoch 18124/20000: Train Loss = 0.434230, Test Loss = 0.252379, Learning Rate = 1.020403e-06\n",
      "Epoch 18125/20000: Train Loss = 0.434175, Test Loss = 0.251988, Learning Rate = 1.020015e-06\n",
      "Epoch 18126/20000: Train Loss = 0.434174, Test Loss = 0.252345, Learning Rate = 1.019628e-06\n",
      "Epoch 18127/20000: Train Loss = 0.434372, Test Loss = 0.252529, Learning Rate = 1.019240e-06\n",
      "Epoch 18128/20000: Train Loss = 0.434235, Test Loss = 0.252650, Learning Rate = 1.018853e-06\n",
      "Epoch 18129/20000: Train Loss = 0.434238, Test Loss = 0.252387, Learning Rate = 1.018466e-06\n",
      "Epoch 18130/20000: Train Loss = 0.434169, Test Loss = 0.252214, Learning Rate = 1.018079e-06\n",
      "Epoch 18131/20000: Train Loss = 0.434191, Test Loss = 0.251655, Learning Rate = 1.017692e-06\n",
      "Epoch 18132/20000: Train Loss = 0.434199, Test Loss = 0.251526, Learning Rate = 1.017305e-06\n",
      "Epoch 18133/20000: Train Loss = 0.434233, Test Loss = 0.251314, Learning Rate = 1.016919e-06\n",
      "Epoch 18134/20000: Train Loss = 0.434265, Test Loss = 0.251633, Learning Rate = 1.016532e-06\n",
      "Epoch 18135/20000: Train Loss = 0.434193, Test Loss = 0.250871, Learning Rate = 1.016146e-06\n",
      "Epoch 18136/20000: Train Loss = 0.434211, Test Loss = 0.251188, Learning Rate = 1.015760e-06\n",
      "Epoch 18137/20000: Train Loss = 0.434320, Test Loss = 0.251621, Learning Rate = 1.015374e-06\n",
      "Epoch 18138/20000: Train Loss = 0.434217, Test Loss = 0.251230, Learning Rate = 1.014988e-06\n",
      "Epoch 18139/20000: Train Loss = 0.434196, Test Loss = 0.251366, Learning Rate = 1.014602e-06\n",
      "Epoch 18140/20000: Train Loss = 0.434226, Test Loss = 0.251505, Learning Rate = 1.014217e-06\n",
      "Epoch 18141/20000: Train Loss = 0.434163, Test Loss = 0.251825, Learning Rate = 1.013832e-06\n",
      "Epoch 18142/20000: Train Loss = 0.434182, Test Loss = 0.250888, Learning Rate = 1.013446e-06\n",
      "Epoch 18143/20000: Train Loss = 0.434148, Test Loss = 0.251255, Learning Rate = 1.013061e-06\n",
      "Epoch 18144/20000: Train Loss = 0.434173, Test Loss = 0.251458, Learning Rate = 1.012676e-06\n",
      "Epoch 18145/20000: Train Loss = 0.434248, Test Loss = 0.251933, Learning Rate = 1.012291e-06\n",
      "Epoch 18146/20000: Train Loss = 0.434139, Test Loss = 0.251501, Learning Rate = 1.011907e-06\n",
      "Epoch 18147/20000: Train Loss = 0.434315, Test Loss = 0.251702, Learning Rate = 1.011522e-06\n",
      "Epoch 18148/20000: Train Loss = 0.434194, Test Loss = 0.251858, Learning Rate = 1.011138e-06\n",
      "Epoch 18149/20000: Train Loss = 0.434143, Test Loss = 0.251796, Learning Rate = 1.010754e-06\n",
      "Epoch 18150/20000: Train Loss = 0.434257, Test Loss = 0.252066, Learning Rate = 1.010370e-06\n",
      "Epoch 18151/20000: Train Loss = 0.434163, Test Loss = 0.252262, Learning Rate = 1.009986e-06\n",
      "Epoch 18152/20000: Train Loss = 0.434283, Test Loss = 0.252685, Learning Rate = 1.009602e-06\n",
      "Epoch 18153/20000: Train Loss = 0.434327, Test Loss = 0.252774, Learning Rate = 1.009218e-06\n",
      "Epoch 18154/20000: Train Loss = 0.434141, Test Loss = 0.251976, Learning Rate = 1.008835e-06\n",
      "Epoch 18155/20000: Train Loss = 0.434167, Test Loss = 0.252214, Learning Rate = 1.008452e-06\n",
      "Epoch 18156/20000: Train Loss = 0.434258, Test Loss = 0.252887, Learning Rate = 1.008068e-06\n",
      "Epoch 18157/20000: Train Loss = 0.434212, Test Loss = 0.253515, Learning Rate = 1.007685e-06\n",
      "Epoch 18158/20000: Train Loss = 0.434352, Test Loss = 0.253967, Learning Rate = 1.007303e-06\n",
      "Epoch 18159/20000: Train Loss = 0.434175, Test Loss = 0.253294, Learning Rate = 1.006920e-06\n",
      "Epoch 18160/20000: Train Loss = 0.434133, Test Loss = 0.253501, Learning Rate = 1.006537e-06\n",
      "Epoch 18161/20000: Train Loss = 0.434183, Test Loss = 0.254194, Learning Rate = 1.006155e-06\n",
      "Epoch 18162/20000: Train Loss = 0.434172, Test Loss = 0.254496, Learning Rate = 1.005772e-06\n",
      "Epoch 18163/20000: Train Loss = 0.434297, Test Loss = 0.254271, Learning Rate = 1.005390e-06\n",
      "Epoch 18164/20000: Train Loss = 0.434230, Test Loss = 0.253517, Learning Rate = 1.005008e-06\n",
      "Epoch 18165/20000: Train Loss = 0.434148, Test Loss = 0.253465, Learning Rate = 1.004626e-06\n",
      "Epoch 18166/20000: Train Loss = 0.434371, Test Loss = 0.252922, Learning Rate = 1.004245e-06\n",
      "Epoch 18167/20000: Train Loss = 0.434226, Test Loss = 0.251753, Learning Rate = 1.003863e-06\n",
      "Epoch 18168/20000: Train Loss = 0.434106, Test Loss = 0.252065, Learning Rate = 1.003482e-06\n",
      "Epoch 18169/20000: Train Loss = 0.434153, Test Loss = 0.252060, Learning Rate = 1.003100e-06\n",
      "Epoch 18170/20000: Train Loss = 0.434147, Test Loss = 0.251981, Learning Rate = 1.002719e-06\n",
      "Epoch 18171/20000: Train Loss = 0.434320, Test Loss = 0.251524, Learning Rate = 1.002338e-06\n",
      "Epoch 18172/20000: Train Loss = 0.434313, Test Loss = 0.252504, Learning Rate = 1.001957e-06\n",
      "Epoch 18173/20000: Train Loss = 0.434239, Test Loss = 0.251975, Learning Rate = 1.001577e-06\n",
      "Epoch 18174/20000: Train Loss = 0.434234, Test Loss = 0.252378, Learning Rate = 1.001196e-06\n",
      "Epoch 18175/20000: Train Loss = 0.434161, Test Loss = 0.252127, Learning Rate = 1.000816e-06\n",
      "Epoch 18176/20000: Train Loss = 0.434207, Test Loss = 0.251552, Learning Rate = 1.000435e-06\n",
      "Epoch 18177/20000: Train Loss = 0.434394, Test Loss = 0.251852, Learning Rate = 1.000055e-06\n",
      "Epoch 18178/20000: Train Loss = 0.434246, Test Loss = 0.251520, Learning Rate = 9.996751e-07\n",
      "Epoch 18179/20000: Train Loss = 0.434153, Test Loss = 0.251656, Learning Rate = 9.992953e-07\n",
      "Epoch 18180/20000: Train Loss = 0.434376, Test Loss = 0.251962, Learning Rate = 9.989156e-07\n",
      "Epoch 18181/20000: Train Loss = 0.434204, Test Loss = 0.251670, Learning Rate = 9.985360e-07\n",
      "Epoch 18182/20000: Train Loss = 0.434182, Test Loss = 0.251922, Learning Rate = 9.981566e-07\n",
      "Epoch 18183/20000: Train Loss = 0.434145, Test Loss = 0.251319, Learning Rate = 9.977773e-07\n",
      "Epoch 18184/20000: Train Loss = 0.434302, Test Loss = 0.251487, Learning Rate = 9.973982e-07\n",
      "Epoch 18185/20000: Train Loss = 0.434249, Test Loss = 0.251160, Learning Rate = 9.970192e-07\n",
      "Epoch 18186/20000: Train Loss = 0.434187, Test Loss = 0.251547, Learning Rate = 9.966404e-07\n",
      "Epoch 18187/20000: Train Loss = 0.434220, Test Loss = 0.252707, Learning Rate = 9.962617e-07\n",
      "Epoch 18188/20000: Train Loss = 0.434183, Test Loss = 0.252807, Learning Rate = 9.958831e-07\n",
      "Epoch 18189/20000: Train Loss = 0.434132, Test Loss = 0.253421, Learning Rate = 9.955047e-07\n",
      "Epoch 18190/20000: Train Loss = 0.434151, Test Loss = 0.253078, Learning Rate = 9.951265e-07\n",
      "Epoch 18191/20000: Train Loss = 0.434154, Test Loss = 0.253130, Learning Rate = 9.947483e-07\n",
      "Epoch 18192/20000: Train Loss = 0.434185, Test Loss = 0.253551, Learning Rate = 9.943704e-07\n",
      "Epoch 18193/20000: Train Loss = 0.434179, Test Loss = 0.253812, Learning Rate = 9.939925e-07\n",
      "Epoch 18194/20000: Train Loss = 0.434115, Test Loss = 0.253558, Learning Rate = 9.936148e-07\n",
      "Epoch 18195/20000: Train Loss = 0.434122, Test Loss = 0.253826, Learning Rate = 9.932373e-07\n",
      "Epoch 18196/20000: Train Loss = 0.434137, Test Loss = 0.253341, Learning Rate = 9.928599e-07\n",
      "Epoch 18197/20000: Train Loss = 0.434198, Test Loss = 0.253798, Learning Rate = 9.924826e-07\n",
      "Epoch 18198/20000: Train Loss = 0.434375, Test Loss = 0.254435, Learning Rate = 9.921055e-07\n",
      "Epoch 18199/20000: Train Loss = 0.434107, Test Loss = 0.253635, Learning Rate = 9.917285e-07\n",
      "Epoch 18200/20000: Train Loss = 0.434224, Test Loss = 0.253049, Learning Rate = 9.913517e-07\n",
      "Epoch 18201/20000: Train Loss = 0.434172, Test Loss = 0.253488, Learning Rate = 9.909750e-07\n",
      "Epoch 18202/20000: Train Loss = 0.434316, Test Loss = 0.253410, Learning Rate = 9.905985e-07\n",
      "Epoch 18203/20000: Train Loss = 0.434270, Test Loss = 0.254671, Learning Rate = 9.902221e-07\n",
      "Epoch 18204/20000: Train Loss = 0.434392, Test Loss = 0.254008, Learning Rate = 9.898458e-07\n",
      "Epoch 18205/20000: Train Loss = 0.434355, Test Loss = 0.254024, Learning Rate = 9.894697e-07\n",
      "Epoch 18206/20000: Train Loss = 0.434296, Test Loss = 0.253874, Learning Rate = 9.890937e-07\n",
      "Epoch 18207/20000: Train Loss = 0.434104, Test Loss = 0.253951, Learning Rate = 9.887179e-07\n",
      "Epoch 18208/20000: Train Loss = 0.434152, Test Loss = 0.254376, Learning Rate = 9.883422e-07\n",
      "Epoch 18209/20000: Train Loss = 0.434306, Test Loss = 0.254755, Learning Rate = 9.879667e-07\n",
      "Epoch 18210/20000: Train Loss = 0.434246, Test Loss = 0.253546, Learning Rate = 9.875913e-07\n",
      "Epoch 18211/20000: Train Loss = 0.434212, Test Loss = 0.254614, Learning Rate = 9.872160e-07\n",
      "Epoch 18212/20000: Train Loss = 0.434286, Test Loss = 0.254421, Learning Rate = 9.868409e-07\n",
      "Epoch 18213/20000: Train Loss = 0.434157, Test Loss = 0.254425, Learning Rate = 9.864659e-07\n",
      "Epoch 18214/20000: Train Loss = 0.434261, Test Loss = 0.254738, Learning Rate = 9.860911e-07\n",
      "Epoch 18215/20000: Train Loss = 0.434134, Test Loss = 0.254852, Learning Rate = 9.857164e-07\n",
      "Epoch 18216/20000: Train Loss = 0.434292, Test Loss = 0.254952, Learning Rate = 9.853419e-07\n",
      "Epoch 18217/20000: Train Loss = 0.434227, Test Loss = 0.254870, Learning Rate = 9.849675e-07\n",
      "Epoch 18218/20000: Train Loss = 0.434244, Test Loss = 0.254775, Learning Rate = 9.845932e-07\n",
      "Epoch 18219/20000: Train Loss = 0.434358, Test Loss = 0.255076, Learning Rate = 9.842191e-07\n",
      "Epoch 18220/20000: Train Loss = 0.434181, Test Loss = 0.254514, Learning Rate = 9.838451e-07\n",
      "Epoch 18221/20000: Train Loss = 0.434206, Test Loss = 0.255579, Learning Rate = 9.834713e-07\n",
      "Epoch 18222/20000: Train Loss = 0.434158, Test Loss = 0.255252, Learning Rate = 9.830976e-07\n",
      "Epoch 18223/20000: Train Loss = 0.434243, Test Loss = 0.255291, Learning Rate = 9.827240e-07\n",
      "Epoch 18224/20000: Train Loss = 0.434340, Test Loss = 0.255172, Learning Rate = 9.823506e-07\n",
      "Epoch 18225/20000: Train Loss = 0.434183, Test Loss = 0.256122, Learning Rate = 9.819773e-07\n",
      "Epoch 18226/20000: Train Loss = 0.434181, Test Loss = 0.254452, Learning Rate = 9.816042e-07\n",
      "Epoch 18227/20000: Train Loss = 0.434188, Test Loss = 0.254424, Learning Rate = 9.812312e-07\n",
      "Epoch 18228/20000: Train Loss = 0.434200, Test Loss = 0.254299, Learning Rate = 9.808584e-07\n",
      "Epoch 18229/20000: Train Loss = 0.434136, Test Loss = 0.253646, Learning Rate = 9.804857e-07\n",
      "Epoch 18230/20000: Train Loss = 0.434188, Test Loss = 0.253964, Learning Rate = 9.801131e-07\n",
      "Epoch 18231/20000: Train Loss = 0.434137, Test Loss = 0.254080, Learning Rate = 9.797407e-07\n",
      "Epoch 18232/20000: Train Loss = 0.434425, Test Loss = 0.253386, Learning Rate = 9.793684e-07\n",
      "Epoch 18233/20000: Train Loss = 0.434293, Test Loss = 0.254343, Learning Rate = 9.789963e-07\n",
      "Epoch 18234/20000: Train Loss = 0.434322, Test Loss = 0.253516, Learning Rate = 9.786243e-07\n",
      "Epoch 18235/20000: Train Loss = 0.434187, Test Loss = 0.254330, Learning Rate = 9.782525e-07\n",
      "Epoch 18236/20000: Train Loss = 0.434130, Test Loss = 0.252911, Learning Rate = 9.778808e-07\n",
      "Epoch 18237/20000: Train Loss = 0.434097, Test Loss = 0.253473, Learning Rate = 9.775092e-07\n",
      "Epoch 18238/20000: Train Loss = 0.434116, Test Loss = 0.253344, Learning Rate = 9.771378e-07\n",
      "Epoch 18239/20000: Train Loss = 0.434209, Test Loss = 0.253434, Learning Rate = 9.767665e-07\n",
      "Epoch 18240/20000: Train Loss = 0.434194, Test Loss = 0.253186, Learning Rate = 9.763953e-07\n",
      "Epoch 18241/20000: Train Loss = 0.434381, Test Loss = 0.253944, Learning Rate = 9.760243e-07\n",
      "Epoch 18242/20000: Train Loss = 0.434232, Test Loss = 0.253246, Learning Rate = 9.756535e-07\n",
      "Epoch 18243/20000: Train Loss = 0.434193, Test Loss = 0.252976, Learning Rate = 9.752827e-07\n",
      "Epoch 18244/20000: Train Loss = 0.434429, Test Loss = 0.253362, Learning Rate = 9.749122e-07\n",
      "Epoch 18245/20000: Train Loss = 0.434236, Test Loss = 0.252033, Learning Rate = 9.745417e-07\n",
      "Epoch 18246/20000: Train Loss = 0.434151, Test Loss = 0.252499, Learning Rate = 9.741714e-07\n",
      "Epoch 18247/20000: Train Loss = 0.434203, Test Loss = 0.252026, Learning Rate = 9.738013e-07\n",
      "Epoch 18248/20000: Train Loss = 0.434241, Test Loss = 0.252587, Learning Rate = 9.734312e-07\n",
      "Epoch 18249/20000: Train Loss = 0.434309, Test Loss = 0.251208, Learning Rate = 9.730614e-07\n",
      "Epoch 18250/20000: Train Loss = 0.434164, Test Loss = 0.251085, Learning Rate = 9.726916e-07\n",
      "Epoch 18251/20000: Train Loss = 0.434232, Test Loss = 0.251260, Learning Rate = 9.723220e-07\n",
      "Epoch 18252/20000: Train Loss = 0.434215, Test Loss = 0.251236, Learning Rate = 9.719526e-07\n",
      "Epoch 18253/20000: Train Loss = 0.434202, Test Loss = 0.250696, Learning Rate = 9.715833e-07\n",
      "Epoch 18254/20000: Train Loss = 0.434108, Test Loss = 0.251221, Learning Rate = 9.712141e-07\n",
      "Epoch 18255/20000: Train Loss = 0.434161, Test Loss = 0.251172, Learning Rate = 9.708451e-07\n",
      "Epoch 18256/20000: Train Loss = 0.434127, Test Loss = 0.251549, Learning Rate = 9.704762e-07\n",
      "Epoch 18257/20000: Train Loss = 0.434173, Test Loss = 0.251678, Learning Rate = 9.701074e-07\n",
      "Epoch 18258/20000: Train Loss = 0.434218, Test Loss = 0.251086, Learning Rate = 9.697388e-07\n",
      "Epoch 18259/20000: Train Loss = 0.434157, Test Loss = 0.251513, Learning Rate = 9.693703e-07\n",
      "Epoch 18260/20000: Train Loss = 0.434105, Test Loss = 0.252045, Learning Rate = 9.690020e-07\n",
      "Epoch 18261/20000: Train Loss = 0.434213, Test Loss = 0.252336, Learning Rate = 9.686338e-07\n",
      "Epoch 18262/20000: Train Loss = 0.434301, Test Loss = 0.251049, Learning Rate = 9.682657e-07\n",
      "Epoch 18263/20000: Train Loss = 0.434219, Test Loss = 0.251464, Learning Rate = 9.678978e-07\n",
      "Epoch 18264/20000: Train Loss = 0.434133, Test Loss = 0.251688, Learning Rate = 9.675300e-07\n",
      "Epoch 18265/20000: Train Loss = 0.434174, Test Loss = 0.251809, Learning Rate = 9.671624e-07\n",
      "Epoch 18266/20000: Train Loss = 0.434168, Test Loss = 0.251966, Learning Rate = 9.667949e-07\n",
      "Epoch 18267/20000: Train Loss = 0.434111, Test Loss = 0.252138, Learning Rate = 9.664276e-07\n",
      "Epoch 18268/20000: Train Loss = 0.434317, Test Loss = 0.251277, Learning Rate = 9.660603e-07\n",
      "Epoch 18269/20000: Train Loss = 0.434187, Test Loss = 0.252343, Learning Rate = 9.656933e-07\n",
      "Epoch 18270/20000: Train Loss = 0.434240, Test Loss = 0.251754, Learning Rate = 9.653263e-07\n",
      "Epoch 18271/20000: Train Loss = 0.434125, Test Loss = 0.252361, Learning Rate = 9.649595e-07\n",
      "Epoch 18272/20000: Train Loss = 0.434202, Test Loss = 0.252922, Learning Rate = 9.645929e-07\n",
      "Epoch 18273/20000: Train Loss = 0.434393, Test Loss = 0.252134, Learning Rate = 9.642264e-07\n",
      "Epoch 18274/20000: Train Loss = 0.434072, Test Loss = 0.252815, Learning Rate = 9.638600e-07\n",
      "Epoch 18275/20000: Train Loss = 0.434205, Test Loss = 0.252629, Learning Rate = 9.634937e-07\n",
      "Epoch 18276/20000: Train Loss = 0.434148, Test Loss = 0.252550, Learning Rate = 9.631276e-07\n",
      "Epoch 18277/20000: Train Loss = 0.434142, Test Loss = 0.253261, Learning Rate = 9.627617e-07\n",
      "Epoch 18278/20000: Train Loss = 0.434109, Test Loss = 0.253076, Learning Rate = 9.623958e-07\n",
      "Epoch 18279/20000: Train Loss = 0.434157, Test Loss = 0.252706, Learning Rate = 9.620302e-07\n",
      "Epoch 18280/20000: Train Loss = 0.434210, Test Loss = 0.253043, Learning Rate = 9.616646e-07\n",
      "Epoch 18281/20000: Train Loss = 0.434160, Test Loss = 0.252771, Learning Rate = 9.612992e-07\n",
      "Epoch 18282/20000: Train Loss = 0.434240, Test Loss = 0.254245, Learning Rate = 9.609339e-07\n",
      "Epoch 18283/20000: Train Loss = 0.434101, Test Loss = 0.252625, Learning Rate = 9.605688e-07\n",
      "Epoch 18284/20000: Train Loss = 0.434499, Test Loss = 0.253985, Learning Rate = 9.602038e-07\n",
      "Epoch 18285/20000: Train Loss = 0.434193, Test Loss = 0.253355, Learning Rate = 9.598390e-07\n",
      "Epoch 18286/20000: Train Loss = 0.434262, Test Loss = 0.253082, Learning Rate = 9.594743e-07\n",
      "Epoch 18287/20000: Train Loss = 0.434166, Test Loss = 0.252809, Learning Rate = 9.591097e-07\n",
      "Epoch 18288/20000: Train Loss = 0.434141, Test Loss = 0.252218, Learning Rate = 9.587452e-07\n",
      "Epoch 18289/20000: Train Loss = 0.434157, Test Loss = 0.252335, Learning Rate = 9.583809e-07\n",
      "Epoch 18290/20000: Train Loss = 0.434218, Test Loss = 0.252526, Learning Rate = 9.580168e-07\n",
      "Epoch 18291/20000: Train Loss = 0.434227, Test Loss = 0.252840, Learning Rate = 9.576528e-07\n",
      "Epoch 18292/20000: Train Loss = 0.434080, Test Loss = 0.252728, Learning Rate = 9.572889e-07\n",
      "Epoch 18293/20000: Train Loss = 0.434132, Test Loss = 0.252451, Learning Rate = 9.569251e-07\n",
      "Epoch 18294/20000: Train Loss = 0.434181, Test Loss = 0.252334, Learning Rate = 9.565615e-07\n",
      "Epoch 18295/20000: Train Loss = 0.434311, Test Loss = 0.252046, Learning Rate = 9.561981e-07\n",
      "Epoch 18296/20000: Train Loss = 0.434215, Test Loss = 0.251950, Learning Rate = 9.558347e-07\n",
      "Epoch 18297/20000: Train Loss = 0.434169, Test Loss = 0.250949, Learning Rate = 9.554715e-07\n",
      "Epoch 18298/20000: Train Loss = 0.434162, Test Loss = 0.252054, Learning Rate = 9.551085e-07\n",
      "Epoch 18299/20000: Train Loss = 0.434210, Test Loss = 0.252374, Learning Rate = 9.547456e-07\n",
      "Epoch 18300/20000: Train Loss = 0.434174, Test Loss = 0.252459, Learning Rate = 9.543828e-07\n",
      "Epoch 18301/20000: Train Loss = 0.434408, Test Loss = 0.252136, Learning Rate = 9.540202e-07\n",
      "Epoch 18302/20000: Train Loss = 0.434120, Test Loss = 0.252024, Learning Rate = 9.536577e-07\n",
      "Epoch 18303/20000: Train Loss = 0.434173, Test Loss = 0.252381, Learning Rate = 9.532953e-07\n",
      "Epoch 18304/20000: Train Loss = 0.434151, Test Loss = 0.252961, Learning Rate = 9.529331e-07\n",
      "Epoch 18305/20000: Train Loss = 0.434138, Test Loss = 0.252407, Learning Rate = 9.525710e-07\n",
      "Epoch 18306/20000: Train Loss = 0.434171, Test Loss = 0.251898, Learning Rate = 9.522090e-07\n",
      "Epoch 18307/20000: Train Loss = 0.434106, Test Loss = 0.251672, Learning Rate = 9.518472e-07\n",
      "Epoch 18308/20000: Train Loss = 0.434185, Test Loss = 0.252433, Learning Rate = 9.514855e-07\n",
      "Epoch 18309/20000: Train Loss = 0.434154, Test Loss = 0.251811, Learning Rate = 9.511240e-07\n",
      "Epoch 18310/20000: Train Loss = 0.434215, Test Loss = 0.252015, Learning Rate = 9.507626e-07\n",
      "Epoch 18311/20000: Train Loss = 0.434129, Test Loss = 0.252725, Learning Rate = 9.504013e-07\n",
      "Epoch 18312/20000: Train Loss = 0.434235, Test Loss = 0.252676, Learning Rate = 9.500402e-07\n",
      "Epoch 18313/20000: Train Loss = 0.434145, Test Loss = 0.252859, Learning Rate = 9.496792e-07\n",
      "Epoch 18314/20000: Train Loss = 0.434183, Test Loss = 0.252694, Learning Rate = 9.493184e-07\n",
      "Epoch 18315/20000: Train Loss = 0.434168, Test Loss = 0.253037, Learning Rate = 9.489577e-07\n",
      "Epoch 18316/20000: Train Loss = 0.434200, Test Loss = 0.252707, Learning Rate = 9.485971e-07\n",
      "Epoch 18317/20000: Train Loss = 0.434377, Test Loss = 0.253127, Learning Rate = 9.482366e-07\n",
      "Epoch 18318/20000: Train Loss = 0.434149, Test Loss = 0.252957, Learning Rate = 9.478763e-07\n",
      "Epoch 18319/20000: Train Loss = 0.434233, Test Loss = 0.252014, Learning Rate = 9.475162e-07\n",
      "Epoch 18320/20000: Train Loss = 0.434149, Test Loss = 0.251386, Learning Rate = 9.471561e-07\n",
      "Epoch 18321/20000: Train Loss = 0.434159, Test Loss = 0.251599, Learning Rate = 9.467962e-07\n",
      "Epoch 18322/20000: Train Loss = 0.434247, Test Loss = 0.251808, Learning Rate = 9.464365e-07\n",
      "Epoch 18323/20000: Train Loss = 0.434160, Test Loss = 0.252067, Learning Rate = 9.460769e-07\n",
      "Epoch 18324/20000: Train Loss = 0.434153, Test Loss = 0.251608, Learning Rate = 9.457174e-07\n",
      "Epoch 18325/20000: Train Loss = 0.434155, Test Loss = 0.252456, Learning Rate = 9.453580e-07\n",
      "Epoch 18326/20000: Train Loss = 0.434164, Test Loss = 0.252002, Learning Rate = 9.449988e-07\n",
      "Epoch 18327/20000: Train Loss = 0.434144, Test Loss = 0.252080, Learning Rate = 9.446397e-07\n",
      "Epoch 18328/20000: Train Loss = 0.434246, Test Loss = 0.252012, Learning Rate = 9.442808e-07\n",
      "Epoch 18329/20000: Train Loss = 0.434179, Test Loss = 0.252447, Learning Rate = 9.439220e-07\n",
      "Epoch 18330/20000: Train Loss = 0.434098, Test Loss = 0.252329, Learning Rate = 9.435633e-07\n",
      "Epoch 18331/20000: Train Loss = 0.434178, Test Loss = 0.252742, Learning Rate = 9.432048e-07\n",
      "Epoch 18332/20000: Train Loss = 0.434247, Test Loss = 0.252834, Learning Rate = 9.428464e-07\n",
      "Epoch 18333/20000: Train Loss = 0.434225, Test Loss = 0.252389, Learning Rate = 9.424882e-07\n",
      "Epoch 18334/20000: Train Loss = 0.434279, Test Loss = 0.253215, Learning Rate = 9.421300e-07\n",
      "Epoch 18335/20000: Train Loss = 0.434213, Test Loss = 0.252594, Learning Rate = 9.417721e-07\n",
      "Epoch 18336/20000: Train Loss = 0.434181, Test Loss = 0.252146, Learning Rate = 9.414142e-07\n",
      "Epoch 18337/20000: Train Loss = 0.434163, Test Loss = 0.252046, Learning Rate = 9.410565e-07\n",
      "Epoch 18338/20000: Train Loss = 0.434347, Test Loss = 0.252849, Learning Rate = 9.406989e-07\n",
      "Epoch 18339/20000: Train Loss = 0.434063, Test Loss = 0.251699, Learning Rate = 9.403415e-07\n",
      "Epoch 18340/20000: Train Loss = 0.434123, Test Loss = 0.251963, Learning Rate = 9.399842e-07\n",
      "Epoch 18341/20000: Train Loss = 0.434381, Test Loss = 0.252618, Learning Rate = 9.396270e-07\n",
      "Epoch 18342/20000: Train Loss = 0.434181, Test Loss = 0.252889, Learning Rate = 9.392700e-07\n",
      "Epoch 18343/20000: Train Loss = 0.434270, Test Loss = 0.252219, Learning Rate = 9.389131e-07\n",
      "Epoch 18344/20000: Train Loss = 0.434096, Test Loss = 0.252367, Learning Rate = 9.385563e-07\n",
      "Epoch 18345/20000: Train Loss = 0.434131, Test Loss = 0.251665, Learning Rate = 9.381997e-07\n",
      "Epoch 18346/20000: Train Loss = 0.434091, Test Loss = 0.252073, Learning Rate = 9.378432e-07\n",
      "Epoch 18347/20000: Train Loss = 0.434142, Test Loss = 0.252965, Learning Rate = 9.374869e-07\n",
      "Epoch 18348/20000: Train Loss = 0.434126, Test Loss = 0.253143, Learning Rate = 9.371306e-07\n",
      "Epoch 18349/20000: Train Loss = 0.434196, Test Loss = 0.252229, Learning Rate = 9.367745e-07\n",
      "Epoch 18350/20000: Train Loss = 0.434116, Test Loss = 0.252252, Learning Rate = 9.364186e-07\n",
      "Epoch 18351/20000: Train Loss = 0.434087, Test Loss = 0.252361, Learning Rate = 9.360628e-07\n",
      "Epoch 18352/20000: Train Loss = 0.434218, Test Loss = 0.253118, Learning Rate = 9.357071e-07\n",
      "Epoch 18353/20000: Train Loss = 0.434376, Test Loss = 0.252599, Learning Rate = 9.353516e-07\n",
      "Epoch 18354/20000: Train Loss = 0.434170, Test Loss = 0.252411, Learning Rate = 9.349962e-07\n",
      "Epoch 18355/20000: Train Loss = 0.434182, Test Loss = 0.253608, Learning Rate = 9.346409e-07\n",
      "Epoch 18356/20000: Train Loss = 0.434218, Test Loss = 0.253258, Learning Rate = 9.342857e-07\n",
      "Epoch 18357/20000: Train Loss = 0.434153, Test Loss = 0.253935, Learning Rate = 9.339307e-07\n",
      "Epoch 18358/20000: Train Loss = 0.434124, Test Loss = 0.253653, Learning Rate = 9.335759e-07\n",
      "Epoch 18359/20000: Train Loss = 0.434133, Test Loss = 0.254116, Learning Rate = 9.332211e-07\n",
      "Epoch 18360/20000: Train Loss = 0.434164, Test Loss = 0.254197, Learning Rate = 9.328665e-07\n",
      "Epoch 18361/20000: Train Loss = 0.434316, Test Loss = 0.253868, Learning Rate = 9.325121e-07\n",
      "Epoch 18362/20000: Train Loss = 0.434159, Test Loss = 0.253972, Learning Rate = 9.321577e-07\n",
      "Epoch 18363/20000: Train Loss = 0.434267, Test Loss = 0.253711, Learning Rate = 9.318036e-07\n",
      "Epoch 18364/20000: Train Loss = 0.434238, Test Loss = 0.253616, Learning Rate = 9.314495e-07\n",
      "Epoch 18365/20000: Train Loss = 0.434124, Test Loss = 0.253669, Learning Rate = 9.310956e-07\n",
      "Epoch 18366/20000: Train Loss = 0.434161, Test Loss = 0.252702, Learning Rate = 9.307418e-07\n",
      "Epoch 18367/20000: Train Loss = 0.434156, Test Loss = 0.252394, Learning Rate = 9.303881e-07\n",
      "Epoch 18368/20000: Train Loss = 0.434265, Test Loss = 0.252746, Learning Rate = 9.300346e-07\n",
      "Epoch 18369/20000: Train Loss = 0.434161, Test Loss = 0.252605, Learning Rate = 9.296812e-07\n",
      "Epoch 18370/20000: Train Loss = 0.434150, Test Loss = 0.253384, Learning Rate = 9.293280e-07\n",
      "Epoch 18371/20000: Train Loss = 0.434226, Test Loss = 0.253307, Learning Rate = 9.289748e-07\n",
      "Epoch 18372/20000: Train Loss = 0.434259, Test Loss = 0.253259, Learning Rate = 9.286218e-07\n",
      "Epoch 18373/20000: Train Loss = 0.434128, Test Loss = 0.252601, Learning Rate = 9.282690e-07\n",
      "Epoch 18374/20000: Train Loss = 0.434139, Test Loss = 0.252864, Learning Rate = 9.279163e-07\n",
      "Epoch 18375/20000: Train Loss = 0.434148, Test Loss = 0.252250, Learning Rate = 9.275637e-07\n",
      "Epoch 18376/20000: Train Loss = 0.434121, Test Loss = 0.252287, Learning Rate = 9.272112e-07\n",
      "Epoch 18377/20000: Train Loss = 0.434208, Test Loss = 0.252595, Learning Rate = 9.268589e-07\n",
      "Epoch 18378/20000: Train Loss = 0.434142, Test Loss = 0.252447, Learning Rate = 9.265068e-07\n",
      "Epoch 18379/20000: Train Loss = 0.434125, Test Loss = 0.251807, Learning Rate = 9.261547e-07\n",
      "Epoch 18380/20000: Train Loss = 0.434191, Test Loss = 0.253838, Learning Rate = 9.258028e-07\n",
      "Epoch 18381/20000: Train Loss = 0.434202, Test Loss = 0.252833, Learning Rate = 9.254510e-07\n",
      "Epoch 18382/20000: Train Loss = 0.434208, Test Loss = 0.252762, Learning Rate = 9.250994e-07\n",
      "Epoch 18383/20000: Train Loss = 0.434245, Test Loss = 0.253481, Learning Rate = 9.247479e-07\n",
      "Epoch 18384/20000: Train Loss = 0.434100, Test Loss = 0.253863, Learning Rate = 9.243965e-07\n",
      "Epoch 18385/20000: Train Loss = 0.434304, Test Loss = 0.253480, Learning Rate = 9.240452e-07\n",
      "Epoch 18386/20000: Train Loss = 0.434145, Test Loss = 0.252736, Learning Rate = 9.236941e-07\n",
      "Epoch 18387/20000: Train Loss = 0.434102, Test Loss = 0.252689, Learning Rate = 9.233431e-07\n",
      "Epoch 18388/20000: Train Loss = 0.434135, Test Loss = 0.253449, Learning Rate = 9.229923e-07\n",
      "Epoch 18389/20000: Train Loss = 0.434140, Test Loss = 0.253616, Learning Rate = 9.226416e-07\n",
      "Epoch 18390/20000: Train Loss = 0.434276, Test Loss = 0.253532, Learning Rate = 9.222910e-07\n",
      "Epoch 18391/20000: Train Loss = 0.434489, Test Loss = 0.253269, Learning Rate = 9.219406e-07\n",
      "Epoch 18392/20000: Train Loss = 0.434399, Test Loss = 0.253849, Learning Rate = 9.215902e-07\n",
      "Epoch 18393/20000: Train Loss = 0.434208, Test Loss = 0.253512, Learning Rate = 9.212401e-07\n",
      "Epoch 18394/20000: Train Loss = 0.434109, Test Loss = 0.253586, Learning Rate = 9.208900e-07\n",
      "Epoch 18395/20000: Train Loss = 0.434187, Test Loss = 0.253287, Learning Rate = 9.205401e-07\n",
      "Epoch 18396/20000: Train Loss = 0.434217, Test Loss = 0.252584, Learning Rate = 9.201903e-07\n",
      "Epoch 18397/20000: Train Loss = 0.434209, Test Loss = 0.253059, Learning Rate = 9.198407e-07\n",
      "Epoch 18398/20000: Train Loss = 0.434159, Test Loss = 0.252406, Learning Rate = 9.194912e-07\n",
      "Epoch 18399/20000: Train Loss = 0.434144, Test Loss = 0.252476, Learning Rate = 9.191418e-07\n",
      "Epoch 18400/20000: Train Loss = 0.434259, Test Loss = 0.252800, Learning Rate = 9.187925e-07\n",
      "Epoch 18401/20000: Train Loss = 0.434143, Test Loss = 0.252233, Learning Rate = 9.184434e-07\n",
      "Epoch 18402/20000: Train Loss = 0.434087, Test Loss = 0.252460, Learning Rate = 9.180944e-07\n",
      "Epoch 18403/20000: Train Loss = 0.434116, Test Loss = 0.252379, Learning Rate = 9.177456e-07\n",
      "Epoch 18404/20000: Train Loss = 0.434154, Test Loss = 0.252362, Learning Rate = 9.173969e-07\n",
      "Epoch 18405/20000: Train Loss = 0.434180, Test Loss = 0.253373, Learning Rate = 9.170483e-07\n",
      "Epoch 18406/20000: Train Loss = 0.434161, Test Loss = 0.253219, Learning Rate = 9.166998e-07\n",
      "Epoch 18407/20000: Train Loss = 0.434070, Test Loss = 0.253044, Learning Rate = 9.163515e-07\n",
      "Epoch 18408/20000: Train Loss = 0.434114, Test Loss = 0.252992, Learning Rate = 9.160033e-07\n",
      "Epoch 18409/20000: Train Loss = 0.434224, Test Loss = 0.253443, Learning Rate = 9.156553e-07\n",
      "Epoch 18410/20000: Train Loss = 0.434134, Test Loss = 0.252197, Learning Rate = 9.153073e-07\n",
      "Epoch 18411/20000: Train Loss = 0.434130, Test Loss = 0.252172, Learning Rate = 9.149595e-07\n",
      "Epoch 18412/20000: Train Loss = 0.434135, Test Loss = 0.251792, Learning Rate = 9.146119e-07\n",
      "Epoch 18413/20000: Train Loss = 0.434234, Test Loss = 0.251987, Learning Rate = 9.142644e-07\n",
      "Epoch 18414/20000: Train Loss = 0.434090, Test Loss = 0.253190, Learning Rate = 9.139170e-07\n",
      "Epoch 18415/20000: Train Loss = 0.434245, Test Loss = 0.253670, Learning Rate = 9.135697e-07\n",
      "Epoch 18416/20000: Train Loss = 0.434504, Test Loss = 0.253345, Learning Rate = 9.132226e-07\n",
      "Epoch 18417/20000: Train Loss = 0.434129, Test Loss = 0.253637, Learning Rate = 9.128756e-07\n",
      "Epoch 18418/20000: Train Loss = 0.434204, Test Loss = 0.253829, Learning Rate = 9.125287e-07\n",
      "Epoch 18419/20000: Train Loss = 0.434220, Test Loss = 0.253785, Learning Rate = 9.121820e-07\n",
      "Epoch 18420/20000: Train Loss = 0.434135, Test Loss = 0.254342, Learning Rate = 9.118354e-07\n",
      "Epoch 18421/20000: Train Loss = 0.434404, Test Loss = 0.254000, Learning Rate = 9.114889e-07\n",
      "Epoch 18422/20000: Train Loss = 0.434169, Test Loss = 0.253192, Learning Rate = 9.111425e-07\n",
      "Epoch 18423/20000: Train Loss = 0.434165, Test Loss = 0.253384, Learning Rate = 9.107963e-07\n",
      "Epoch 18424/20000: Train Loss = 0.434127, Test Loss = 0.252531, Learning Rate = 9.104502e-07\n",
      "Epoch 18425/20000: Train Loss = 0.434257, Test Loss = 0.253084, Learning Rate = 9.101043e-07\n",
      "Epoch 18426/20000: Train Loss = 0.434211, Test Loss = 0.253097, Learning Rate = 9.097585e-07\n",
      "Epoch 18427/20000: Train Loss = 0.434164, Test Loss = 0.252653, Learning Rate = 9.094128e-07\n",
      "Epoch 18428/20000: Train Loss = 0.434199, Test Loss = 0.253442, Learning Rate = 9.090673e-07\n",
      "Epoch 18429/20000: Train Loss = 0.434466, Test Loss = 0.253018, Learning Rate = 9.087218e-07\n",
      "Epoch 18430/20000: Train Loss = 0.434372, Test Loss = 0.253067, Learning Rate = 9.083765e-07\n",
      "Epoch 18431/20000: Train Loss = 0.434322, Test Loss = 0.252711, Learning Rate = 9.080314e-07\n",
      "Epoch 18432/20000: Train Loss = 0.434163, Test Loss = 0.253119, Learning Rate = 9.076864e-07\n",
      "Epoch 18433/20000: Train Loss = 0.434109, Test Loss = 0.253409, Learning Rate = 9.073415e-07\n",
      "Epoch 18434/20000: Train Loss = 0.434150, Test Loss = 0.253522, Learning Rate = 9.069967e-07\n",
      "Epoch 18435/20000: Train Loss = 0.434152, Test Loss = 0.253580, Learning Rate = 9.066521e-07\n",
      "Epoch 18436/20000: Train Loss = 0.434186, Test Loss = 0.253283, Learning Rate = 9.063076e-07\n",
      "Epoch 18437/20000: Train Loss = 0.434081, Test Loss = 0.253401, Learning Rate = 9.059632e-07\n",
      "Epoch 18438/20000: Train Loss = 0.434132, Test Loss = 0.252874, Learning Rate = 9.056189e-07\n",
      "Epoch 18439/20000: Train Loss = 0.434112, Test Loss = 0.253018, Learning Rate = 9.052748e-07\n",
      "Epoch 18440/20000: Train Loss = 0.434376, Test Loss = 0.253066, Learning Rate = 9.049309e-07\n",
      "Epoch 18441/20000: Train Loss = 0.434189, Test Loss = 0.253411, Learning Rate = 9.045870e-07\n",
      "Epoch 18442/20000: Train Loss = 0.434135, Test Loss = 0.253553, Learning Rate = 9.042433e-07\n",
      "Epoch 18443/20000: Train Loss = 0.434146, Test Loss = 0.253594, Learning Rate = 9.038997e-07\n",
      "Epoch 18444/20000: Train Loss = 0.434231, Test Loss = 0.253198, Learning Rate = 9.035562e-07\n",
      "Epoch 18445/20000: Train Loss = 0.434215, Test Loss = 0.253383, Learning Rate = 9.032129e-07\n",
      "Epoch 18446/20000: Train Loss = 0.434082, Test Loss = 0.251890, Learning Rate = 9.028697e-07\n",
      "Epoch 18447/20000: Train Loss = 0.434299, Test Loss = 0.252327, Learning Rate = 9.025267e-07\n",
      "Epoch 18448/20000: Train Loss = 0.434087, Test Loss = 0.252185, Learning Rate = 9.021837e-07\n",
      "Epoch 18449/20000: Train Loss = 0.434311, Test Loss = 0.252083, Learning Rate = 9.018409e-07\n",
      "Epoch 18450/20000: Train Loss = 0.434126, Test Loss = 0.251507, Learning Rate = 9.014982e-07\n",
      "Epoch 18451/20000: Train Loss = 0.434102, Test Loss = 0.251598, Learning Rate = 9.011557e-07\n",
      "Epoch 18452/20000: Train Loss = 0.434215, Test Loss = 0.251866, Learning Rate = 9.008133e-07\n",
      "Epoch 18453/20000: Train Loss = 0.434129, Test Loss = 0.251064, Learning Rate = 9.004710e-07\n",
      "Epoch 18454/20000: Train Loss = 0.434258, Test Loss = 0.250988, Learning Rate = 9.001288e-07\n",
      "Epoch 18455/20000: Train Loss = 0.434205, Test Loss = 0.250864, Learning Rate = 8.997868e-07\n",
      "Epoch 18456/20000: Train Loss = 0.434090, Test Loss = 0.251294, Learning Rate = 8.994449e-07\n",
      "Epoch 18457/20000: Train Loss = 0.434230, Test Loss = 0.251559, Learning Rate = 8.991032e-07\n",
      "Epoch 18458/20000: Train Loss = 0.434253, Test Loss = 0.251741, Learning Rate = 8.987615e-07\n",
      "Epoch 18459/20000: Train Loss = 0.434143, Test Loss = 0.250666, Learning Rate = 8.984200e-07\n",
      "Epoch 18460/20000: Train Loss = 0.434260, Test Loss = 0.251068, Learning Rate = 8.980786e-07\n",
      "Epoch 18461/20000: Train Loss = 0.434298, Test Loss = 0.251468, Learning Rate = 8.977374e-07\n",
      "Epoch 18462/20000: Train Loss = 0.434109, Test Loss = 0.251812, Learning Rate = 8.973963e-07\n",
      "Epoch 18463/20000: Train Loss = 0.434121, Test Loss = 0.252201, Learning Rate = 8.970553e-07\n",
      "Epoch 18464/20000: Train Loss = 0.434282, Test Loss = 0.252280, Learning Rate = 8.967144e-07\n",
      "Epoch 18465/20000: Train Loss = 0.434163, Test Loss = 0.252118, Learning Rate = 8.963737e-07\n",
      "Epoch 18466/20000: Train Loss = 0.434126, Test Loss = 0.252286, Learning Rate = 8.960331e-07\n",
      "Epoch 18467/20000: Train Loss = 0.434091, Test Loss = 0.251726, Learning Rate = 8.956926e-07\n",
      "Epoch 18468/20000: Train Loss = 0.434076, Test Loss = 0.252033, Learning Rate = 8.953523e-07\n",
      "Epoch 18469/20000: Train Loss = 0.434247, Test Loss = 0.252621, Learning Rate = 8.950121e-07\n",
      "Epoch 18470/20000: Train Loss = 0.434110, Test Loss = 0.251962, Learning Rate = 8.946720e-07\n",
      "Epoch 18471/20000: Train Loss = 0.434135, Test Loss = 0.252887, Learning Rate = 8.943321e-07\n",
      "Epoch 18472/20000: Train Loss = 0.434152, Test Loss = 0.252444, Learning Rate = 8.939922e-07\n",
      "Epoch 18473/20000: Train Loss = 0.434188, Test Loss = 0.251779, Learning Rate = 8.936525e-07\n",
      "Epoch 18474/20000: Train Loss = 0.434559, Test Loss = 0.253626, Learning Rate = 8.933130e-07\n",
      "Epoch 18475/20000: Train Loss = 0.434081, Test Loss = 0.252674, Learning Rate = 8.929735e-07\n",
      "Epoch 18476/20000: Train Loss = 0.434140, Test Loss = 0.252848, Learning Rate = 8.926342e-07\n",
      "Epoch 18477/20000: Train Loss = 0.434126, Test Loss = 0.252652, Learning Rate = 8.922951e-07\n",
      "Epoch 18478/20000: Train Loss = 0.434131, Test Loss = 0.253327, Learning Rate = 8.919560e-07\n",
      "Epoch 18479/20000: Train Loss = 0.434164, Test Loss = 0.252988, Learning Rate = 8.916171e-07\n",
      "Epoch 18480/20000: Train Loss = 0.434123, Test Loss = 0.253303, Learning Rate = 8.912783e-07\n",
      "Epoch 18481/20000: Train Loss = 0.434135, Test Loss = 0.253312, Learning Rate = 8.909396e-07\n",
      "Epoch 18482/20000: Train Loss = 0.434123, Test Loss = 0.253504, Learning Rate = 8.906011e-07\n",
      "Epoch 18483/20000: Train Loss = 0.434204, Test Loss = 0.253159, Learning Rate = 8.902627e-07\n",
      "Epoch 18484/20000: Train Loss = 0.434205, Test Loss = 0.253945, Learning Rate = 8.899244e-07\n",
      "Epoch 18485/20000: Train Loss = 0.434178, Test Loss = 0.253450, Learning Rate = 8.895863e-07\n",
      "Epoch 18486/20000: Train Loss = 0.434188, Test Loss = 0.254489, Learning Rate = 8.892483e-07\n",
      "Epoch 18487/20000: Train Loss = 0.434252, Test Loss = 0.253518, Learning Rate = 8.889104e-07\n",
      "Epoch 18488/20000: Train Loss = 0.434126, Test Loss = 0.252988, Learning Rate = 8.885726e-07\n",
      "Epoch 18489/20000: Train Loss = 0.434280, Test Loss = 0.252833, Learning Rate = 8.882350e-07\n",
      "Epoch 18490/20000: Train Loss = 0.434102, Test Loss = 0.253314, Learning Rate = 8.878975e-07\n",
      "Epoch 18491/20000: Train Loss = 0.434091, Test Loss = 0.253480, Learning Rate = 8.875601e-07\n",
      "Epoch 18492/20000: Train Loss = 0.434064, Test Loss = 0.253451, Learning Rate = 8.872228e-07\n",
      "Epoch 18493/20000: Train Loss = 0.434141, Test Loss = 0.253632, Learning Rate = 8.868857e-07\n",
      "Epoch 18494/20000: Train Loss = 0.434300, Test Loss = 0.254054, Learning Rate = 8.865487e-07\n",
      "Epoch 18495/20000: Train Loss = 0.433976, Test Loss = 0.253958, Learning Rate = 8.862119e-07\n",
      "Epoch 18496/20000: Train Loss = 0.434405, Test Loss = 0.253625, Learning Rate = 8.858751e-07\n",
      "Epoch 18497/20000: Train Loss = 0.434609, Test Loss = 0.254665, Learning Rate = 8.855385e-07\n",
      "Epoch 18498/20000: Train Loss = 0.434153, Test Loss = 0.254056, Learning Rate = 8.852020e-07\n",
      "Epoch 18499/20000: Train Loss = 0.434183, Test Loss = 0.253151, Learning Rate = 8.848657e-07\n",
      "Epoch 18500/20000: Train Loss = 0.434122, Test Loss = 0.253375, Learning Rate = 8.845295e-07\n",
      "Epoch 18501/20000: Train Loss = 0.434113, Test Loss = 0.253311, Learning Rate = 8.841934e-07\n",
      "Epoch 18502/20000: Train Loss = 0.434225, Test Loss = 0.252893, Learning Rate = 8.838574e-07\n",
      "Epoch 18503/20000: Train Loss = 0.434184, Test Loss = 0.253055, Learning Rate = 8.835216e-07\n",
      "Epoch 18504/20000: Train Loss = 0.434061, Test Loss = 0.253181, Learning Rate = 8.831858e-07\n",
      "Epoch 18505/20000: Train Loss = 0.434201, Test Loss = 0.252690, Learning Rate = 8.828503e-07\n",
      "Epoch 18506/20000: Train Loss = 0.434067, Test Loss = 0.253291, Learning Rate = 8.825148e-07\n",
      "Epoch 18507/20000: Train Loss = 0.434197, Test Loss = 0.253108, Learning Rate = 8.821795e-07\n",
      "Epoch 18508/20000: Train Loss = 0.434122, Test Loss = 0.252787, Learning Rate = 8.818443e-07\n",
      "Epoch 18509/20000: Train Loss = 0.434072, Test Loss = 0.252766, Learning Rate = 8.815092e-07\n",
      "Epoch 18510/20000: Train Loss = 0.434113, Test Loss = 0.253237, Learning Rate = 8.811742e-07\n",
      "Epoch 18511/20000: Train Loss = 0.434091, Test Loss = 0.252558, Learning Rate = 8.808394e-07\n",
      "Epoch 18512/20000: Train Loss = 0.434352, Test Loss = 0.252958, Learning Rate = 8.805047e-07\n",
      "Epoch 18513/20000: Train Loss = 0.434254, Test Loss = 0.252649, Learning Rate = 8.801702e-07\n",
      "Epoch 18514/20000: Train Loss = 0.434174, Test Loss = 0.253274, Learning Rate = 8.798357e-07\n",
      "Epoch 18515/20000: Train Loss = 0.434115, Test Loss = 0.253206, Learning Rate = 8.795014e-07\n",
      "Epoch 18516/20000: Train Loss = 0.434094, Test Loss = 0.252497, Learning Rate = 8.791672e-07\n",
      "Epoch 18517/20000: Train Loss = 0.434098, Test Loss = 0.252996, Learning Rate = 8.788331e-07\n",
      "Epoch 18518/20000: Train Loss = 0.434087, Test Loss = 0.253110, Learning Rate = 8.784992e-07\n",
      "Epoch 18519/20000: Train Loss = 0.434156, Test Loss = 0.252823, Learning Rate = 8.781654e-07\n",
      "Epoch 18520/20000: Train Loss = 0.434100, Test Loss = 0.252771, Learning Rate = 8.778317e-07\n",
      "Epoch 18521/20000: Train Loss = 0.434563, Test Loss = 0.252806, Learning Rate = 8.774982e-07\n",
      "Epoch 18522/20000: Train Loss = 0.434026, Test Loss = 0.252264, Learning Rate = 8.771648e-07\n",
      "Epoch 18523/20000: Train Loss = 0.434197, Test Loss = 0.252945, Learning Rate = 8.768315e-07\n",
      "Epoch 18524/20000: Train Loss = 0.434177, Test Loss = 0.252165, Learning Rate = 8.764983e-07\n",
      "Epoch 18525/20000: Train Loss = 0.434164, Test Loss = 0.252343, Learning Rate = 8.761652e-07\n",
      "Epoch 18526/20000: Train Loss = 0.434126, Test Loss = 0.252283, Learning Rate = 8.758323e-07\n",
      "Epoch 18527/20000: Train Loss = 0.434081, Test Loss = 0.251759, Learning Rate = 8.754995e-07\n",
      "Epoch 18528/20000: Train Loss = 0.434079, Test Loss = 0.252171, Learning Rate = 8.751669e-07\n",
      "Epoch 18529/20000: Train Loss = 0.434116, Test Loss = 0.253231, Learning Rate = 8.748343e-07\n",
      "Epoch 18530/20000: Train Loss = 0.434217, Test Loss = 0.252970, Learning Rate = 8.745019e-07\n",
      "Epoch 18531/20000: Train Loss = 0.434110, Test Loss = 0.252736, Learning Rate = 8.741696e-07\n",
      "Epoch 18532/20000: Train Loss = 0.434103, Test Loss = 0.252810, Learning Rate = 8.738375e-07\n",
      "Epoch 18533/20000: Train Loss = 0.434044, Test Loss = 0.253375, Learning Rate = 8.735054e-07\n",
      "Epoch 18534/20000: Train Loss = 0.434113, Test Loss = 0.254034, Learning Rate = 8.731735e-07\n",
      "Epoch 18535/20000: Train Loss = 0.434194, Test Loss = 0.253152, Learning Rate = 8.728417e-07\n",
      "Epoch 18536/20000: Train Loss = 0.434107, Test Loss = 0.252774, Learning Rate = 8.725101e-07\n",
      "Epoch 18537/20000: Train Loss = 0.434146, Test Loss = 0.252920, Learning Rate = 8.721785e-07\n",
      "Epoch 18538/20000: Train Loss = 0.434111, Test Loss = 0.253604, Learning Rate = 8.718471e-07\n",
      "Epoch 18539/20000: Train Loss = 0.434305, Test Loss = 0.253429, Learning Rate = 8.715159e-07\n",
      "Epoch 18540/20000: Train Loss = 0.434184, Test Loss = 0.253355, Learning Rate = 8.711847e-07\n",
      "Epoch 18541/20000: Train Loss = 0.434169, Test Loss = 0.253473, Learning Rate = 8.708537e-07\n",
      "Epoch 18542/20000: Train Loss = 0.434100, Test Loss = 0.253290, Learning Rate = 8.705228e-07\n",
      "Epoch 18543/20000: Train Loss = 0.434166, Test Loss = 0.252890, Learning Rate = 8.701920e-07\n",
      "Epoch 18544/20000: Train Loss = 0.434101, Test Loss = 0.252835, Learning Rate = 8.698614e-07\n",
      "Epoch 18545/20000: Train Loss = 0.434154, Test Loss = 0.252533, Learning Rate = 8.695308e-07\n",
      "Epoch 18546/20000: Train Loss = 0.434130, Test Loss = 0.253771, Learning Rate = 8.692004e-07\n",
      "Epoch 18547/20000: Train Loss = 0.434269, Test Loss = 0.253508, Learning Rate = 8.688702e-07\n",
      "Epoch 18548/20000: Train Loss = 0.434141, Test Loss = 0.253078, Learning Rate = 8.685400e-07\n",
      "Epoch 18549/20000: Train Loss = 0.434104, Test Loss = 0.253413, Learning Rate = 8.682100e-07\n",
      "Epoch 18550/20000: Train Loss = 0.434119, Test Loss = 0.253865, Learning Rate = 8.678801e-07\n",
      "Epoch 18551/20000: Train Loss = 0.434053, Test Loss = 0.253313, Learning Rate = 8.675503e-07\n",
      "Epoch 18552/20000: Train Loss = 0.434172, Test Loss = 0.252845, Learning Rate = 8.672207e-07\n",
      "Epoch 18553/20000: Train Loss = 0.434205, Test Loss = 0.252949, Learning Rate = 8.668912e-07\n",
      "Epoch 18554/20000: Train Loss = 0.434131, Test Loss = 0.252707, Learning Rate = 8.665618e-07\n",
      "Epoch 18555/20000: Train Loss = 0.434160, Test Loss = 0.252359, Learning Rate = 8.662325e-07\n",
      "Epoch 18556/20000: Train Loss = 0.434059, Test Loss = 0.252557, Learning Rate = 8.659034e-07\n",
      "Epoch 18557/20000: Train Loss = 0.434190, Test Loss = 0.252698, Learning Rate = 8.655743e-07\n",
      "Epoch 18558/20000: Train Loss = 0.434119, Test Loss = 0.251972, Learning Rate = 8.652454e-07\n",
      "Epoch 18559/20000: Train Loss = 0.434127, Test Loss = 0.252129, Learning Rate = 8.649167e-07\n",
      "Epoch 18560/20000: Train Loss = 0.434260, Test Loss = 0.252601, Learning Rate = 8.645880e-07\n",
      "Epoch 18561/20000: Train Loss = 0.434352, Test Loss = 0.252253, Learning Rate = 8.642595e-07\n",
      "Epoch 18562/20000: Train Loss = 0.434117, Test Loss = 0.252382, Learning Rate = 8.639311e-07\n",
      "Epoch 18563/20000: Train Loss = 0.434234, Test Loss = 0.251681, Learning Rate = 8.636028e-07\n",
      "Epoch 18564/20000: Train Loss = 0.434204, Test Loss = 0.252174, Learning Rate = 8.632747e-07\n",
      "Epoch 18565/20000: Train Loss = 0.434140, Test Loss = 0.252277, Learning Rate = 8.629467e-07\n",
      "Epoch 18566/20000: Train Loss = 0.434123, Test Loss = 0.252027, Learning Rate = 8.626188e-07\n",
      "Epoch 18567/20000: Train Loss = 0.434219, Test Loss = 0.252044, Learning Rate = 8.622910e-07\n",
      "Epoch 18568/20000: Train Loss = 0.434109, Test Loss = 0.251905, Learning Rate = 8.619634e-07\n",
      "Epoch 18569/20000: Train Loss = 0.434111, Test Loss = 0.252297, Learning Rate = 8.616358e-07\n",
      "Epoch 18570/20000: Train Loss = 0.434077, Test Loss = 0.251849, Learning Rate = 8.613084e-07\n",
      "Epoch 18571/20000: Train Loss = 0.434224, Test Loss = 0.252421, Learning Rate = 8.609812e-07\n",
      "Epoch 18572/20000: Train Loss = 0.434080, Test Loss = 0.251774, Learning Rate = 8.606540e-07\n",
      "Epoch 18573/20000: Train Loss = 0.434108, Test Loss = 0.251441, Learning Rate = 8.603270e-07\n",
      "Epoch 18574/20000: Train Loss = 0.434138, Test Loss = 0.252008, Learning Rate = 8.600001e-07\n",
      "Epoch 18575/20000: Train Loss = 0.434212, Test Loss = 0.252604, Learning Rate = 8.596733e-07\n",
      "Epoch 18576/20000: Train Loss = 0.434097, Test Loss = 0.252697, Learning Rate = 8.593467e-07\n",
      "Epoch 18577/20000: Train Loss = 0.434044, Test Loss = 0.252279, Learning Rate = 8.590201e-07\n",
      "Epoch 18578/20000: Train Loss = 0.434071, Test Loss = 0.251990, Learning Rate = 8.586937e-07\n",
      "Epoch 18579/20000: Train Loss = 0.434179, Test Loss = 0.252249, Learning Rate = 8.583674e-07\n",
      "Epoch 18580/20000: Train Loss = 0.434066, Test Loss = 0.251972, Learning Rate = 8.580413e-07\n",
      "Epoch 18581/20000: Train Loss = 0.434102, Test Loss = 0.251565, Learning Rate = 8.577153e-07\n",
      "Epoch 18582/20000: Train Loss = 0.434159, Test Loss = 0.252742, Learning Rate = 8.573893e-07\n",
      "Epoch 18583/20000: Train Loss = 0.434144, Test Loss = 0.251659, Learning Rate = 8.570636e-07\n",
      "Epoch 18584/20000: Train Loss = 0.434102, Test Loss = 0.252517, Learning Rate = 8.567379e-07\n",
      "Epoch 18585/20000: Train Loss = 0.434119, Test Loss = 0.251972, Learning Rate = 8.564124e-07\n",
      "Epoch 18586/20000: Train Loss = 0.434158, Test Loss = 0.252377, Learning Rate = 8.560869e-07\n",
      "Epoch 18587/20000: Train Loss = 0.434124, Test Loss = 0.251911, Learning Rate = 8.557617e-07\n",
      "Epoch 18588/20000: Train Loss = 0.434127, Test Loss = 0.251114, Learning Rate = 8.554365e-07\n",
      "Epoch 18589/20000: Train Loss = 0.434346, Test Loss = 0.251605, Learning Rate = 8.551115e-07\n",
      "Epoch 18590/20000: Train Loss = 0.434261, Test Loss = 0.252123, Learning Rate = 8.547865e-07\n",
      "Epoch 18591/20000: Train Loss = 0.434172, Test Loss = 0.252195, Learning Rate = 8.544617e-07\n",
      "Epoch 18592/20000: Train Loss = 0.434086, Test Loss = 0.252175, Learning Rate = 8.541371e-07\n",
      "Epoch 18593/20000: Train Loss = 0.434118, Test Loss = 0.252210, Learning Rate = 8.538125e-07\n",
      "Epoch 18594/20000: Train Loss = 0.434118, Test Loss = 0.252588, Learning Rate = 8.534881e-07\n",
      "Epoch 18595/20000: Train Loss = 0.434193, Test Loss = 0.252745, Learning Rate = 8.531638e-07\n",
      "Epoch 18596/20000: Train Loss = 0.434170, Test Loss = 0.253605, Learning Rate = 8.528396e-07\n",
      "Epoch 18597/20000: Train Loss = 0.434177, Test Loss = 0.253817, Learning Rate = 8.525156e-07\n",
      "Epoch 18598/20000: Train Loss = 0.434104, Test Loss = 0.253773, Learning Rate = 8.521916e-07\n",
      "Epoch 18599/20000: Train Loss = 0.434149, Test Loss = 0.254134, Learning Rate = 8.518678e-07\n",
      "Epoch 18600/20000: Train Loss = 0.434087, Test Loss = 0.254042, Learning Rate = 8.515441e-07\n",
      "Epoch 18601/20000: Train Loss = 0.434191, Test Loss = 0.254201, Learning Rate = 8.512206e-07\n",
      "Epoch 18602/20000: Train Loss = 0.434086, Test Loss = 0.254114, Learning Rate = 8.508971e-07\n",
      "Epoch 18603/20000: Train Loss = 0.434313, Test Loss = 0.253398, Learning Rate = 8.505738e-07\n",
      "Epoch 18604/20000: Train Loss = 0.434176, Test Loss = 0.253184, Learning Rate = 8.502506e-07\n",
      "Epoch 18605/20000: Train Loss = 0.434080, Test Loss = 0.253016, Learning Rate = 8.499275e-07\n",
      "Epoch 18606/20000: Train Loss = 0.434184, Test Loss = 0.253527, Learning Rate = 8.496046e-07\n",
      "Epoch 18607/20000: Train Loss = 0.434222, Test Loss = 0.252854, Learning Rate = 8.492818e-07\n",
      "Epoch 18608/20000: Train Loss = 0.434175, Test Loss = 0.252710, Learning Rate = 8.489591e-07\n",
      "Epoch 18609/20000: Train Loss = 0.434120, Test Loss = 0.253020, Learning Rate = 8.486365e-07\n",
      "Epoch 18610/20000: Train Loss = 0.434090, Test Loss = 0.252021, Learning Rate = 8.483140e-07\n",
      "Epoch 18611/20000: Train Loss = 0.434181, Test Loss = 0.252871, Learning Rate = 8.479917e-07\n",
      "Epoch 18612/20000: Train Loss = 0.434064, Test Loss = 0.253113, Learning Rate = 8.476695e-07\n",
      "Epoch 18613/20000: Train Loss = 0.434114, Test Loss = 0.252810, Learning Rate = 8.473474e-07\n",
      "Epoch 18614/20000: Train Loss = 0.434188, Test Loss = 0.253321, Learning Rate = 8.470254e-07\n",
      "Epoch 18615/20000: Train Loss = 0.434138, Test Loss = 0.252922, Learning Rate = 8.467036e-07\n",
      "Epoch 18616/20000: Train Loss = 0.434260, Test Loss = 0.253280, Learning Rate = 8.463818e-07\n",
      "Epoch 18617/20000: Train Loss = 0.434049, Test Loss = 0.253220, Learning Rate = 8.460602e-07\n",
      "Epoch 18618/20000: Train Loss = 0.434191, Test Loss = 0.253540, Learning Rate = 8.457387e-07\n",
      "Epoch 18619/20000: Train Loss = 0.434081, Test Loss = 0.253033, Learning Rate = 8.454174e-07\n",
      "Epoch 18620/20000: Train Loss = 0.434221, Test Loss = 0.253330, Learning Rate = 8.450962e-07\n",
      "Epoch 18621/20000: Train Loss = 0.434198, Test Loss = 0.253908, Learning Rate = 8.447750e-07\n",
      "Epoch 18622/20000: Train Loss = 0.434214, Test Loss = 0.253400, Learning Rate = 8.444540e-07\n",
      "Epoch 18623/20000: Train Loss = 0.434298, Test Loss = 0.253166, Learning Rate = 8.441332e-07\n",
      "Epoch 18624/20000: Train Loss = 0.434185, Test Loss = 0.254170, Learning Rate = 8.438124e-07\n",
      "Epoch 18625/20000: Train Loss = 0.434173, Test Loss = 0.253617, Learning Rate = 8.434918e-07\n",
      "Epoch 18626/20000: Train Loss = 0.434147, Test Loss = 0.253141, Learning Rate = 8.431713e-07\n",
      "Epoch 18627/20000: Train Loss = 0.434166, Test Loss = 0.252528, Learning Rate = 8.428509e-07\n",
      "Epoch 18628/20000: Train Loss = 0.434113, Test Loss = 0.252875, Learning Rate = 8.425307e-07\n",
      "Epoch 18629/20000: Train Loss = 0.434154, Test Loss = 0.253100, Learning Rate = 8.422105e-07\n",
      "Epoch 18630/20000: Train Loss = 0.434200, Test Loss = 0.253530, Learning Rate = 8.418905e-07\n",
      "Epoch 18631/20000: Train Loss = 0.434437, Test Loss = 0.252357, Learning Rate = 8.415706e-07\n",
      "Epoch 18632/20000: Train Loss = 0.434103, Test Loss = 0.253238, Learning Rate = 8.412508e-07\n",
      "Epoch 18633/20000: Train Loss = 0.434197, Test Loss = 0.252667, Learning Rate = 8.409312e-07\n",
      "Epoch 18634/20000: Train Loss = 0.434115, Test Loss = 0.252675, Learning Rate = 8.406116e-07\n",
      "Epoch 18635/20000: Train Loss = 0.434121, Test Loss = 0.252909, Learning Rate = 8.402922e-07\n",
      "Epoch 18636/20000: Train Loss = 0.434132, Test Loss = 0.252593, Learning Rate = 8.399730e-07\n",
      "Epoch 18637/20000: Train Loss = 0.434055, Test Loss = 0.252465, Learning Rate = 8.396538e-07\n",
      "Epoch 18638/20000: Train Loss = 0.434112, Test Loss = 0.252383, Learning Rate = 8.393347e-07\n",
      "Epoch 18639/20000: Train Loss = 0.434074, Test Loss = 0.252377, Learning Rate = 8.390158e-07\n",
      "Epoch 18640/20000: Train Loss = 0.434097, Test Loss = 0.252277, Learning Rate = 8.386970e-07\n",
      "Epoch 18641/20000: Train Loss = 0.434075, Test Loss = 0.252138, Learning Rate = 8.383783e-07\n",
      "Epoch 18642/20000: Train Loss = 0.434135, Test Loss = 0.251942, Learning Rate = 8.380598e-07\n",
      "Epoch 18643/20000: Train Loss = 0.434119, Test Loss = 0.252061, Learning Rate = 8.377413e-07\n",
      "Epoch 18644/20000: Train Loss = 0.434105, Test Loss = 0.252310, Learning Rate = 8.374230e-07\n",
      "Epoch 18645/20000: Train Loss = 0.434126, Test Loss = 0.252002, Learning Rate = 8.371048e-07\n",
      "Epoch 18646/20000: Train Loss = 0.434069, Test Loss = 0.251665, Learning Rate = 8.367867e-07\n",
      "Epoch 18647/20000: Train Loss = 0.434106, Test Loss = 0.251774, Learning Rate = 8.364688e-07\n",
      "Epoch 18648/20000: Train Loss = 0.434121, Test Loss = 0.252067, Learning Rate = 8.361509e-07\n",
      "Epoch 18649/20000: Train Loss = 0.434109, Test Loss = 0.252126, Learning Rate = 8.358332e-07\n",
      "Epoch 18650/20000: Train Loss = 0.434198, Test Loss = 0.251562, Learning Rate = 8.355156e-07\n",
      "Epoch 18651/20000: Train Loss = 0.434110, Test Loss = 0.252113, Learning Rate = 8.351982e-07\n",
      "Epoch 18652/20000: Train Loss = 0.434097, Test Loss = 0.252397, Learning Rate = 8.348808e-07\n",
      "Epoch 18653/20000: Train Loss = 0.434093, Test Loss = 0.251888, Learning Rate = 8.345636e-07\n",
      "Epoch 18654/20000: Train Loss = 0.434105, Test Loss = 0.252252, Learning Rate = 8.342465e-07\n",
      "Epoch 18655/20000: Train Loss = 0.434077, Test Loss = 0.252142, Learning Rate = 8.339295e-07\n",
      "Epoch 18656/20000: Train Loss = 0.434264, Test Loss = 0.252755, Learning Rate = 8.336126e-07\n",
      "Epoch 18657/20000: Train Loss = 0.434062, Test Loss = 0.252590, Learning Rate = 8.332959e-07\n",
      "Epoch 18658/20000: Train Loss = 0.434091, Test Loss = 0.253012, Learning Rate = 8.329792e-07\n",
      "Epoch 18659/20000: Train Loss = 0.434177, Test Loss = 0.253840, Learning Rate = 8.326627e-07\n",
      "Epoch 18660/20000: Train Loss = 0.434098, Test Loss = 0.252874, Learning Rate = 8.323463e-07\n",
      "Epoch 18661/20000: Train Loss = 0.434118, Test Loss = 0.252951, Learning Rate = 8.320301e-07\n",
      "Epoch 18662/20000: Train Loss = 0.434087, Test Loss = 0.252602, Learning Rate = 8.317139e-07\n",
      "Epoch 18663/20000: Train Loss = 0.434087, Test Loss = 0.253360, Learning Rate = 8.313979e-07\n",
      "Epoch 18664/20000: Train Loss = 0.434073, Test Loss = 0.253080, Learning Rate = 8.310820e-07\n",
      "Epoch 18665/20000: Train Loss = 0.434223, Test Loss = 0.253069, Learning Rate = 8.307662e-07\n",
      "Epoch 18666/20000: Train Loss = 0.434155, Test Loss = 0.252409, Learning Rate = 8.304505e-07\n",
      "Epoch 18667/20000: Train Loss = 0.434064, Test Loss = 0.252717, Learning Rate = 8.301350e-07\n",
      "Epoch 18668/20000: Train Loss = 0.434192, Test Loss = 0.252948, Learning Rate = 8.298195e-07\n",
      "Epoch 18669/20000: Train Loss = 0.434374, Test Loss = 0.253152, Learning Rate = 8.295042e-07\n",
      "Epoch 18670/20000: Train Loss = 0.434242, Test Loss = 0.253521, Learning Rate = 8.291890e-07\n",
      "Epoch 18671/20000: Train Loss = 0.434118, Test Loss = 0.253542, Learning Rate = 8.288740e-07\n",
      "Epoch 18672/20000: Train Loss = 0.434087, Test Loss = 0.253552, Learning Rate = 8.285590e-07\n",
      "Epoch 18673/20000: Train Loss = 0.434072, Test Loss = 0.253526, Learning Rate = 8.282442e-07\n",
      "Epoch 18674/20000: Train Loss = 0.434049, Test Loss = 0.253827, Learning Rate = 8.279295e-07\n",
      "Epoch 18675/20000: Train Loss = 0.434072, Test Loss = 0.253914, Learning Rate = 8.276149e-07\n",
      "Epoch 18676/20000: Train Loss = 0.434044, Test Loss = 0.253164, Learning Rate = 8.273004e-07\n",
      "Epoch 18677/20000: Train Loss = 0.434111, Test Loss = 0.252636, Learning Rate = 8.269861e-07\n",
      "Epoch 18678/20000: Train Loss = 0.434138, Test Loss = 0.253255, Learning Rate = 8.266718e-07\n",
      "Epoch 18679/20000: Train Loss = 0.434080, Test Loss = 0.252581, Learning Rate = 8.263577e-07\n",
      "Epoch 18680/20000: Train Loss = 0.434099, Test Loss = 0.252941, Learning Rate = 8.260437e-07\n",
      "Epoch 18681/20000: Train Loss = 0.434084, Test Loss = 0.252845, Learning Rate = 8.257298e-07\n",
      "Epoch 18682/20000: Train Loss = 0.434067, Test Loss = 0.253509, Learning Rate = 8.254161e-07\n",
      "Epoch 18683/20000: Train Loss = 0.434186, Test Loss = 0.252951, Learning Rate = 8.251025e-07\n",
      "Epoch 18684/20000: Train Loss = 0.434150, Test Loss = 0.253449, Learning Rate = 8.247889e-07\n",
      "Epoch 18685/20000: Train Loss = 0.434158, Test Loss = 0.253164, Learning Rate = 8.244755e-07\n",
      "Epoch 18686/20000: Train Loss = 0.434056, Test Loss = 0.253564, Learning Rate = 8.241623e-07\n",
      "Epoch 18687/20000: Train Loss = 0.434121, Test Loss = 0.253372, Learning Rate = 8.238491e-07\n",
      "Epoch 18688/20000: Train Loss = 0.434068, Test Loss = 0.253034, Learning Rate = 8.235361e-07\n",
      "Epoch 18689/20000: Train Loss = 0.434042, Test Loss = 0.253115, Learning Rate = 8.232231e-07\n",
      "Epoch 18690/20000: Train Loss = 0.434110, Test Loss = 0.253073, Learning Rate = 8.229103e-07\n",
      "Epoch 18691/20000: Train Loss = 0.434109, Test Loss = 0.252822, Learning Rate = 8.225977e-07\n",
      "Epoch 18692/20000: Train Loss = 0.434119, Test Loss = 0.252793, Learning Rate = 8.222851e-07\n",
      "Epoch 18693/20000: Train Loss = 0.434132, Test Loss = 0.252627, Learning Rate = 8.219726e-07\n",
      "Epoch 18694/20000: Train Loss = 0.434052, Test Loss = 0.252247, Learning Rate = 8.216603e-07\n",
      "Epoch 18695/20000: Train Loss = 0.434117, Test Loss = 0.252642, Learning Rate = 8.213481e-07\n",
      "Epoch 18696/20000: Train Loss = 0.434167, Test Loss = 0.252737, Learning Rate = 8.210360e-07\n",
      "Epoch 18697/20000: Train Loss = 0.434148, Test Loss = 0.252395, Learning Rate = 8.207241e-07\n",
      "Epoch 18698/20000: Train Loss = 0.434141, Test Loss = 0.252203, Learning Rate = 8.204122e-07\n",
      "Epoch 18699/20000: Train Loss = 0.434131, Test Loss = 0.252105, Learning Rate = 8.201005e-07\n",
      "Epoch 18700/20000: Train Loss = 0.434151, Test Loss = 0.252003, Learning Rate = 8.197888e-07\n",
      "Epoch 18701/20000: Train Loss = 0.434051, Test Loss = 0.252250, Learning Rate = 8.194773e-07\n",
      "Epoch 18702/20000: Train Loss = 0.434133, Test Loss = 0.252601, Learning Rate = 8.191660e-07\n",
      "Epoch 18703/20000: Train Loss = 0.434115, Test Loss = 0.251937, Learning Rate = 8.188547e-07\n",
      "Epoch 18704/20000: Train Loss = 0.433969, Test Loss = 0.252621, Learning Rate = 8.185436e-07\n",
      "Epoch 18705/20000: Train Loss = 0.434164, Test Loss = 0.252889, Learning Rate = 8.182325e-07\n",
      "Epoch 18706/20000: Train Loss = 0.434105, Test Loss = 0.253440, Learning Rate = 8.179216e-07\n",
      "Epoch 18707/20000: Train Loss = 0.434095, Test Loss = 0.253089, Learning Rate = 8.176108e-07\n",
      "Epoch 18708/20000: Train Loss = 0.434144, Test Loss = 0.254009, Learning Rate = 8.173002e-07\n",
      "Epoch 18709/20000: Train Loss = 0.434127, Test Loss = 0.253227, Learning Rate = 8.169896e-07\n",
      "Epoch 18710/20000: Train Loss = 0.434079, Test Loss = 0.252644, Learning Rate = 8.166792e-07\n",
      "Epoch 18711/20000: Train Loss = 0.434156, Test Loss = 0.252828, Learning Rate = 8.163689e-07\n",
      "Epoch 18712/20000: Train Loss = 0.434292, Test Loss = 0.252787, Learning Rate = 8.160587e-07\n",
      "Epoch 18713/20000: Train Loss = 0.434129, Test Loss = 0.253272, Learning Rate = 8.157486e-07\n",
      "Epoch 18714/20000: Train Loss = 0.434056, Test Loss = 0.252934, Learning Rate = 8.154386e-07\n",
      "Epoch 18715/20000: Train Loss = 0.434136, Test Loss = 0.253591, Learning Rate = 8.151288e-07\n",
      "Epoch 18716/20000: Train Loss = 0.434177, Test Loss = 0.253564, Learning Rate = 8.148191e-07\n",
      "Epoch 18717/20000: Train Loss = 0.434069, Test Loss = 0.253382, Learning Rate = 8.145095e-07\n",
      "Epoch 18718/20000: Train Loss = 0.434090, Test Loss = 0.253462, Learning Rate = 8.142000e-07\n",
      "Epoch 18719/20000: Train Loss = 0.434128, Test Loss = 0.253708, Learning Rate = 8.138906e-07\n",
      "Epoch 18720/20000: Train Loss = 0.434090, Test Loss = 0.252737, Learning Rate = 8.135813e-07\n",
      "Epoch 18721/20000: Train Loss = 0.434122, Test Loss = 0.252904, Learning Rate = 8.132722e-07\n",
      "Epoch 18722/20000: Train Loss = 0.434139, Test Loss = 0.252505, Learning Rate = 8.129632e-07\n",
      "Epoch 18723/20000: Train Loss = 0.434100, Test Loss = 0.252689, Learning Rate = 8.126543e-07\n",
      "Epoch 18724/20000: Train Loss = 0.434138, Test Loss = 0.252771, Learning Rate = 8.123455e-07\n",
      "Epoch 18725/20000: Train Loss = 0.434215, Test Loss = 0.252823, Learning Rate = 8.120368e-07\n",
      "Epoch 18726/20000: Train Loss = 0.434132, Test Loss = 0.252349, Learning Rate = 8.117283e-07\n",
      "Epoch 18727/20000: Train Loss = 0.434061, Test Loss = 0.252226, Learning Rate = 8.114198e-07\n",
      "Epoch 18728/20000: Train Loss = 0.434436, Test Loss = 0.252838, Learning Rate = 8.111115e-07\n",
      "Epoch 18729/20000: Train Loss = 0.434111, Test Loss = 0.252776, Learning Rate = 8.108033e-07\n",
      "Epoch 18730/20000: Train Loss = 0.434125, Test Loss = 0.252634, Learning Rate = 8.104952e-07\n",
      "Epoch 18731/20000: Train Loss = 0.434144, Test Loss = 0.252819, Learning Rate = 8.101873e-07\n",
      "Epoch 18732/20000: Train Loss = 0.434165, Test Loss = 0.252772, Learning Rate = 8.098794e-07\n",
      "Epoch 18733/20000: Train Loss = 0.434327, Test Loss = 0.253507, Learning Rate = 8.095717e-07\n",
      "Epoch 18734/20000: Train Loss = 0.434146, Test Loss = 0.252698, Learning Rate = 8.092641e-07\n",
      "Epoch 18735/20000: Train Loss = 0.434046, Test Loss = 0.252793, Learning Rate = 8.089566e-07\n",
      "Epoch 18736/20000: Train Loss = 0.434078, Test Loss = 0.252845, Learning Rate = 8.086492e-07\n",
      "Epoch 18737/20000: Train Loss = 0.434176, Test Loss = 0.252878, Learning Rate = 8.083419e-07\n",
      "Epoch 18738/20000: Train Loss = 0.434125, Test Loss = 0.252360, Learning Rate = 8.080348e-07\n",
      "Epoch 18739/20000: Train Loss = 0.434049, Test Loss = 0.252554, Learning Rate = 8.077277e-07\n",
      "Epoch 18740/20000: Train Loss = 0.434096, Test Loss = 0.252685, Learning Rate = 8.074208e-07\n",
      "Epoch 18741/20000: Train Loss = 0.434093, Test Loss = 0.252654, Learning Rate = 8.071140e-07\n",
      "Epoch 18742/20000: Train Loss = 0.434184, Test Loss = 0.253245, Learning Rate = 8.068073e-07\n",
      "Epoch 18743/20000: Train Loss = 0.434187, Test Loss = 0.253568, Learning Rate = 8.065008e-07\n",
      "Epoch 18744/20000: Train Loss = 0.434137, Test Loss = 0.252518, Learning Rate = 8.061943e-07\n",
      "Epoch 18745/20000: Train Loss = 0.434202, Test Loss = 0.252715, Learning Rate = 8.058880e-07\n",
      "Epoch 18746/20000: Train Loss = 0.434196, Test Loss = 0.253299, Learning Rate = 8.055818e-07\n",
      "Epoch 18747/20000: Train Loss = 0.434102, Test Loss = 0.252463, Learning Rate = 8.052757e-07\n",
      "Epoch 18748/20000: Train Loss = 0.434083, Test Loss = 0.252259, Learning Rate = 8.049697e-07\n",
      "Epoch 18749/20000: Train Loss = 0.434066, Test Loss = 0.252266, Learning Rate = 8.046638e-07\n",
      "Epoch 18750/20000: Train Loss = 0.434235, Test Loss = 0.252107, Learning Rate = 8.043581e-07\n",
      "Epoch 18751/20000: Train Loss = 0.434084, Test Loss = 0.252683, Learning Rate = 8.040524e-07\n",
      "Epoch 18752/20000: Train Loss = 0.434079, Test Loss = 0.252181, Learning Rate = 8.037469e-07\n",
      "Epoch 18753/20000: Train Loss = 0.434141, Test Loss = 0.252541, Learning Rate = 8.034415e-07\n",
      "Epoch 18754/20000: Train Loss = 0.434062, Test Loss = 0.252389, Learning Rate = 8.031362e-07\n",
      "Epoch 18755/20000: Train Loss = 0.434150, Test Loss = 0.252966, Learning Rate = 8.028311e-07\n",
      "Epoch 18756/20000: Train Loss = 0.434068, Test Loss = 0.252524, Learning Rate = 8.025260e-07\n",
      "Epoch 18757/20000: Train Loss = 0.434047, Test Loss = 0.252525, Learning Rate = 8.022211e-07\n",
      "Epoch 18758/20000: Train Loss = 0.434072, Test Loss = 0.252482, Learning Rate = 8.019163e-07\n",
      "Epoch 18759/20000: Train Loss = 0.434080, Test Loss = 0.252379, Learning Rate = 8.016116e-07\n",
      "Epoch 18760/20000: Train Loss = 0.434017, Test Loss = 0.252139, Learning Rate = 8.013070e-07\n",
      "Epoch 18761/20000: Train Loss = 0.434096, Test Loss = 0.252407, Learning Rate = 8.010025e-07\n",
      "Epoch 18762/20000: Train Loss = 0.434120, Test Loss = 0.252270, Learning Rate = 8.006981e-07\n",
      "Epoch 18763/20000: Train Loss = 0.434090, Test Loss = 0.252300, Learning Rate = 8.003939e-07\n",
      "Epoch 18764/20000: Train Loss = 0.434083, Test Loss = 0.252336, Learning Rate = 8.000898e-07\n",
      "Epoch 18765/20000: Train Loss = 0.434077, Test Loss = 0.252567, Learning Rate = 7.997857e-07\n",
      "Epoch 18766/20000: Train Loss = 0.434163, Test Loss = 0.253303, Learning Rate = 7.994818e-07\n",
      "Epoch 18767/20000: Train Loss = 0.434079, Test Loss = 0.253215, Learning Rate = 7.991781e-07\n",
      "Epoch 18768/20000: Train Loss = 0.434114, Test Loss = 0.252828, Learning Rate = 7.988744e-07\n",
      "Epoch 18769/20000: Train Loss = 0.434091, Test Loss = 0.252828, Learning Rate = 7.985708e-07\n",
      "Epoch 18770/20000: Train Loss = 0.434057, Test Loss = 0.253226, Learning Rate = 7.982674e-07\n",
      "Epoch 18771/20000: Train Loss = 0.434029, Test Loss = 0.252599, Learning Rate = 7.979641e-07\n",
      "Epoch 18772/20000: Train Loss = 0.434120, Test Loss = 0.252157, Learning Rate = 7.976609e-07\n",
      "Epoch 18773/20000: Train Loss = 0.434148, Test Loss = 0.253436, Learning Rate = 7.973578e-07\n",
      "Epoch 18774/20000: Train Loss = 0.434215, Test Loss = 0.253567, Learning Rate = 7.970548e-07\n",
      "Epoch 18775/20000: Train Loss = 0.434107, Test Loss = 0.253574, Learning Rate = 7.967520e-07\n",
      "Epoch 18776/20000: Train Loss = 0.434068, Test Loss = 0.253667, Learning Rate = 7.964492e-07\n",
      "Epoch 18777/20000: Train Loss = 0.434080, Test Loss = 0.252911, Learning Rate = 7.961466e-07\n",
      "Epoch 18778/20000: Train Loss = 0.434095, Test Loss = 0.253195, Learning Rate = 7.958441e-07\n",
      "Epoch 18779/20000: Train Loss = 0.434094, Test Loss = 0.252930, Learning Rate = 7.955417e-07\n",
      "Epoch 18780/20000: Train Loss = 0.434063, Test Loss = 0.252960, Learning Rate = 7.952394e-07\n",
      "Epoch 18781/20000: Train Loss = 0.434121, Test Loss = 0.253097, Learning Rate = 7.949372e-07\n",
      "Epoch 18782/20000: Train Loss = 0.434057, Test Loss = 0.253679, Learning Rate = 7.946352e-07\n",
      "Epoch 18783/20000: Train Loss = 0.434070, Test Loss = 0.253173, Learning Rate = 7.943332e-07\n",
      "Epoch 18784/20000: Train Loss = 0.434056, Test Loss = 0.253334, Learning Rate = 7.940314e-07\n",
      "Epoch 18785/20000: Train Loss = 0.434079, Test Loss = 0.253702, Learning Rate = 7.937297e-07\n",
      "Epoch 18786/20000: Train Loss = 0.434121, Test Loss = 0.253390, Learning Rate = 7.934281e-07\n",
      "Epoch 18787/20000: Train Loss = 0.434051, Test Loss = 0.253575, Learning Rate = 7.931266e-07\n",
      "Epoch 18788/20000: Train Loss = 0.434187, Test Loss = 0.253657, Learning Rate = 7.928253e-07\n",
      "Epoch 18789/20000: Train Loss = 0.434158, Test Loss = 0.253981, Learning Rate = 7.925240e-07\n",
      "Epoch 18790/20000: Train Loss = 0.434072, Test Loss = 0.254639, Learning Rate = 7.922229e-07\n",
      "Epoch 18791/20000: Train Loss = 0.434100, Test Loss = 0.254238, Learning Rate = 7.919218e-07\n",
      "Epoch 18792/20000: Train Loss = 0.434111, Test Loss = 0.254395, Learning Rate = 7.916209e-07\n",
      "Epoch 18793/20000: Train Loss = 0.434133, Test Loss = 0.254378, Learning Rate = 7.913201e-07\n",
      "Epoch 18794/20000: Train Loss = 0.434236, Test Loss = 0.253925, Learning Rate = 7.910195e-07\n",
      "Epoch 18795/20000: Train Loss = 0.434040, Test Loss = 0.253084, Learning Rate = 7.907189e-07\n",
      "Epoch 18796/20000: Train Loss = 0.434172, Test Loss = 0.252857, Learning Rate = 7.904184e-07\n",
      "Epoch 18797/20000: Train Loss = 0.434016, Test Loss = 0.253545, Learning Rate = 7.901181e-07\n",
      "Epoch 18798/20000: Train Loss = 0.434130, Test Loss = 0.253982, Learning Rate = 7.898179e-07\n",
      "Epoch 18799/20000: Train Loss = 0.434053, Test Loss = 0.253266, Learning Rate = 7.895178e-07\n",
      "Epoch 18800/20000: Train Loss = 0.434212, Test Loss = 0.252677, Learning Rate = 7.892178e-07\n",
      "Epoch 18801/20000: Train Loss = 0.434327, Test Loss = 0.253596, Learning Rate = 7.889179e-07\n",
      "Epoch 18802/20000: Train Loss = 0.434083, Test Loss = 0.253614, Learning Rate = 7.886181e-07\n",
      "Epoch 18803/20000: Train Loss = 0.434385, Test Loss = 0.252985, Learning Rate = 7.883185e-07\n",
      "Epoch 18804/20000: Train Loss = 0.434290, Test Loss = 0.254335, Learning Rate = 7.880189e-07\n",
      "Epoch 18805/20000: Train Loss = 0.434238, Test Loss = 0.253290, Learning Rate = 7.877195e-07\n",
      "Epoch 18806/20000: Train Loss = 0.434099, Test Loss = 0.252640, Learning Rate = 7.874202e-07\n",
      "Epoch 18807/20000: Train Loss = 0.434197, Test Loss = 0.253699, Learning Rate = 7.871210e-07\n",
      "Epoch 18808/20000: Train Loss = 0.434098, Test Loss = 0.253017, Learning Rate = 7.868219e-07\n",
      "Epoch 18809/20000: Train Loss = 0.434112, Test Loss = 0.253268, Learning Rate = 7.865229e-07\n",
      "Epoch 18810/20000: Train Loss = 0.434193, Test Loss = 0.253174, Learning Rate = 7.862241e-07\n",
      "Epoch 18811/20000: Train Loss = 0.434008, Test Loss = 0.253393, Learning Rate = 7.859253e-07\n",
      "Epoch 18812/20000: Train Loss = 0.434130, Test Loss = 0.252972, Learning Rate = 7.856267e-07\n",
      "Epoch 18813/20000: Train Loss = 0.434050, Test Loss = 0.253000, Learning Rate = 7.853282e-07\n",
      "Epoch 18814/20000: Train Loss = 0.434033, Test Loss = 0.253006, Learning Rate = 7.850298e-07\n",
      "Epoch 18815/20000: Train Loss = 0.434069, Test Loss = 0.252347, Learning Rate = 7.847315e-07\n",
      "Epoch 18816/20000: Train Loss = 0.434096, Test Loss = 0.252971, Learning Rate = 7.844333e-07\n",
      "Epoch 18817/20000: Train Loss = 0.434040, Test Loss = 0.252227, Learning Rate = 7.841353e-07\n",
      "Epoch 18818/20000: Train Loss = 0.434096, Test Loss = 0.252791, Learning Rate = 7.838373e-07\n",
      "Epoch 18819/20000: Train Loss = 0.434068, Test Loss = 0.252793, Learning Rate = 7.835395e-07\n",
      "Epoch 18820/20000: Train Loss = 0.434300, Test Loss = 0.253402, Learning Rate = 7.832417e-07\n",
      "Epoch 18821/20000: Train Loss = 0.434081, Test Loss = 0.253398, Learning Rate = 7.829441e-07\n",
      "Epoch 18822/20000: Train Loss = 0.434081, Test Loss = 0.252979, Learning Rate = 7.826466e-07\n",
      "Epoch 18823/20000: Train Loss = 0.434066, Test Loss = 0.253045, Learning Rate = 7.823493e-07\n",
      "Epoch 18824/20000: Train Loss = 0.434180, Test Loss = 0.252926, Learning Rate = 7.820520e-07\n",
      "Epoch 18825/20000: Train Loss = 0.434032, Test Loss = 0.253567, Learning Rate = 7.817548e-07\n",
      "Epoch 18826/20000: Train Loss = 0.434125, Test Loss = 0.253168, Learning Rate = 7.814578e-07\n",
      "Epoch 18827/20000: Train Loss = 0.434265, Test Loss = 0.252634, Learning Rate = 7.811608e-07\n",
      "Epoch 18828/20000: Train Loss = 0.434063, Test Loss = 0.252356, Learning Rate = 7.808640e-07\n",
      "Epoch 18829/20000: Train Loss = 0.434241, Test Loss = 0.252164, Learning Rate = 7.805673e-07\n",
      "Epoch 18830/20000: Train Loss = 0.434113, Test Loss = 0.252762, Learning Rate = 7.802707e-07\n",
      "Epoch 18831/20000: Train Loss = 0.434080, Test Loss = 0.252248, Learning Rate = 7.799742e-07\n",
      "Epoch 18832/20000: Train Loss = 0.434068, Test Loss = 0.252746, Learning Rate = 7.796779e-07\n",
      "Epoch 18833/20000: Train Loss = 0.434056, Test Loss = 0.252529, Learning Rate = 7.793816e-07\n",
      "Epoch 18834/20000: Train Loss = 0.434104, Test Loss = 0.253143, Learning Rate = 7.790855e-07\n",
      "Epoch 18835/20000: Train Loss = 0.434066, Test Loss = 0.252807, Learning Rate = 7.787894e-07\n",
      "Epoch 18836/20000: Train Loss = 0.434172, Test Loss = 0.252154, Learning Rate = 7.784935e-07\n",
      "Epoch 18837/20000: Train Loss = 0.434065, Test Loss = 0.252275, Learning Rate = 7.781977e-07\n",
      "Epoch 18838/20000: Train Loss = 0.434076, Test Loss = 0.252128, Learning Rate = 7.779020e-07\n",
      "Epoch 18839/20000: Train Loss = 0.434121, Test Loss = 0.253082, Learning Rate = 7.776064e-07\n",
      "Epoch 18840/20000: Train Loss = 0.434083, Test Loss = 0.253198, Learning Rate = 7.773110e-07\n",
      "Epoch 18841/20000: Train Loss = 0.434270, Test Loss = 0.252709, Learning Rate = 7.770156e-07\n",
      "Epoch 18842/20000: Train Loss = 0.434160, Test Loss = 0.252916, Learning Rate = 7.767204e-07\n",
      "Epoch 18843/20000: Train Loss = 0.434078, Test Loss = 0.253125, Learning Rate = 7.764252e-07\n",
      "Epoch 18844/20000: Train Loss = 0.434567, Test Loss = 0.252506, Learning Rate = 7.761302e-07\n",
      "Epoch 18845/20000: Train Loss = 0.434293, Test Loss = 0.253505, Learning Rate = 7.758353e-07\n",
      "Epoch 18846/20000: Train Loss = 0.434195, Test Loss = 0.252843, Learning Rate = 7.755405e-07\n",
      "Epoch 18847/20000: Train Loss = 0.434026, Test Loss = 0.252426, Learning Rate = 7.752458e-07\n",
      "Epoch 18848/20000: Train Loss = 0.434083, Test Loss = 0.252657, Learning Rate = 7.749513e-07\n",
      "Epoch 18849/20000: Train Loss = 0.434036, Test Loss = 0.252441, Learning Rate = 7.746568e-07\n",
      "Epoch 18850/20000: Train Loss = 0.434174, Test Loss = 0.251472, Learning Rate = 7.743624e-07\n",
      "Epoch 18851/20000: Train Loss = 0.434022, Test Loss = 0.252079, Learning Rate = 7.740682e-07\n",
      "Epoch 18852/20000: Train Loss = 0.434059, Test Loss = 0.252336, Learning Rate = 7.737741e-07\n",
      "Epoch 18853/20000: Train Loss = 0.434007, Test Loss = 0.252286, Learning Rate = 7.734801e-07\n",
      "Epoch 18854/20000: Train Loss = 0.434095, Test Loss = 0.252442, Learning Rate = 7.731862e-07\n",
      "Epoch 18855/20000: Train Loss = 0.434062, Test Loss = 0.252380, Learning Rate = 7.728924e-07\n",
      "Epoch 18856/20000: Train Loss = 0.434040, Test Loss = 0.252257, Learning Rate = 7.725987e-07\n",
      "Epoch 18857/20000: Train Loss = 0.434134, Test Loss = 0.252250, Learning Rate = 7.723051e-07\n",
      "Epoch 18858/20000: Train Loss = 0.434104, Test Loss = 0.252275, Learning Rate = 7.720117e-07\n",
      "Epoch 18859/20000: Train Loss = 0.434105, Test Loss = 0.252050, Learning Rate = 7.717183e-07\n",
      "Epoch 18860/20000: Train Loss = 0.434067, Test Loss = 0.251996, Learning Rate = 7.714251e-07\n",
      "Epoch 18861/20000: Train Loss = 0.434065, Test Loss = 0.252261, Learning Rate = 7.711320e-07\n",
      "Epoch 18862/20000: Train Loss = 0.434083, Test Loss = 0.251893, Learning Rate = 7.708390e-07\n",
      "Epoch 18863/20000: Train Loss = 0.434026, Test Loss = 0.251989, Learning Rate = 7.705461e-07\n",
      "Epoch 18864/20000: Train Loss = 0.434139, Test Loss = 0.251445, Learning Rate = 7.702533e-07\n",
      "Epoch 18865/20000: Train Loss = 0.434141, Test Loss = 0.251549, Learning Rate = 7.699606e-07\n",
      "Epoch 18866/20000: Train Loss = 0.434041, Test Loss = 0.251276, Learning Rate = 7.696680e-07\n",
      "Epoch 18867/20000: Train Loss = 0.434111, Test Loss = 0.251918, Learning Rate = 7.693756e-07\n",
      "Epoch 18868/20000: Train Loss = 0.434170, Test Loss = 0.251998, Learning Rate = 7.690833e-07\n",
      "Epoch 18869/20000: Train Loss = 0.434355, Test Loss = 0.251695, Learning Rate = 7.687910e-07\n",
      "Epoch 18870/20000: Train Loss = 0.434035, Test Loss = 0.252022, Learning Rate = 7.684989e-07\n",
      "Epoch 18871/20000: Train Loss = 0.434120, Test Loss = 0.251162, Learning Rate = 7.682069e-07\n",
      "Epoch 18872/20000: Train Loss = 0.434020, Test Loss = 0.250792, Learning Rate = 7.679150e-07\n",
      "Epoch 18873/20000: Train Loss = 0.434078, Test Loss = 0.251309, Learning Rate = 7.676232e-07\n",
      "Epoch 18874/20000: Train Loss = 0.434047, Test Loss = 0.250940, Learning Rate = 7.673315e-07\n",
      "Epoch 18875/20000: Train Loss = 0.434134, Test Loss = 0.251213, Learning Rate = 7.670400e-07\n",
      "Epoch 18876/20000: Train Loss = 0.434040, Test Loss = 0.251133, Learning Rate = 7.667485e-07\n",
      "Epoch 18877/20000: Train Loss = 0.434023, Test Loss = 0.251394, Learning Rate = 7.664572e-07\n",
      "Epoch 18878/20000: Train Loss = 0.434157, Test Loss = 0.251162, Learning Rate = 7.661659e-07\n",
      "Epoch 18879/20000: Train Loss = 0.434049, Test Loss = 0.251064, Learning Rate = 7.658748e-07\n",
      "Epoch 18880/20000: Train Loss = 0.434106, Test Loss = 0.250790, Learning Rate = 7.655838e-07\n",
      "Epoch 18881/20000: Train Loss = 0.434120, Test Loss = 0.250237, Learning Rate = 7.652929e-07\n",
      "Epoch 18882/20000: Train Loss = 0.434040, Test Loss = 0.250845, Learning Rate = 7.650021e-07\n",
      "Epoch 18883/20000: Train Loss = 0.434283, Test Loss = 0.251052, Learning Rate = 7.647114e-07\n",
      "Epoch 18884/20000: Train Loss = 0.434088, Test Loss = 0.251553, Learning Rate = 7.644209e-07\n",
      "Epoch 18885/20000: Train Loss = 0.434041, Test Loss = 0.250920, Learning Rate = 7.641304e-07\n",
      "Epoch 18886/20000: Train Loss = 0.434115, Test Loss = 0.250787, Learning Rate = 7.638401e-07\n",
      "Epoch 18887/20000: Train Loss = 0.434145, Test Loss = 0.250954, Learning Rate = 7.635498e-07\n",
      "Epoch 18888/20000: Train Loss = 0.434089, Test Loss = 0.250931, Learning Rate = 7.632597e-07\n",
      "Epoch 18889/20000: Train Loss = 0.434189, Test Loss = 0.250960, Learning Rate = 7.629697e-07\n",
      "Epoch 18890/20000: Train Loss = 0.434051, Test Loss = 0.250644, Learning Rate = 7.626798e-07\n",
      "Epoch 18891/20000: Train Loss = 0.434051, Test Loss = 0.251057, Learning Rate = 7.623900e-07\n",
      "Epoch 18892/20000: Train Loss = 0.434090, Test Loss = 0.251054, Learning Rate = 7.621003e-07\n",
      "Epoch 18893/20000: Train Loss = 0.434104, Test Loss = 0.251455, Learning Rate = 7.618107e-07\n",
      "Epoch 18894/20000: Train Loss = 0.434038, Test Loss = 0.250947, Learning Rate = 7.615212e-07\n",
      "Epoch 18895/20000: Train Loss = 0.434122, Test Loss = 0.251362, Learning Rate = 7.612319e-07\n",
      "Epoch 18896/20000: Train Loss = 0.434191, Test Loss = 0.252074, Learning Rate = 7.609426e-07\n",
      "Epoch 18897/20000: Train Loss = 0.434112, Test Loss = 0.252008, Learning Rate = 7.606535e-07\n",
      "Epoch 18898/20000: Train Loss = 0.434057, Test Loss = 0.251555, Learning Rate = 7.603645e-07\n",
      "Epoch 18899/20000: Train Loss = 0.434189, Test Loss = 0.251573, Learning Rate = 7.600755e-07\n",
      "Epoch 18900/20000: Train Loss = 0.434151, Test Loss = 0.251785, Learning Rate = 7.597867e-07\n",
      "Epoch 18901/20000: Train Loss = 0.434124, Test Loss = 0.252123, Learning Rate = 7.594980e-07\n",
      "Epoch 18902/20000: Train Loss = 0.434053, Test Loss = 0.251870, Learning Rate = 7.592094e-07\n",
      "Epoch 18903/20000: Train Loss = 0.434094, Test Loss = 0.251417, Learning Rate = 7.589210e-07\n",
      "Epoch 18904/20000: Train Loss = 0.434162, Test Loss = 0.251995, Learning Rate = 7.586326e-07\n",
      "Epoch 18905/20000: Train Loss = 0.434094, Test Loss = 0.253050, Learning Rate = 7.583443e-07\n",
      "Epoch 18906/20000: Train Loss = 0.434037, Test Loss = 0.252912, Learning Rate = 7.580562e-07\n",
      "Epoch 18907/20000: Train Loss = 0.434399, Test Loss = 0.252449, Learning Rate = 7.577681e-07\n",
      "Epoch 18908/20000: Train Loss = 0.434097, Test Loss = 0.253482, Learning Rate = 7.574802e-07\n",
      "Epoch 18909/20000: Train Loss = 0.434073, Test Loss = 0.253625, Learning Rate = 7.571924e-07\n",
      "Epoch 18910/20000: Train Loss = 0.434142, Test Loss = 0.253323, Learning Rate = 7.569047e-07\n",
      "Epoch 18911/20000: Train Loss = 0.434108, Test Loss = 0.254073, Learning Rate = 7.566171e-07\n",
      "Epoch 18912/20000: Train Loss = 0.434091, Test Loss = 0.253507, Learning Rate = 7.563296e-07\n",
      "Epoch 18913/20000: Train Loss = 0.434067, Test Loss = 0.253841, Learning Rate = 7.560422e-07\n",
      "Epoch 18914/20000: Train Loss = 0.434016, Test Loss = 0.253433, Learning Rate = 7.557549e-07\n",
      "Epoch 18915/20000: Train Loss = 0.434476, Test Loss = 0.253074, Learning Rate = 7.554678e-07\n",
      "Epoch 18916/20000: Train Loss = 0.434056, Test Loss = 0.253538, Learning Rate = 7.551807e-07\n",
      "Epoch 18917/20000: Train Loss = 0.434066, Test Loss = 0.253808, Learning Rate = 7.548938e-07\n",
      "Epoch 18918/20000: Train Loss = 0.434066, Test Loss = 0.253086, Learning Rate = 7.546069e-07\n",
      "Epoch 18919/20000: Train Loss = 0.434029, Test Loss = 0.253709, Learning Rate = 7.543202e-07\n",
      "Epoch 18920/20000: Train Loss = 0.434036, Test Loss = 0.253651, Learning Rate = 7.540336e-07\n",
      "Epoch 18921/20000: Train Loss = 0.434119, Test Loss = 0.253825, Learning Rate = 7.537470e-07\n",
      "Epoch 18922/20000: Train Loss = 0.434205, Test Loss = 0.253833, Learning Rate = 7.534606e-07\n",
      "Epoch 18923/20000: Train Loss = 0.434034, Test Loss = 0.253841, Learning Rate = 7.531743e-07\n",
      "Epoch 18924/20000: Train Loss = 0.434099, Test Loss = 0.253210, Learning Rate = 7.528882e-07\n",
      "Epoch 18925/20000: Train Loss = 0.434072, Test Loss = 0.253370, Learning Rate = 7.526021e-07\n",
      "Epoch 18926/20000: Train Loss = 0.434574, Test Loss = 0.254052, Learning Rate = 7.523161e-07\n",
      "Epoch 18927/20000: Train Loss = 0.434106, Test Loss = 0.253642, Learning Rate = 7.520303e-07\n",
      "Epoch 18928/20000: Train Loss = 0.434107, Test Loss = 0.253900, Learning Rate = 7.517445e-07\n",
      "Epoch 18929/20000: Train Loss = 0.434089, Test Loss = 0.254262, Learning Rate = 7.514589e-07\n",
      "Epoch 18930/20000: Train Loss = 0.434078, Test Loss = 0.253836, Learning Rate = 7.511733e-07\n",
      "Epoch 18931/20000: Train Loss = 0.434061, Test Loss = 0.253934, Learning Rate = 7.508879e-07\n",
      "Epoch 18932/20000: Train Loss = 0.434184, Test Loss = 0.254707, Learning Rate = 7.506026e-07\n",
      "Epoch 18933/20000: Train Loss = 0.434070, Test Loss = 0.254276, Learning Rate = 7.503174e-07\n",
      "Epoch 18934/20000: Train Loss = 0.434133, Test Loss = 0.253902, Learning Rate = 7.500323e-07\n",
      "Epoch 18935/20000: Train Loss = 0.434058, Test Loss = 0.254010, Learning Rate = 7.497473e-07\n",
      "Epoch 18936/20000: Train Loss = 0.434030, Test Loss = 0.254405, Learning Rate = 7.494624e-07\n",
      "Epoch 18937/20000: Train Loss = 0.434142, Test Loss = 0.254673, Learning Rate = 7.491776e-07\n",
      "Epoch 18938/20000: Train Loss = 0.434092, Test Loss = 0.253543, Learning Rate = 7.488930e-07\n",
      "Epoch 18939/20000: Train Loss = 0.434051, Test Loss = 0.253597, Learning Rate = 7.486084e-07\n",
      "Epoch 18940/20000: Train Loss = 0.434075, Test Loss = 0.253352, Learning Rate = 7.483240e-07\n",
      "Epoch 18941/20000: Train Loss = 0.434231, Test Loss = 0.253139, Learning Rate = 7.480396e-07\n",
      "Epoch 18942/20000: Train Loss = 0.434336, Test Loss = 0.253308, Learning Rate = 7.477554e-07\n",
      "Epoch 18943/20000: Train Loss = 0.434019, Test Loss = 0.253608, Learning Rate = 7.474712e-07\n",
      "Epoch 18944/20000: Train Loss = 0.434077, Test Loss = 0.254211, Learning Rate = 7.471872e-07\n",
      "Epoch 18945/20000: Train Loss = 0.434101, Test Loss = 0.253864, Learning Rate = 7.469033e-07\n",
      "Epoch 18946/20000: Train Loss = 0.434084, Test Loss = 0.253549, Learning Rate = 7.466195e-07\n",
      "Epoch 18947/20000: Train Loss = 0.434116, Test Loss = 0.253722, Learning Rate = 7.463358e-07\n",
      "Epoch 18948/20000: Train Loss = 0.434653, Test Loss = 0.253194, Learning Rate = 7.460522e-07\n",
      "Epoch 18949/20000: Train Loss = 0.434126, Test Loss = 0.253945, Learning Rate = 7.457688e-07\n",
      "Epoch 18950/20000: Train Loss = 0.434148, Test Loss = 0.254384, Learning Rate = 7.454854e-07\n",
      "Epoch 18951/20000: Train Loss = 0.434115, Test Loss = 0.253920, Learning Rate = 7.452021e-07\n",
      "Epoch 18952/20000: Train Loss = 0.434155, Test Loss = 0.253147, Learning Rate = 7.449190e-07\n",
      "Epoch 18953/20000: Train Loss = 0.434106, Test Loss = 0.253280, Learning Rate = 7.446359e-07\n",
      "Epoch 18954/20000: Train Loss = 0.434019, Test Loss = 0.253116, Learning Rate = 7.443530e-07\n",
      "Epoch 18955/20000: Train Loss = 0.434069, Test Loss = 0.253244, Learning Rate = 7.440701e-07\n",
      "Epoch 18956/20000: Train Loss = 0.434130, Test Loss = 0.253142, Learning Rate = 7.437874e-07\n",
      "Epoch 18957/20000: Train Loss = 0.434273, Test Loss = 0.252836, Learning Rate = 7.435048e-07\n",
      "Epoch 18958/20000: Train Loss = 0.434044, Test Loss = 0.253056, Learning Rate = 7.432223e-07\n",
      "Epoch 18959/20000: Train Loss = 0.434062, Test Loss = 0.253254, Learning Rate = 7.429399e-07\n",
      "Epoch 18960/20000: Train Loss = 0.434034, Test Loss = 0.253179, Learning Rate = 7.426576e-07\n",
      "Epoch 18961/20000: Train Loss = 0.434029, Test Loss = 0.253337, Learning Rate = 7.423754e-07\n",
      "Epoch 18962/20000: Train Loss = 0.434082, Test Loss = 0.253205, Learning Rate = 7.420933e-07\n",
      "Epoch 18963/20000: Train Loss = 0.434147, Test Loss = 0.253485, Learning Rate = 7.418113e-07\n",
      "Epoch 18964/20000: Train Loss = 0.434034, Test Loss = 0.253448, Learning Rate = 7.415295e-07\n",
      "Epoch 18965/20000: Train Loss = 0.434038, Test Loss = 0.253537, Learning Rate = 7.412477e-07\n",
      "Epoch 18966/20000: Train Loss = 0.434134, Test Loss = 0.253504, Learning Rate = 7.409660e-07\n",
      "Epoch 18967/20000: Train Loss = 0.434039, Test Loss = 0.253064, Learning Rate = 7.406845e-07\n",
      "Epoch 18968/20000: Train Loss = 0.434056, Test Loss = 0.253499, Learning Rate = 7.404031e-07\n",
      "Epoch 18969/20000: Train Loss = 0.434079, Test Loss = 0.253108, Learning Rate = 7.401217e-07\n",
      "Epoch 18970/20000: Train Loss = 0.434031, Test Loss = 0.253596, Learning Rate = 7.398405e-07\n",
      "Epoch 18971/20000: Train Loss = 0.434127, Test Loss = 0.254175, Learning Rate = 7.395594e-07\n",
      "Epoch 18972/20000: Train Loss = 0.434073, Test Loss = 0.253756, Learning Rate = 7.392784e-07\n",
      "Epoch 18973/20000: Train Loss = 0.434179, Test Loss = 0.254173, Learning Rate = 7.389975e-07\n",
      "Epoch 18974/20000: Train Loss = 0.434062, Test Loss = 0.253561, Learning Rate = 7.387167e-07\n",
      "Epoch 18975/20000: Train Loss = 0.434122, Test Loss = 0.253498, Learning Rate = 7.384360e-07\n",
      "Epoch 18976/20000: Train Loss = 0.434119, Test Loss = 0.253237, Learning Rate = 7.381554e-07\n",
      "Epoch 18977/20000: Train Loss = 0.434037, Test Loss = 0.253076, Learning Rate = 7.378749e-07\n",
      "Epoch 18978/20000: Train Loss = 0.434037, Test Loss = 0.253427, Learning Rate = 7.375945e-07\n",
      "Epoch 18979/20000: Train Loss = 0.434058, Test Loss = 0.253340, Learning Rate = 7.373143e-07\n",
      "Epoch 18980/20000: Train Loss = 0.433999, Test Loss = 0.254176, Learning Rate = 7.370341e-07\n",
      "Epoch 18981/20000: Train Loss = 0.434062, Test Loss = 0.253660, Learning Rate = 7.367541e-07\n",
      "Epoch 18982/20000: Train Loss = 0.434095, Test Loss = 0.254025, Learning Rate = 7.364741e-07\n",
      "Epoch 18983/20000: Train Loss = 0.434110, Test Loss = 0.253820, Learning Rate = 7.361943e-07\n",
      "Epoch 18984/20000: Train Loss = 0.434121, Test Loss = 0.253583, Learning Rate = 7.359145e-07\n",
      "Epoch 18985/20000: Train Loss = 0.434049, Test Loss = 0.253666, Learning Rate = 7.356349e-07\n",
      "Epoch 18986/20000: Train Loss = 0.434091, Test Loss = 0.253264, Learning Rate = 7.353554e-07\n",
      "Epoch 18987/20000: Train Loss = 0.434122, Test Loss = 0.253769, Learning Rate = 7.350760e-07\n",
      "Epoch 18988/20000: Train Loss = 0.434113, Test Loss = 0.253316, Learning Rate = 7.347967e-07\n",
      "Epoch 18989/20000: Train Loss = 0.434142, Test Loss = 0.253495, Learning Rate = 7.345175e-07\n",
      "Epoch 18990/20000: Train Loss = 0.434155, Test Loss = 0.253602, Learning Rate = 7.342384e-07\n",
      "Epoch 18991/20000: Train Loss = 0.434101, Test Loss = 0.253811, Learning Rate = 7.339594e-07\n",
      "Epoch 18992/20000: Train Loss = 0.434101, Test Loss = 0.253558, Learning Rate = 7.336805e-07\n",
      "Epoch 18993/20000: Train Loss = 0.434056, Test Loss = 0.253303, Learning Rate = 7.334017e-07\n",
      "Epoch 18994/20000: Train Loss = 0.434026, Test Loss = 0.253425, Learning Rate = 7.331230e-07\n",
      "Epoch 18995/20000: Train Loss = 0.434007, Test Loss = 0.253531, Learning Rate = 7.328445e-07\n",
      "Epoch 18996/20000: Train Loss = 0.434136, Test Loss = 0.254009, Learning Rate = 7.325660e-07\n",
      "Epoch 18997/20000: Train Loss = 0.434049, Test Loss = 0.253067, Learning Rate = 7.322877e-07\n",
      "Epoch 18998/20000: Train Loss = 0.434055, Test Loss = 0.252542, Learning Rate = 7.320094e-07\n",
      "Epoch 18999/20000: Train Loss = 0.434108, Test Loss = 0.253026, Learning Rate = 7.317313e-07\n",
      "Epoch 19000/20000: Train Loss = 0.434046, Test Loss = 0.253054, Learning Rate = 7.314532e-07\n",
      "Epoch 19001/20000: Train Loss = 0.434052, Test Loss = 0.253074, Learning Rate = 7.311753e-07\n",
      "Epoch 19002/20000: Train Loss = 0.434088, Test Loss = 0.252836, Learning Rate = 7.308975e-07\n",
      "Epoch 19003/20000: Train Loss = 0.434002, Test Loss = 0.253157, Learning Rate = 7.306197e-07\n",
      "Epoch 19004/20000: Train Loss = 0.434098, Test Loss = 0.252769, Learning Rate = 7.303421e-07\n",
      "Epoch 19005/20000: Train Loss = 0.434035, Test Loss = 0.252914, Learning Rate = 7.300646e-07\n",
      "Epoch 19006/20000: Train Loss = 0.434171, Test Loss = 0.252429, Learning Rate = 7.297872e-07\n",
      "Epoch 19007/20000: Train Loss = 0.433984, Test Loss = 0.252372, Learning Rate = 7.295099e-07\n",
      "Epoch 19008/20000: Train Loss = 0.434122, Test Loss = 0.252520, Learning Rate = 7.292327e-07\n",
      "Epoch 19009/20000: Train Loss = 0.433994, Test Loss = 0.252274, Learning Rate = 7.289556e-07\n",
      "Epoch 19010/20000: Train Loss = 0.434161, Test Loss = 0.251914, Learning Rate = 7.286786e-07\n",
      "Epoch 19011/20000: Train Loss = 0.434077, Test Loss = 0.252411, Learning Rate = 7.284018e-07\n",
      "Epoch 19012/20000: Train Loss = 0.434021, Test Loss = 0.252617, Learning Rate = 7.281250e-07\n",
      "Epoch 19013/20000: Train Loss = 0.434143, Test Loss = 0.252632, Learning Rate = 7.278483e-07\n",
      "Epoch 19014/20000: Train Loss = 0.434068, Test Loss = 0.252280, Learning Rate = 7.275718e-07\n",
      "Epoch 19015/20000: Train Loss = 0.434070, Test Loss = 0.252645, Learning Rate = 7.272953e-07\n",
      "Epoch 19016/20000: Train Loss = 0.434094, Test Loss = 0.252934, Learning Rate = 7.270190e-07\n",
      "Epoch 19017/20000: Train Loss = 0.434043, Test Loss = 0.253182, Learning Rate = 7.267427e-07\n",
      "Epoch 19018/20000: Train Loss = 0.434014, Test Loss = 0.252925, Learning Rate = 7.264666e-07\n",
      "Epoch 19019/20000: Train Loss = 0.434195, Test Loss = 0.253062, Learning Rate = 7.261905e-07\n",
      "Epoch 19020/20000: Train Loss = 0.434053, Test Loss = 0.252660, Learning Rate = 7.259146e-07\n",
      "Epoch 19021/20000: Train Loss = 0.434102, Test Loss = 0.252412, Learning Rate = 7.256388e-07\n",
      "Epoch 19022/20000: Train Loss = 0.434015, Test Loss = 0.253163, Learning Rate = 7.253630e-07\n",
      "Epoch 19023/20000: Train Loss = 0.434229, Test Loss = 0.252851, Learning Rate = 7.250874e-07\n",
      "Epoch 19024/20000: Train Loss = 0.434041, Test Loss = 0.252663, Learning Rate = 7.248119e-07\n",
      "Epoch 19025/20000: Train Loss = 0.434072, Test Loss = 0.252673, Learning Rate = 7.245365e-07\n",
      "Epoch 19026/20000: Train Loss = 0.434242, Test Loss = 0.252515, Learning Rate = 7.242612e-07\n",
      "Epoch 19027/20000: Train Loss = 0.434108, Test Loss = 0.252173, Learning Rate = 7.239860e-07\n",
      "Epoch 19028/20000: Train Loss = 0.434039, Test Loss = 0.252600, Learning Rate = 7.237109e-07\n",
      "Epoch 19029/20000: Train Loss = 0.434125, Test Loss = 0.252522, Learning Rate = 7.234359e-07\n",
      "Epoch 19030/20000: Train Loss = 0.434137, Test Loss = 0.253016, Learning Rate = 7.231610e-07\n",
      "Epoch 19031/20000: Train Loss = 0.434157, Test Loss = 0.253161, Learning Rate = 7.228862e-07\n",
      "Epoch 19032/20000: Train Loss = 0.434122, Test Loss = 0.252889, Learning Rate = 7.226116e-07\n",
      "Epoch 19033/20000: Train Loss = 0.434109, Test Loss = 0.252932, Learning Rate = 7.223370e-07\n",
      "Epoch 19034/20000: Train Loss = 0.434026, Test Loss = 0.253530, Learning Rate = 7.220625e-07\n",
      "Epoch 19035/20000: Train Loss = 0.434124, Test Loss = 0.253246, Learning Rate = 7.217882e-07\n",
      "Epoch 19036/20000: Train Loss = 0.434121, Test Loss = 0.253314, Learning Rate = 7.215139e-07\n",
      "Epoch 19037/20000: Train Loss = 0.434093, Test Loss = 0.253404, Learning Rate = 7.212397e-07\n",
      "Epoch 19038/20000: Train Loss = 0.434057, Test Loss = 0.253038, Learning Rate = 7.209657e-07\n",
      "Epoch 19039/20000: Train Loss = 0.434080, Test Loss = 0.253845, Learning Rate = 7.206917e-07\n",
      "Epoch 19040/20000: Train Loss = 0.434082, Test Loss = 0.253175, Learning Rate = 7.204179e-07\n",
      "Epoch 19041/20000: Train Loss = 0.434047, Test Loss = 0.253801, Learning Rate = 7.201442e-07\n",
      "Epoch 19042/20000: Train Loss = 0.434110, Test Loss = 0.253811, Learning Rate = 7.198705e-07\n",
      "Epoch 19043/20000: Train Loss = 0.434074, Test Loss = 0.253220, Learning Rate = 7.195970e-07\n",
      "Epoch 19044/20000: Train Loss = 0.434069, Test Loss = 0.253086, Learning Rate = 7.193236e-07\n",
      "Epoch 19045/20000: Train Loss = 0.434065, Test Loss = 0.253050, Learning Rate = 7.190502e-07\n",
      "Epoch 19046/20000: Train Loss = 0.434001, Test Loss = 0.252982, Learning Rate = 7.187770e-07\n",
      "Epoch 19047/20000: Train Loss = 0.434059, Test Loss = 0.252964, Learning Rate = 7.185039e-07\n",
      "Epoch 19048/20000: Train Loss = 0.434053, Test Loss = 0.252653, Learning Rate = 7.182309e-07\n",
      "Epoch 19049/20000: Train Loss = 0.434015, Test Loss = 0.252952, Learning Rate = 7.179580e-07\n",
      "Epoch 19050/20000: Train Loss = 0.434052, Test Loss = 0.252689, Learning Rate = 7.176852e-07\n",
      "Epoch 19051/20000: Train Loss = 0.434050, Test Loss = 0.253147, Learning Rate = 7.174125e-07\n",
      "Epoch 19052/20000: Train Loss = 0.434037, Test Loss = 0.252733, Learning Rate = 7.171399e-07\n",
      "Epoch 19053/20000: Train Loss = 0.434049, Test Loss = 0.252810, Learning Rate = 7.168674e-07\n",
      "Epoch 19054/20000: Train Loss = 0.434144, Test Loss = 0.252740, Learning Rate = 7.165950e-07\n",
      "Epoch 19055/20000: Train Loss = 0.434031, Test Loss = 0.251971, Learning Rate = 7.163227e-07\n",
      "Epoch 19056/20000: Train Loss = 0.434110, Test Loss = 0.252063, Learning Rate = 7.160505e-07\n",
      "Epoch 19057/20000: Train Loss = 0.434043, Test Loss = 0.251974, Learning Rate = 7.157785e-07\n",
      "Epoch 19058/20000: Train Loss = 0.434078, Test Loss = 0.252373, Learning Rate = 7.155065e-07\n",
      "Epoch 19059/20000: Train Loss = 0.434042, Test Loss = 0.252217, Learning Rate = 7.152346e-07\n",
      "Epoch 19060/20000: Train Loss = 0.434004, Test Loss = 0.252959, Learning Rate = 7.149628e-07\n",
      "Epoch 19061/20000: Train Loss = 0.434083, Test Loss = 0.252956, Learning Rate = 7.146912e-07\n",
      "Epoch 19062/20000: Train Loss = 0.434104, Test Loss = 0.252819, Learning Rate = 7.144196e-07\n",
      "Epoch 19063/20000: Train Loss = 0.434059, Test Loss = 0.252885, Learning Rate = 7.141481e-07\n",
      "Epoch 19064/20000: Train Loss = 0.434020, Test Loss = 0.252269, Learning Rate = 7.138768e-07\n",
      "Epoch 19065/20000: Train Loss = 0.434243, Test Loss = 0.252191, Learning Rate = 7.136055e-07\n",
      "Epoch 19066/20000: Train Loss = 0.434067, Test Loss = 0.252114, Learning Rate = 7.133344e-07\n",
      "Epoch 19067/20000: Train Loss = 0.434043, Test Loss = 0.252204, Learning Rate = 7.130633e-07\n",
      "Epoch 19068/20000: Train Loss = 0.434022, Test Loss = 0.252492, Learning Rate = 7.127924e-07\n",
      "Epoch 19069/20000: Train Loss = 0.434050, Test Loss = 0.252182, Learning Rate = 7.125215e-07\n",
      "Epoch 19070/20000: Train Loss = 0.434080, Test Loss = 0.252402, Learning Rate = 7.122508e-07\n",
      "Epoch 19071/20000: Train Loss = 0.434082, Test Loss = 0.252351, Learning Rate = 7.119802e-07\n",
      "Epoch 19072/20000: Train Loss = 0.434037, Test Loss = 0.252773, Learning Rate = 7.117096e-07\n",
      "Epoch 19073/20000: Train Loss = 0.434053, Test Loss = 0.252929, Learning Rate = 7.114392e-07\n",
      "Epoch 19074/20000: Train Loss = 0.434054, Test Loss = 0.252919, Learning Rate = 7.111689e-07\n",
      "Epoch 19075/20000: Train Loss = 0.434108, Test Loss = 0.252728, Learning Rate = 7.108987e-07\n",
      "Epoch 19076/20000: Train Loss = 0.434131, Test Loss = 0.252581, Learning Rate = 7.106285e-07\n",
      "Epoch 19077/20000: Train Loss = 0.434078, Test Loss = 0.253290, Learning Rate = 7.103585e-07\n",
      "Epoch 19078/20000: Train Loss = 0.433957, Test Loss = 0.252816, Learning Rate = 7.100886e-07\n",
      "Epoch 19079/20000: Train Loss = 0.434112, Test Loss = 0.253455, Learning Rate = 7.098188e-07\n",
      "Epoch 19080/20000: Train Loss = 0.434070, Test Loss = 0.253310, Learning Rate = 7.095491e-07\n",
      "Epoch 19081/20000: Train Loss = 0.434009, Test Loss = 0.253503, Learning Rate = 7.092795e-07\n",
      "Epoch 19082/20000: Train Loss = 0.434084, Test Loss = 0.253557, Learning Rate = 7.090100e-07\n",
      "Epoch 19083/20000: Train Loss = 0.434018, Test Loss = 0.253831, Learning Rate = 7.087405e-07\n",
      "Epoch 19084/20000: Train Loss = 0.434170, Test Loss = 0.253051, Learning Rate = 7.084712e-07\n",
      "Epoch 19085/20000: Train Loss = 0.434124, Test Loss = 0.253438, Learning Rate = 7.082020e-07\n",
      "Epoch 19086/20000: Train Loss = 0.434163, Test Loss = 0.253668, Learning Rate = 7.079330e-07\n",
      "Epoch 19087/20000: Train Loss = 0.434131, Test Loss = 0.253319, Learning Rate = 7.076640e-07\n",
      "Epoch 19088/20000: Train Loss = 0.434063, Test Loss = 0.253677, Learning Rate = 7.073951e-07\n",
      "Epoch 19089/20000: Train Loss = 0.434045, Test Loss = 0.253545, Learning Rate = 7.071263e-07\n",
      "Epoch 19090/20000: Train Loss = 0.434034, Test Loss = 0.253328, Learning Rate = 7.068576e-07\n",
      "Epoch 19091/20000: Train Loss = 0.434148, Test Loss = 0.253511, Learning Rate = 7.065890e-07\n",
      "Epoch 19092/20000: Train Loss = 0.434013, Test Loss = 0.253603, Learning Rate = 7.063205e-07\n",
      "Epoch 19093/20000: Train Loss = 0.434160, Test Loss = 0.253039, Learning Rate = 7.060521e-07\n",
      "Epoch 19094/20000: Train Loss = 0.434194, Test Loss = 0.253493, Learning Rate = 7.057838e-07\n",
      "Epoch 19095/20000: Train Loss = 0.434051, Test Loss = 0.253337, Learning Rate = 7.055157e-07\n",
      "Epoch 19096/20000: Train Loss = 0.434015, Test Loss = 0.253001, Learning Rate = 7.052476e-07\n",
      "Epoch 19097/20000: Train Loss = 0.434060, Test Loss = 0.253209, Learning Rate = 7.049796e-07\n",
      "Epoch 19098/20000: Train Loss = 0.434139, Test Loss = 0.252841, Learning Rate = 7.047117e-07\n",
      "Epoch 19099/20000: Train Loss = 0.434098, Test Loss = 0.253083, Learning Rate = 7.044440e-07\n",
      "Epoch 19100/20000: Train Loss = 0.434124, Test Loss = 0.253292, Learning Rate = 7.041763e-07\n",
      "Epoch 19101/20000: Train Loss = 0.434027, Test Loss = 0.253309, Learning Rate = 7.039087e-07\n",
      "Epoch 19102/20000: Train Loss = 0.434032, Test Loss = 0.253160, Learning Rate = 7.036413e-07\n",
      "Epoch 19103/20000: Train Loss = 0.434045, Test Loss = 0.253131, Learning Rate = 7.033739e-07\n",
      "Epoch 19104/20000: Train Loss = 0.434058, Test Loss = 0.253357, Learning Rate = 7.031066e-07\n",
      "Epoch 19105/20000: Train Loss = 0.434007, Test Loss = 0.252704, Learning Rate = 7.028395e-07\n",
      "Epoch 19106/20000: Train Loss = 0.434078, Test Loss = 0.252853, Learning Rate = 7.025724e-07\n",
      "Epoch 19107/20000: Train Loss = 0.434025, Test Loss = 0.252779, Learning Rate = 7.023055e-07\n",
      "Epoch 19108/20000: Train Loss = 0.434118, Test Loss = 0.252538, Learning Rate = 7.020386e-07\n",
      "Epoch 19109/20000: Train Loss = 0.434066, Test Loss = 0.252801, Learning Rate = 7.017718e-07\n",
      "Epoch 19110/20000: Train Loss = 0.433991, Test Loss = 0.253410, Learning Rate = 7.015052e-07\n",
      "Epoch 19111/20000: Train Loss = 0.434055, Test Loss = 0.252780, Learning Rate = 7.012386e-07\n",
      "Epoch 19112/20000: Train Loss = 0.434068, Test Loss = 0.253179, Learning Rate = 7.009722e-07\n",
      "Epoch 19113/20000: Train Loss = 0.434044, Test Loss = 0.253539, Learning Rate = 7.007058e-07\n",
      "Epoch 19114/20000: Train Loss = 0.434164, Test Loss = 0.253956, Learning Rate = 7.004396e-07\n",
      "Epoch 19115/20000: Train Loss = 0.434101, Test Loss = 0.254150, Learning Rate = 7.001734e-07\n",
      "Epoch 19116/20000: Train Loss = 0.434016, Test Loss = 0.254104, Learning Rate = 6.999074e-07\n",
      "Epoch 19117/20000: Train Loss = 0.434000, Test Loss = 0.253631, Learning Rate = 6.996414e-07\n",
      "Epoch 19118/20000: Train Loss = 0.434012, Test Loss = 0.253450, Learning Rate = 6.993756e-07\n",
      "Epoch 19119/20000: Train Loss = 0.434102, Test Loss = 0.254188, Learning Rate = 6.991099e-07\n",
      "Epoch 19120/20000: Train Loss = 0.434078, Test Loss = 0.253782, Learning Rate = 6.988442e-07\n",
      "Epoch 19121/20000: Train Loss = 0.434038, Test Loss = 0.253635, Learning Rate = 6.985787e-07\n",
      "Epoch 19122/20000: Train Loss = 0.434155, Test Loss = 0.253498, Learning Rate = 6.983132e-07\n",
      "Epoch 19123/20000: Train Loss = 0.434064, Test Loss = 0.253488, Learning Rate = 6.980479e-07\n",
      "Epoch 19124/20000: Train Loss = 0.434001, Test Loss = 0.254016, Learning Rate = 6.977827e-07\n",
      "Epoch 19125/20000: Train Loss = 0.434086, Test Loss = 0.254393, Learning Rate = 6.975175e-07\n",
      "Epoch 19126/20000: Train Loss = 0.434014, Test Loss = 0.254039, Learning Rate = 6.972525e-07\n",
      "Epoch 19127/20000: Train Loss = 0.434113, Test Loss = 0.253987, Learning Rate = 6.969875e-07\n",
      "Epoch 19128/20000: Train Loss = 0.434146, Test Loss = 0.254499, Learning Rate = 6.967227e-07\n",
      "Epoch 19129/20000: Train Loss = 0.434108, Test Loss = 0.253868, Learning Rate = 6.964580e-07\n",
      "Epoch 19130/20000: Train Loss = 0.434042, Test Loss = 0.253753, Learning Rate = 6.961933e-07\n",
      "Epoch 19131/20000: Train Loss = 0.434076, Test Loss = 0.253625, Learning Rate = 6.959288e-07\n",
      "Epoch 19132/20000: Train Loss = 0.433993, Test Loss = 0.253679, Learning Rate = 6.956644e-07\n",
      "Epoch 19133/20000: Train Loss = 0.434033, Test Loss = 0.253401, Learning Rate = 6.954000e-07\n",
      "Epoch 19134/20000: Train Loss = 0.434144, Test Loss = 0.252681, Learning Rate = 6.951358e-07\n",
      "Epoch 19135/20000: Train Loss = 0.434288, Test Loss = 0.252954, Learning Rate = 6.948717e-07\n",
      "Epoch 19136/20000: Train Loss = 0.434047, Test Loss = 0.252798, Learning Rate = 6.946076e-07\n",
      "Epoch 19137/20000: Train Loss = 0.433987, Test Loss = 0.251944, Learning Rate = 6.943437e-07\n",
      "Epoch 19138/20000: Train Loss = 0.434045, Test Loss = 0.251615, Learning Rate = 6.940799e-07\n",
      "Epoch 19139/20000: Train Loss = 0.434063, Test Loss = 0.251952, Learning Rate = 6.938161e-07\n",
      "Epoch 19140/20000: Train Loss = 0.433991, Test Loss = 0.252536, Learning Rate = 6.935525e-07\n",
      "Epoch 19141/20000: Train Loss = 0.434182, Test Loss = 0.252253, Learning Rate = 6.932890e-07\n",
      "Epoch 19142/20000: Train Loss = 0.433968, Test Loss = 0.251666, Learning Rate = 6.930255e-07\n",
      "Epoch 19143/20000: Train Loss = 0.434067, Test Loss = 0.251531, Learning Rate = 6.927622e-07\n",
      "Epoch 19144/20000: Train Loss = 0.434110, Test Loss = 0.251380, Learning Rate = 6.924990e-07\n",
      "Epoch 19145/20000: Train Loss = 0.434130, Test Loss = 0.251759, Learning Rate = 6.922359e-07\n",
      "Epoch 19146/20000: Train Loss = 0.434097, Test Loss = 0.251627, Learning Rate = 6.919728e-07\n",
      "Epoch 19147/20000: Train Loss = 0.434355, Test Loss = 0.251466, Learning Rate = 6.917099e-07\n",
      "Epoch 19148/20000: Train Loss = 0.434042, Test Loss = 0.251704, Learning Rate = 6.914471e-07\n",
      "Epoch 19149/20000: Train Loss = 0.434114, Test Loss = 0.251675, Learning Rate = 6.911843e-07\n",
      "Epoch 19150/20000: Train Loss = 0.434113, Test Loss = 0.251908, Learning Rate = 6.909217e-07\n",
      "Epoch 19151/20000: Train Loss = 0.434018, Test Loss = 0.251962, Learning Rate = 6.906592e-07\n",
      "Epoch 19152/20000: Train Loss = 0.434086, Test Loss = 0.252092, Learning Rate = 6.903967e-07\n",
      "Epoch 19153/20000: Train Loss = 0.434098, Test Loss = 0.252786, Learning Rate = 6.901344e-07\n",
      "Epoch 19154/20000: Train Loss = 0.434154, Test Loss = 0.252629, Learning Rate = 6.898722e-07\n",
      "Epoch 19155/20000: Train Loss = 0.434023, Test Loss = 0.252714, Learning Rate = 6.896100e-07\n",
      "Epoch 19156/20000: Train Loss = 0.434028, Test Loss = 0.252224, Learning Rate = 6.893480e-07\n",
      "Epoch 19157/20000: Train Loss = 0.434048, Test Loss = 0.251975, Learning Rate = 6.890861e-07\n",
      "Epoch 19158/20000: Train Loss = 0.434069, Test Loss = 0.252965, Learning Rate = 6.888242e-07\n",
      "Epoch 19159/20000: Train Loss = 0.433998, Test Loss = 0.252259, Learning Rate = 6.885625e-07\n",
      "Epoch 19160/20000: Train Loss = 0.434045, Test Loss = 0.252354, Learning Rate = 6.883009e-07\n",
      "Epoch 19161/20000: Train Loss = 0.434132, Test Loss = 0.252685, Learning Rate = 6.880393e-07\n",
      "Epoch 19162/20000: Train Loss = 0.434015, Test Loss = 0.252187, Learning Rate = 6.877779e-07\n",
      "Epoch 19163/20000: Train Loss = 0.434036, Test Loss = 0.252498, Learning Rate = 6.875166e-07\n",
      "Epoch 19164/20000: Train Loss = 0.434079, Test Loss = 0.253238, Learning Rate = 6.872553e-07\n",
      "Epoch 19165/20000: Train Loss = 0.434054, Test Loss = 0.253214, Learning Rate = 6.869942e-07\n",
      "Epoch 19166/20000: Train Loss = 0.434164, Test Loss = 0.254093, Learning Rate = 6.867331e-07\n",
      "Epoch 19167/20000: Train Loss = 0.434162, Test Loss = 0.253311, Learning Rate = 6.864722e-07\n",
      "Epoch 19168/20000: Train Loss = 0.433993, Test Loss = 0.253766, Learning Rate = 6.862114e-07\n",
      "Epoch 19169/20000: Train Loss = 0.434064, Test Loss = 0.253716, Learning Rate = 6.859506e-07\n",
      "Epoch 19170/20000: Train Loss = 0.434021, Test Loss = 0.253085, Learning Rate = 6.856900e-07\n",
      "Epoch 19171/20000: Train Loss = 0.434007, Test Loss = 0.253584, Learning Rate = 6.854294e-07\n",
      "Epoch 19172/20000: Train Loss = 0.434080, Test Loss = 0.253653, Learning Rate = 6.851690e-07\n",
      "Epoch 19173/20000: Train Loss = 0.434014, Test Loss = 0.253106, Learning Rate = 6.849086e-07\n",
      "Epoch 19174/20000: Train Loss = 0.434048, Test Loss = 0.252931, Learning Rate = 6.846484e-07\n",
      "Epoch 19175/20000: Train Loss = 0.434020, Test Loss = 0.253033, Learning Rate = 6.843882e-07\n",
      "Epoch 19176/20000: Train Loss = 0.434117, Test Loss = 0.252595, Learning Rate = 6.841282e-07\n",
      "Epoch 19177/20000: Train Loss = 0.434005, Test Loss = 0.253222, Learning Rate = 6.838682e-07\n",
      "Epoch 19178/20000: Train Loss = 0.434113, Test Loss = 0.252801, Learning Rate = 6.836084e-07\n",
      "Epoch 19179/20000: Train Loss = 0.434162, Test Loss = 0.254008, Learning Rate = 6.833486e-07\n",
      "Epoch 19180/20000: Train Loss = 0.434007, Test Loss = 0.253565, Learning Rate = 6.830890e-07\n",
      "Epoch 19181/20000: Train Loss = 0.434007, Test Loss = 0.253577, Learning Rate = 6.828294e-07\n",
      "Epoch 19182/20000: Train Loss = 0.434019, Test Loss = 0.253200, Learning Rate = 6.825700e-07\n",
      "Epoch 19183/20000: Train Loss = 0.434006, Test Loss = 0.253508, Learning Rate = 6.823106e-07\n",
      "Epoch 19184/20000: Train Loss = 0.434013, Test Loss = 0.254049, Learning Rate = 6.820514e-07\n",
      "Epoch 19185/20000: Train Loss = 0.434084, Test Loss = 0.253620, Learning Rate = 6.817922e-07\n",
      "Epoch 19186/20000: Train Loss = 0.434013, Test Loss = 0.253957, Learning Rate = 6.815331e-07\n",
      "Epoch 19187/20000: Train Loss = 0.434034, Test Loss = 0.253869, Learning Rate = 6.812742e-07\n",
      "Epoch 19188/20000: Train Loss = 0.434069, Test Loss = 0.254309, Learning Rate = 6.810153e-07\n",
      "Epoch 19189/20000: Train Loss = 0.434019, Test Loss = 0.254139, Learning Rate = 6.807565e-07\n",
      "Epoch 19190/20000: Train Loss = 0.434185, Test Loss = 0.253890, Learning Rate = 6.804979e-07\n",
      "Epoch 19191/20000: Train Loss = 0.434107, Test Loss = 0.253788, Learning Rate = 6.802393e-07\n",
      "Epoch 19192/20000: Train Loss = 0.434047, Test Loss = 0.254495, Learning Rate = 6.799808e-07\n",
      "Epoch 19193/20000: Train Loss = 0.434023, Test Loss = 0.254144, Learning Rate = 6.797225e-07\n",
      "Epoch 19194/20000: Train Loss = 0.434036, Test Loss = 0.254152, Learning Rate = 6.794642e-07\n",
      "Epoch 19195/20000: Train Loss = 0.434016, Test Loss = 0.254212, Learning Rate = 6.792060e-07\n",
      "Epoch 19196/20000: Train Loss = 0.434037, Test Loss = 0.253844, Learning Rate = 6.789479e-07\n",
      "Epoch 19197/20000: Train Loss = 0.434073, Test Loss = 0.253466, Learning Rate = 6.786899e-07\n",
      "Epoch 19198/20000: Train Loss = 0.434081, Test Loss = 0.253688, Learning Rate = 6.784321e-07\n",
      "Epoch 19199/20000: Train Loss = 0.434042, Test Loss = 0.253329, Learning Rate = 6.781743e-07\n",
      "Epoch 19200/20000: Train Loss = 0.434147, Test Loss = 0.253896, Learning Rate = 6.779166e-07\n",
      "Epoch 19201/20000: Train Loss = 0.434211, Test Loss = 0.253876, Learning Rate = 6.776590e-07\n",
      "Epoch 19202/20000: Train Loss = 0.434174, Test Loss = 0.253465, Learning Rate = 6.774015e-07\n",
      "Epoch 19203/20000: Train Loss = 0.434165, Test Loss = 0.254274, Learning Rate = 6.771441e-07\n",
      "Epoch 19204/20000: Train Loss = 0.434089, Test Loss = 0.254182, Learning Rate = 6.768868e-07\n",
      "Epoch 19205/20000: Train Loss = 0.434066, Test Loss = 0.253978, Learning Rate = 6.766296e-07\n",
      "Epoch 19206/20000: Train Loss = 0.434045, Test Loss = 0.254061, Learning Rate = 6.763725e-07\n",
      "Epoch 19207/20000: Train Loss = 0.434098, Test Loss = 0.254081, Learning Rate = 6.761155e-07\n",
      "Epoch 19208/20000: Train Loss = 0.434027, Test Loss = 0.253395, Learning Rate = 6.758586e-07\n",
      "Epoch 19209/20000: Train Loss = 0.434018, Test Loss = 0.253137, Learning Rate = 6.756018e-07\n",
      "Epoch 19210/20000: Train Loss = 0.434013, Test Loss = 0.253573, Learning Rate = 6.753451e-07\n",
      "Epoch 19211/20000: Train Loss = 0.434005, Test Loss = 0.253607, Learning Rate = 6.750885e-07\n",
      "Epoch 19212/20000: Train Loss = 0.434130, Test Loss = 0.254504, Learning Rate = 6.748320e-07\n",
      "Epoch 19213/20000: Train Loss = 0.433984, Test Loss = 0.253926, Learning Rate = 6.745755e-07\n",
      "Epoch 19214/20000: Train Loss = 0.434017, Test Loss = 0.253327, Learning Rate = 6.743192e-07\n",
      "Epoch 19215/20000: Train Loss = 0.434005, Test Loss = 0.252942, Learning Rate = 6.740630e-07\n",
      "Epoch 19216/20000: Train Loss = 0.434189, Test Loss = 0.253037, Learning Rate = 6.738069e-07\n",
      "Epoch 19217/20000: Train Loss = 0.434037, Test Loss = 0.253502, Learning Rate = 6.735508e-07\n",
      "Epoch 19218/20000: Train Loss = 0.434075, Test Loss = 0.253450, Learning Rate = 6.732949e-07\n",
      "Epoch 19219/20000: Train Loss = 0.434034, Test Loss = 0.253747, Learning Rate = 6.730391e-07\n",
      "Epoch 19220/20000: Train Loss = 0.433997, Test Loss = 0.254027, Learning Rate = 6.727833e-07\n",
      "Epoch 19221/20000: Train Loss = 0.434062, Test Loss = 0.253671, Learning Rate = 6.725277e-07\n",
      "Epoch 19222/20000: Train Loss = 0.434113, Test Loss = 0.253818, Learning Rate = 6.722722e-07\n",
      "Epoch 19223/20000: Train Loss = 0.434011, Test Loss = 0.254077, Learning Rate = 6.720167e-07\n",
      "Epoch 19224/20000: Train Loss = 0.434023, Test Loss = 0.253843, Learning Rate = 6.717614e-07\n",
      "Epoch 19225/20000: Train Loss = 0.434159, Test Loss = 0.253862, Learning Rate = 6.715061e-07\n",
      "Epoch 19226/20000: Train Loss = 0.434027, Test Loss = 0.254487, Learning Rate = 6.712510e-07\n",
      "Epoch 19227/20000: Train Loss = 0.434027, Test Loss = 0.253614, Learning Rate = 6.709959e-07\n",
      "Epoch 19228/20000: Train Loss = 0.434107, Test Loss = 0.253883, Learning Rate = 6.707409e-07\n",
      "Epoch 19229/20000: Train Loss = 0.434156, Test Loss = 0.253614, Learning Rate = 6.704861e-07\n",
      "Epoch 19230/20000: Train Loss = 0.434047, Test Loss = 0.253459, Learning Rate = 6.702313e-07\n",
      "Epoch 19231/20000: Train Loss = 0.434033, Test Loss = 0.253973, Learning Rate = 6.699766e-07\n",
      "Epoch 19232/20000: Train Loss = 0.434101, Test Loss = 0.253802, Learning Rate = 6.697221e-07\n",
      "Epoch 19233/20000: Train Loss = 0.434054, Test Loss = 0.253621, Learning Rate = 6.694676e-07\n",
      "Epoch 19234/20000: Train Loss = 0.434004, Test Loss = 0.253576, Learning Rate = 6.692132e-07\n",
      "Epoch 19235/20000: Train Loss = 0.434103, Test Loss = 0.253231, Learning Rate = 6.689589e-07\n",
      "Epoch 19236/20000: Train Loss = 0.434140, Test Loss = 0.253766, Learning Rate = 6.687047e-07\n",
      "Epoch 19237/20000: Train Loss = 0.433977, Test Loss = 0.253343, Learning Rate = 6.684507e-07\n",
      "Epoch 19238/20000: Train Loss = 0.434052, Test Loss = 0.253459, Learning Rate = 6.681967e-07\n",
      "Epoch 19239/20000: Train Loss = 0.434126, Test Loss = 0.253088, Learning Rate = 6.679428e-07\n",
      "Epoch 19240/20000: Train Loss = 0.433971, Test Loss = 0.253595, Learning Rate = 6.676890e-07\n",
      "Epoch 19241/20000: Train Loss = 0.434030, Test Loss = 0.253781, Learning Rate = 6.674353e-07\n",
      "Epoch 19242/20000: Train Loss = 0.434007, Test Loss = 0.253775, Learning Rate = 6.671816e-07\n",
      "Epoch 19243/20000: Train Loss = 0.434065, Test Loss = 0.253356, Learning Rate = 6.669281e-07\n",
      "Epoch 19244/20000: Train Loss = 0.434015, Test Loss = 0.253571, Learning Rate = 6.666747e-07\n",
      "Epoch 19245/20000: Train Loss = 0.434110, Test Loss = 0.253810, Learning Rate = 6.664214e-07\n",
      "Epoch 19246/20000: Train Loss = 0.434021, Test Loss = 0.253480, Learning Rate = 6.661682e-07\n",
      "Epoch 19247/20000: Train Loss = 0.434081, Test Loss = 0.253066, Learning Rate = 6.659151e-07\n",
      "Epoch 19248/20000: Train Loss = 0.434172, Test Loss = 0.253782, Learning Rate = 6.656620e-07\n",
      "Epoch 19249/20000: Train Loss = 0.433998, Test Loss = 0.253673, Learning Rate = 6.654091e-07\n",
      "Epoch 19250/20000: Train Loss = 0.434062, Test Loss = 0.253214, Learning Rate = 6.651563e-07\n",
      "Epoch 19251/20000: Train Loss = 0.433982, Test Loss = 0.253602, Learning Rate = 6.649035e-07\n",
      "Epoch 19252/20000: Train Loss = 0.434131, Test Loss = 0.253935, Learning Rate = 6.646509e-07\n",
      "Epoch 19253/20000: Train Loss = 0.434090, Test Loss = 0.253596, Learning Rate = 6.643983e-07\n",
      "Epoch 19254/20000: Train Loss = 0.434016, Test Loss = 0.253698, Learning Rate = 6.641459e-07\n",
      "Epoch 19255/20000: Train Loss = 0.434017, Test Loss = 0.253666, Learning Rate = 6.638935e-07\n",
      "Epoch 19256/20000: Train Loss = 0.434001, Test Loss = 0.253421, Learning Rate = 6.636412e-07\n",
      "Epoch 19257/20000: Train Loss = 0.434027, Test Loss = 0.253576, Learning Rate = 6.633891e-07\n",
      "Epoch 19258/20000: Train Loss = 0.434003, Test Loss = 0.253673, Learning Rate = 6.631370e-07\n",
      "Epoch 19259/20000: Train Loss = 0.433997, Test Loss = 0.253554, Learning Rate = 6.628850e-07\n",
      "Epoch 19260/20000: Train Loss = 0.434080, Test Loss = 0.253638, Learning Rate = 6.626332e-07\n",
      "Epoch 19261/20000: Train Loss = 0.434014, Test Loss = 0.253896, Learning Rate = 6.623814e-07\n",
      "Epoch 19262/20000: Train Loss = 0.434284, Test Loss = 0.253906, Learning Rate = 6.621297e-07\n",
      "Epoch 19263/20000: Train Loss = 0.434126, Test Loss = 0.254088, Learning Rate = 6.618781e-07\n",
      "Epoch 19264/20000: Train Loss = 0.434026, Test Loss = 0.253437, Learning Rate = 6.616266e-07\n",
      "Epoch 19265/20000: Train Loss = 0.434019, Test Loss = 0.253150, Learning Rate = 6.613752e-07\n",
      "Epoch 19266/20000: Train Loss = 0.434101, Test Loss = 0.253443, Learning Rate = 6.611239e-07\n",
      "Epoch 19267/20000: Train Loss = 0.434072, Test Loss = 0.253449, Learning Rate = 6.608727e-07\n",
      "Epoch 19268/20000: Train Loss = 0.434332, Test Loss = 0.254094, Learning Rate = 6.606216e-07\n",
      "Epoch 19269/20000: Train Loss = 0.433994, Test Loss = 0.253427, Learning Rate = 6.603706e-07\n",
      "Epoch 19270/20000: Train Loss = 0.434031, Test Loss = 0.253575, Learning Rate = 6.601196e-07\n",
      "Epoch 19271/20000: Train Loss = 0.434120, Test Loss = 0.253142, Learning Rate = 6.598688e-07\n",
      "Epoch 19272/20000: Train Loss = 0.433968, Test Loss = 0.253223, Learning Rate = 6.596181e-07\n",
      "Epoch 19273/20000: Train Loss = 0.434007, Test Loss = 0.253452, Learning Rate = 6.593674e-07\n",
      "Epoch 19274/20000: Train Loss = 0.433979, Test Loss = 0.253151, Learning Rate = 6.591169e-07\n",
      "Epoch 19275/20000: Train Loss = 0.434098, Test Loss = 0.253014, Learning Rate = 6.588664e-07\n",
      "Epoch 19276/20000: Train Loss = 0.434016, Test Loss = 0.252922, Learning Rate = 6.586161e-07\n",
      "Epoch 19277/20000: Train Loss = 0.434026, Test Loss = 0.252575, Learning Rate = 6.583658e-07\n",
      "Epoch 19278/20000: Train Loss = 0.434026, Test Loss = 0.253285, Learning Rate = 6.581157e-07\n",
      "Epoch 19279/20000: Train Loss = 0.433979, Test Loss = 0.253462, Learning Rate = 6.578656e-07\n",
      "Epoch 19280/20000: Train Loss = 0.434036, Test Loss = 0.253333, Learning Rate = 6.576156e-07\n",
      "Epoch 19281/20000: Train Loss = 0.434025, Test Loss = 0.253595, Learning Rate = 6.573658e-07\n",
      "Epoch 19282/20000: Train Loss = 0.434192, Test Loss = 0.252819, Learning Rate = 6.571160e-07\n",
      "Epoch 19283/20000: Train Loss = 0.434025, Test Loss = 0.252849, Learning Rate = 6.568663e-07\n",
      "Epoch 19284/20000: Train Loss = 0.434008, Test Loss = 0.252836, Learning Rate = 6.566167e-07\n",
      "Epoch 19285/20000: Train Loss = 0.434039, Test Loss = 0.252960, Learning Rate = 6.563672e-07\n",
      "Epoch 19286/20000: Train Loss = 0.433999, Test Loss = 0.252869, Learning Rate = 6.561178e-07\n",
      "Epoch 19287/20000: Train Loss = 0.434009, Test Loss = 0.253373, Learning Rate = 6.558685e-07\n",
      "Epoch 19288/20000: Train Loss = 0.434066, Test Loss = 0.252848, Learning Rate = 6.556193e-07\n",
      "Epoch 19289/20000: Train Loss = 0.434052, Test Loss = 0.252729, Learning Rate = 6.553702e-07\n",
      "Epoch 19290/20000: Train Loss = 0.434072, Test Loss = 0.252739, Learning Rate = 6.551212e-07\n",
      "Epoch 19291/20000: Train Loss = 0.434098, Test Loss = 0.252777, Learning Rate = 6.548722e-07\n",
      "Epoch 19292/20000: Train Loss = 0.434045, Test Loss = 0.252743, Learning Rate = 6.546234e-07\n",
      "Epoch 19293/20000: Train Loss = 0.434027, Test Loss = 0.252573, Learning Rate = 6.543746e-07\n",
      "Epoch 19294/20000: Train Loss = 0.434077, Test Loss = 0.252502, Learning Rate = 6.541260e-07\n",
      "Epoch 19295/20000: Train Loss = 0.434037, Test Loss = 0.252240, Learning Rate = 6.538775e-07\n",
      "Epoch 19296/20000: Train Loss = 0.434012, Test Loss = 0.252103, Learning Rate = 6.536290e-07\n",
      "Epoch 19297/20000: Train Loss = 0.434037, Test Loss = 0.252499, Learning Rate = 6.533806e-07\n",
      "Epoch 19298/20000: Train Loss = 0.434072, Test Loss = 0.252889, Learning Rate = 6.531324e-07\n",
      "Epoch 19299/20000: Train Loss = 0.434021, Test Loss = 0.252563, Learning Rate = 6.528842e-07\n",
      "Epoch 19300/20000: Train Loss = 0.434058, Test Loss = 0.252527, Learning Rate = 6.526361e-07\n",
      "Epoch 19301/20000: Train Loss = 0.434010, Test Loss = 0.252416, Learning Rate = 6.523881e-07\n",
      "Epoch 19302/20000: Train Loss = 0.434013, Test Loss = 0.252099, Learning Rate = 6.521402e-07\n",
      "Epoch 19303/20000: Train Loss = 0.434005, Test Loss = 0.252376, Learning Rate = 6.518924e-07\n",
      "Epoch 19304/20000: Train Loss = 0.434109, Test Loss = 0.252530, Learning Rate = 6.516447e-07\n",
      "Epoch 19305/20000: Train Loss = 0.434044, Test Loss = 0.252439, Learning Rate = 6.513971e-07\n",
      "Epoch 19306/20000: Train Loss = 0.434171, Test Loss = 0.252555, Learning Rate = 6.511496e-07\n",
      "Epoch 19307/20000: Train Loss = 0.434025, Test Loss = 0.252474, Learning Rate = 6.509022e-07\n",
      "Epoch 19308/20000: Train Loss = 0.434088, Test Loss = 0.252620, Learning Rate = 6.506549e-07\n",
      "Epoch 19309/20000: Train Loss = 0.434080, Test Loss = 0.252503, Learning Rate = 6.504077e-07\n",
      "Epoch 19310/20000: Train Loss = 0.434050, Test Loss = 0.252185, Learning Rate = 6.501605e-07\n",
      "Epoch 19311/20000: Train Loss = 0.434019, Test Loss = 0.252752, Learning Rate = 6.499135e-07\n",
      "Epoch 19312/20000: Train Loss = 0.434069, Test Loss = 0.251965, Learning Rate = 6.496665e-07\n",
      "Epoch 19313/20000: Train Loss = 0.433988, Test Loss = 0.252687, Learning Rate = 6.494197e-07\n",
      "Epoch 19314/20000: Train Loss = 0.434038, Test Loss = 0.252615, Learning Rate = 6.491729e-07\n",
      "Epoch 19315/20000: Train Loss = 0.434046, Test Loss = 0.251799, Learning Rate = 6.489262e-07\n",
      "Epoch 19316/20000: Train Loss = 0.434020, Test Loss = 0.251734, Learning Rate = 6.486797e-07\n",
      "Epoch 19317/20000: Train Loss = 0.434031, Test Loss = 0.252406, Learning Rate = 6.484332e-07\n",
      "Epoch 19318/20000: Train Loss = 0.434019, Test Loss = 0.252222, Learning Rate = 6.481868e-07\n",
      "Epoch 19319/20000: Train Loss = 0.434074, Test Loss = 0.252247, Learning Rate = 6.479405e-07\n",
      "Epoch 19320/20000: Train Loss = 0.434038, Test Loss = 0.252178, Learning Rate = 6.476943e-07\n",
      "Epoch 19321/20000: Train Loss = 0.434011, Test Loss = 0.252016, Learning Rate = 6.474482e-07\n",
      "Epoch 19322/20000: Train Loss = 0.434037, Test Loss = 0.252034, Learning Rate = 6.472022e-07\n",
      "Epoch 19323/20000: Train Loss = 0.434020, Test Loss = 0.252573, Learning Rate = 6.469563e-07\n",
      "Epoch 19324/20000: Train Loss = 0.434022, Test Loss = 0.252131, Learning Rate = 6.467104e-07\n",
      "Epoch 19325/20000: Train Loss = 0.433993, Test Loss = 0.252610, Learning Rate = 6.464647e-07\n",
      "Epoch 19326/20000: Train Loss = 0.434014, Test Loss = 0.252132, Learning Rate = 6.462191e-07\n",
      "Epoch 19327/20000: Train Loss = 0.433991, Test Loss = 0.252358, Learning Rate = 6.459735e-07\n",
      "Epoch 19328/20000: Train Loss = 0.434067, Test Loss = 0.253020, Learning Rate = 6.457281e-07\n",
      "Epoch 19329/20000: Train Loss = 0.434043, Test Loss = 0.252502, Learning Rate = 6.454827e-07\n",
      "Epoch 19330/20000: Train Loss = 0.433995, Test Loss = 0.252380, Learning Rate = 6.452374e-07\n",
      "Epoch 19331/20000: Train Loss = 0.434003, Test Loss = 0.252472, Learning Rate = 6.449923e-07\n",
      "Epoch 19332/20000: Train Loss = 0.434048, Test Loss = 0.251932, Learning Rate = 6.447472e-07\n",
      "Epoch 19333/20000: Train Loss = 0.434017, Test Loss = 0.252437, Learning Rate = 6.445022e-07\n",
      "Epoch 19334/20000: Train Loss = 0.434016, Test Loss = 0.251977, Learning Rate = 6.442573e-07\n",
      "Epoch 19335/20000: Train Loss = 0.434024, Test Loss = 0.252257, Learning Rate = 6.440125e-07\n",
      "Epoch 19336/20000: Train Loss = 0.434126, Test Loss = 0.252708, Learning Rate = 6.437678e-07\n",
      "Epoch 19337/20000: Train Loss = 0.434068, Test Loss = 0.252897, Learning Rate = 6.435232e-07\n",
      "Epoch 19338/20000: Train Loss = 0.433991, Test Loss = 0.252220, Learning Rate = 6.432787e-07\n",
      "Epoch 19339/20000: Train Loss = 0.434105, Test Loss = 0.252202, Learning Rate = 6.430342e-07\n",
      "Epoch 19340/20000: Train Loss = 0.434105, Test Loss = 0.251957, Learning Rate = 6.427899e-07\n",
      "Epoch 19341/20000: Train Loss = 0.434000, Test Loss = 0.252498, Learning Rate = 6.425457e-07\n",
      "Epoch 19342/20000: Train Loss = 0.434119, Test Loss = 0.252666, Learning Rate = 6.423015e-07\n",
      "Epoch 19343/20000: Train Loss = 0.433989, Test Loss = 0.252007, Learning Rate = 6.420575e-07\n",
      "Epoch 19344/20000: Train Loss = 0.434118, Test Loss = 0.252205, Learning Rate = 6.418135e-07\n",
      "Epoch 19345/20000: Train Loss = 0.434024, Test Loss = 0.251899, Learning Rate = 6.415696e-07\n",
      "Epoch 19346/20000: Train Loss = 0.434027, Test Loss = 0.252166, Learning Rate = 6.413258e-07\n",
      "Epoch 19347/20000: Train Loss = 0.434114, Test Loss = 0.251412, Learning Rate = 6.410822e-07\n",
      "Epoch 19348/20000: Train Loss = 0.433986, Test Loss = 0.251812, Learning Rate = 6.408386e-07\n",
      "Epoch 19349/20000: Train Loss = 0.433986, Test Loss = 0.251793, Learning Rate = 6.405951e-07\n",
      "Epoch 19350/20000: Train Loss = 0.434012, Test Loss = 0.251785, Learning Rate = 6.403516e-07\n",
      "Epoch 19351/20000: Train Loss = 0.434038, Test Loss = 0.251862, Learning Rate = 6.401083e-07\n",
      "Epoch 19352/20000: Train Loss = 0.434073, Test Loss = 0.252299, Learning Rate = 6.398651e-07\n",
      "Epoch 19353/20000: Train Loss = 0.434020, Test Loss = 0.252229, Learning Rate = 6.396220e-07\n",
      "Epoch 19354/20000: Train Loss = 0.433995, Test Loss = 0.252284, Learning Rate = 6.393789e-07\n",
      "Epoch 19355/20000: Train Loss = 0.433980, Test Loss = 0.251662, Learning Rate = 6.391360e-07\n",
      "Epoch 19356/20000: Train Loss = 0.434037, Test Loss = 0.251787, Learning Rate = 6.388931e-07\n",
      "Epoch 19357/20000: Train Loss = 0.434081, Test Loss = 0.252048, Learning Rate = 6.386504e-07\n",
      "Epoch 19358/20000: Train Loss = 0.434055, Test Loss = 0.251817, Learning Rate = 6.384077e-07\n",
      "Epoch 19359/20000: Train Loss = 0.434060, Test Loss = 0.252033, Learning Rate = 6.381651e-07\n",
      "Epoch 19360/20000: Train Loss = 0.434261, Test Loss = 0.252033, Learning Rate = 6.379226e-07\n",
      "Epoch 19361/20000: Train Loss = 0.434010, Test Loss = 0.251924, Learning Rate = 6.376802e-07\n",
      "Epoch 19362/20000: Train Loss = 0.434025, Test Loss = 0.251541, Learning Rate = 6.374379e-07\n",
      "Epoch 19363/20000: Train Loss = 0.434000, Test Loss = 0.251678, Learning Rate = 6.371957e-07\n",
      "Epoch 19364/20000: Train Loss = 0.434079, Test Loss = 0.251921, Learning Rate = 6.369536e-07\n",
      "Epoch 19365/20000: Train Loss = 0.433972, Test Loss = 0.252242, Learning Rate = 6.367116e-07\n",
      "Epoch 19366/20000: Train Loss = 0.433990, Test Loss = 0.251838, Learning Rate = 6.364697e-07\n",
      "Epoch 19367/20000: Train Loss = 0.434032, Test Loss = 0.252135, Learning Rate = 6.362278e-07\n",
      "Epoch 19368/20000: Train Loss = 0.434018, Test Loss = 0.252418, Learning Rate = 6.359861e-07\n",
      "Epoch 19369/20000: Train Loss = 0.434250, Test Loss = 0.252580, Learning Rate = 6.357444e-07\n",
      "Epoch 19370/20000: Train Loss = 0.434187, Test Loss = 0.253724, Learning Rate = 6.355028e-07\n",
      "Epoch 19371/20000: Train Loss = 0.434087, Test Loss = 0.252699, Learning Rate = 6.352614e-07\n",
      "Epoch 19372/20000: Train Loss = 0.434008, Test Loss = 0.252996, Learning Rate = 6.350200e-07\n",
      "Epoch 19373/20000: Train Loss = 0.434003, Test Loss = 0.253373, Learning Rate = 6.347787e-07\n",
      "Epoch 19374/20000: Train Loss = 0.434032, Test Loss = 0.253663, Learning Rate = 6.345375e-07\n",
      "Epoch 19375/20000: Train Loss = 0.434004, Test Loss = 0.253576, Learning Rate = 6.342964e-07\n",
      "Epoch 19376/20000: Train Loss = 0.434116, Test Loss = 0.253850, Learning Rate = 6.340554e-07\n",
      "Epoch 19377/20000: Train Loss = 0.434048, Test Loss = 0.253326, Learning Rate = 6.338145e-07\n",
      "Epoch 19378/20000: Train Loss = 0.434052, Test Loss = 0.253329, Learning Rate = 6.335736e-07\n",
      "Epoch 19379/20000: Train Loss = 0.433934, Test Loss = 0.253144, Learning Rate = 6.333329e-07\n",
      "Epoch 19380/20000: Train Loss = 0.434200, Test Loss = 0.252662, Learning Rate = 6.330922e-07\n",
      "Epoch 19381/20000: Train Loss = 0.434135, Test Loss = 0.254190, Learning Rate = 6.328517e-07\n",
      "Epoch 19382/20000: Train Loss = 0.434230, Test Loss = 0.253683, Learning Rate = 6.326112e-07\n",
      "Epoch 19383/20000: Train Loss = 0.434132, Test Loss = 0.252922, Learning Rate = 6.323708e-07\n",
      "Epoch 19384/20000: Train Loss = 0.434255, Test Loss = 0.252405, Learning Rate = 6.321306e-07\n",
      "Epoch 19385/20000: Train Loss = 0.434007, Test Loss = 0.253212, Learning Rate = 6.318904e-07\n",
      "Epoch 19386/20000: Train Loss = 0.433998, Test Loss = 0.253963, Learning Rate = 6.316503e-07\n",
      "Epoch 19387/20000: Train Loss = 0.434041, Test Loss = 0.254011, Learning Rate = 6.314102e-07\n",
      "Epoch 19388/20000: Train Loss = 0.434040, Test Loss = 0.253586, Learning Rate = 6.311703e-07\n",
      "Epoch 19389/20000: Train Loss = 0.433988, Test Loss = 0.253259, Learning Rate = 6.309305e-07\n",
      "Epoch 19390/20000: Train Loss = 0.433975, Test Loss = 0.253486, Learning Rate = 6.306908e-07\n",
      "Epoch 19391/20000: Train Loss = 0.433992, Test Loss = 0.253683, Learning Rate = 6.304511e-07\n",
      "Epoch 19392/20000: Train Loss = 0.434054, Test Loss = 0.253242, Learning Rate = 6.302116e-07\n",
      "Epoch 19393/20000: Train Loss = 0.434017, Test Loss = 0.253641, Learning Rate = 6.299721e-07\n",
      "Epoch 19394/20000: Train Loss = 0.433967, Test Loss = 0.253510, Learning Rate = 6.297327e-07\n",
      "Epoch 19395/20000: Train Loss = 0.434103, Test Loss = 0.253601, Learning Rate = 6.294934e-07\n",
      "Epoch 19396/20000: Train Loss = 0.433972, Test Loss = 0.254265, Learning Rate = 6.292543e-07\n",
      "Epoch 19397/20000: Train Loss = 0.434054, Test Loss = 0.253561, Learning Rate = 6.290152e-07\n",
      "Epoch 19398/20000: Train Loss = 0.434112, Test Loss = 0.253950, Learning Rate = 6.287761e-07\n",
      "Epoch 19399/20000: Train Loss = 0.433979, Test Loss = 0.253398, Learning Rate = 6.285372e-07\n",
      "Epoch 19400/20000: Train Loss = 0.434020, Test Loss = 0.253629, Learning Rate = 6.282984e-07\n",
      "Epoch 19401/20000: Train Loss = 0.434049, Test Loss = 0.253750, Learning Rate = 6.280597e-07\n",
      "Epoch 19402/20000: Train Loss = 0.434090, Test Loss = 0.253368, Learning Rate = 6.278210e-07\n",
      "Epoch 19403/20000: Train Loss = 0.434035, Test Loss = 0.253860, Learning Rate = 6.275825e-07\n",
      "Epoch 19404/20000: Train Loss = 0.433986, Test Loss = 0.253742, Learning Rate = 6.273440e-07\n",
      "Epoch 19405/20000: Train Loss = 0.434016, Test Loss = 0.253228, Learning Rate = 6.271056e-07\n",
      "Epoch 19406/20000: Train Loss = 0.434087, Test Loss = 0.253591, Learning Rate = 6.268673e-07\n",
      "Epoch 19407/20000: Train Loss = 0.434097, Test Loss = 0.253029, Learning Rate = 6.266292e-07\n",
      "Epoch 19408/20000: Train Loss = 0.433794, Test Loss = 0.253408, Learning Rate = 6.263910e-07\n",
      "Epoch 19409/20000: Train Loss = 0.434052, Test Loss = 0.253428, Learning Rate = 6.261530e-07\n",
      "Epoch 19410/20000: Train Loss = 0.434018, Test Loss = 0.253479, Learning Rate = 6.259151e-07\n",
      "Epoch 19411/20000: Train Loss = 0.434078, Test Loss = 0.253560, Learning Rate = 6.256773e-07\n",
      "Epoch 19412/20000: Train Loss = 0.433973, Test Loss = 0.253222, Learning Rate = 6.254395e-07\n",
      "Epoch 19413/20000: Train Loss = 0.434018, Test Loss = 0.253405, Learning Rate = 6.252019e-07\n",
      "Epoch 19414/20000: Train Loss = 0.434063, Test Loss = 0.253714, Learning Rate = 6.249643e-07\n",
      "Epoch 19415/20000: Train Loss = 0.434061, Test Loss = 0.253095, Learning Rate = 6.247269e-07\n",
      "Epoch 19416/20000: Train Loss = 0.433989, Test Loss = 0.253561, Learning Rate = 6.244895e-07\n",
      "Epoch 19417/20000: Train Loss = 0.434006, Test Loss = 0.253267, Learning Rate = 6.242522e-07\n",
      "Epoch 19418/20000: Train Loss = 0.433985, Test Loss = 0.253248, Learning Rate = 6.240150e-07\n",
      "Epoch 19419/20000: Train Loss = 0.434140, Test Loss = 0.253152, Learning Rate = 6.237779e-07\n",
      "Epoch 19420/20000: Train Loss = 0.433996, Test Loss = 0.253295, Learning Rate = 6.235409e-07\n",
      "Epoch 19421/20000: Train Loss = 0.434008, Test Loss = 0.253225, Learning Rate = 6.233039e-07\n",
      "Epoch 19422/20000: Train Loss = 0.433999, Test Loss = 0.253161, Learning Rate = 6.230671e-07\n",
      "Epoch 19423/20000: Train Loss = 0.433999, Test Loss = 0.253223, Learning Rate = 6.228304e-07\n",
      "Epoch 19424/20000: Train Loss = 0.434016, Test Loss = 0.252847, Learning Rate = 6.225937e-07\n",
      "Epoch 19425/20000: Train Loss = 0.434026, Test Loss = 0.252729, Learning Rate = 6.223571e-07\n",
      "Epoch 19426/20000: Train Loss = 0.434060, Test Loss = 0.252666, Learning Rate = 6.221206e-07\n",
      "Epoch 19427/20000: Train Loss = 0.433977, Test Loss = 0.252695, Learning Rate = 6.218843e-07\n",
      "Epoch 19428/20000: Train Loss = 0.433990, Test Loss = 0.252864, Learning Rate = 6.216480e-07\n",
      "Epoch 19429/20000: Train Loss = 0.433995, Test Loss = 0.252685, Learning Rate = 6.214118e-07\n",
      "Epoch 19430/20000: Train Loss = 0.434128, Test Loss = 0.252919, Learning Rate = 6.211756e-07\n",
      "Epoch 19431/20000: Train Loss = 0.434020, Test Loss = 0.252394, Learning Rate = 6.209396e-07\n",
      "Epoch 19432/20000: Train Loss = 0.434025, Test Loss = 0.252390, Learning Rate = 6.207037e-07\n",
      "Epoch 19433/20000: Train Loss = 0.434019, Test Loss = 0.252609, Learning Rate = 6.204678e-07\n",
      "Epoch 19434/20000: Train Loss = 0.434033, Test Loss = 0.252800, Learning Rate = 6.202321e-07\n",
      "Epoch 19435/20000: Train Loss = 0.434028, Test Loss = 0.252718, Learning Rate = 6.199964e-07\n",
      "Epoch 19436/20000: Train Loss = 0.434023, Test Loss = 0.253298, Learning Rate = 6.197608e-07\n",
      "Epoch 19437/20000: Train Loss = 0.434069, Test Loss = 0.253873, Learning Rate = 6.195253e-07\n",
      "Epoch 19438/20000: Train Loss = 0.433934, Test Loss = 0.253588, Learning Rate = 6.192899e-07\n",
      "Epoch 19439/20000: Train Loss = 0.433993, Test Loss = 0.253450, Learning Rate = 6.190546e-07\n",
      "Epoch 19440/20000: Train Loss = 0.434028, Test Loss = 0.252527, Learning Rate = 6.188194e-07\n",
      "Epoch 19441/20000: Train Loss = 0.434012, Test Loss = 0.252689, Learning Rate = 6.185842e-07\n",
      "Epoch 19442/20000: Train Loss = 0.434039, Test Loss = 0.253126, Learning Rate = 6.183492e-07\n",
      "Epoch 19443/20000: Train Loss = 0.434009, Test Loss = 0.253053, Learning Rate = 6.181142e-07\n",
      "Epoch 19444/20000: Train Loss = 0.434024, Test Loss = 0.253280, Learning Rate = 6.178794e-07\n",
      "Epoch 19445/20000: Train Loss = 0.434040, Test Loss = 0.253095, Learning Rate = 6.176446e-07\n",
      "Epoch 19446/20000: Train Loss = 0.434017, Test Loss = 0.253190, Learning Rate = 6.174099e-07\n",
      "Epoch 19447/20000: Train Loss = 0.434008, Test Loss = 0.253155, Learning Rate = 6.171753e-07\n",
      "Epoch 19448/20000: Train Loss = 0.434019, Test Loss = 0.253143, Learning Rate = 6.169408e-07\n",
      "Epoch 19449/20000: Train Loss = 0.433983, Test Loss = 0.252940, Learning Rate = 6.167064e-07\n",
      "Epoch 19450/20000: Train Loss = 0.434043, Test Loss = 0.252935, Learning Rate = 6.164720e-07\n",
      "Epoch 19451/20000: Train Loss = 0.434101, Test Loss = 0.252487, Learning Rate = 6.162378e-07\n",
      "Epoch 19452/20000: Train Loss = 0.433999, Test Loss = 0.252875, Learning Rate = 6.160036e-07\n",
      "Epoch 19453/20000: Train Loss = 0.434050, Test Loss = 0.252679, Learning Rate = 6.157696e-07\n",
      "Epoch 19454/20000: Train Loss = 0.434045, Test Loss = 0.252510, Learning Rate = 6.155356e-07\n",
      "Epoch 19455/20000: Train Loss = 0.434033, Test Loss = 0.252460, Learning Rate = 6.153017e-07\n",
      "Epoch 19456/20000: Train Loss = 0.433968, Test Loss = 0.252719, Learning Rate = 6.150679e-07\n",
      "Epoch 19457/20000: Train Loss = 0.433995, Test Loss = 0.252398, Learning Rate = 6.148342e-07\n",
      "Epoch 19458/20000: Train Loss = 0.434049, Test Loss = 0.252939, Learning Rate = 6.146006e-07\n",
      "Epoch 19459/20000: Train Loss = 0.434028, Test Loss = 0.252347, Learning Rate = 6.143671e-07\n",
      "Epoch 19460/20000: Train Loss = 0.434055, Test Loss = 0.252503, Learning Rate = 6.141336e-07\n",
      "Epoch 19461/20000: Train Loss = 0.434168, Test Loss = 0.253436, Learning Rate = 6.139003e-07\n",
      "Epoch 19462/20000: Train Loss = 0.434001, Test Loss = 0.252895, Learning Rate = 6.136670e-07\n",
      "Epoch 19463/20000: Train Loss = 0.434276, Test Loss = 0.252486, Learning Rate = 6.134338e-07\n",
      "Epoch 19464/20000: Train Loss = 0.434027, Test Loss = 0.253099, Learning Rate = 6.132007e-07\n",
      "Epoch 19465/20000: Train Loss = 0.434022, Test Loss = 0.253013, Learning Rate = 6.129677e-07\n",
      "Epoch 19466/20000: Train Loss = 0.434083, Test Loss = 0.253058, Learning Rate = 6.127348e-07\n",
      "Epoch 19467/20000: Train Loss = 0.434032, Test Loss = 0.253378, Learning Rate = 6.125020e-07\n",
      "Epoch 19468/20000: Train Loss = 0.433976, Test Loss = 0.253092, Learning Rate = 6.122693e-07\n",
      "Epoch 19469/20000: Train Loss = 0.434077, Test Loss = 0.253107, Learning Rate = 6.120366e-07\n",
      "Epoch 19470/20000: Train Loss = 0.434012, Test Loss = 0.253759, Learning Rate = 6.118041e-07\n",
      "Epoch 19471/20000: Train Loss = 0.434045, Test Loss = 0.253486, Learning Rate = 6.115716e-07\n",
      "Epoch 19472/20000: Train Loss = 0.434059, Test Loss = 0.253645, Learning Rate = 6.113392e-07\n",
      "Epoch 19473/20000: Train Loss = 0.434091, Test Loss = 0.253712, Learning Rate = 6.111069e-07\n",
      "Epoch 19474/20000: Train Loss = 0.433937, Test Loss = 0.254144, Learning Rate = 6.108747e-07\n",
      "Epoch 19475/20000: Train Loss = 0.433968, Test Loss = 0.254021, Learning Rate = 6.106426e-07\n",
      "Epoch 19476/20000: Train Loss = 0.433968, Test Loss = 0.253899, Learning Rate = 6.104106e-07\n",
      "Epoch 19477/20000: Train Loss = 0.434012, Test Loss = 0.253672, Learning Rate = 6.101786e-07\n",
      "Epoch 19478/20000: Train Loss = 0.434032, Test Loss = 0.254127, Learning Rate = 6.099468e-07\n",
      "Epoch 19479/20000: Train Loss = 0.434038, Test Loss = 0.254377, Learning Rate = 6.097150e-07\n",
      "Epoch 19480/20000: Train Loss = 0.434053, Test Loss = 0.253952, Learning Rate = 6.094833e-07\n",
      "Epoch 19481/20000: Train Loss = 0.434005, Test Loss = 0.253533, Learning Rate = 6.092517e-07\n",
      "Epoch 19482/20000: Train Loss = 0.434010, Test Loss = 0.253689, Learning Rate = 6.090202e-07\n",
      "Epoch 19483/20000: Train Loss = 0.434109, Test Loss = 0.254204, Learning Rate = 6.087888e-07\n",
      "Epoch 19484/20000: Train Loss = 0.434050, Test Loss = 0.254233, Learning Rate = 6.085575e-07\n",
      "Epoch 19485/20000: Train Loss = 0.433998, Test Loss = 0.254315, Learning Rate = 6.083263e-07\n",
      "Epoch 19486/20000: Train Loss = 0.433937, Test Loss = 0.254536, Learning Rate = 6.080951e-07\n",
      "Epoch 19487/20000: Train Loss = 0.434005, Test Loss = 0.254047, Learning Rate = 6.078641e-07\n",
      "Epoch 19488/20000: Train Loss = 0.433985, Test Loss = 0.254224, Learning Rate = 6.076331e-07\n",
      "Epoch 19489/20000: Train Loss = 0.433963, Test Loss = 0.253934, Learning Rate = 6.074022e-07\n",
      "Epoch 19490/20000: Train Loss = 0.434009, Test Loss = 0.253802, Learning Rate = 6.071714e-07\n",
      "Epoch 19491/20000: Train Loss = 0.433994, Test Loss = 0.253698, Learning Rate = 6.069407e-07\n",
      "Epoch 19492/20000: Train Loss = 0.434064, Test Loss = 0.254152, Learning Rate = 6.067101e-07\n",
      "Epoch 19493/20000: Train Loss = 0.434034, Test Loss = 0.253600, Learning Rate = 6.064796e-07\n",
      "Epoch 19494/20000: Train Loss = 0.434058, Test Loss = 0.253196, Learning Rate = 6.062491e-07\n",
      "Epoch 19495/20000: Train Loss = 0.433995, Test Loss = 0.253469, Learning Rate = 6.060188e-07\n",
      "Epoch 19496/20000: Train Loss = 0.433967, Test Loss = 0.253496, Learning Rate = 6.057885e-07\n",
      "Epoch 19497/20000: Train Loss = 0.434008, Test Loss = 0.253709, Learning Rate = 6.055583e-07\n",
      "Epoch 19498/20000: Train Loss = 0.434043, Test Loss = 0.253408, Learning Rate = 6.053282e-07\n",
      "Epoch 19499/20000: Train Loss = 0.433962, Test Loss = 0.253482, Learning Rate = 6.050982e-07\n",
      "Epoch 19500/20000: Train Loss = 0.434047, Test Loss = 0.253662, Learning Rate = 6.048683e-07\n",
      "Epoch 19501/20000: Train Loss = 0.433964, Test Loss = 0.253818, Learning Rate = 6.046384e-07\n",
      "Epoch 19502/20000: Train Loss = 0.434049, Test Loss = 0.253668, Learning Rate = 6.044087e-07\n",
      "Epoch 19503/20000: Train Loss = 0.434009, Test Loss = 0.254261, Learning Rate = 6.041790e-07\n",
      "Epoch 19504/20000: Train Loss = 0.434052, Test Loss = 0.253913, Learning Rate = 6.039495e-07\n",
      "Epoch 19505/20000: Train Loss = 0.434060, Test Loss = 0.253909, Learning Rate = 6.037200e-07\n",
      "Epoch 19506/20000: Train Loss = 0.433988, Test Loss = 0.253927, Learning Rate = 6.034906e-07\n",
      "Epoch 19507/20000: Train Loss = 0.433990, Test Loss = 0.253563, Learning Rate = 6.032613e-07\n",
      "Epoch 19508/20000: Train Loss = 0.433955, Test Loss = 0.253348, Learning Rate = 6.030320e-07\n",
      "Epoch 19509/20000: Train Loss = 0.434024, Test Loss = 0.253168, Learning Rate = 6.028029e-07\n",
      "Epoch 19510/20000: Train Loss = 0.433967, Test Loss = 0.253379, Learning Rate = 6.025739e-07\n",
      "Epoch 19511/20000: Train Loss = 0.434022, Test Loss = 0.254011, Learning Rate = 6.023449e-07\n",
      "Epoch 19512/20000: Train Loss = 0.434020, Test Loss = 0.254141, Learning Rate = 6.021160e-07\n",
      "Epoch 19513/20000: Train Loss = 0.434007, Test Loss = 0.253843, Learning Rate = 6.018872e-07\n",
      "Epoch 19514/20000: Train Loss = 0.433968, Test Loss = 0.254037, Learning Rate = 6.016585e-07\n",
      "Epoch 19515/20000: Train Loss = 0.434076, Test Loss = 0.253829, Learning Rate = 6.014299e-07\n",
      "Epoch 19516/20000: Train Loss = 0.434064, Test Loss = 0.253927, Learning Rate = 6.012014e-07\n",
      "Epoch 19517/20000: Train Loss = 0.434072, Test Loss = 0.253639, Learning Rate = 6.009730e-07\n",
      "Epoch 19518/20000: Train Loss = 0.434013, Test Loss = 0.253660, Learning Rate = 6.007446e-07\n",
      "Epoch 19519/20000: Train Loss = 0.434026, Test Loss = 0.253731, Learning Rate = 6.005163e-07\n",
      "Epoch 19520/20000: Train Loss = 0.434004, Test Loss = 0.253301, Learning Rate = 6.002882e-07\n",
      "Epoch 19521/20000: Train Loss = 0.434012, Test Loss = 0.253205, Learning Rate = 6.000601e-07\n",
      "Epoch 19522/20000: Train Loss = 0.433989, Test Loss = 0.253408, Learning Rate = 5.998321e-07\n",
      "Epoch 19523/20000: Train Loss = 0.434069, Test Loss = 0.253286, Learning Rate = 5.996041e-07\n",
      "Epoch 19524/20000: Train Loss = 0.434051, Test Loss = 0.252828, Learning Rate = 5.993763e-07\n",
      "Epoch 19525/20000: Train Loss = 0.434102, Test Loss = 0.253692, Learning Rate = 5.991486e-07\n",
      "Epoch 19526/20000: Train Loss = 0.434101, Test Loss = 0.253200, Learning Rate = 5.989209e-07\n",
      "Epoch 19527/20000: Train Loss = 0.434110, Test Loss = 0.253690, Learning Rate = 5.986933e-07\n",
      "Epoch 19528/20000: Train Loss = 0.434012, Test Loss = 0.253757, Learning Rate = 5.984658e-07\n",
      "Epoch 19529/20000: Train Loss = 0.434006, Test Loss = 0.253007, Learning Rate = 5.982384e-07\n",
      "Epoch 19530/20000: Train Loss = 0.433972, Test Loss = 0.253406, Learning Rate = 5.980111e-07\n",
      "Epoch 19531/20000: Train Loss = 0.433973, Test Loss = 0.253138, Learning Rate = 5.977839e-07\n",
      "Epoch 19532/20000: Train Loss = 0.433993, Test Loss = 0.252875, Learning Rate = 5.975568e-07\n",
      "Epoch 19533/20000: Train Loss = 0.433996, Test Loss = 0.252329, Learning Rate = 5.973297e-07\n",
      "Epoch 19534/20000: Train Loss = 0.434140, Test Loss = 0.252596, Learning Rate = 5.971027e-07\n",
      "Epoch 19535/20000: Train Loss = 0.434132, Test Loss = 0.252380, Learning Rate = 5.968758e-07\n",
      "Epoch 19536/20000: Train Loss = 0.434099, Test Loss = 0.253086, Learning Rate = 5.966490e-07\n",
      "Epoch 19537/20000: Train Loss = 0.434022, Test Loss = 0.253046, Learning Rate = 5.964223e-07\n",
      "Epoch 19538/20000: Train Loss = 0.434003, Test Loss = 0.253223, Learning Rate = 5.961957e-07\n",
      "Epoch 19539/20000: Train Loss = 0.434000, Test Loss = 0.252707, Learning Rate = 5.959692e-07\n",
      "Epoch 19540/20000: Train Loss = 0.433994, Test Loss = 0.252525, Learning Rate = 5.957427e-07\n",
      "Epoch 19541/20000: Train Loss = 0.434010, Test Loss = 0.252743, Learning Rate = 5.955164e-07\n",
      "Epoch 19542/20000: Train Loss = 0.433958, Test Loss = 0.252401, Learning Rate = 5.952901e-07\n",
      "Epoch 19543/20000: Train Loss = 0.433978, Test Loss = 0.252306, Learning Rate = 5.950639e-07\n",
      "Epoch 19544/20000: Train Loss = 0.434017, Test Loss = 0.252715, Learning Rate = 5.948378e-07\n",
      "Epoch 19545/20000: Train Loss = 0.434058, Test Loss = 0.253230, Learning Rate = 5.946118e-07\n",
      "Epoch 19546/20000: Train Loss = 0.434009, Test Loss = 0.252785, Learning Rate = 5.943858e-07\n",
      "Epoch 19547/20000: Train Loss = 0.434037, Test Loss = 0.252888, Learning Rate = 5.941600e-07\n",
      "Epoch 19548/20000: Train Loss = 0.434121, Test Loss = 0.252355, Learning Rate = 5.939342e-07\n",
      "Epoch 19549/20000: Train Loss = 0.434078, Test Loss = 0.252459, Learning Rate = 5.937085e-07\n",
      "Epoch 19550/20000: Train Loss = 0.434016, Test Loss = 0.252424, Learning Rate = 5.934829e-07\n",
      "Epoch 19551/20000: Train Loss = 0.433969, Test Loss = 0.252665, Learning Rate = 5.932574e-07\n",
      "Epoch 19552/20000: Train Loss = 0.433972, Test Loss = 0.252587, Learning Rate = 5.930320e-07\n",
      "Epoch 19553/20000: Train Loss = 0.434010, Test Loss = 0.253004, Learning Rate = 5.928067e-07\n",
      "Epoch 19554/20000: Train Loss = 0.434137, Test Loss = 0.252469, Learning Rate = 5.925814e-07\n",
      "Epoch 19555/20000: Train Loss = 0.434048, Test Loss = 0.252472, Learning Rate = 5.923562e-07\n",
      "Epoch 19556/20000: Train Loss = 0.434028, Test Loss = 0.252599, Learning Rate = 5.921312e-07\n",
      "Epoch 19557/20000: Train Loss = 0.434023, Test Loss = 0.252176, Learning Rate = 5.919062e-07\n",
      "Epoch 19558/20000: Train Loss = 0.434117, Test Loss = 0.252665, Learning Rate = 5.916813e-07\n",
      "Epoch 19559/20000: Train Loss = 0.433992, Test Loss = 0.252940, Learning Rate = 5.914564e-07\n",
      "Epoch 19560/20000: Train Loss = 0.433961, Test Loss = 0.253332, Learning Rate = 5.912317e-07\n",
      "Epoch 19561/20000: Train Loss = 0.434030, Test Loss = 0.253131, Learning Rate = 5.910071e-07\n",
      "Epoch 19562/20000: Train Loss = 0.433963, Test Loss = 0.252878, Learning Rate = 5.907825e-07\n",
      "Epoch 19563/20000: Train Loss = 0.433970, Test Loss = 0.252579, Learning Rate = 5.905580e-07\n",
      "Epoch 19564/20000: Train Loss = 0.434006, Test Loss = 0.252314, Learning Rate = 5.903336e-07\n",
      "Epoch 19565/20000: Train Loss = 0.433991, Test Loss = 0.252097, Learning Rate = 5.901093e-07\n",
      "Epoch 19566/20000: Train Loss = 0.434035, Test Loss = 0.252208, Learning Rate = 5.898851e-07\n",
      "Epoch 19567/20000: Train Loss = 0.434017, Test Loss = 0.252338, Learning Rate = 5.896609e-07\n",
      "Epoch 19568/20000: Train Loss = 0.434168, Test Loss = 0.251919, Learning Rate = 5.894369e-07\n",
      "Epoch 19569/20000: Train Loss = 0.433973, Test Loss = 0.252153, Learning Rate = 5.892129e-07\n",
      "Epoch 19570/20000: Train Loss = 0.434011, Test Loss = 0.252040, Learning Rate = 5.889890e-07\n",
      "Epoch 19571/20000: Train Loss = 0.433983, Test Loss = 0.252567, Learning Rate = 5.887652e-07\n",
      "Epoch 19572/20000: Train Loss = 0.434040, Test Loss = 0.252672, Learning Rate = 5.885415e-07\n",
      "Epoch 19573/20000: Train Loss = 0.433963, Test Loss = 0.252649, Learning Rate = 5.883179e-07\n",
      "Epoch 19574/20000: Train Loss = 0.433994, Test Loss = 0.253144, Learning Rate = 5.880943e-07\n",
      "Epoch 19575/20000: Train Loss = 0.434047, Test Loss = 0.252565, Learning Rate = 5.878709e-07\n",
      "Epoch 19576/20000: Train Loss = 0.434092, Test Loss = 0.252792, Learning Rate = 5.876475e-07\n",
      "Epoch 19577/20000: Train Loss = 0.433961, Test Loss = 0.252875, Learning Rate = 5.874242e-07\n",
      "Epoch 19578/20000: Train Loss = 0.433988, Test Loss = 0.252548, Learning Rate = 5.872010e-07\n",
      "Epoch 19579/20000: Train Loss = 0.433999, Test Loss = 0.252853, Learning Rate = 5.869779e-07\n",
      "Epoch 19580/20000: Train Loss = 0.434004, Test Loss = 0.253242, Learning Rate = 5.867548e-07\n",
      "Epoch 19581/20000: Train Loss = 0.434005, Test Loss = 0.253272, Learning Rate = 5.865319e-07\n",
      "Epoch 19582/20000: Train Loss = 0.433987, Test Loss = 0.253482, Learning Rate = 5.863090e-07\n",
      "Epoch 19583/20000: Train Loss = 0.433963, Test Loss = 0.253329, Learning Rate = 5.860862e-07\n",
      "Epoch 19584/20000: Train Loss = 0.433966, Test Loss = 0.253073, Learning Rate = 5.858636e-07\n",
      "Epoch 19585/20000: Train Loss = 0.433971, Test Loss = 0.253143, Learning Rate = 5.856409e-07\n",
      "Epoch 19586/20000: Train Loss = 0.433966, Test Loss = 0.253103, Learning Rate = 5.854184e-07\n",
      "Epoch 19587/20000: Train Loss = 0.434008, Test Loss = 0.252915, Learning Rate = 5.851960e-07\n",
      "Epoch 19588/20000: Train Loss = 0.433991, Test Loss = 0.253628, Learning Rate = 5.849736e-07\n",
      "Epoch 19589/20000: Train Loss = 0.434015, Test Loss = 0.254254, Learning Rate = 5.847513e-07\n",
      "Epoch 19590/20000: Train Loss = 0.434153, Test Loss = 0.253887, Learning Rate = 5.845291e-07\n",
      "Epoch 19591/20000: Train Loss = 0.433954, Test Loss = 0.253345, Learning Rate = 5.843070e-07\n",
      "Epoch 19592/20000: Train Loss = 0.434080, Test Loss = 0.253091, Learning Rate = 5.840850e-07\n",
      "Epoch 19593/20000: Train Loss = 0.434020, Test Loss = 0.253574, Learning Rate = 5.838631e-07\n",
      "Epoch 19594/20000: Train Loss = 0.434177, Test Loss = 0.253973, Learning Rate = 5.836412e-07\n",
      "Epoch 19595/20000: Train Loss = 0.433996, Test Loss = 0.253719, Learning Rate = 5.834195e-07\n",
      "Epoch 19596/20000: Train Loss = 0.433969, Test Loss = 0.253749, Learning Rate = 5.831978e-07\n",
      "Epoch 19597/20000: Train Loss = 0.434038, Test Loss = 0.253277, Learning Rate = 5.829762e-07\n",
      "Epoch 19598/20000: Train Loss = 0.433987, Test Loss = 0.253158, Learning Rate = 5.827547e-07\n",
      "Epoch 19599/20000: Train Loss = 0.434183, Test Loss = 0.253229, Learning Rate = 5.825332e-07\n",
      "Epoch 19600/20000: Train Loss = 0.434081, Test Loss = 0.253787, Learning Rate = 5.823119e-07\n",
      "Epoch 19601/20000: Train Loss = 0.433971, Test Loss = 0.253481, Learning Rate = 5.820906e-07\n",
      "Epoch 19602/20000: Train Loss = 0.433984, Test Loss = 0.253185, Learning Rate = 5.818694e-07\n",
      "Epoch 19603/20000: Train Loss = 0.434040, Test Loss = 0.252958, Learning Rate = 5.816484e-07\n",
      "Epoch 19604/20000: Train Loss = 0.433950, Test Loss = 0.253251, Learning Rate = 5.814273e-07\n",
      "Epoch 19605/20000: Train Loss = 0.433962, Test Loss = 0.253294, Learning Rate = 5.812064e-07\n",
      "Epoch 19606/20000: Train Loss = 0.433965, Test Loss = 0.253488, Learning Rate = 5.809856e-07\n",
      "Epoch 19607/20000: Train Loss = 0.434058, Test Loss = 0.254308, Learning Rate = 5.807648e-07\n",
      "Epoch 19608/20000: Train Loss = 0.433958, Test Loss = 0.253890, Learning Rate = 5.805441e-07\n",
      "Epoch 19609/20000: Train Loss = 0.433973, Test Loss = 0.253739, Learning Rate = 5.803235e-07\n",
      "Epoch 19610/20000: Train Loss = 0.433969, Test Loss = 0.253593, Learning Rate = 5.801030e-07\n",
      "Epoch 19611/20000: Train Loss = 0.433965, Test Loss = 0.253592, Learning Rate = 5.798826e-07\n",
      "Epoch 19612/20000: Train Loss = 0.434086, Test Loss = 0.253143, Learning Rate = 5.796623e-07\n",
      "Epoch 19613/20000: Train Loss = 0.434053, Test Loss = 0.253301, Learning Rate = 5.794420e-07\n",
      "Epoch 19614/20000: Train Loss = 0.433961, Test Loss = 0.252981, Learning Rate = 5.792218e-07\n",
      "Epoch 19615/20000: Train Loss = 0.434052, Test Loss = 0.253260, Learning Rate = 5.790018e-07\n",
      "Epoch 19616/20000: Train Loss = 0.434074, Test Loss = 0.252823, Learning Rate = 5.787818e-07\n",
      "Epoch 19617/20000: Train Loss = 0.433985, Test Loss = 0.253137, Learning Rate = 5.785618e-07\n",
      "Epoch 19618/20000: Train Loss = 0.434016, Test Loss = 0.253375, Learning Rate = 5.783420e-07\n",
      "Epoch 19619/20000: Train Loss = 0.433968, Test Loss = 0.253291, Learning Rate = 5.781222e-07\n",
      "Epoch 19620/20000: Train Loss = 0.434011, Test Loss = 0.253104, Learning Rate = 5.779026e-07\n",
      "Epoch 19621/20000: Train Loss = 0.434059, Test Loss = 0.253237, Learning Rate = 5.776830e-07\n",
      "Epoch 19622/20000: Train Loss = 0.434097, Test Loss = 0.253471, Learning Rate = 5.774635e-07\n",
      "Epoch 19623/20000: Train Loss = 0.434060, Test Loss = 0.253424, Learning Rate = 5.772441e-07\n",
      "Epoch 19624/20000: Train Loss = 0.434003, Test Loss = 0.252403, Learning Rate = 5.770247e-07\n",
      "Epoch 19625/20000: Train Loss = 0.434027, Test Loss = 0.252593, Learning Rate = 5.768055e-07\n",
      "Epoch 19626/20000: Train Loss = 0.433984, Test Loss = 0.252497, Learning Rate = 5.765863e-07\n",
      "Epoch 19627/20000: Train Loss = 0.433962, Test Loss = 0.252464, Learning Rate = 5.763672e-07\n",
      "Epoch 19628/20000: Train Loss = 0.433983, Test Loss = 0.252246, Learning Rate = 5.761482e-07\n",
      "Epoch 19629/20000: Train Loss = 0.433981, Test Loss = 0.252430, Learning Rate = 5.759293e-07\n",
      "Epoch 19630/20000: Train Loss = 0.434081, Test Loss = 0.252160, Learning Rate = 5.757104e-07\n",
      "Epoch 19631/20000: Train Loss = 0.434082, Test Loss = 0.252858, Learning Rate = 5.754917e-07\n",
      "Epoch 19632/20000: Train Loss = 0.433963, Test Loss = 0.252438, Learning Rate = 5.752730e-07\n",
      "Epoch 19633/20000: Train Loss = 0.434024, Test Loss = 0.252265, Learning Rate = 5.750544e-07\n",
      "Epoch 19634/20000: Train Loss = 0.434016, Test Loss = 0.252055, Learning Rate = 5.748359e-07\n",
      "Epoch 19635/20000: Train Loss = 0.434044, Test Loss = 0.251653, Learning Rate = 5.746175e-07\n",
      "Epoch 19636/20000: Train Loss = 0.433949, Test Loss = 0.252408, Learning Rate = 5.743992e-07\n",
      "Epoch 19637/20000: Train Loss = 0.433971, Test Loss = 0.252418, Learning Rate = 5.741809e-07\n",
      "Epoch 19638/20000: Train Loss = 0.433992, Test Loss = 0.252377, Learning Rate = 5.739627e-07\n",
      "Epoch 19639/20000: Train Loss = 0.433961, Test Loss = 0.252321, Learning Rate = 5.737446e-07\n",
      "Epoch 19640/20000: Train Loss = 0.434063, Test Loss = 0.252527, Learning Rate = 5.735266e-07\n",
      "Epoch 19641/20000: Train Loss = 0.433928, Test Loss = 0.252265, Learning Rate = 5.733087e-07\n",
      "Epoch 19642/20000: Train Loss = 0.434001, Test Loss = 0.252097, Learning Rate = 5.730909e-07\n",
      "Epoch 19643/20000: Train Loss = 0.434014, Test Loss = 0.251785, Learning Rate = 5.728731e-07\n",
      "Epoch 19644/20000: Train Loss = 0.433982, Test Loss = 0.252362, Learning Rate = 5.726554e-07\n",
      "Epoch 19645/20000: Train Loss = 0.433968, Test Loss = 0.252043, Learning Rate = 5.724378e-07\n",
      "Epoch 19646/20000: Train Loss = 0.434097, Test Loss = 0.252290, Learning Rate = 5.722203e-07\n",
      "Epoch 19647/20000: Train Loss = 0.434036, Test Loss = 0.251975, Learning Rate = 5.720029e-07\n",
      "Epoch 19648/20000: Train Loss = 0.434150, Test Loss = 0.251791, Learning Rate = 5.717856e-07\n",
      "Epoch 19649/20000: Train Loss = 0.433948, Test Loss = 0.252248, Learning Rate = 5.715683e-07\n",
      "Epoch 19650/20000: Train Loss = 0.433969, Test Loss = 0.252091, Learning Rate = 5.713511e-07\n",
      "Epoch 19651/20000: Train Loss = 0.434030, Test Loss = 0.251641, Learning Rate = 5.711340e-07\n",
      "Epoch 19652/20000: Train Loss = 0.434144, Test Loss = 0.251565, Learning Rate = 5.709170e-07\n",
      "Epoch 19653/20000: Train Loss = 0.433992, Test Loss = 0.252269, Learning Rate = 5.707001e-07\n",
      "Epoch 19654/20000: Train Loss = 0.433975, Test Loss = 0.252520, Learning Rate = 5.704832e-07\n",
      "Epoch 19655/20000: Train Loss = 0.434029, Test Loss = 0.252570, Learning Rate = 5.702665e-07\n",
      "Epoch 19656/20000: Train Loss = 0.433963, Test Loss = 0.252506, Learning Rate = 5.700498e-07\n",
      "Epoch 19657/20000: Train Loss = 0.434088, Test Loss = 0.252158, Learning Rate = 5.698332e-07\n",
      "Epoch 19658/20000: Train Loss = 0.434027, Test Loss = 0.252013, Learning Rate = 5.696166e-07\n",
      "Epoch 19659/20000: Train Loss = 0.433973, Test Loss = 0.252708, Learning Rate = 5.694002e-07\n",
      "Epoch 19660/20000: Train Loss = 0.433979, Test Loss = 0.252776, Learning Rate = 5.691838e-07\n",
      "Epoch 19661/20000: Train Loss = 0.433974, Test Loss = 0.252684, Learning Rate = 5.689676e-07\n",
      "Epoch 19662/20000: Train Loss = 0.434037, Test Loss = 0.252749, Learning Rate = 5.687514e-07\n",
      "Epoch 19663/20000: Train Loss = 0.434078, Test Loss = 0.252785, Learning Rate = 5.685353e-07\n",
      "Epoch 19664/20000: Train Loss = 0.434013, Test Loss = 0.252751, Learning Rate = 5.683192e-07\n",
      "Epoch 19665/20000: Train Loss = 0.434014, Test Loss = 0.252647, Learning Rate = 5.681033e-07\n",
      "Epoch 19666/20000: Train Loss = 0.433996, Test Loss = 0.252717, Learning Rate = 5.678874e-07\n",
      "Epoch 19667/20000: Train Loss = 0.433994, Test Loss = 0.253212, Learning Rate = 5.676716e-07\n",
      "Epoch 19668/20000: Train Loss = 0.433971, Test Loss = 0.253226, Learning Rate = 5.674559e-07\n",
      "Epoch 19669/20000: Train Loss = 0.434003, Test Loss = 0.252678, Learning Rate = 5.672403e-07\n",
      "Epoch 19670/20000: Train Loss = 0.433977, Test Loss = 0.252689, Learning Rate = 5.670248e-07\n",
      "Epoch 19671/20000: Train Loss = 0.434006, Test Loss = 0.253211, Learning Rate = 5.668093e-07\n",
      "Epoch 19672/20000: Train Loss = 0.434058, Test Loss = 0.252931, Learning Rate = 5.665940e-07\n",
      "Epoch 19673/20000: Train Loss = 0.434104, Test Loss = 0.252724, Learning Rate = 5.663787e-07\n",
      "Epoch 19674/20000: Train Loss = 0.434026, Test Loss = 0.253187, Learning Rate = 5.661635e-07\n",
      "Epoch 19675/20000: Train Loss = 0.433960, Test Loss = 0.252974, Learning Rate = 5.659483e-07\n",
      "Epoch 19676/20000: Train Loss = 0.433995, Test Loss = 0.252892, Learning Rate = 5.657333e-07\n",
      "Epoch 19677/20000: Train Loss = 0.434056, Test Loss = 0.252906, Learning Rate = 5.655183e-07\n",
      "Epoch 19678/20000: Train Loss = 0.434152, Test Loss = 0.253042, Learning Rate = 5.653035e-07\n",
      "Epoch 19679/20000: Train Loss = 0.434067, Test Loss = 0.252776, Learning Rate = 5.650887e-07\n",
      "Epoch 19680/20000: Train Loss = 0.434020, Test Loss = 0.252609, Learning Rate = 5.648739e-07\n",
      "Epoch 19681/20000: Train Loss = 0.433970, Test Loss = 0.252550, Learning Rate = 5.646593e-07\n",
      "Epoch 19682/20000: Train Loss = 0.434035, Test Loss = 0.252427, Learning Rate = 5.644447e-07\n",
      "Epoch 19683/20000: Train Loss = 0.433988, Test Loss = 0.252720, Learning Rate = 5.642303e-07\n",
      "Epoch 19684/20000: Train Loss = 0.434028, Test Loss = 0.252910, Learning Rate = 5.640159e-07\n",
      "Epoch 19685/20000: Train Loss = 0.434011, Test Loss = 0.252620, Learning Rate = 5.638016e-07\n",
      "Epoch 19686/20000: Train Loss = 0.433947, Test Loss = 0.252607, Learning Rate = 5.635873e-07\n",
      "Epoch 19687/20000: Train Loss = 0.433954, Test Loss = 0.252993, Learning Rate = 5.633732e-07\n",
      "Epoch 19688/20000: Train Loss = 0.434000, Test Loss = 0.252773, Learning Rate = 5.631591e-07\n",
      "Epoch 19689/20000: Train Loss = 0.433982, Test Loss = 0.252438, Learning Rate = 5.629451e-07\n",
      "Epoch 19690/20000: Train Loss = 0.434056, Test Loss = 0.252285, Learning Rate = 5.627312e-07\n",
      "Epoch 19691/20000: Train Loss = 0.433995, Test Loss = 0.252726, Learning Rate = 5.625174e-07\n",
      "Epoch 19692/20000: Train Loss = 0.434225, Test Loss = 0.252624, Learning Rate = 5.623037e-07\n",
      "Epoch 19693/20000: Train Loss = 0.433970, Test Loss = 0.252430, Learning Rate = 5.620900e-07\n",
      "Epoch 19694/20000: Train Loss = 0.433952, Test Loss = 0.252486, Learning Rate = 5.618764e-07\n",
      "Epoch 19695/20000: Train Loss = 0.433994, Test Loss = 0.252446, Learning Rate = 5.616629e-07\n",
      "Epoch 19696/20000: Train Loss = 0.434058, Test Loss = 0.252954, Learning Rate = 5.614495e-07\n",
      "Epoch 19697/20000: Train Loss = 0.434078, Test Loss = 0.252491, Learning Rate = 5.612362e-07\n",
      "Epoch 19698/20000: Train Loss = 0.433973, Test Loss = 0.252412, Learning Rate = 5.610229e-07\n",
      "Epoch 19699/20000: Train Loss = 0.433990, Test Loss = 0.252058, Learning Rate = 5.608098e-07\n",
      "Epoch 19700/20000: Train Loss = 0.434056, Test Loss = 0.252141, Learning Rate = 5.605967e-07\n",
      "Epoch 19701/20000: Train Loss = 0.433960, Test Loss = 0.252330, Learning Rate = 5.603836e-07\n",
      "Epoch 19702/20000: Train Loss = 0.434072, Test Loss = 0.251826, Learning Rate = 5.601707e-07\n",
      "Epoch 19703/20000: Train Loss = 0.433996, Test Loss = 0.252614, Learning Rate = 5.599579e-07\n",
      "Epoch 19704/20000: Train Loss = 0.433943, Test Loss = 0.252632, Learning Rate = 5.597451e-07\n",
      "Epoch 19705/20000: Train Loss = 0.433948, Test Loss = 0.252412, Learning Rate = 5.595324e-07\n",
      "Epoch 19706/20000: Train Loss = 0.433958, Test Loss = 0.252316, Learning Rate = 5.593198e-07\n",
      "Epoch 19707/20000: Train Loss = 0.434032, Test Loss = 0.252638, Learning Rate = 5.591073e-07\n",
      "Epoch 19708/20000: Train Loss = 0.433977, Test Loss = 0.253038, Learning Rate = 5.588948e-07\n",
      "Epoch 19709/20000: Train Loss = 0.433966, Test Loss = 0.252777, Learning Rate = 5.586825e-07\n",
      "Epoch 19710/20000: Train Loss = 0.433996, Test Loss = 0.253296, Learning Rate = 5.584702e-07\n",
      "Epoch 19711/20000: Train Loss = 0.434055, Test Loss = 0.253115, Learning Rate = 5.582580e-07\n",
      "Epoch 19712/20000: Train Loss = 0.433952, Test Loss = 0.253512, Learning Rate = 5.580459e-07\n",
      "Epoch 19713/20000: Train Loss = 0.434042, Test Loss = 0.253439, Learning Rate = 5.578338e-07\n",
      "Epoch 19714/20000: Train Loss = 0.434026, Test Loss = 0.254010, Learning Rate = 5.576219e-07\n",
      "Epoch 19715/20000: Train Loss = 0.433995, Test Loss = 0.253294, Learning Rate = 5.574100e-07\n",
      "Epoch 19716/20000: Train Loss = 0.434011, Test Loss = 0.253676, Learning Rate = 5.571982e-07\n",
      "Epoch 19717/20000: Train Loss = 0.434021, Test Loss = 0.253489, Learning Rate = 5.569865e-07\n",
      "Epoch 19718/20000: Train Loss = 0.434153, Test Loss = 0.253046, Learning Rate = 5.567748e-07\n",
      "Epoch 19719/20000: Train Loss = 0.434001, Test Loss = 0.253608, Learning Rate = 5.565633e-07\n",
      "Epoch 19720/20000: Train Loss = 0.433954, Test Loss = 0.253116, Learning Rate = 5.563518e-07\n",
      "Epoch 19721/20000: Train Loss = 0.434044, Test Loss = 0.253062, Learning Rate = 5.561404e-07\n",
      "Epoch 19722/20000: Train Loss = 0.434143, Test Loss = 0.252652, Learning Rate = 5.559291e-07\n",
      "Epoch 19723/20000: Train Loss = 0.433943, Test Loss = 0.252309, Learning Rate = 5.557178e-07\n",
      "Epoch 19724/20000: Train Loss = 0.433999, Test Loss = 0.252806, Learning Rate = 5.555067e-07\n",
      "Epoch 19725/20000: Train Loss = 0.434032, Test Loss = 0.252918, Learning Rate = 5.552956e-07\n",
      "Epoch 19726/20000: Train Loss = 0.433977, Test Loss = 0.252806, Learning Rate = 5.550846e-07\n",
      "Epoch 19727/20000: Train Loss = 0.434001, Test Loss = 0.252759, Learning Rate = 5.548737e-07\n",
      "Epoch 19728/20000: Train Loss = 0.433991, Test Loss = 0.252700, Learning Rate = 5.546628e-07\n",
      "Epoch 19729/20000: Train Loss = 0.433918, Test Loss = 0.253229, Learning Rate = 5.544521e-07\n",
      "Epoch 19730/20000: Train Loss = 0.433995, Test Loss = 0.253414, Learning Rate = 5.542414e-07\n",
      "Epoch 19731/20000: Train Loss = 0.433990, Test Loss = 0.253364, Learning Rate = 5.540308e-07\n",
      "Epoch 19732/20000: Train Loss = 0.434009, Test Loss = 0.253507, Learning Rate = 5.538203e-07\n",
      "Epoch 19733/20000: Train Loss = 0.433971, Test Loss = 0.253626, Learning Rate = 5.536098e-07\n",
      "Epoch 19734/20000: Train Loss = 0.433995, Test Loss = 0.253193, Learning Rate = 5.533995e-07\n",
      "Epoch 19735/20000: Train Loss = 0.433991, Test Loss = 0.253176, Learning Rate = 5.531892e-07\n",
      "Epoch 19736/20000: Train Loss = 0.433986, Test Loss = 0.253061, Learning Rate = 5.529790e-07\n",
      "Epoch 19737/20000: Train Loss = 0.434012, Test Loss = 0.253418, Learning Rate = 5.527689e-07\n",
      "Epoch 19738/20000: Train Loss = 0.433993, Test Loss = 0.253377, Learning Rate = 5.525589e-07\n",
      "Epoch 19739/20000: Train Loss = 0.433966, Test Loss = 0.253565, Learning Rate = 5.523489e-07\n",
      "Epoch 19740/20000: Train Loss = 0.434111, Test Loss = 0.254170, Learning Rate = 5.521390e-07\n",
      "Epoch 19741/20000: Train Loss = 0.433987, Test Loss = 0.254186, Learning Rate = 5.519292e-07\n",
      "Epoch 19742/20000: Train Loss = 0.433996, Test Loss = 0.254380, Learning Rate = 5.517195e-07\n",
      "Epoch 19743/20000: Train Loss = 0.433946, Test Loss = 0.253960, Learning Rate = 5.515099e-07\n",
      "Epoch 19744/20000: Train Loss = 0.433972, Test Loss = 0.253617, Learning Rate = 5.513003e-07\n",
      "Epoch 19745/20000: Train Loss = 0.434007, Test Loss = 0.253973, Learning Rate = 5.510908e-07\n",
      "Epoch 19746/20000: Train Loss = 0.434040, Test Loss = 0.253499, Learning Rate = 5.508814e-07\n",
      "Epoch 19747/20000: Train Loss = 0.434143, Test Loss = 0.253522, Learning Rate = 5.506721e-07\n",
      "Epoch 19748/20000: Train Loss = 0.433950, Test Loss = 0.253259, Learning Rate = 5.504629e-07\n",
      "Epoch 19749/20000: Train Loss = 0.434021, Test Loss = 0.253697, Learning Rate = 5.502537e-07\n",
      "Epoch 19750/20000: Train Loss = 0.433932, Test Loss = 0.253761, Learning Rate = 5.500446e-07\n",
      "Epoch 19751/20000: Train Loss = 0.434001, Test Loss = 0.253564, Learning Rate = 5.498356e-07\n",
      "Epoch 19752/20000: Train Loss = 0.433986, Test Loss = 0.253718, Learning Rate = 5.496267e-07\n",
      "Epoch 19753/20000: Train Loss = 0.433981, Test Loss = 0.253504, Learning Rate = 5.494179e-07\n",
      "Epoch 19754/20000: Train Loss = 0.434004, Test Loss = 0.253411, Learning Rate = 5.492091e-07\n",
      "Epoch 19755/20000: Train Loss = 0.433939, Test Loss = 0.253410, Learning Rate = 5.490004e-07\n",
      "Epoch 19756/20000: Train Loss = 0.433963, Test Loss = 0.253729, Learning Rate = 5.487918e-07\n",
      "Epoch 19757/20000: Train Loss = 0.433939, Test Loss = 0.253907, Learning Rate = 5.485833e-07\n",
      "Epoch 19758/20000: Train Loss = 0.433984, Test Loss = 0.253799, Learning Rate = 5.483748e-07\n",
      "Epoch 19759/20000: Train Loss = 0.433947, Test Loss = 0.254017, Learning Rate = 5.481665e-07\n",
      "Epoch 19760/20000: Train Loss = 0.433992, Test Loss = 0.253631, Learning Rate = 5.479582e-07\n",
      "Epoch 19761/20000: Train Loss = 0.433963, Test Loss = 0.254147, Learning Rate = 5.477500e-07\n",
      "Epoch 19762/20000: Train Loss = 0.433950, Test Loss = 0.254061, Learning Rate = 5.475418e-07\n",
      "Epoch 19763/20000: Train Loss = 0.433974, Test Loss = 0.254082, Learning Rate = 5.473338e-07\n",
      "Epoch 19764/20000: Train Loss = 0.433981, Test Loss = 0.254086, Learning Rate = 5.471258e-07\n",
      "Epoch 19765/20000: Train Loss = 0.434009, Test Loss = 0.254371, Learning Rate = 5.469179e-07\n",
      "Epoch 19766/20000: Train Loss = 0.434006, Test Loss = 0.254432, Learning Rate = 5.467101e-07\n",
      "Epoch 19767/20000: Train Loss = 0.433954, Test Loss = 0.254211, Learning Rate = 5.465024e-07\n",
      "Epoch 19768/20000: Train Loss = 0.434066, Test Loss = 0.254469, Learning Rate = 5.462947e-07\n",
      "Epoch 19769/20000: Train Loss = 0.433953, Test Loss = 0.254114, Learning Rate = 5.460871e-07\n",
      "Epoch 19770/20000: Train Loss = 0.433982, Test Loss = 0.253825, Learning Rate = 5.458796e-07\n",
      "Epoch 19771/20000: Train Loss = 0.433994, Test Loss = 0.253925, Learning Rate = 5.456722e-07\n",
      "Epoch 19772/20000: Train Loss = 0.434045, Test Loss = 0.254139, Learning Rate = 5.454649e-07\n",
      "Epoch 19773/20000: Train Loss = 0.433948, Test Loss = 0.253780, Learning Rate = 5.452576e-07\n",
      "Epoch 19774/20000: Train Loss = 0.433978, Test Loss = 0.253654, Learning Rate = 5.450504e-07\n",
      "Epoch 19775/20000: Train Loss = 0.434098, Test Loss = 0.253926, Learning Rate = 5.448433e-07\n",
      "Epoch 19776/20000: Train Loss = 0.434065, Test Loss = 0.254205, Learning Rate = 5.446363e-07\n",
      "Epoch 19777/20000: Train Loss = 0.433982, Test Loss = 0.253902, Learning Rate = 5.444294e-07\n",
      "Epoch 19778/20000: Train Loss = 0.433958, Test Loss = 0.253989, Learning Rate = 5.442225e-07\n",
      "Epoch 19779/20000: Train Loss = 0.433985, Test Loss = 0.253267, Learning Rate = 5.440157e-07\n",
      "Epoch 19780/20000: Train Loss = 0.433966, Test Loss = 0.252967, Learning Rate = 5.438090e-07\n",
      "Epoch 19781/20000: Train Loss = 0.434059, Test Loss = 0.252873, Learning Rate = 5.436024e-07\n",
      "Epoch 19782/20000: Train Loss = 0.433967, Test Loss = 0.253043, Learning Rate = 5.433958e-07\n",
      "Epoch 19783/20000: Train Loss = 0.433973, Test Loss = 0.253114, Learning Rate = 5.431893e-07\n",
      "Epoch 19784/20000: Train Loss = 0.434073, Test Loss = 0.253472, Learning Rate = 5.429829e-07\n",
      "Epoch 19785/20000: Train Loss = 0.434006, Test Loss = 0.253380, Learning Rate = 5.427766e-07\n",
      "Epoch 19786/20000: Train Loss = 0.433986, Test Loss = 0.253207, Learning Rate = 5.425704e-07\n",
      "Epoch 19787/20000: Train Loss = 0.434059, Test Loss = 0.252970, Learning Rate = 5.423642e-07\n",
      "Epoch 19788/20000: Train Loss = 0.433977, Test Loss = 0.252991, Learning Rate = 5.421581e-07\n",
      "Epoch 19789/20000: Train Loss = 0.433957, Test Loss = 0.253189, Learning Rate = 5.419521e-07\n",
      "Epoch 19790/20000: Train Loss = 0.433963, Test Loss = 0.252883, Learning Rate = 5.417462e-07\n",
      "Epoch 19791/20000: Train Loss = 0.433936, Test Loss = 0.252936, Learning Rate = 5.415403e-07\n",
      "Epoch 19792/20000: Train Loss = 0.433952, Test Loss = 0.253213, Learning Rate = 5.413346e-07\n",
      "Epoch 19793/20000: Train Loss = 0.434030, Test Loss = 0.252628, Learning Rate = 5.411289e-07\n",
      "Epoch 19794/20000: Train Loss = 0.433976, Test Loss = 0.252797, Learning Rate = 5.409233e-07\n",
      "Epoch 19795/20000: Train Loss = 0.434117, Test Loss = 0.252499, Learning Rate = 5.407177e-07\n",
      "Epoch 19796/20000: Train Loss = 0.433945, Test Loss = 0.252626, Learning Rate = 5.405123e-07\n",
      "Epoch 19797/20000: Train Loss = 0.433968, Test Loss = 0.252844, Learning Rate = 5.403069e-07\n",
      "Epoch 19798/20000: Train Loss = 0.434151, Test Loss = 0.252457, Learning Rate = 5.401016e-07\n",
      "Epoch 19799/20000: Train Loss = 0.434049, Test Loss = 0.252238, Learning Rate = 5.398964e-07\n",
      "Epoch 19800/20000: Train Loss = 0.433928, Test Loss = 0.252779, Learning Rate = 5.396912e-07\n",
      "Epoch 19801/20000: Train Loss = 0.434010, Test Loss = 0.252750, Learning Rate = 5.394862e-07\n",
      "Epoch 19802/20000: Train Loss = 0.434033, Test Loss = 0.252584, Learning Rate = 5.392812e-07\n",
      "Epoch 19803/20000: Train Loss = 0.434064, Test Loss = 0.252941, Learning Rate = 5.390763e-07\n",
      "Epoch 19804/20000: Train Loss = 0.434040, Test Loss = 0.252700, Learning Rate = 5.388714e-07\n",
      "Epoch 19805/20000: Train Loss = 0.433986, Test Loss = 0.252457, Learning Rate = 5.386667e-07\n",
      "Epoch 19806/20000: Train Loss = 0.433973, Test Loss = 0.252932, Learning Rate = 5.384620e-07\n",
      "Epoch 19807/20000: Train Loss = 0.434051, Test Loss = 0.252680, Learning Rate = 5.382574e-07\n",
      "Epoch 19808/20000: Train Loss = 0.434123, Test Loss = 0.253015, Learning Rate = 5.380529e-07\n",
      "Epoch 19809/20000: Train Loss = 0.433959, Test Loss = 0.253197, Learning Rate = 5.378484e-07\n",
      "Epoch 19810/20000: Train Loss = 0.434001, Test Loss = 0.252865, Learning Rate = 5.376440e-07\n",
      "Epoch 19811/20000: Train Loss = 0.434112, Test Loss = 0.252761, Learning Rate = 5.374398e-07\n",
      "Epoch 19812/20000: Train Loss = 0.433971, Test Loss = 0.252975, Learning Rate = 5.372355e-07\n",
      "Epoch 19813/20000: Train Loss = 0.433966, Test Loss = 0.252727, Learning Rate = 5.370314e-07\n",
      "Epoch 19814/20000: Train Loss = 0.433949, Test Loss = 0.252372, Learning Rate = 5.368274e-07\n",
      "Epoch 19815/20000: Train Loss = 0.433973, Test Loss = 0.253047, Learning Rate = 5.366234e-07\n",
      "Epoch 19816/20000: Train Loss = 0.433949, Test Loss = 0.253063, Learning Rate = 5.364195e-07\n",
      "Epoch 19817/20000: Train Loss = 0.433947, Test Loss = 0.253138, Learning Rate = 5.362156e-07\n",
      "Epoch 19818/20000: Train Loss = 0.433973, Test Loss = 0.253344, Learning Rate = 5.360119e-07\n",
      "Epoch 19819/20000: Train Loss = 0.433956, Test Loss = 0.252935, Learning Rate = 5.358082e-07\n",
      "Epoch 19820/20000: Train Loss = 0.434077, Test Loss = 0.253238, Learning Rate = 5.356046e-07\n",
      "Epoch 19821/20000: Train Loss = 0.433963, Test Loss = 0.252923, Learning Rate = 5.354011e-07\n",
      "Epoch 19822/20000: Train Loss = 0.433977, Test Loss = 0.253174, Learning Rate = 5.351977e-07\n",
      "Epoch 19823/20000: Train Loss = 0.433987, Test Loss = 0.253211, Learning Rate = 5.349943e-07\n",
      "Epoch 19824/20000: Train Loss = 0.433992, Test Loss = 0.253122, Learning Rate = 5.347910e-07\n",
      "Epoch 19825/20000: Train Loss = 0.433943, Test Loss = 0.252840, Learning Rate = 5.345878e-07\n",
      "Epoch 19826/20000: Train Loss = 0.434053, Test Loss = 0.252506, Learning Rate = 5.343847e-07\n",
      "Epoch 19827/20000: Train Loss = 0.433969, Test Loss = 0.252724, Learning Rate = 5.341817e-07\n",
      "Epoch 19828/20000: Train Loss = 0.434030, Test Loss = 0.253135, Learning Rate = 5.339787e-07\n",
      "Epoch 19829/20000: Train Loss = 0.433969, Test Loss = 0.252662, Learning Rate = 5.337758e-07\n",
      "Epoch 19830/20000: Train Loss = 0.433943, Test Loss = 0.253105, Learning Rate = 5.335730e-07\n",
      "Epoch 19831/20000: Train Loss = 0.433967, Test Loss = 0.253207, Learning Rate = 5.333702e-07\n",
      "Epoch 19832/20000: Train Loss = 0.433989, Test Loss = 0.253238, Learning Rate = 5.331675e-07\n",
      "Epoch 19833/20000: Train Loss = 0.433976, Test Loss = 0.253250, Learning Rate = 5.329650e-07\n",
      "Epoch 19834/20000: Train Loss = 0.433929, Test Loss = 0.252967, Learning Rate = 5.327624e-07\n",
      "Epoch 19835/20000: Train Loss = 0.433968, Test Loss = 0.253032, Learning Rate = 5.325600e-07\n",
      "Epoch 19836/20000: Train Loss = 0.433929, Test Loss = 0.253072, Learning Rate = 5.323577e-07\n",
      "Epoch 19837/20000: Train Loss = 0.433942, Test Loss = 0.252861, Learning Rate = 5.321554e-07\n",
      "Epoch 19838/20000: Train Loss = 0.434056, Test Loss = 0.253135, Learning Rate = 5.319532e-07\n",
      "Epoch 19839/20000: Train Loss = 0.433989, Test Loss = 0.252710, Learning Rate = 5.317510e-07\n",
      "Epoch 19840/20000: Train Loss = 0.433976, Test Loss = 0.252392, Learning Rate = 5.315490e-07\n",
      "Epoch 19841/20000: Train Loss = 0.433961, Test Loss = 0.252703, Learning Rate = 5.313470e-07\n",
      "Epoch 19842/20000: Train Loss = 0.433982, Test Loss = 0.253010, Learning Rate = 5.311451e-07\n",
      "Epoch 19843/20000: Train Loss = 0.434040, Test Loss = 0.252794, Learning Rate = 5.309433e-07\n",
      "Epoch 19844/20000: Train Loss = 0.433944, Test Loss = 0.252918, Learning Rate = 5.307416e-07\n",
      "Epoch 19845/20000: Train Loss = 0.433956, Test Loss = 0.252970, Learning Rate = 5.305399e-07\n",
      "Epoch 19846/20000: Train Loss = 0.434072, Test Loss = 0.253074, Learning Rate = 5.303383e-07\n",
      "Epoch 19847/20000: Train Loss = 0.433945, Test Loss = 0.253112, Learning Rate = 5.301368e-07\n",
      "Epoch 19848/20000: Train Loss = 0.433957, Test Loss = 0.252954, Learning Rate = 5.299353e-07\n",
      "Epoch 19849/20000: Train Loss = 0.433967, Test Loss = 0.253014, Learning Rate = 5.297340e-07\n",
      "Epoch 19850/20000: Train Loss = 0.434005, Test Loss = 0.252811, Learning Rate = 5.295327e-07\n",
      "Epoch 19851/20000: Train Loss = 0.434118, Test Loss = 0.253542, Learning Rate = 5.293315e-07\n",
      "Epoch 19852/20000: Train Loss = 0.433948, Test Loss = 0.253534, Learning Rate = 5.291304e-07\n",
      "Epoch 19853/20000: Train Loss = 0.433992, Test Loss = 0.253515, Learning Rate = 5.289293e-07\n",
      "Epoch 19854/20000: Train Loss = 0.433986, Test Loss = 0.253909, Learning Rate = 5.287283e-07\n",
      "Epoch 19855/20000: Train Loss = 0.433957, Test Loss = 0.254153, Learning Rate = 5.285274e-07\n",
      "Epoch 19856/20000: Train Loss = 0.433984, Test Loss = 0.253956, Learning Rate = 5.283266e-07\n",
      "Epoch 19857/20000: Train Loss = 0.433983, Test Loss = 0.253849, Learning Rate = 5.281258e-07\n",
      "Epoch 19858/20000: Train Loss = 0.433946, Test Loss = 0.253620, Learning Rate = 5.279252e-07\n",
      "Epoch 19859/20000: Train Loss = 0.434009, Test Loss = 0.253755, Learning Rate = 5.277246e-07\n",
      "Epoch 19860/20000: Train Loss = 0.433991, Test Loss = 0.253689, Learning Rate = 5.275241e-07\n",
      "Epoch 19861/20000: Train Loss = 0.433977, Test Loss = 0.253675, Learning Rate = 5.273236e-07\n",
      "Epoch 19862/20000: Train Loss = 0.434135, Test Loss = 0.253588, Learning Rate = 5.271232e-07\n",
      "Epoch 19863/20000: Train Loss = 0.433972, Test Loss = 0.253234, Learning Rate = 5.269229e-07\n",
      "Epoch 19864/20000: Train Loss = 0.433964, Test Loss = 0.253619, Learning Rate = 5.267227e-07\n",
      "Epoch 19865/20000: Train Loss = 0.434083, Test Loss = 0.253758, Learning Rate = 5.265226e-07\n",
      "Epoch 19866/20000: Train Loss = 0.434035, Test Loss = 0.253385, Learning Rate = 5.263225e-07\n",
      "Epoch 19867/20000: Train Loss = 0.433966, Test Loss = 0.253508, Learning Rate = 5.261225e-07\n",
      "Epoch 19868/20000: Train Loss = 0.433998, Test Loss = 0.254154, Learning Rate = 5.259226e-07\n",
      "Epoch 19869/20000: Train Loss = 0.434007, Test Loss = 0.254215, Learning Rate = 5.257228e-07\n",
      "Epoch 19870/20000: Train Loss = 0.434031, Test Loss = 0.253596, Learning Rate = 5.255230e-07\n",
      "Epoch 19871/20000: Train Loss = 0.434015, Test Loss = 0.253609, Learning Rate = 5.253233e-07\n",
      "Epoch 19872/20000: Train Loss = 0.433988, Test Loss = 0.253247, Learning Rate = 5.251237e-07\n",
      "Epoch 19873/20000: Train Loss = 0.434024, Test Loss = 0.253355, Learning Rate = 5.249242e-07\n",
      "Epoch 19874/20000: Train Loss = 0.433977, Test Loss = 0.252977, Learning Rate = 5.247247e-07\n",
      "Epoch 19875/20000: Train Loss = 0.433941, Test Loss = 0.252943, Learning Rate = 5.245254e-07\n",
      "Epoch 19876/20000: Train Loss = 0.433947, Test Loss = 0.253075, Learning Rate = 5.243261e-07\n",
      "Epoch 19877/20000: Train Loss = 0.433980, Test Loss = 0.253723, Learning Rate = 5.241268e-07\n",
      "Epoch 19878/20000: Train Loss = 0.434021, Test Loss = 0.253188, Learning Rate = 5.239277e-07\n",
      "Epoch 19879/20000: Train Loss = 0.433926, Test Loss = 0.253581, Learning Rate = 5.237286e-07\n",
      "Epoch 19880/20000: Train Loss = 0.434058, Test Loss = 0.253629, Learning Rate = 5.235296e-07\n",
      "Epoch 19881/20000: Train Loss = 0.434005, Test Loss = 0.254076, Learning Rate = 5.233307e-07\n",
      "Epoch 19882/20000: Train Loss = 0.434013, Test Loss = 0.254188, Learning Rate = 5.231318e-07\n",
      "Epoch 19883/20000: Train Loss = 0.433968, Test Loss = 0.253464, Learning Rate = 5.229330e-07\n",
      "Epoch 19884/20000: Train Loss = 0.433988, Test Loss = 0.253352, Learning Rate = 5.227343e-07\n",
      "Epoch 19885/20000: Train Loss = 0.433905, Test Loss = 0.253751, Learning Rate = 5.225357e-07\n",
      "Epoch 19886/20000: Train Loss = 0.434032, Test Loss = 0.253631, Learning Rate = 5.223372e-07\n",
      "Epoch 19887/20000: Train Loss = 0.433939, Test Loss = 0.253324, Learning Rate = 5.221387e-07\n",
      "Epoch 19888/20000: Train Loss = 0.434107, Test Loss = 0.253230, Learning Rate = 5.219403e-07\n",
      "Epoch 19889/20000: Train Loss = 0.434146, Test Loss = 0.253793, Learning Rate = 5.217420e-07\n",
      "Epoch 19890/20000: Train Loss = 0.433974, Test Loss = 0.253442, Learning Rate = 5.215437e-07\n",
      "Epoch 19891/20000: Train Loss = 0.434010, Test Loss = 0.253147, Learning Rate = 5.213455e-07\n",
      "Epoch 19892/20000: Train Loss = 0.433979, Test Loss = 0.253835, Learning Rate = 5.211475e-07\n",
      "Epoch 19893/20000: Train Loss = 0.433982, Test Loss = 0.253535, Learning Rate = 5.209494e-07\n",
      "Epoch 19894/20000: Train Loss = 0.433933, Test Loss = 0.253801, Learning Rate = 5.207515e-07\n",
      "Epoch 19895/20000: Train Loss = 0.433931, Test Loss = 0.253743, Learning Rate = 5.205536e-07\n",
      "Epoch 19896/20000: Train Loss = 0.433958, Test Loss = 0.253676, Learning Rate = 5.203558e-07\n",
      "Epoch 19897/20000: Train Loss = 0.433965, Test Loss = 0.253323, Learning Rate = 5.201581e-07\n",
      "Epoch 19898/20000: Train Loss = 0.434005, Test Loss = 0.253312, Learning Rate = 5.199604e-07\n",
      "Epoch 19899/20000: Train Loss = 0.434038, Test Loss = 0.252943, Learning Rate = 5.197629e-07\n",
      "Epoch 19900/20000: Train Loss = 0.434024, Test Loss = 0.253303, Learning Rate = 5.195654e-07\n",
      "Epoch 19901/20000: Train Loss = 0.434050, Test Loss = 0.253141, Learning Rate = 5.193680e-07\n",
      "Epoch 19902/20000: Train Loss = 0.433956, Test Loss = 0.253087, Learning Rate = 5.191706e-07\n",
      "Epoch 19903/20000: Train Loss = 0.433953, Test Loss = 0.253157, Learning Rate = 5.189733e-07\n",
      "Epoch 19904/20000: Train Loss = 0.433975, Test Loss = 0.253192, Learning Rate = 5.187761e-07\n",
      "Epoch 19905/20000: Train Loss = 0.433954, Test Loss = 0.252914, Learning Rate = 5.185790e-07\n",
      "Epoch 19906/20000: Train Loss = 0.433938, Test Loss = 0.253009, Learning Rate = 5.183820e-07\n",
      "Epoch 19907/20000: Train Loss = 0.434047, Test Loss = 0.253363, Learning Rate = 5.181850e-07\n",
      "Epoch 19908/20000: Train Loss = 0.434028, Test Loss = 0.252893, Learning Rate = 5.179881e-07\n",
      "Epoch 19909/20000: Train Loss = 0.433941, Test Loss = 0.253259, Learning Rate = 5.177913e-07\n",
      "Epoch 19910/20000: Train Loss = 0.433977, Test Loss = 0.253224, Learning Rate = 5.175945e-07\n",
      "Epoch 19911/20000: Train Loss = 0.433942, Test Loss = 0.253177, Learning Rate = 5.173979e-07\n",
      "Epoch 19912/20000: Train Loss = 0.433966, Test Loss = 0.253015, Learning Rate = 5.172013e-07\n",
      "Epoch 19913/20000: Train Loss = 0.434159, Test Loss = 0.252995, Learning Rate = 5.170048e-07\n",
      "Epoch 19914/20000: Train Loss = 0.433985, Test Loss = 0.252606, Learning Rate = 5.168083e-07\n",
      "Epoch 19915/20000: Train Loss = 0.433973, Test Loss = 0.252704, Learning Rate = 5.166119e-07\n",
      "Epoch 19916/20000: Train Loss = 0.433987, Test Loss = 0.252748, Learning Rate = 5.164156e-07\n",
      "Epoch 19917/20000: Train Loss = 0.433964, Test Loss = 0.252899, Learning Rate = 5.162194e-07\n",
      "Epoch 19918/20000: Train Loss = 0.433927, Test Loss = 0.252990, Learning Rate = 5.160233e-07\n",
      "Epoch 19919/20000: Train Loss = 0.433998, Test Loss = 0.253437, Learning Rate = 5.158272e-07\n",
      "Epoch 19920/20000: Train Loss = 0.433950, Test Loss = 0.252909, Learning Rate = 5.156312e-07\n",
      "Epoch 19921/20000: Train Loss = 0.433968, Test Loss = 0.252861, Learning Rate = 5.154353e-07\n",
      "Epoch 19922/20000: Train Loss = 0.433958, Test Loss = 0.252787, Learning Rate = 5.152394e-07\n",
      "Epoch 19923/20000: Train Loss = 0.433931, Test Loss = 0.252354, Learning Rate = 5.150436e-07\n",
      "Epoch 19924/20000: Train Loss = 0.433979, Test Loss = 0.252938, Learning Rate = 5.148479e-07\n",
      "Epoch 19925/20000: Train Loss = 0.433995, Test Loss = 0.252240, Learning Rate = 5.146523e-07\n",
      "Epoch 19926/20000: Train Loss = 0.434004, Test Loss = 0.251992, Learning Rate = 5.144567e-07\n",
      "Epoch 19927/20000: Train Loss = 0.433966, Test Loss = 0.252028, Learning Rate = 5.142613e-07\n",
      "Epoch 19928/20000: Train Loss = 0.433948, Test Loss = 0.252203, Learning Rate = 5.140659e-07\n",
      "Epoch 19929/20000: Train Loss = 0.434038, Test Loss = 0.251974, Learning Rate = 5.138705e-07\n",
      "Epoch 19930/20000: Train Loss = 0.433955, Test Loss = 0.251824, Learning Rate = 5.136753e-07\n",
      "Epoch 19931/20000: Train Loss = 0.433935, Test Loss = 0.251842, Learning Rate = 5.134801e-07\n",
      "Epoch 19932/20000: Train Loss = 0.434018, Test Loss = 0.251858, Learning Rate = 5.132850e-07\n",
      "Epoch 19933/20000: Train Loss = 0.433944, Test Loss = 0.251802, Learning Rate = 5.130899e-07\n",
      "Epoch 19934/20000: Train Loss = 0.433977, Test Loss = 0.251813, Learning Rate = 5.128950e-07\n",
      "Epoch 19935/20000: Train Loss = 0.433964, Test Loss = 0.251422, Learning Rate = 5.127001e-07\n",
      "Epoch 19936/20000: Train Loss = 0.433967, Test Loss = 0.252076, Learning Rate = 5.125053e-07\n",
      "Epoch 19937/20000: Train Loss = 0.433982, Test Loss = 0.251882, Learning Rate = 5.123106e-07\n",
      "Epoch 19938/20000: Train Loss = 0.433947, Test Loss = 0.252184, Learning Rate = 5.121159e-07\n",
      "Epoch 19939/20000: Train Loss = 0.433939, Test Loss = 0.251912, Learning Rate = 5.119213e-07\n",
      "Epoch 19940/20000: Train Loss = 0.433977, Test Loss = 0.251872, Learning Rate = 5.117268e-07\n",
      "Epoch 19941/20000: Train Loss = 0.433985, Test Loss = 0.252014, Learning Rate = 5.115323e-07\n",
      "Epoch 19942/20000: Train Loss = 0.433965, Test Loss = 0.251911, Learning Rate = 5.113380e-07\n",
      "Epoch 19943/20000: Train Loss = 0.433969, Test Loss = 0.251929, Learning Rate = 5.111437e-07\n",
      "Epoch 19944/20000: Train Loss = 0.433957, Test Loss = 0.252195, Learning Rate = 5.109495e-07\n",
      "Epoch 19945/20000: Train Loss = 0.434096, Test Loss = 0.252044, Learning Rate = 5.107553e-07\n",
      "Epoch 19946/20000: Train Loss = 0.434076, Test Loss = 0.251552, Learning Rate = 5.105612e-07\n",
      "Epoch 19947/20000: Train Loss = 0.434081, Test Loss = 0.252131, Learning Rate = 5.103672e-07\n",
      "Epoch 19948/20000: Train Loss = 0.433940, Test Loss = 0.252090, Learning Rate = 5.101733e-07\n",
      "Epoch 19949/20000: Train Loss = 0.433968, Test Loss = 0.252254, Learning Rate = 5.099795e-07\n",
      "Epoch 19950/20000: Train Loss = 0.433967, Test Loss = 0.252537, Learning Rate = 5.097857e-07\n",
      "Epoch 19951/20000: Train Loss = 0.433934, Test Loss = 0.252649, Learning Rate = 5.095920e-07\n",
      "Epoch 19952/20000: Train Loss = 0.434014, Test Loss = 0.252478, Learning Rate = 5.093983e-07\n",
      "Epoch 19953/20000: Train Loss = 0.433952, Test Loss = 0.252938, Learning Rate = 5.092048e-07\n",
      "Epoch 19954/20000: Train Loss = 0.433955, Test Loss = 0.252525, Learning Rate = 5.090113e-07\n",
      "Epoch 19955/20000: Train Loss = 0.434022, Test Loss = 0.252579, Learning Rate = 5.088179e-07\n",
      "Epoch 19956/20000: Train Loss = 0.434114, Test Loss = 0.252117, Learning Rate = 5.086246e-07\n",
      "Epoch 19957/20000: Train Loss = 0.433967, Test Loss = 0.252793, Learning Rate = 5.084313e-07\n",
      "Epoch 19958/20000: Train Loss = 0.433869, Test Loss = 0.252045, Learning Rate = 5.082381e-07\n",
      "Epoch 19959/20000: Train Loss = 0.434065, Test Loss = 0.252014, Learning Rate = 5.080450e-07\n",
      "Epoch 19960/20000: Train Loss = 0.433947, Test Loss = 0.252466, Learning Rate = 5.078519e-07\n",
      "Epoch 19961/20000: Train Loss = 0.434029, Test Loss = 0.252189, Learning Rate = 5.076590e-07\n",
      "Epoch 19962/20000: Train Loss = 0.434078, Test Loss = 0.252299, Learning Rate = 5.074661e-07\n",
      "Epoch 19963/20000: Train Loss = 0.433914, Test Loss = 0.251951, Learning Rate = 5.072733e-07\n",
      "Epoch 19964/20000: Train Loss = 0.433984, Test Loss = 0.251871, Learning Rate = 5.070805e-07\n",
      "Epoch 19965/20000: Train Loss = 0.433962, Test Loss = 0.251805, Learning Rate = 5.068878e-07\n",
      "Epoch 19966/20000: Train Loss = 0.433977, Test Loss = 0.252098, Learning Rate = 5.066952e-07\n",
      "Epoch 19967/20000: Train Loss = 0.433951, Test Loss = 0.252217, Learning Rate = 5.065027e-07\n",
      "Epoch 19968/20000: Train Loss = 0.433948, Test Loss = 0.252060, Learning Rate = 5.063102e-07\n",
      "Epoch 19969/20000: Train Loss = 0.433956, Test Loss = 0.251638, Learning Rate = 5.061178e-07\n",
      "Epoch 19970/20000: Train Loss = 0.433947, Test Loss = 0.251792, Learning Rate = 5.059255e-07\n",
      "Epoch 19971/20000: Train Loss = 0.433946, Test Loss = 0.251730, Learning Rate = 5.057333e-07\n",
      "Epoch 19972/20000: Train Loss = 0.433949, Test Loss = 0.251826, Learning Rate = 5.055411e-07\n",
      "Epoch 19973/20000: Train Loss = 0.433987, Test Loss = 0.251669, Learning Rate = 5.053490e-07\n",
      "Epoch 19974/20000: Train Loss = 0.433967, Test Loss = 0.251770, Learning Rate = 5.051570e-07\n",
      "Epoch 19975/20000: Train Loss = 0.433953, Test Loss = 0.251734, Learning Rate = 5.049651e-07\n",
      "Epoch 19976/20000: Train Loss = 0.433924, Test Loss = 0.251903, Learning Rate = 5.047732e-07\n",
      "Epoch 19977/20000: Train Loss = 0.433965, Test Loss = 0.251705, Learning Rate = 5.045814e-07\n",
      "Epoch 19978/20000: Train Loss = 0.433995, Test Loss = 0.251758, Learning Rate = 5.043897e-07\n",
      "Epoch 19979/20000: Train Loss = 0.433995, Test Loss = 0.251944, Learning Rate = 5.041980e-07\n",
      "Epoch 19980/20000: Train Loss = 0.433914, Test Loss = 0.251760, Learning Rate = 5.040064e-07\n",
      "Epoch 19981/20000: Train Loss = 0.434003, Test Loss = 0.251432, Learning Rate = 5.038149e-07\n",
      "Epoch 19982/20000: Train Loss = 0.433975, Test Loss = 0.252207, Learning Rate = 5.036235e-07\n",
      "Epoch 19983/20000: Train Loss = 0.433955, Test Loss = 0.251852, Learning Rate = 5.034321e-07\n",
      "Epoch 19984/20000: Train Loss = 0.434042, Test Loss = 0.251642, Learning Rate = 5.032408e-07\n",
      "Epoch 19985/20000: Train Loss = 0.434045, Test Loss = 0.252016, Learning Rate = 5.030496e-07\n",
      "Epoch 19986/20000: Train Loss = 0.433962, Test Loss = 0.251822, Learning Rate = 5.028585e-07\n",
      "Epoch 19987/20000: Train Loss = 0.433971, Test Loss = 0.251922, Learning Rate = 5.026674e-07\n",
      "Epoch 19988/20000: Train Loss = 0.433953, Test Loss = 0.252386, Learning Rate = 5.024764e-07\n",
      "Epoch 19989/20000: Train Loss = 0.433953, Test Loss = 0.252491, Learning Rate = 5.022855e-07\n",
      "Epoch 19990/20000: Train Loss = 0.433998, Test Loss = 0.251769, Learning Rate = 5.020946e-07\n",
      "Epoch 19991/20000: Train Loss = 0.433969, Test Loss = 0.251834, Learning Rate = 5.019038e-07\n",
      "Epoch 19992/20000: Train Loss = 0.433923, Test Loss = 0.252319, Learning Rate = 5.017131e-07\n",
      "Epoch 19993/20000: Train Loss = 0.433967, Test Loss = 0.251954, Learning Rate = 5.015225e-07\n",
      "Epoch 19994/20000: Train Loss = 0.433961, Test Loss = 0.252326, Learning Rate = 5.013319e-07\n",
      "Epoch 19995/20000: Train Loss = 0.433927, Test Loss = 0.252257, Learning Rate = 5.011414e-07\n",
      "Epoch 19996/20000: Train Loss = 0.433935, Test Loss = 0.252227, Learning Rate = 5.009510e-07\n",
      "Epoch 19997/20000: Train Loss = 0.433997, Test Loss = 0.252369, Learning Rate = 5.007607e-07\n",
      "Epoch 19998/20000: Train Loss = 0.433972, Test Loss = 0.252673, Learning Rate = 5.005704e-07\n",
      "Epoch 19999/20000: Train Loss = 0.433949, Test Loss = 0.252185, Learning Rate = 5.003802e-07\n",
      "Epoch 20000/20000: Train Loss = 0.433969, Test Loss = 0.252367, Learning Rate = 5.001901e-07\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of predictions saved to ./cloud_column_model/comparison_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ==============================================\n",
    "# PART 1: Data Loading and Preprocessing\n",
    "# ==============================================\n",
    "\n",
    "# Load input and output data from CSV files\n",
    "input_csv_path = './cloud_column_model/ensemble_input.csv'\n",
    "output_csv_path = './cloud_column_model/ensemble_output.csv'\n",
    "new_input_csv_path = './cloud_column_model/ensemble_new_input_ag_vary.csv'\n",
    "output_comparison_csv_path = './cloud_column_model/ensemble_output_ag_vary.csv'\n",
    "\n",
    "# Load the input data\n",
    "input_data = pd.read_csv(input_csv_path)\n",
    "output_data = pd.read_csv(output_csv_path)\n",
    "new_input_data = pd.read_csv(new_input_csv_path)\n",
    "comparison_output_data = pd.read_csv(output_comparison_csv_path)\n",
    "\n",
    "# Select only the 'ag' input and 'PCP_t4' output\n",
    "input_data_ag = input_data[['ag']]\n",
    "output_data_t4 = output_data[['PCP_t4']]\n",
    "test_input_data_ag = new_input_data[['ag']]  # Renamed for clarity\n",
    "comparison_output_data_t4 = comparison_output_data[['PCP_t4']]\n",
    "\n",
    "# Convert input and output data to numpy arrays\n",
    "Xf = input_data_ag.values  # Input features\n",
    "HXf_t4 = output_data_t4.values  # Outputs (only PCP_t4)\n",
    "test_Xf = test_input_data_ag.values\n",
    "comparison_Y = comparison_output_data_t4.values\n",
    "\n",
    "# Normalize the data using StandardScaler\n",
    "scaler_X = StandardScaler()\n",
    "scaler_Y = StandardScaler()\n",
    "\n",
    "Xf_scaled = scaler_X.fit_transform(Xf)\n",
    "HXf_t4_scaled = scaler_Y.fit_transform(HXf_t4)\n",
    "test_Xf_scaled = scaler_X.transform(test_Xf)\n",
    "comparison_Y_scaled = scaler_Y.transform(comparison_Y)\n",
    "\n",
    "# ==============================================\n",
    "# PART 2: Define and Train the Neural Network Model\n",
    "# ==============================================\n",
    "\n",
    "# Model parameters\n",
    "layers = 10  # Number of hidden layers\n",
    "nodes_per_layer = 100  # Number of nodes per hidden layer\n",
    "total_epochs = 20000  # Total number of training epochs\n",
    "init_rate = 0.001  # Initial learning rate\n",
    "batch_size = 32  # Batch size for training\n",
    "\n",
    "# Define the DNN model\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, layers, nodes_per_layer):\n",
    "        super(DNN, self).__init__()\n",
    "        layers_list = [nn.Linear(input_dim, nodes_per_layer), nn.Tanh()]\n",
    "        for _ in range(layers):\n",
    "            layers_list.extend([nn.Linear(nodes_per_layer, nodes_per_layer), nn.Tanh()])\n",
    "        layers_list.append(nn.Linear(nodes_per_layer, output_dim))\n",
    "        self.layers = nn.Sequential(*layers_list)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Weight initialization function\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Initialize the model and apply weight initialization\n",
    "model = DNN(Xf_scaled.shape[1], HXf_t4_scaled.shape[1], layers, nodes_per_layer).to(device)\n",
    "model.apply(weights_init)\n",
    "\n",
    "# Set up optimizer, scheduler, and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=init_rate)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=(5e-7 / init_rate) ** (1 / total_epochs))\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Prepare dataloader for training\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(Xf_scaled).float(), torch.from_numpy(HXf_t4_scaled).float()),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "best_model = copy.deepcopy(model)\n",
    "best_loss = float('inf')\n",
    "patience = 50000\n",
    "patience_counter = 0\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "lrs = []\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    model.train()\n",
    "    epoch_training_loss = 0.0\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_training_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "    epoch_training_loss /= len(Xf_scaled)\n",
    "    train_losses.append(epoch_training_loss)\n",
    "\n",
    "    # Testing on the test dataset\n",
    "    model.eval()\n",
    "    epoch_testing_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(torch.from_numpy(test_Xf_scaled).float().to(device))\n",
    "        epoch_testing_loss = criterion(\n",
    "            test_outputs,\n",
    "            torch.from_numpy(comparison_Y_scaled).float().to(device)\n",
    "        ).item()\n",
    "    test_losses.append(epoch_testing_loss)\n",
    "\n",
    "    # Check for improvement\n",
    "    if epoch_testing_loss < best_loss:\n",
    "        best_loss = epoch_testing_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping triggered')\n",
    "            break\n",
    "\n",
    "    # Record and print the current learning rate\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    lrs.append(current_lr)\n",
    "    print(f\"Epoch {epoch+1}/{total_epochs}: \"\n",
    "          f\"Train Loss = {epoch_training_loss:.6f}, \"\n",
    "          f\"Test Loss = {epoch_testing_loss:.6f}, \"\n",
    "          f\"Learning Rate = {current_lr:.6e}\")\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "# Restore the best model obtained during training\n",
    "model = best_model\n",
    "\n",
    "# ==============================================\n",
    "# PART 3: Predict and Compare Results\n",
    "# ==============================================\n",
    "\n",
    "# Generate new 'ag' values\n",
    "new_ag_values = np.arange(50, 1201, 1.151)  # From 50 to 1200 with increment of 1\n",
    "new_ag_values_scaled = scaler_X.transform(new_ag_values.reshape(-1, 1))\n",
    "\n",
    "# Predict using the trained DNN model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions_scaled_new_ag = model(torch.from_numpy(new_ag_values_scaled).float().to(device)).cpu().numpy()\n",
    "\n",
    "# Inverse transform the predictions\n",
    "predictions_new_ag = scaler_Y.inverse_transform(predictions_scaled_new_ag)\n",
    "\n",
    "# Load original outputs for the new 'ag' values\n",
    "original_outputs_new_ag = comparison_output_data['PCP_t4'].values\n",
    "\n",
    "# ==============================================\n",
    "# PART 4: Plotting Results\n",
    "# ==============================================\n",
    "\n",
    "# Plot Training and Testing Loss\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(test_losses, label='Testing Loss')\n",
    "plt.title('Training and Testing Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Learning Rate vs. Epochs\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(lrs, label='Learning Rate')\n",
    "plt.title('Learning Rate Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Comparison: Original vs Predicted Outputs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(new_ag_values, original_outputs_new_ag, label='Original Model Outputs', marker='o', linestyle='-', color='blue')\n",
    "plt.plot(new_ag_values, predictions_new_ag, label='DNN Predicted Outputs', marker='x', linestyle='--', color='orange')\n",
    "plt.title('Comparison of DNN Predictions and Original Model Outputs')\n",
    "plt.xlabel('ag (Input Parameter)')\n",
    "plt.ylabel('PCP_t4 (Output)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save predictions and comparison data to a CSV file\n",
    "comparison_df = pd.DataFrame({\n",
    "    'ag': new_ag_values,\n",
    "    'Original_Output': original_outputs_new_ag,\n",
    "    'DNN_Predicted_Output': predictions_new_ag.flatten()\n",
    "})\n",
    "comparison_csv_path = './cloud_column_model/comparison_predictions.csv'\n",
    "comparison_df.to_csv(comparison_csv_path, index=False)\n",
    "print(f\"Comparison of predictions saved to {comparison_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xf_scaled mean and std: 1.2434497875801754e-16 0.9999999999999999\n",
      "HXf_t4_scaled mean and std: -3.659295089164516e-16 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Xf_scaled mean and std:\", Xf_scaled.mean(), Xf_scaled.std())\n",
    "print(\"HXf_t4_scaled mean and std:\", HXf_t4_scaled.mean(), HXf_t4_scaled.std()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[--------------------- rho_g, the graupel density Vs other outputs----------------------------------]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input for first iteration: [200, 0.3, 400, 0.4, 0.5, 0.5, 0.5, 0.2, 0.1, 0.001, 0.0006]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "parmap <function runcrm at 0x15dc99a80>: Running in mode par with numPartitions 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for first ensemble member: [3.7471554279, 0.0863199979, 5.0838737488, 0.0021726273, 240.758682251, 896.8100585938, 9.7006654739, 4.1479549408, 4.5859365463, 4.1558990479, 154.1160430908, 785.2816772461, 7.2581114769, 8.1785850525, 3.3773915768, 10.9536771774, 166.9486846924, 808.3519287109, 16.7680702209, 13.5256023407, 6.2727732658, 11.7516775131, 204.241394043, 765.5327758789, 17.2996482849, 23.8721046448, 5.0507102013, 9.2141370773, 213.2725830078, 738.844543457, 16.8713703156, 31.6702823639, 5.5008354187, 3.2548453808, 232.6736907959, 486.527923584]\n",
      "len(HXf), len(HXf[0]): 1000 36\n",
      "New ensemble outputs with varying 'rhog' saved to ./cloud_column_model/ensemble_output_rhog_vary.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cloud Column Model Parameter Sweep with Parallel Processing\n",
    "\n",
    "This script performs a parameter sweep for the 'rhog' parameter in a cloud column model simulation\n",
    "It uses a parallel execution framework to run multiple instances of the CRM model with varying 'rhog' values \n",
    "and saves the ensemble output to a CSV file\n",
    "\n",
    "Key Features:\n",
    "1. Fixed input parameters for the CRM model are defined in the `fixed_values` dictionary\n",
    "2. The 'rhog' parameter varies from 0.1 to 1, incremented by 0.0009\n",
    "3. Input data is prepared for each run, including both fixed and varying parameters\n",
    "4. Parallel processing is implemented using DASK via the `parmap_framework` package\n",
    "5. Outputs of the CRM model are aggregated into a DataFrame and saved as a CSV\n",
    "\n",
    "Dependencies:\n",
    "- pandas\n",
    "- numpy\n",
    "- parmap_framework\n",
    "- module_runcrm (containing the `runcrm` function)\n",
    "\n",
    "Workflow:\n",
    "1. Prepare a list of input configurations (`runs`) for all parameter sweep iterations\n",
    "2. Execute the runs in parallel using the DASK cluster\n",
    "3. Process and organize the output data into a DataFrame with appropriate column names\n",
    "4. Save the DataFrame to a CSV file for further analysis\n",
    "\n",
    "Output:\n",
    "- CSV file containing the ensemble output for all 'rhog' values, with 36 output variables \n",
    "  (6 variables x 6 time steps) and the corresponding 'rhog' values\n",
    "\n",
    "Usage:\n",
    "Ensure all dependencies are installed, and paths for input/output files are correct before execution\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from parmap_framework import parmap\n",
    "from module_runcrm import runcrm\n",
    "\n",
    "# File path for saving the output\n",
    "output_csv_path_rhog = './cloud_column_model/ensemble_output_rhog_vary.csv'\n",
    "\n",
    "# Fixed values for input parameters\n",
    "fixed_values = {\n",
    "    'as': 200,\n",
    "    'bs': 0.3,\n",
    "    'ag': 400,\n",
    "    'bg': 0.4,\n",
    "    'N0r':0.5,\n",
    "    'N0s': 0.5,\n",
    "    'N0g': 0.5,\n",
    "    'rhos': 0.2,\n",
    "    'qc0': 0.001,\n",
    "    'qi0': 0.0006\n",
    "}\n",
    "\n",
    "# Define the range for 'ag' parameter\n",
    "rhog_values = np.arange(0.1, 1, 0.0009)  # From 0.1 to 1 with increment of 0.0009\n",
    "\n",
    "# Prepare input data for each iteration\n",
    "runs = []\n",
    "for rhog in rhog_values:\n",
    "    input_params = list(fixed_values.values())\n",
    "    input_params.insert(8, rhog)  # Insert the varying 'rhog' value in the correct position\n",
    "    runs.append([\n",
    "        './cloud_column_model/run_one_crm1d.txt',  # Input file (placeholder)\n",
    "        './cloud_column_model/crm1d_output.txt',  # Output file (placeholder)\n",
    "        './cloud_column_model/namelist_3h_t30-180.f90',  # Namelist file\n",
    "        len(runs) + 1,  # Run number\n",
    "        input_params  # Parameters for this run\n",
    "    ])\n",
    "\n",
    "# Print a sample input for debugging\n",
    "print(f\"Sample input for first iteration: {runs[0][-1]}\")\n",
    "\n",
    "# Parallel execution using DASK\n",
    "DASK_URL = 'scispark6.jpl.nasa.gov:8786'\n",
    "parmode = 'par'\n",
    "num_Workers = 12\n",
    "pmap = parmap.Parmap(master=DASK_URL, mode=parmode, numWorkers=num_Workers)\n",
    "HXf = pmap(runcrm, runs)\n",
    "\n",
    "# Print a sample output for debugging\n",
    "print('Output for first ensemble member:', HXf[0])\n",
    "print('len(HXf), len(HXf[0]):', len(HXf), len(HXf[0]))\n",
    "\n",
    "# Define column names for the output variables (6 variables x 6 time steps)\n",
    "column_names_output = [f\"{var}_t{t}\" for t in range(1, 7) for var in ['PCP', 'ACC', 'LWP', 'IWP', 'OLR', 'OSR']]\n",
    "\n",
    "# Convert the full output (36 columns per member) to a DataFrame\n",
    "HXf_df = pd.DataFrame(HXf, columns=column_names_output)\n",
    "\n",
    "# Add the 'ag' values as a separate column for reference\n",
    "HXf_df.insert(0, 'rhog', rhog_values)\n",
    "\n",
    "# Save to CSV\n",
    "HXf_df.to_csv(output_csv_path_rhog, index=False)\n",
    "print(f\"New ensemble outputs with varying 'rhog' saved to {output_csv_path_rhog}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Load the output data\n",
    "output_data = pd.read_csv(output_csv_path_rhog)\n",
    "\n",
    "# Extract 'rhog' and 'PCP_t4' values\n",
    "rhog_values_plot = output_data['rhog']\n",
    "pcp_t4_values = output_data['PCP_t4']\n",
    "\n",
    "# Plot 'ag' vs 'PCP_t4'\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rhog_values_plot,pcp_t4_values, label='PCP_t4', linewidth=2)\n",
    "plt.xlabel('rhog (Varying Parameter)', fontsize=12)\n",
    "plt.ylabel('PCP_t4 (Output Variable)', fontsize=12)\n",
    "plt.title('PCP_t4 vs rhog', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cloud Model where the previous step file has been used as input file to generate output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (1000, 1), indices imply (1000, 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m column_names_input \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mas\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN0r\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN0s\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN0g\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrhos\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrhog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqc0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqi0\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Save input ensemble (Xf) to a CSV file\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m Xf_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(Xf, columns\u001b[38;5;241m=\u001b[39mcolumn_names_input)\n\u001b[1;32m     12\u001b[0m Xf_df\u001b[38;5;241m.\u001b[39mto_csv(input_csv_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput parameters for 1000 ensembles saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_csv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:758\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    747\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    748\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[1;32m    749\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    755\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[1;32m    756\u001b[0m         )\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 758\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m ndarray_to_mgr(\n\u001b[1;32m    759\u001b[0m             data,\n\u001b[1;32m    760\u001b[0m             index,\n\u001b[1;32m    761\u001b[0m             columns,\n\u001b[1;32m    762\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    763\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    764\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    765\u001b[0m         )\n\u001b[1;32m    767\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:337\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[1;32m    333\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[1;32m    334\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[1;32m    335\u001b[0m )\n\u001b[0;32m--> 337\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:408\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    406\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    407\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[0;32m--> 408\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (1000, 1), indices imply (1000, 11)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths for saving input and output\n",
    "input_csv_path = './cloud_column_model/ensemble_input_1000.csv'\n",
    "output_csv_path = './cloud_column_model/ensemble_output_1000.csv'\n",
    "\n",
    "# Define column names for the input parameters\n",
    "column_names_input = ['as', 'bs', 'ag', 'bg', 'N0r', 'N0s', 'N0g', 'rhos', 'rhog', 'qc0', 'qi0']\n",
    "\n",
    "# Save input ensemble (Xf) to a CSV file\n",
    "Xf_df = pd.DataFrame(Xf, columns=column_names_input)\n",
    "Xf_df.to_csv(input_csv_path, index=False)\n",
    "print(f\"Input parameters for 1000 ensembles saved to {input_csv_path}\")\n",
    "\n",
    "# Define column names for the output variables\n",
    "column_names_output = [f\"{var}_t{t}\" for t in range(1, nt + 1) for var in ynames]\n",
    "\n",
    "# Save output ensemble (HXf) to a CSV file\n",
    "HXf_df = pd.DataFrame(HXf, columns=column_names_output)\n",
    "HXf_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"Output variables for 1000 ensembles saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from parmap_framework import parmap\n",
    "from module_runcrm import runcrm\n",
    "\n",
    "# File paths\n",
    "new_input_csv_path = './cloud_column_model/ensemble_new_input_ag_vary.csv'\n",
    "output_csv_path = './cloud_column_model/ensemble_output_ag_vary.csv'\n",
    "\n",
    "# Load the new input file where 'ag' and 'bg' vary\n",
    "ensemble_input = pd.read_csv(new_input_csv_path)\n",
    "\n",
    "# Sort the input data by 'ag' and then by 'bg' values\n",
    "ensemble_input_sorted = ensemble_input.sort_values(by=['ag', 'bg']).reset_index(drop=True)\n",
    "\n",
    "# Define other required parameters\n",
    "n_ens = len(ensemble_input_sorted)  # Number of ensemble members\n",
    "input_file_list = ['./cloud_column_model/run_one_crm1d.txt'] * n_ens\n",
    "output_file_list = ['./cloud_column_model/crm1d_output.txt'] * n_ens\n",
    "namelist_file_list = ['./cloud_column_model/namelist_3h_t30-180.f90'] * n_ens\n",
    "run_num_list = list(range(1, n_ens + 1))\n",
    "\n",
    "# Prepare input data for CRM runs\n",
    "runs = [list(x) for x in zip(input_file_list, output_file_list, namelist_file_list, run_num_list, ensemble_input_sorted.values.tolist())]\n",
    "\n",
    "# Print a sample input for debugging\n",
    "print(f\"Full input to the first ensemble member: {runs[0]}\")\n",
    "\n",
    "# Parallel execution using DASK\n",
    "DASK_URL = 'scispark6.jpl.nasa.gov:8786'\n",
    "parmode = 'par'\n",
    "num_Workers = 12\n",
    "pmap = parmap.Parmap(master=DASK_URL, mode=parmode, numWorkers=num_Workers)\n",
    "HXf = pmap(runcrm, runs)\n",
    "\n",
    "# Print a sample output for debugging\n",
    "print('Output for ensemble member Ne/2:', HXf[np.int32(n_ens / 2)])\n",
    "print('len(HXf), len(HXf[0]):', len(HXf), len(HXf[0]))\n",
    "\n",
    "# Define column names for the output variables (6 variables x 6 time steps)\n",
    "column_names_output = [f\"{var}_t{t}\" for t in range(1, 7) for var in ['PCP', 'ACC', 'LWP', 'IWP', 'OLR', 'OSR']]\n",
    "\n",
    "# Convert the full output (36 columns per member) to a DataFrame\n",
    "HXf_df = pd.DataFrame(HXf, columns=column_names_output)\n",
    "\n",
    "# Save to CSV\n",
    "HXf_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"New ensemble outputs saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the input (ag) and PCP_t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# File paths\n",
    "new_input_csv_path = './cloud_column_model/ensemble_new_input_ag_vary.csv'\n",
    "output_csv_path = './cloud_column_model/ensemble_output_ag_vary.csv'\n",
    "\n",
    "# Load the input and output data\n",
    "input_data = pd.read_csv(new_input_csv_path)\n",
    "output_data = pd.read_csv(output_csv_path)\n",
    "\n",
    "# Sort the input data by 'ag' and align the output data\n",
    "sorted_data = input_data.sort_values(by=['ag']).reset_index(drop=True)\n",
    "sorted_output_data = output_data.loc[sorted_data.index].reset_index(drop=True)\n",
    "\n",
    "# Extract 'ag' and 'PCP_t4' for plotting\n",
    "x_values_sorted = sorted_data['ag']\n",
    "y_values_sorted = sorted_output_data['PCP_t4']\n",
    "\n",
    "# Plot the sorted graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_values_sorted, y_values_sorted, label='PCP_t4', color='blue')\n",
    "plt.xlabel('ag (Coefficient in Graupel Fallspeed-Diameter Relationship)')\n",
    "plt.ylabel('PCP_t4 (Precipitation Rate at Time Step 4)')\n",
    "plt.title('output_test_CRM_model (Actual - Sorted)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the sorted plot\n",
    "plt.show()\n",
    "\n",
    "# Check the number of points in the sorted x-axis and y-axis\n",
    "num_x_points_sorted = len(x_values_sorted)\n",
    "num_y_points_sorted = len(y_values_sorted)\n",
    "\n",
    "print(f\"Number of points mapped on the sorted x-axis (ag): {num_x_points_sorted}\")\n",
    "print(f\"Number of points mapped on the sorted y-axis (PCP_t4): {num_y_points_sorted}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Downsample and cubic Spline to make the graph smoother "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# File paths\n",
    "new_input_csv_path = './cloud_column_model/ensemble_new_input_ag_vary.csv'\n",
    "output_csv_path = './cloud_column_model/ensemble_output_ag_vary.csv'\n",
    "\n",
    "# Load the input and output data\n",
    "input_data = pd.read_csv(new_input_csv_path)\n",
    "output_data = pd.read_csv(output_csv_path) \n",
    "\n",
    "# Extract 'ag' values from input data and 'PCP_t4' from output data\n",
    "x_values = input_data['ag']\n",
    "y_values = output_data['PCP_t4']\n",
    "\n",
    "# Downsample the data by considering every 10 points\n",
    "x_downsampled = x_values[::60]\n",
    "y_downsampled = y_values[::60]\n",
    "\n",
    "# Create cubic interpolation using the downsampled data\n",
    "interp_func = interp1d(x_downsampled, y_downsampled, kind='cubic')\n",
    "\n",
    "# Generate smooth points for a curve\n",
    "x_smooth = np.linspace(x_downsampled.min(), x_downsampled.max(), 1000)  # 1000 evenly spaced points\n",
    "y_smooth = interp_func(x_smooth)\n",
    "\n",
    "\n",
    "# Plot the graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_smooth, y_smooth, label='PCP_t4 (Interpolated)', color='blue', linewidth=2)\n",
    "\n",
    "# Add downsampled data points for reference\n",
    "plt.scatter(x_downsampled, y_downsampled, color='red', s=10, label='Downsampled Data')\n",
    "\n",
    "# Adjust x and y ticks at intervals of 100 and 10\n",
    "x_ticks = np.arange(x_smooth.min(), x_smooth.max() + 1, 100)\n",
    "y_ticks = np.arange(y_smooth.min(), y_smooth.max() + 1, 5)\n",
    "plt.xticks(x_ticks)\n",
    "plt.yticks(y_ticks)\n",
    "\n",
    "# Add labels, title, and grid\n",
    "plt.xlabel('ag (Coefficient in Graupel Fallspeed-Diameter Relationship)')\n",
    "plt.ylabel('PCP_t4 (Precipitation Rate at Time Step 4)')\n",
    "plt.title('output_test_CRM_model (Actual) with Downsampling and Interpolation')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout() \n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ==============================================\n",
    "# PART 1: Data Loading and Preprocessing\n",
    "# ==============================================\n",
    "\n",
    "# Load input and output data from CSV files\n",
    "input_csv_path = './cloud_column_model/ensemble_input.csv'\n",
    "output_csv_path = './cloud_column_model/ensemble_output.csv'\n",
    "new_input_csv_path = './cloud_column_model/ensemble_new_input_ag_vary.csv'\n",
    "output_comparison_csv_path = './cloud_column_model/ensemble_output_ag_vary.csv'\n",
    "\n",
    "# Load the input data\n",
    "input_data = pd.read_csv(input_csv_path)\n",
    "output_data = pd.read_csv(output_csv_path)\n",
    "new_input_data = pd.read_csv(new_input_csv_path)\n",
    "comparison_output_data = pd.read_csv(output_comparison_csv_path)\n",
    "\n",
    "# Select only the 'ag' input and 'PCP_t4' output\n",
    "input_data_ag = input_data[['ag']]\n",
    "output_data_t4 = output_data[['PCP_t4']]\n",
    "test_input_data_ag = new_input_data[['ag']]  # Renamed for clarity\n",
    "comparison_output_data_t4 = comparison_output_data[['PCP_t4']]\n",
    "\n",
    "# Convert input and output data to numpy arrays\n",
    "Xf = input_data_ag.values  # Input features\n",
    "HXf_t4 = output_data_t4.values  # Outputs (only PCP_t4)\n",
    "test_Xf = test_input_data_ag.values\n",
    "comparison_Y = comparison_output_data_t4.values\n",
    "\n",
    "# Normalize the data using StandardScaler\n",
    "scaler_X = StandardScaler()\n",
    "scaler_Y = StandardScaler()\n",
    "\n",
    "Xf_scaled = scaler_X.fit_transform(Xf)\n",
    "HXf_t4_scaled = scaler_Y.fit_transform(HXf_t4)\n",
    "test_Xf_scaled = scaler_X.transform(test_Xf)\n",
    "comparison_Y_scaled = scaler_Y.transform(comparison_Y)\n",
    "\n",
    "# ==============================================\n",
    "# PART 2: Define and Train the Neural Network Model\n",
    "# ==============================================\n",
    "\n",
    "# Model parameters\n",
    "layers = 10  # Number of hidden layers\n",
    "nodes_per_layer = 100  # Number of nodes per hidden layer\n",
    "total_epochs = 500  # Total number of training epochs\n",
    "init_rate = 0.001  # Initial learning rate\n",
    "batch_size = 32  # Batch size for training\n",
    "\n",
    "# Define the DNN model\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, layers, nodes_per_layer):\n",
    "        super(DNN, self).__init__()\n",
    "        layers_list = [nn.Linear(input_dim, nodes_per_layer), nn.Tanh()]\n",
    "        for _ in range(layers):\n",
    "            layers_list.extend([nn.Linear(nodes_per_layer, nodes_per_layer), nn.Tanh()])\n",
    "        layers_list.append(nn.Linear(nodes_per_layer, output_dim))\n",
    "        self.layers = nn.Sequential(*layers_list)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Weight initialization function\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Initialize the model and apply weight initialization\n",
    "model = DNN(Xf_scaled.shape[1], HXf_t4_scaled.shape[1], layers, nodes_per_layer).to(device)\n",
    "model.apply(weights_init)\n",
    "\n",
    "# Set up optimizer, scheduler, and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=init_rate)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=(5e-7 / init_rate) ** (1 / total_epochs))\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Prepare dataloader for training\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(Xf_scaled).float(), torch.from_numpy(HXf_t4_scaled).float()),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "best_model = copy.deepcopy(model)\n",
    "best_loss = float('inf')\n",
    "patience = 50000\n",
    "patience_counter = 0\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "lrs = []\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    model.train()\n",
    "    epoch_training_loss = 0.0\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_training_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "    epoch_training_loss /= len(Xf_scaled)\n",
    "    train_losses.append(epoch_training_loss)\n",
    "\n",
    "    # Testing on the test dataset\n",
    "    model.eval()\n",
    "    epoch_testing_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(torch.from_numpy(test_Xf_scaled).float().to(device))\n",
    "        epoch_testing_loss = criterion(\n",
    "            test_outputs,\n",
    "            torch.from_numpy(comparison_Y_scaled).float().to(device)\n",
    "        ).item()\n",
    "    test_losses.append(epoch_testing_loss)\n",
    "\n",
    "    # Check for improvement\n",
    "    if epoch_testing_loss < best_loss:\n",
    "        best_loss = epoch_testing_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping triggered')\n",
    "            break\n",
    "\n",
    "    # Record and print the current learning rate\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    lrs.append(current_lr)\n",
    "    print(f\"Epoch {epoch+1}/{total_epochs}: \"\n",
    "          f\"Train Loss = {epoch_training_loss:.6f}, \"\n",
    "          f\"Test Loss = {epoch_testing_loss:.6f}, \"\n",
    "          f\"Learning Rate = {current_lr:.6e}\")\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "# Restore the best model obtained during training\n",
    "model = best_model\n",
    "\n",
    "# ==============================================\n",
    "# PART 3: Predict and Compare Results\n",
    "# ==============================================\n",
    "\n",
    "# Make predictions on the test input data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions_scaled = model(torch.from_numpy(test_Xf_scaled).float().to(device)).cpu().numpy()\n",
    "\n",
    "# Inverse transform the predictions and comparison data\n",
    "predictions = scaler_Y.inverse_transform(predictions_scaled)\n",
    "comparison_Y = scaler_Y.inverse_transform(comparison_Y_scaled)\n",
    "\n",
    "# ==============================================\n",
    "# PART 4: Plotting Results\n",
    "# ==============================================\n",
    "\n",
    "# Plot Training and Testing Loss\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(test_losses, label='Testing Loss')\n",
    "plt.title('Training and Testing Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Learning Rate\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(lrs)\n",
    "plt.title('Learning Rate Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Predictions vs Ground Truth\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(test_Xf, predictions, label='DNN Predictions', marker='o', linestyle='--')\n",
    "plt.plot(test_Xf, comparison_Y, label='Ground Truth', marker='x', linestyle='-')\n",
    "plt.title('Comparison of DNN Predictions and Ground Truth')\n",
    "plt.xlabel('ag (Input Parameter)')\n",
    "plt.ylabel('PCP_t4 (Output)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# Save ag, Predictions, and Ground Truth to CSV\n",
    "# ==============================================\n",
    "\n",
    "# Create a DataFrame with ag, predictions, and ground truth\n",
    "results_df = pd.DataFrame({\n",
    "    'ag': test_Xf.flatten(),  # Flatten to 1D for saving\n",
    "    'DNN_Predictions_PCP_t4': predictions.flatten(),\n",
    "    'Ground_Truth_PCP_t4': comparison_Y.flatten()\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv_path = './cloud_column_model/predictions_vs_ground_truth.csv'\n",
    "results_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Results saved to {output_csv_path}\")\n",
    "\n",
    "# ==============================================\n",
    "# Visualize the Results\n",
    "# ==============================================\n",
    "\n",
    "# Load the saved CSV file\n",
    "results_df = pd.read_csv(output_csv_path)\n",
    "\n",
    "# Downsample the data by selecting every 15th value\n",
    "downsampled_results = results_df.iloc[::15]\n",
    "\n",
    "# Plot the downsampled graph for DNN Predictions vs Ground Truth\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(downsampled_results['ag'], downsampled_results['DNN_Predictions_PCP_t4'], 'o--', label='DNN Predictions')\n",
    "plt.plot(downsampled_results['ag'], downsampled_results['Ground_Truth_PCP_t4'], 'x-', label='Ground Truth')\n",
    "plt.title('Comparison of DNN Predictions and Ground Truth (Downsampled)')\n",
    "plt.xlabel('ag (Input Parameter)')\n",
    "plt.ylabel('PCP_t4 (Output)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
